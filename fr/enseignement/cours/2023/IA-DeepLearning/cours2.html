<html>

<head>
    <meta charset="utf-8" />
    <title>Intelligence artificielle (2023-2024): Cours: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            flex-direction: column;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 3vmin;
            flex-wrap: wrap;
        }

        .codeexample {
            background-color: #eeeeee;
        }

        /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
        pre {
            line-height: 125%;
        }

        td.linenos .normal {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        td.linenos .special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos.special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        body .hll {
            background-color: #ffffcc
        }

        body {
            background: #f8f8f8;
        }

        body .c {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment */
        body .err {
            border: 1px solid #FF0000
        }

        /* Error */
        body .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */
        body .o {
            color: #666666
        }

        /* Operator */
        body .ch {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Hashbang */
        body .cm {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Multiline */
        body .cp {
            color: #9C6500
        }

        /* Comment.Preproc */
        body .cpf {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.PreprocFile */
        body .c1 {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Single */
        body .cs {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Special */
        body .gd {
            color: #A00000
        }

        /* Generic.Deleted */
        body .ge {
            font-style: italic
        }

        /* Generic.Emph */
        body .gr {
            color: #E40000
        }

        /* Generic.Error */
        body .gh {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Heading */
        body .gi {
            color: #008400
        }

        /* Generic.Inserted */
        body .go {
            color: #717171
        }

        /* Generic.Output */
        body .gp {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Prompt */
        body .gs {
            font-weight: bold
        }

        /* Generic.Strong */
        body .gu {
            color: #800080;
            font-weight: bold
        }

        /* Generic.Subheading */
        body .gt {
            color: #0044DD
        }

        /* Generic.Traceback */
        body .kc {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Constant */
        body .kd {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Declaration */
        body .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */
        body .kp {
            color: #008000
        }

        /* Keyword.Pseudo */
        body .kr {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Reserved */
        body .kt {
            color: #B00040
        }

        /* Keyword.Type */
        body .m {
            color: #666666
        }

        /* Literal.Number */
        body .s {
            color: #BA2121
        }

        /* Literal.String */
        body .na {
            color: #687822
        }

        /* Name.Attribute */
        body .nb {
            color: #008000
        }

        /* Name.Builtin */
        body .nc {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Class */
        body .no {
            color: #880000
        }

        /* Name.Constant */
        body .nd {
            color: #AA22FF
        }

        /* Name.Decorator */
        body .ni {
            color: #717171;
            font-weight: bold
        }

        /* Name.Entity */
        body .ne {
            color: #CB3F38;
            font-weight: bold
        }

        /* Name.Exception */
        body .nf {
            color: #0000FF
        }

        /* Name.Function */
        body .nl {
            color: #767600
        }

        /* Name.Label */
        body .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */
        body .nt {
            color: #008000;
            font-weight: bold
        }

        /* Name.Tag */
        body .nv {
            color: #19177C
        }

        /* Name.Variable */
        body .ow {
            color: #AA22FF;
            font-weight: bold
        }

        /* Operator.Word */
        body .w {
            color: #bbbbbb
        }

        /* Text.Whitespace */
        body .mb {
            color: #666666
        }

        /* Literal.Number.Bin */
        body .mf {
            color: #666666
        }

        /* Literal.Number.Float */
        body .mh {
            color: #666666
        }

        /* Literal.Number.Hex */
        body .mi {
            color: #666666
        }

        /* Literal.Number.Integer */
        body .mo {
            color: #666666
        }

        /* Literal.Number.Oct */
        body .sa {
            color: #BA2121
        }

        /* Literal.String.Affix */
        body .sb {
            color: #BA2121
        }

        /* Literal.String.Backtick */
        body .sc {
            color: #BA2121
        }

        /* Literal.String.Char */
        body .dl {
            color: #BA2121
        }

        /* Literal.String.Delimiter */
        body .sd {
            color: #BA2121;
            font-style: italic
        }

        /* Literal.String.Doc */
        body .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */
        body .se {
            color: #AA5D1F;
            font-weight: bold
        }

        /* Literal.String.Escape */
        body .sh {
            color: #BA2121
        }

        /* Literal.String.Heredoc */
        body .si {
            color: #A45A77;
            font-weight: bold
        }

        /* Literal.String.Interpol */
        body .sx {
            color: #008000
        }

        /* Literal.String.Other */
        body .sr {
            color: #A45A77
        }

        /* Literal.String.Regex */
        body .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */
        body .ss {
            color: #19177C
        }

        /* Literal.String.Symbol */
        body .bp {
            color: #008000
        }

        /* Name.Builtin.Pseudo */
        body .fm {
            color: #0000FF
        }

        /* Name.Function.Magic */
        body .vc {
            color: #19177C
        }

        /* Name.Variable.Class */
        body .vg {
            color: #19177C
        }

        /* Name.Variable.Global */
        body .vi {
            color: #19177C
        }

        /* Name.Variable.Instance */
        body .vm {
            color: #19177C
        }

        /* Name.Variable.Magic */
        body .il {
            color: #666666
        }

        /* Literal.Number.Integer.Long */


        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 3vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 70%;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 100%;
            max-height: 100%;
        }

        .content figure img {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 4vh;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }


        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                height: 76%;
                width: 90vw;
                display: flex;
                color: #000000;
                text-align: left;
                padding: 5vw;
                font-size: 3vmin;
                flex-wrap: wrap;
            }

            .content figure img {
                max-width: 80%;
                max-height: 80%;
            }

            .content figcaption {
                max-width: 80%;
                max-height: 80%;
            }
        }
    </style>
    <script src="../../2021/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:2.5vw">Intelligence artificielle</h1>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Year</b>: 2023-2024<br />
                <b>Email</b>: john(dot)samuel(at)cpe(dot)fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles (NLP) </h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Intelligence artificielle</h3>
            <figure>
                <img src="../../../../../images/art/courses/deeplearningposition.svg" height="450px" />
                <figcaption>Intelligence artificielle</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <p>Le <b>traitement automatique des langues (TAL)</b> est un domaine interdisciplinaire de la linguistique
                informatique qui se concentre sur l'analyse et la compréhension du <b>langage naturel</b> (celui utilisé
                par les humains). Cette section aborde plusieurs aspects clés du TAL, notamment :</p>
            <ul>
                <li><b>Analyser et comprendre le langage naturel (humain)</b>: Le TAL se consacre à la compréhension du
                    langage naturel dans divers contextes, qu'il s'agisse de textes écrits ou de discours verbal.</li>
                <li>Interaction homme-machine</li>
                <li><b>Syntaxe d'une langue</b>
                    <ul>
                        <li>Parsing : Le parsing consiste à analyser la structure grammaticale des phrases.</li>
                        <li>L'étiquetage en parties du discours (PoS) : L'étiquetage PoS consiste à assigner des
                            catégories grammaticales (comme verbe, nom, adjectif, etc.) aux mots d'une phrase.</li>
                    </ul>
                </li>
                <li><b>Sémantique d'une langue</b>
                    <ul>
                        <li>Traduction automatique</li>
                        <li>Reconnaissance d'entités nommées (NER): La NER consiste à identifier des entités spécifiques
                            (comme des noms de personnes, de lieux ou d'organisations) dans un texte.</li>
                        <li>Analyse des sentiments</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Analyse de systèmes TAL</h3>
            <ul>
                <li><b>Racinisation</b> : La racinisation est le processus de réduction des mots à leur forme de base ou
                    de racine. </li>
                <li><b>Étiquetage morpho-syntaxique</b> : Cette étape consiste à attribuer des balises ou des étiquettes
                    aux mots dans un texte en fonction de leur rôle grammatical et de leur structure. </li>
                <li><b>Lemmatisation</b> : Contrairement à la racinisation, la lemmatisation consiste à ramener les mots
                    à leur forme canonique ou lemmes. </li>
                <li><b>Morphologie</b> : La morphologie concerne l'étude de la structure des mots, notamment comment ils
                    sont formés à partir de morphèmes (unités de sens). </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation [Frakes 2003]</h3>
            <ul>
                <li>grouper les mots en fonction de leur similarité sémantique.</li>
                <li>Algorithmes de suppression des affixes: supprimer les suffixes ou préfixes des mots produisant une
                    racine </li>
                <li>Exemples
                    <ul>
                        <li>engineer: engineer, engineered, engineering</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: mesures d'évaluation [Frakes 2003]</h3>
            <ul>
                <li>La mesure dans laquelle un algorithm modifie des mots qu'elle réduit à ses racines est appelée la
                    <b>force</b> de l'algorithme
                </li>
                <li>Une métrique de <b>similarité</b> des algorithmes met en correspondance les n-tuples d'algorithmes
                    (n au moins 2), avec un nombre indiquant la similarité des algorithmes. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: distance de Hamming [Frakes 2003]</h3>
            <ol>
                <li>La distance de Hamming entre deux chaînes de longueur égale est définie comme le nombre de
                    caractères des deux chaînes qui sont différents à la même position.</li>
                <li>Pour les chaînes de longueur inégale, ajouter la différence de longueur à la distance de Hamming
                    pour obtenir une fonction de distance de Hamming modifiée \(d\)</li>
                <li>Exemples
                    <ul>
                        <li>tri: try, tried, trying</li>
                        <li>\(d\)(tri, try)= 1</li>
                        <li>\(d\)(tri, tried)= 2</li>
                        <li>\(d\)(tri, trying)= 4</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: force [Frakes 2003]</h3>
            <ol>
                <li>Le nombre moyen de mots par classe</li>
                <li>Facteur de compression de l'indice. Soit n est le nombre de mots dans le corpus et s est le nombre
                    de racines. \[\frac{n - s}{n}\]
                </li>
                <li>Le nombre de mots et de racines qui diffèrent</li>
                <li>Le nombre moyen de caractères supprimés lors de la formation des racines</li>
                <li>La médiane et la moyenne de la distance de Hamming modifiée entre les mots et leur racine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: similarité [Frakes 2003]</h3>
            <ol>
                <li>Soit \(A1\) et \(A2\) sont deux algorithmes</li>
                <li>Soit \(W\) une liste de mots et \(n\) le nombre de mots dans \(W\) \[ M(A1,A2,W) = \frac{n}{\Sigma
                    d(x_i, y_i)}\]
                </li>
                <li>pour tous les mots \(w_i\) en W, \(x_i\) est le résultat de l'application de \(A1\) à \(w_i\) et
                    \(y_i\) est le résultat de l'application de \(A2\) à \(w_i\)</li>
                <li>des algorithmes plus similaires auront des valeurs plus élevées de M</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: nltk</h3>
            <p>L'objectif est de réduire les mots à leur forme de base ou racine, en éliminant les suffixes, ce qui
                permet de regrouper différentes formes d'un mot sous une forme commune. </p>
            <ul>
                <li><b>Porter [Porter 1980]</b> : Le Porter Stemming Algorithm, créé par Martin Porter en 1980, est basé
                    sur un ensemble de règles heuristiques. Il suit une approche itérative en appliquant une série de
                    transformations séquentielles aux mots.</li>
                <li><b>Snowball</b> Le Snowball (anciennement appelé Porter2) est une amélioration du Porter Stemmer. Il
                    suit également une approche basée sur des règles, mais il est plus systématique dans son traitement
                    des différents cas de racination. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Porter</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Snowball</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">snowball</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">snowball</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <ul>
                <li>Part of Speech (PoS) Tagging</li>
                <li>attribution à chaque mot d'un texte de la balise morphosyntaxique appropriée dans son contexte
                    d'apparition
                </li>
                <li>Exemples des balises
                    <ul>
                        <li>noms</li>
                        <li>verbes</li>
                        <li>adjectifs</li>
                        <li>adverbes</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
            <ol>
                <li>approche manuelle
                    <ul>
                        <li>constuction des règles)</li>
                    </ul>
                </li>
                <li>approche statistique
                    <ul>
                        <li>collection de n-grammes (bi-grammes, tri-grammes, ...)</li>
                        <li>ensemble de fréquences de cooccurrence</li>
                        <li>l'estimation de la probabilité d'une séquence de longueur n est calculée en tenant compte de
                            son occurrence dans le corpus d'entraînement</li>
                    </ul>
                </li>
                <li>apprentissage machine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{}</span><span class="s2">-grams&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
1-grams<br/>
('He',) ('went',) ('to',) ('school',) ('yesterday',) ('and',) ('attended',) ('the',) ('classes',) <br/>
2-grams<br/>
('He', 'went') ('went', 'to') ('to', 'school') ('school', 'yesterday') ('yesterday', 'and') ('and', 'attended') ('attended', 'the') ('the', 'classes') <br/>
3-grams<br/>
('He', 'went', 'to') ('went', 'to', 'school') ('to', 'school', 'yesterday') ('school', 'yesterday', 'and') ('yesterday', 'and', 'attended') ('and', 'attended', 'the') ('attended', 'the', 'classes') <br/>
4-grams<br/>
('He', 'went', 'to', 'school') ('went', 'to', 'school', 'yesterday') ('to', 'school', 'yesterday', 'and') ('school', 'yesterday', 'and', 'attended') ('yesterday', 'and', 'attended', 'the') ('and', 'attended', 'the', 'classes')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>

            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He goes to school daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>PRP</td>
                    <td>pronoun, personal</td>
                </tr>
                <tr>
                    <td>VBZ</td>
                    <td>verb, present tense, 3rd person singular</td>
                </tr>
                <tr>
                    <td>TO</td>
                    <td>"to" as preposition</td>
                </tr>
                <tr>
                    <td>NN</td>
                    <td>"noun, common, singular or mass</td>
                </tr>
                <tr>
                    <td>RB</td>
                    <td>adverb</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				 $ pip3 install spacy<br/>
				 $ python3 -m spacy download en_core_web_sm<br/>
                         </code>
            </p>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				  import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("He goes to school daily")<br/>
                                  <br/>
                                  for token in doc:<br/>
                                  &nbsp;&nbsp;print(token.text, token.pos_, token.dep_)<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
				  He PRON nsubj<br/>
                                  goes VERB ROOT<br/>
                                  to ADP prep<br/>
                                  school NOUN pobj<br/>
                                  daily ADV advmod<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("He goes to school daily")<br/>
                                  <br/>
                                  for token in doc:<br/>
				  &nbsp;&nbsp;print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,<br/>
				  &nbsp;&nbsp;&nbsp;&nbsp;token.shape_, token.is_alpha, token.is_stop)<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
				  He -PRON- PRON PRP nsubj Xx True True<br/>
                                  goes go VERB VBZ ROOT xxxx True False<br/>
                                  to to ADP IN prep xx True True<br/>
                                  school school NOUN NN pobj xxxx True False<br/>
                                  daily daily ADV RB advmod xxxx True False<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Gesmundo 2012]</h3>
            <ul>
                <li>regrouper les formes de mots qui appartiennent au même paradigme morphologique flexionnel et
                    attribuer à chaque paradigme son lemme correspondant.</li>
                <li>Exemples
                    <ul>
                        <li>go: go, goes, going, went, gone</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Chrupała 2006, Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation comme une tâche d'étiquetage</li>
                <li>Attribuer un label pour chaque transformation d'un label en lemme</li>
                <li>4 étapes [Gesmundo 2012]
                    <ol>
                        <li>supprimer un suffixe de longueur \(N_s\)</li>
                        <li>ajouter un nouveau suffixe de lemme \(L_s\)</li>
                        <li>supprimer un préfixe de longueur \(N_p\)</li>
                        <li>ajouter un nouveau préfixe lemme, \(L_p\)</li>
                    </ol>
                </li>
                <li>Transformation \(\tau = \langle N_s, L_s, N_p, L_p \rangle\)</li>
                <li>(going, go) = \(\langle 3, \emptyset, 0, \emptyset \rangle \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer</h3>
            <ul>
                <li>WordNet [Miller 1995]</li>
            </ul>
            <p class="codeexample">
                <code>
			  import nltk<br/>
                          nltk.download('punkt')<br/>
                          nltk.download('wordnet')<br/>
                          nltk.download('averaged_perceptron_tagger')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (sans les balises PoS)</h3>
            <p class="codeexample">
                <code>
			  from nltk.stem import WordNetLemmatizer<br/>
<br/>
                          sentence = "He went to school yesterday and attended the classes"<br/>
                          lemmatizer = WordNetLemmatizer()<br/>
                          <br/>
                          for word in sentence.split():<br/>
                          &nbsp;&nbsp;print(lemmatizer.lemmatize(word), end=' ')<br/>
                         </code>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     He went to school yesterday and attended the class
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
                <code>
                          from nltk.stem import WordNetLemmatizer<br/>
                          from nltk import word_tokenize, pos_tag<br/>
                          from nltk.corpus import wordnet as wn<br/>
                          <br/>
                          # Check the complete list of tags http://www.nltk.org/book/ch05.html<br/>
                          def wntag(tag):<br/>
                          &nbsp;&nbsp;if tag.startswith("J"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.ADJ<br/>
                          &nbsp;&nbsp;elif tag.startswith("R"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.ADV<br/>
                          &nbsp;&nbsp;elif tag.startswith("N"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.NOUN<br/>
                          &nbsp;&nbsp;elif tag.startswith("V"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.VERB<br/>
                          &nbsp;&nbsp;return None<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
                <code>
                          lemmatizer = WordNetLemmatizer()<br/>
                          <br/>
                          sentence = "I went to school today and he goes daily"<br/>
                          tokens = word_tokenize(sentence)<br/>
                          for token, tag in pos_tag(tokens):<br/>
                          &nbsp;&nbsp;if wntag(tag):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;print(lemmatizer.lemmatize(token, wntag(tag)), end=' ')<br/>
                          &nbsp;&nbsp;else:<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;print(lemmatizer.lemmatize(token), end=' ')<br/>
                         </code>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     I go to school today and he go daily
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("I went to school today and he goes daily")<br/>
                                  <br/>
                                  for token in doc:<br/>
				  &nbsp;&nbsp;print(token.lemma_, end=' ')<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
				  -PRON- go to school today and -PRON- go daily<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>4.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Morphologie</h3>
            <ul>
                <li>l'étude des mots, de leurs les paradigmes et de l’organisation des catégories grammaticales</li>
                <li>examine les parties du discours, l'intonation et l'accent, ainsi que la façon dont le contexte peut
                    modifier la prononciation et le sens d'un mot</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>4.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
				 from spacy import displacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("He goes to school daily")<br/>
                                  <br/>
                                  displacy.serve(doc, style="dep")<br/>
                         </code>
            </p>
            <figure>
                <img src="../../2021/MachineLearning/spacy-dep-output.svg" height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Les embeddings de mots sont une technique d'apprentissage de caractéristiques où des mots ou des phrases
                du vocabulaire sont associés à des vecteurs de nombres réels.</p>
            <ul>
                <li>L'idée principale est de représenter chaque mot par un vecteur dense dans un espace continu, de
                    telle sorte que des mots similaires aient des vecteurs similaires, capturant ainsi les relations
                    sémantiques entre les mots.</li>
                <li>Quantifier et catégoriser les similarités sémantiques entre les éléments linguistiques en fonction
                    de leurs propriétés de distribution dans de grands échantillons de données linguistiques.</li>
                <li>En d'autres termes, les mots qui ont des contextes similaires ou qui apparaissent dans des contextes
                    similaires auront des embeddings de mots similaires.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Avantages de Word Embeddings</p>
            <ul>
                <li><b>Représentation dense</b> : Les embeddings fournissent une représentation dense, contrairement à
                    une représentation creuse où chaque mot serait représenté par un vecteur binaire indiquant sa
                    présence ou son absence.</li>
                <li><b>Capture des relations sémantiques</b> : Les embeddings captent les relations sémantiques et les
                    similitudes entre les mots, ce qui les rend utiles dans de nombreuses tâches de traitement du
                    langage naturel.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Applications de Word Embeddings</p>
            <ul>
                <li><b>Similarité sémantique</b> : Mesurer la similarité sémantique entre les mots.</li>
                <li><b>Traduction automatique</b> : Améliorer les performances des systèmes de traduction automatique.
                </li>
                <li><b>Analyse des sentiments</b> : Mieux comprendre le contexte et les relations sémantiques dans
                    l'analyse des sentiments, entre autres applications.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy est une bibliothèque open-source pour le traitement du langage naturel (NLP) en Python. Elle offre
                des outils performants et efficaces pour effectuer diverses tâches de traitement du langage naturel, de
                l'analyse syntaxique à la reconnaissance d'entités nommées. spaCy est conçu pour être rapide, précis et
                facile à utiliser.</p>
            <ul>
                <li><b>Collecte de données</b> : Les modèles spaCy sont souvent entraînés sur de vastes ensembles de
                    données annotées, qui peuvent inclure des corpus textuels avec des annotations pour l'analyse
                    syntaxique, la reconnaissance d'entités nommées, etc.</li>
                <li><b>Annotation des données</b> : Les données collectées sont annotées manuellement avec des
                    informations linguistiques spécifiques telles que les parties du discours, les entités nommées, les
                    relations syntaxiques, etc.</li>
                <li><b>Entraînement initial</b> : Les modèles spaCy sont initialement entraînés sur ces ensembles de
                    données annotées pour apprendre les structures linguistiques. Ce processus peut inclure
                    l'utilisation d'algorithmes d'apprentissage automatique tels que les réseaux de neurones.</li>
                <li><b>Optimisation et réglage</b> : Les modèles sont ensuite optimisés et réglés pour améliorer leurs
                    performances sur des tâches spécifiques. Cela peut impliquer des itérations sur le processus
                    d'entraînement en ajustant les hyperparamètres du modèle.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>Évaluation</b> : Les modèles sont évalués sur des ensembles de données de test distincts pour
                    mesurer leur précision, leur rappel et d'autres métriques spécifiques à la tâche.</li>
                <li><b>Construction des modèles linguistiques pré-entraînés</b> : Une fois le modèle entraîné et évalué,
                    spaCy construit des modèles linguistiques pré-entraînés qui encapsulent les connaissances acquises
                    sur la structure linguistique.</li>
                <li><b>Téléchargement et utilisation</b> : Les utilisateurs peuvent télécharger ces modèles
                    pré-entraînés via spaCy et les utiliser dans leurs applications pour effectuer diverses tâches de
                    traitement du langage naturel sans avoir à entraîner un modèle de zéro.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy propose différents modèles linguistiques pré-entraînés pour différentes langues et tâches. Le
                modèle en_core_web_lg est un modèle vectoriel large d'anglais.</p>
            <h4>Installation du Modèle spaCy (en_core_web_lg) :</h4>
            <p class="codeexample">
                <code>
				 $ python3 -m spacy download en_core_web_lg<br/>
                         </code>
            </p>
            <h4>Chargement du Modèle spaCy :</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Avantages de spaCy :</p>
            <ul>
                <li><b>Performance élevée</b> : spaCy est reconnu pour sa rapidité d'exécution, ce qui le rend adapté au
                    traitement de grands volumes de texte en temps réel.</li>
                <li><b>Modèles pré-entraînés</b> : spaCy propose des modèles linguistiques pré-entraînés pour plusieurs
                    langues, ce qui facilite l'analyse de texte sans nécessiter d'entraînement à partir de zéro.</li>
                <li><b>Extraction d'informations linguistiques riches</b> : spaCy fournit des informations linguistiques
                    détaillées telles que les parties du discours, les entités nommées, les relations syntaxiques, et
                    plus encore.</li>
                <li><b>API conviviale</b> : L'API spaCy est conçue pour être intuitive et conviviale. Elle facilite la
                    réalisation de tâches complexes avec des lignes de code concises.</li>
                <li><b>Intégration avec d'autres bibliothèques</b> : spaCy s'intègre bien avec d'autres bibliothèques
                    Python populaires, facilitant son utilisation dans des projets plus larges.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Limites de spaCy :</p>
            <ul>
                <li><b>Dépendance des modèles linguistiques</b> : L'utilisation de modèles pré-entraînés signifie que la
                    qualité des résultats dépend de la qualité du modèle. Dans des domaines de spécialité ou pour des
                    langues moins courantes, les modèles peuvent ne pas être aussi performants.</li>
                <li><b>Gestion des entités nommées</b> : Bien que spaCy excelle dans la reconnaissance d'entités
                    nommées, il peut parfois avoir du mal avec des tâches plus complexes impliquant des variations
                    contextuelles.</li>
                <li><b>Taille des modèles</b> : Les modèles pré-entraînés peuvent être relativement volumineux, ce qui
                    peut être un inconvénient dans des environnements avec des restrictions de mémoire ou pour des
                    applications mobiles.</li>
                <li><b>Personnalisation limitée</b> : Bien que spaCy offre des fonctionnalités de personnalisation,
                    elles peuvent être limitées par rapport à d'autres bibliothèques NLP plus flexibles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>

<span class="c1"># Définir les mots à comparer</span>
<span class="n">words_to_compare</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;apple&quot;</span><span class="p">]</span>

<span class="c1"># Calculer la similarité entre les paires de mots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
        <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="o">=</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">doc1</span><span class="p">,</span> <span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word1</span><span class="p">),</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
        <span class="n">similarity_score</span> <span class="o">=</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité (</span><span class="si">{}</span><span class="s2"> / </span><span class="si">{}</span><span class="s2">): </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">similarity_score</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <h4>Affichage</h4>
            <p class="codeexample">
                <code>
                <pre>
Similarité (dog / cat): ...
Similarité (dog / apple): ...
Similarité (cat / apple): ...
                </pre>

                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: vector</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>

<span class="c1"># Imprimer les vecteurs de chaque jeton sur une seule ligne</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteurs de &#39;</span><span class="si">{}</span><span class="s2">&#39; : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">,</span> <span class="n">vector_list</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>Word2Vec a marqué un tournant significatif dans la <b>représentation des mots</b> dans le domaine de
                l'apprentissage automatique.</p>
            <ul>
                <li>C'est une technique publiée en <b>2013</b> par une équipe de chercheurs dirigée par <b>Tomas
                        Mikolov</b> chez Google.</li>
                <li><b>Représentation vectorielle</b> : Word2Vec représente chaque mot distinct avec un vecteur dans un
                    espace continu. Ces vecteurs captent les relations sémantiques et syntaxiques entre les mots.</li>
                <li><b>Apprentissage basé sur un réseau neuronal</b> : Le modèle utilise un réseau neuronal pour
                    apprendre des associations de mots à partir d'un vaste corpus de texte. Cette approche permet de
                    capturer des nuances complexes dans la signification des mots.</li>
                <li><b>Entrée et sortie</b> : Word2Vec prend en entrée un large corpus de texte et produit un espace
                    vectoriel, généralement de plusieurs centaines de dimensions. Cette représentation vectorielle
                    permet de mesurer la similarité sémantique entre les mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>L'implémentation de Word2Vec se déroule en plusieurs étapes :</p>
            <ol>
                <li><b>Prétraitement des données</b> : Le texte est nettoyé et prétraité pour éliminer les éléments
                    indésirables tels que la ponctuation et les stopwords.</li>
                <li><b>Création d'un vocabulaire</b> : Les mots uniques du corpus sont utilisés pour construire un
                    vocabulaire. Chaque mot est ensuite associé à un index.</li>
                <li><b>Génération de paires mot-contexte</b> : Pour chaque mot du corpus, des paires mot-contexte sont
                    créées en utilisant une fenêtre contextuelle glissante. Ces paires servent d'exemples
                    d'entraînement.</li>
                <li><b>Construction du modèle Word2Vec</b> : Un modèle de réseau neuronal est créé, avec une couche
                    d'entrée représentant les mots, une couche cachée (skip-gram ou CBOW), et une couche de sortie pour
                    prédire le mot suivant dans le contexte.</li>
                <li><b>Entraînement du modèle</b> : Le modèle est entraîné sur les paires mot-contexte générées,
                    ajustant les poids du réseau pour minimiser la différence entre les prédictions et les vrais mots du
                    contexte.</li>
                <li><b>Obtention des embeddings</b> : Les vecteurs de mots appris pendant l'entraînement, appelés
                    embeddings, sont extraits. Chaque mot du vocabulaire est maintenant représenté par un vecteur dense
                    dans l'espace continu.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec</h3>
            <ul>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
                <li>une simple fonction mathématique (par exemple, la similarité cosinus entre les vecteurs) indique le
                    niveau de similarité sémantique entre les mots représentés par ces vecteurs \[\text{similarity} =
                    \cos(\theta) = {\mathbf{A} \cdot \mathbf{B}
                    \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i B_i} }{
                    \sqrt{\sum\limits_{i=1}^{n}{A_i^2}} \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} },\]
                </li>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <p>CBOW est un modèle spécifique de Word2Vec. Dans ce modèle, la prédiction du mot courant se fait en
                utilisant une fenêtre de mots contextuels voisins. L'ordre des mots de contexte n'influence pas la
                prédiction, ce qui en fait une approche robuste.</p>
            <ul>
                <li><b>Modèle prédictif</b> : CBOW prédit le mot cible en se basant sur le contexte qui l'entoure, mais
                    contrairement à d'autres modèles, l'ordre spécifique des mots dans ce contexte n'est pas pris en
                    compte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <ul>
                <li><b>Entrée</b> : La donnée d'entrée du modèle CBOW est une fenêtre de mots contextuels entourant le
                    mot cible. Cette fenêtre est définie par un paramètre appelé la taille de la fenêtre.</li>
                <li><b>Architecture</b> : CBOW utilise une architecture de réseau neuronal à une seule couche cachée. La
                    couche d'entrée représente les mots du contexte, et la couche de sortie représente le mot cible à
                    prédire.</li>
                <li><b>Entraînement</b> : Le modèle est entraîné en ajustant les poids du réseau pour minimiser la
                    différence entre les prédictions du modèle et le mot cible réel. Cela se fait à travers des
                    techniques d'optimisation comme la rétropropagation du gradient.</li>
                <li><b>Sortie</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les relations sémantiques entre les mots, permettant
                    ainsi de représenter chaque mot par un vecteur dans un espace continu.</li>
                <li><b>Avantages</b> : CBOW est souvent plus rapide à entraîner que d'autres modèles comme le Skip-gram
                    (une autre variante de Word2Vec) et peut être plus efficace dans des contextes où l'ordre séquentiel
                    des mots n'est pas critique.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle CBOW avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>

<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des deux mots les plus probables suivant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots suivant &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <p>Le modèle Skip-gram est une autre variante de Word2Vec qui se concentre sur la prédiction de la fenêtre
                voisine des mots de contexte à partir du mot courant. </p>
            <ul>
                <li><b>Objectif</b> : L'objectif principal du modèle Skip-gram est de prendre un mot source (le mot
                    courant) et de prédire les mots qui l'entourent dans une fenêtre de contexte donnée.</li>
                <li><b>Entrée</b> : Le mot source est utilisé comme donnée d'entrée du modèle, et la sortie souhaitée
                    est la distribution des probabilités des mots du contexte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <ul>
                <li><b>Architecture</b> : Skip-gram utilise une architecture de réseau neuronal à une seule couche
                    cachée. La couche d'entrée représente le mot source, et la couche de sortie représente les mots du
                    contexte.</li>
                <li><b>Entraînement</b> : Pendant l'entraînement, les poids du réseau sont ajustés pour minimiser la
                    différence entre les prédictions du modèle et la véritable distribution des mots du contexte. Cela
                    se fait généralement à l'aide de techniques d'optimisation comme la rétropropagation du gradient.
                </li>
                <li><b>Pondération du contexte</b> : Une caractéristique importante du modèle Skip-gram est que
                    l'architecture accorde plus de poids aux mots de contexte proches du mot source que ceux plus
                    éloignés. Cela permet de mieux capturer les relations sémantiques et syntaxiques locales.</li>
                <li><b>Embeddings</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les similitudes sémantiques entre les mots, permettant
                    de représenter chaque mot par un vecteur dans un espace continu.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle Skip-gram avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="c1"># sg: 1 pour skip-gram ; sinon CBOW.</span>
<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des mots les plus probables dans le contexte entourant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots dans le contexte de &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <p>La Reconnaissance d'Entités Nommées (NER) consiste à identifier et classer des entités spécifiques dans
                un texte. Ces entités peuvent inclure des personnes, des lieux, des organisations, des dates, des
                montants monétaires, etc. Le but est d'extraire des informations structurées à partir de données
                textuelles non structurées.</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/datarepresentation.svg"
                    height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <ul>
                <li><b>Identification d'entités</b> : La première étape de la NER consiste à identifier les mots ou
                    groupes de mots qui représentent des entités dans le texte. Ces entités peuvent être des noms de
                    personnes, des noms de lieux, des noms d'organisations, etc.</li>
                <li><b>Classification des entités</b> : Une fois les entités identifiées, elles sont classifiées dans
                    des catégories spécifiques. Par exemple, une entité peut être classée comme "PERSON" si elle
                    représente une personne, "LOCATION" si elle représente un lieu, "ORGANIZATION" si elle représente
                    une organisation, et ainsi de suite.</li>
                <li><b>Contextualisation</b> : La NER tient compte du contexte dans lequel une entité apparaît. Par
                    exemple, le mot "banc" peut être classé comme une entité financière dans le contexte d'une
                    discussion sur l'économie, mais comme une entité physique dans le contexte d'un parc.</li>
                <li><b>Relations entre entités</b> : Dans certains cas, la NER peut également inclure la détection des
                    relations entre différentes entités dans le texte. Par exemple, la relation entre une personne et
                    l'organisation qu'elle travaille.</li>
                <li><b>Applications pratiques</b> : Les résultats de la NER peuvent être utilisés dans diverses
                    applications, telles que l'amélioration de la recherche d'informations, l'extraction de relations,
                    la catégorisation de documents, la création de résumés automatiques, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <p>La Reconnaissance d'Entités Nommées (NER) est souvent réalisée à l'aide de modèles d'apprentissage
                automatique, et plusieurs algorithmes peuvent être utilisés dans ce contexte. Voici quelques-uns des
                algorithmes couramment employés :</p>
            <ul>
                <li><b>Modèles de markov cachés (HMM - Hidden Markov Models)</b> : Les HMM ont été utilisés pour la NER,
                    où l'idée est de modéliser la séquence des étiquettes d'entités en tant que séquence cachée derrière
                    la séquence observable de mots.</li>
                <li><b>Réseaux de neurones</b> : Les architectures de réseaux de neurones, y compris les réseaux de
                    neurones récurrents (RNN), les réseaux de neurones récurrents bidirectionnels (BiRNN), et les
                    réseaux de neurones récurrents à mémoire à court terme (LSTM), ont montré des performances
                    significatives dans la NER.</li>
                <li><b>Transformers</b> : Les modèles basés sur les transformers, tels que BERT (Bidirectional Encoder
                    Representations from Transformers) et ses variantes, ont considérablement amélioré les performances
                    en NER. Ces modèles sont pré-entraînés sur de grandes quantités de données textuelles et captent des
                    représentations contextuelles riches.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <ul>
                <li><b>Modèles statistiques traditionnels</b> : Des approches statistiques plus traditionnelles, comme
                    les modèles de séquence et les classificateurs basés sur des caractéristiques, ont également été
                    utilisées dans des scénarios où des quantités limitées de données annotées sont disponibles.</li>
                <li><b>Règles et expressions régulières</b> : Dans certains cas, des règles manuelles ou des expressions
                    régulières peuvent être utilisées pour extraire des entités spécifiques, surtout lorsque des motifs
                    clairs et récurrents peuvent être définis.</li>
                <li><b>Entraînement supervisé</b> : Les méthodes d'entraînement supervisé consistent à annoter
                    manuellement un ensemble de données avec des entités nommées, puis à entraîner un modèle sur ces
                    données annotées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Analyser le texte</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Afficher les informations sur les entités</span>
<span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">:</span>
    <span class="n">entity_text</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">text</span>
    <span class="n">start_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">start_char</span>
    <span class="n">end_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">end_char</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">label_</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entité: </span><span class="si">{}</span><span class="s2">, Début: </span><span class="si">{}</span><span class="s2">, Fin: </span><span class="si">{}</span><span class="s2">, Catégorie: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entity_text</span><span class="p">,
                 </span> <span class="n">start_char</span><span class="p">,</span> <span class="n">end_char</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
                <code>
                <pre>
Entité: Paris, Début: 0, Fin: 5, Catégorie: GPE
Entité: France, Début: 24, Fin: 30, Catégorie: GPE
Entité: 2015, Début: 35, Fin: 39, Catégorie: DATE
Entité: 2,206,488, Début: 72, Fin: 81, Catégorie: CARDINAL
                         </pre>
                </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">serve</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>GPE</td>
                    <td>Pays, villes, états.</td>
                </tr>
                <tr>
                    <td>DATE</td>
                    <td>Dates ou périodes absolues ou relatives</td>
                </tr>
                <tr>
                    <td>CARDINAL</td>
                    <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Installation</p>
            <p class="codeexample">
                <code>
			import nltk</br>
                        nltk.download('vader_lexicon')</br>
                         </code>
            </p>
            <p>Usage</p>
            <p class="codeexample">
                <code>
			from nltk.sentiment.vader import SentimentIntensityAnalyzer</br>
                        sia = SentimentIntensityAnalyzer()</br>
                        sentiment = sia.polarity_scores("this movie is good")</br>
                        print(sentiment)</br>
                        sentiment = sia.polarity_scores("this movie is not very good")</br>
                        print(sentiment)</br>
                        sentiment = sia.polarity_scores("this movie is bad")</br>
                        print(sentiment)</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}</br>
                        {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'compound': -0.3865}</br>
                        {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique</h3>
            <p>La traduction automatique est le processus d'utilisation de logiciels pour traduire un texte ou un
                discours d'une langue à une autre. Il existe plusieurs approches pour aborder ce problème complexe :</p>
            <ul>
                <li><b>Approche manuelle (règles)</b> : Cette approche repose sur des règles linguistiques définies
                    manuellement par des linguistes ou des experts en langues. Les règles peuvent inclure des
                    correspondances mot à mot, des règles de grammaire, et d'autres connaissances linguistiques
                    spécifiques.</li>
                <li><b>Approche statistique</b> : Dans cette approche, les modèles statistiques sont utilisés pour
                    apprendre les relations entre les phrases dans différentes langues à partir de grands ensembles de
                    données parallèles. Ces modèles peuvent être basés sur des probabilités conditionnelles et des
                    méthodes statistiques avancées.</li>
                <li><b>Approche hybride (règles et statistique)</b> : L'approche hybride combine des éléments des
                    approches manuelles et statistiques. Elle peut utiliser des règles pour des aspects spécifiques de
                    la traduction tout en tirant parti de l'apprentissage statistique pour d'autres parties du
                    processus.</li>
                <li><b>Apprentissage machine</b> : L'apprentissage machine, en particulier les modèles neuronaux, a
                    considérablement amélioré les performances de la traduction automatique. Les réseaux de neurones,
                    tels que les réseaux de neurones récurrents (RNN) et les transformers, ont montré des résultats
                    impressionnants en capturant des structures linguistiques complexes.</li>
            </ul>
            </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche manuelle (règles)</h3>
            <p>L'approche manuelle dans la traduction automatique repose sur l'utilisation de règles linguistiques
                préalablement définies par des linguistes ou des experts en langues. Ces règles spécifient comment
                traduire des éléments spécifiques d'une langue source vers une langue cible.</p>
            <h4>Exemple : Traduction de phrases simples (anglais vers français)</h4>
            <p>Supposons que nous avons une règle manuelle qui dit que le mot "hello" en anglais doit être traduit par
                "bonjour" en français. De plus, nous avons une règle indiquant que la phrase "I love" doit être traduite
                par "j'aime". Ces règles sont simples et sont utilisées mot à mot.</p>
            <ul>
                <li><b>Application des règles manuelles</b> : "Hello" est traduit en "bonjour." "I love" est traduit en
                    "j'aime." "programming" n'a pas de règle spécifique, donc nous le laissons tel quel.</li>
                <li><b>Traduction résultante</b> : La phrase anglaise "Hello, I love programming" serait traduite en
                    français par "Bonjour, j'aime programming."</li>
            </ul>
            <p> Dans des situations réelles, les règles peuvent devenir extrêmement complexes, impliquant des contextes
                grammaticaux, des variations lexicales, des idiomes, etc.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
            <p>L'approche statistique de la traduction automatique repose sur l'utilisation de modèles statistiques pour
                apprendre les relations entre les phrases dans différentes langues à partir de grands ensembles de
                données parallèles. Contrairement à l'approche manuelle basée sur des règles linguistiques définies par
                des experts, l'approche statistique utilise des statistiques et des probabilités pour estimer la
                probabilité d'une traduction donnée.</p>
            <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
            <ul>
                <li><b>Ensembles de données parallèles</b> : Pour entraîner un modèle statistique, des ensembles de
                    données parallèles sont nécessaires. Ces ensembles contiennent des phrases dans la langue source et
                    leurs traductions correspondantes dans la langue cible.</li>
                <li><b>Alignement de phrases</b> : Les phrases équivalentes dans les deux langues sont alignées dans
                    l'ensemble de données parallèles. Cela crée des paires de phrases qui serviront de données
                    d'entraînement pour le modèle.</li>
                <li><b>Extraction de caractéristiques</b> : Des caractéristiques pertinentes sont extraites à partir des
                    paires de phrases alignées. Ces caractéristiques peuvent inclure des n-grammes (groupes de mots
                    consécutifs), des séquences de mots, des informations sur la syntaxe, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
            <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
            <ul>
                <li><b>Entraînement du modèle</b> : Un modèle statistique, souvent basé sur des méthodes probabilistes
                    comme les modèles de langue conditionnels, est entraîné sur ces caractéristiques extraites. Le
                    modèle apprend les probabilités conditionnelles des traductions données les phrases sources.</li>
                <li><b>Estimation des probabilités</b> : Lors de la traduction d'une nouvelle phrase, le modèle estime
                    les probabilités des différentes traductions possibles en fonction des caractéristiques de cette
                    phrase.</li>
                <li><b>Sélection de la meilleure traduction</b> : La traduction avec la probabilité la plus élevée est
                    sélectionnée comme la traduction finale de la phrase source.</li>
                <li><b>Évaluation et ajustement</b> : Le modèle est évalué sur des ensembles de données de test pour
                    mesurer sa performance. Si nécessaire, le modèle peut être ajusté pour améliorer ses performances.
                </li>
            </ul>
            <p>Un exemple concret de cette approche pourrait être l'utilisation de modèles de langues conditionnels (par
                exemple, les modèles de Markov conditionnels) pour estimer la probabilité d'une traduction donnée une
                phrase source.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche Hybride (Règles et Statistique)</h3>
            <p>L'approche hybride dans la traduction automatique combine des éléments des approches manuelles (basées
                sur des règles) et des approches statistiques. L'idée est d'utiliser des règles linguistiques pour
                certaines parties de la traduction tout en tirant parti de modèles statistiques pour d'autres parties du
                processus. Cette combinaison vise à exploiter les avantages des deux approches pour obtenir des
                traductions de meilleure qualité.</p>
            <h4>Les étapes clés :</h4>
            <li><b>Règles linguistiques</b> : Des règles linguistiques sont définies manuellement pour certaines
                constructions grammaticales, expressions idiomatiques, ou autres aspects linguistiques spécifiques. Ces
                règles peuvent être élaborées par des experts linguistes.</li>
            <li><b>Ensembles de données parallèles</b> : Comme dans l'approche statistique, des ensembles de données
                parallèles contenant des phrases dans la langue source et leurs traductions dans la langue cible sont
                nécessaires.</li>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche Hybride (Règles et Statistique)</h3>
            <h4>Les étapes clés :</h4>
            <li><b>Apprentissage statistique</b> : Un modèle statistique est entraîné sur ces ensembles de données
                parallèles, en utilisant des méthodes statistiques pour apprendre les relations entre les phrases dans
                les deux langues.</li>
            <li><b>Application des règles</b> : Lors du processus de traduction, les règles linguistiques sont
                appliquées en priorité. Si une partie du texte correspond à une règle prédéfinie, la traduction basée
                sur la règle est utilisée.</li>
            <li><b>Utilisation du modèle statistique</b> : Pour les parties du texte qui ne correspondent pas à des
                règles prédéfinies, le modèle statistique est utilisé pour générer la traduction en se basant sur les
                relations apprises à partir des ensembles de données parallèles.</li>
            <li><b>Intégration des traductions partielles</b> : Les traductions générées par les règles et celles
                générées par le modèle statistique sont intégrées pour former la traduction finale du texte.</li>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
            <p>L'approche machine dans la traduction automatique fait référence à l'utilisation de techniques
                d'apprentissage machine, en particulier d'algorithmes d'apprentissage automatique, pour automatiser le
                processus de traduction entre langues. Cette approche a considérablement évolué avec le développement de
                modèles plus avancés basés sur l'apprentissage profond, notamment les modèles de séquence à séquence.
            </p>
            <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
            <ul>
                <li><b>Ensembles de données parallèles</b> : Des ensembles de données parallèles sont nécessaires,
                    contenant des phrases dans la langue source et leurs traductions correspondantes dans la langue
                    cible. Ces ensembles serviront de données d'entraînement pour le modèle.</li>
                <li><b>Modèle de séquence à séquence (Seq2Seq)</b> : Le modèle central dans cette approche est
                    généralement un modèle de séquence à séquence (Seq2Seq). Ce type de modèle utilise un réseau
                    neuronal récurrent (RNN) ou une architecture de transformer pour traiter des séquences de données.
                </li>
                <li><b>Encodage de la phrase source</b> : La phrase source est encodée en une représentation vectorielle
                    par le réseau neuronal. Cela capture les informations sémantiques et syntaxiques de la phrase
                    source.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
            <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
            <ul>
                <li><b>Décodage vers la langue cible</b> : Le modèle décode ensuite la représentation encodée pour
                    générer la séquence de mots dans la langue cible. C'est le processus de génération de la traduction.
                </li>
                <li><b>Entraînement supervisé</b> : Le modèle est entraîné sur les ensembles de données parallèles en
                    utilisant une approche supervisée. Il apprend à minimiser la différence entre la séquence générée et
                    la traduction attendue dans la langue cible.</li>
                <li><b>Optimisation</b> : Des techniques d'optimisation, telles que la descente de gradient stochastique
                    (SGD) ou des optimiseurs plus avancés comme Adam, sont utilisées pour ajuster les poids du modèle
                    pendant l'entraînement.</li>
                <li><b>Évaluation et ajustement</b> : Le modèle est évalué sur des ensembles de données de test
                    indépendants pour mesurer sa performance. Des ajustements peuvent être effectués pour améliorer les
                    résultats.</li>
                <li><b>Incorporation de méthodes plus avancées</b> : Les modèles de traduction automatique basés sur
                    l'apprentissage machine peuvent être améliorés en incorporant des techniques plus avancées telles
                    que l'attention, qui permettent au modèle de se concentrer sur des parties spécifiques de la phrase
                    source lors de la génération de la traduction.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Transformer</h3>
            <p>Le Transformer est une architecture de réseau de neurones qui a révolutionné le domaine du traitement du
                langage naturel depuis son introduction par Vaswani et al. en 2017. Deux des modèles les plus célèbres
                basés sur l'architecture Transformer sont : </p>
            <ul>
                <li> BERT (Bidirectional Encoder Representations from Transformers)</li>
                <li> GPT (Generative Pre-trained Transformer)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT (Bidirectional Encoder Representations from Transformers) :</h3>
            <p> BERT, développé par Google, est un modèle pré-entraîné qui a été formé sur de vastes corpus de texte. Ce
                qui distingue BERT, c'est son approche bidirectionnelle pour la représentation des mots. Contrairement à
                des modèles précédents qui utilisaient une compréhension unidirectionnelle du contexte, BERT prend en
                compte le contexte à la fois avant et après un mot dans une phrase, améliorant ainsi la compréhension du
                sens.</p>
            <ul>
                <li><b>Utilisation</b> : BERT a été pré-entraîné sur des tâches telles que la prédiction de mots masqués
                    dans une phrase (Masked Language Model) et la prédiction de la relation entre deux phrases (Next
                    Sentence Prediction). Ces pré-entraînements permettent à BERT d'acquérir une compréhension profonde
                    du langage.</li>
                <li><b>Applications</b> : BERT est souvent utilisé comme base pour des tâches spécifiques telles que la
                    classification de texte, l'extraction d'entités nommées, la compréhension de texte, etc. Des
                    versions pré-entraînées de BERT sont disponibles, et les modèles peuvent être fine-tunés pour des
                    tâches spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : composants</h3>
            <p> Un modèle BERT (Bidirectional Encoder Representations from Transformers) est composé de plusieurs
                éléments clés, reflétant l'architecture Transformer. Voici les composants principaux d'un modèle BERT :
            </p>
            <ul>
                <li><b>Embedding token</b> : Cette couche transforme les tokens (mots ou sous-mots) d'une séquence en
                    vecteurs d'embedding. Chaque token est représenté par un vecteur qui capture son sens sémantique.
                    Ces embeddings peuvent également inclure des informations de position pour indiquer la position de
                    chaque token dans la séquence.</li>
                <li><b>Embedding de segment</b> : BERT prend en compte le contexte global d'une séquence, même lorsque
                    celle-ci contient plusieurs phrases. Pour ce faire, une couche d'embedding de segment est utilisée.
                    Elle attribue un segment spécifique à chaque token pour indiquer à quel segment il appartient. Cela
                    permet au modèle de distinguer différentes parties de la séquence.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : composants</h3>
            <ul>
                <li><b>Encoder BERT (Transformateur)</b> : L'élément central de BERT est l'encodeur, qui suit
                    l'architecture du Transformer. L'encodeur est composé de plusieurs couches d'attention multi-têtes.
                    Chaque couche prend en compte les relations entre les tokens et utilise l'attention pour attribuer
                    des poids aux différents tokens en fonction de leur importance contextuelle.</li>
                <li><b>Attention multi-tête</b> : Chaque couche d'attention contient plusieurs "têtes" d'attention.
                    Chaque tête capture des aspects différents des relations entre les tokens. L'utilisation de
                    plusieurs têtes permet au modèle de capturer des relations complexes dans le contexte.</li>
                <li><b>Couche de pooling</b> : BERT utilise souvent une couche de pooling pour agréger les
                    représentations de tous les tokens en une seule représentation. Cette représentation agrégée peut
                    être utilisée pour des tâches spécifiques, comme la classification de texte.</li>
                <li><b>Couche de classification</b> : Pour les tâches de classification, une couche de classification
                    est ajoutée au-dessus du modèle BERT. Cette couche peut consister en une ou plusieurs couches denses
                    qui projettent la représentation agrégée sur l'espace de sortie de la tâche.</li>
                <li><b>Fine-tuning et couches spécifiques à la tâche</b> : Lorsque BERT est fine-tuné pour une tâche
                    spécifique, des couches supplémentaires peuvent être ajoutées pour adapter le modèle à la tâche en
                    question. Cela peut inclure des couches de classification, des couches de régression, ou d'autres
                    couches spécifiques à la sortie souhaitée.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : fonctionnement</h3>
            <ul>
                <li><b>Prétraitement des données</b> : Avant d'entraîner ou d'utiliser BERT, les données doivent être
                    prétraitées. Cela inclut la tokenisation, où les phrases sont divisées en tokens (mots ou
                    sous-mots), et l'ajout de tokens spéciaux, tels que [CLS] (pour le début de la phrase) et [SEP]
                    (pour la séparation entre les phrases). De plus, des embeddings de segment peuvent être ajoutés pour
                    indiquer à quel segment appartient chaque token, ce qui est utile pour les tâches de compréhension
                    de texte.</li>
                <li><b>Architecture du modèle BERT</b> : BERT utilise une architecture Transformer avec un encodeur
                    bidirectionnel. La partie "bidirectionnelle" signifie que le modèle prend en compte le contexte à la
                    fois avant et après chaque mot dans une phrase, ce qui améliore la compréhension du sens.</li>
                <li><b>Pré-entraînement</b> : BERT est pré-entraîné sur de grandes quantités de données textuelles non
                    annotées. Il est formé à prédire les mots masqués dans une séquence (Masked Language Model, MLM) et
                    à prédire la relation entre deux phrases (Next Sentence Prediction, NSP). Le modèle apprend ainsi
                    une représentation profonde et contextuelle du langage.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : fonctionnement</h3>
            <ul>
                <li><b>Fine-tuning</b> : Après le pré-entraînement, BERT peut être fine-tuné sur des tâches spécifiques.
                    Par exemple, pour la classification de texte, une couche de classification peut être ajoutée
                    au-dessus de la représentation BERT, et le modèle peut être entraîné sur des données annotées pour
                    la tâche spécifique.</li>
                <li><b>Utilisation du modèle fine-tuné</b> : Une fois fine-tuné, le modèle BERT peut être utilisé pour
                    effectuer des tâches spécifiques, telles que la classification de texte, la reconnaissance d'entités
                    nommées, ou d'autres tâches de traitement du langage naturel.</li>
                <li><b>Gestion de la longueur des séquences</b> : BERT a une limitation sur la longueur maximale des
                    séquences qu'il peut traiter. Pour les textes plus longs, des techniques comme le fractionnement du
                    texte ou le choix d'une sous-séquence significative peuvent être utilisées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer)</h3>
            <p>GPT, développé par OpenAI, est un modèle basé sur l'architecture Transformer, mais avec une approche de
                génération de texte. GPT utilise un modèle de langage pré-entraîné qui a appris à prédire le mot suivant
                dans une séquence de mots. Il est capable de générer du texte cohérent et contextuellement approprié.
            </p>
            <ul>
                <li><b>Utilisation</b> : GPT est pré-entraîné sur un large corpus de texte, et la pré-entraînement vise
                    à enseigner au modèle la structure et les motifs du langage. Il peut ensuite être fine-tuné sur des
                    tâches spécifiques selon les besoins.</li>
                <li><b>Applications</b> : GPT est souvent utilisé pour des tâches de génération de texte, comme la
                    rédaction automatique de contenu, la complétion de texte, et d'autres applications où la création de
                    texte naturel est requise.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : composants</h3>
            <p> Le modèle GPT (Generative Pre-trained Transformer) est composé de plusieurs éléments clés qui suivent
                l'architecture Transformer. Voici les composants principaux de GPT : </p>
            <ul>
                <li><b>Embedding token</b> : Tout comme dans BERT, GPT utilise une couche d'embedding pour convertir les
                    tokens (mots ou sous-mots) en vecteurs d'embedding. Chaque token est représenté par un vecteur qui
                    capture son sens sémantique.</li>
                <li><b>Positional encoding</b> : GPT prend en compte la position des mots dans une séquence. Pour ce
                    faire, une couche de Positional Encoding est ajoutée aux embeddings de token pour introduire des
                    informations de position.</li>
                <li><b>Encodeur-décodeur transformer</b> : Contrairement à BERT, GPT utilise une architecture
                    encodeur-décodeur basée sur le Transformer. L'encodeur capture les relations entre les tokens dans
                    une séquence, tandis que le décodeur est utilisé pour générer la séquence de sortie.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : composants</h3>
            <ul>
                <li><b>Attention multi-tête</b> : Comme dans BERT, GPT utilise une attention multi-tête pour capturer
                    des relations complexes entre les mots. Chaque tête d'attention se concentre sur différents aspects
                    des relations entre les tokens.</li>
                <li><b>Couche de pooling (ou moyenne)</b> : GPT utilise souvent une couche de pooling pour agréger les
                    représentations de tous les tokens en une seule représentation. Cette représentation agrégée est
                    ensuite utilisée comme entrée pour le décodeur.</li>
                <li><b>Décodeur GPT</b> : Le décodeur est la partie du modèle qui génère la séquence de sortie. Il prend
                    la représentation agrégée en entrée et génère séquentiellement les tokens de sortie un par un.</li>
                <li><b>Fine-tuning</b> : Après le pré-entraînement, GPT peut être fine-tuné pour des tâches spécifiques
                    en ajoutant des couches spécifiques à la tâche. Ces couches supplémentaires sont souvent des couches
                    de classification ou de régression, selon la nature de la tâche.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : fonctionnement</h3>
            <ul>
                <li><b>Pré-entraînement</b> : GPT est pré-entraîné sur un vaste corpus de texte non annoté. L'objectif
                    du pré-entraînement est d'apprendre une représentation riche et contextuelle du langage. Pendant
                    cette phase, le modèle apprend à prédire le mot suivant dans une séquence de mots. Il utilise une
                    architecture encodeur-décodeur basée sur le Transformer.</li>
                <li><b>Embedding et positional encoding</b> : Chaque mot dans une séquence est représenté par un vecteur
                    d'embedding. GPT prend également en compte la position de chaque mot dans la séquence en utilisant
                    une couche de Positional Encoding.</li>
                <li><b>Encodeur-décodeur transformer</b> : GPT utilise une architecture de Transformer où l'encodeur
                    capture les relations entre les mots dans une séquence, et le décodeur est utilisé pour générer la
                    séquence de sortie. Cependant, GPT est principalement utilisé de manière autoregressive, ce qui
                    signifie qu'il génère séquentiellement un mot à la fois en utilisant les mots précédemment générés
                    comme contexte.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : fonctionnement</h3>
            <ul>
                <li><b>Attention multi-tête</b> : GPT utilise l'attention multi-tête pour capturer des relations
                    complexes entre les mots. Chaque tête d'attention se concentre sur différents aspects du contexte,
                    permettant au modèle de saisir des dépendances à long terme et des relations subtiles.</li>
                <li><b>Génération de texte</b> : Lors de la génération de texte, le modèle prend une séquence initiale
                    en entrée et génère séquentiellement les mots suivants. À chaque étape, le modèle utilise les mots
                    générés précédemment comme contexte pour prédire le mot suivant. Ce processus se répète jusqu'à ce
                    qu'une séquence complète soit générée.</li>
                <li><b>Fine-tuning pour des tâches spécifiques</b> : Après le pré-entraînement, GPT peut être fine-tuné
                    pour des tâches spécifiques en ajoutant des couches supplémentaires spécifiques à la tâche. Par
                    exemple, pour la classification de texte, des couches de classification peuvent être ajoutées
                    au-dessus du modèle pré-entraîné.</li>
                <li><b>Gestion de la longueur des séquences</b> : GPT peut gérer des séquences de longueur variable,
                    mais il a une limite pratique sur la longueur maximale de la séquence qu'il peut générer.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT vs. GPT</h3>
            <table>
                <tr>
                    <th> Caractéristique </th>
                    <th> BERT </th>
                    <th> GPT </th>
                </tr>
                <tr>
                    <td> Objectif du Pré-entraînement </td>
                    <td> Prédiction bidirectionnelle des mots (MLM) et prédiction de relation entre deux phrases (NSP)
                    </td>
                    <td> Génération de texte autoregressive </td>
                </tr>
                <tr>
                    <td> Architecture </td>
                    <td> Encodeur bidirectionnel </td>
                    <td> Encodeur-décodeur avec orientation autoregressive </td>
                </tr>
                <tr>
                    <td> Utilisation en Fine-Tuning </td>
                    <td> Classification de texte, extraction d'entités nommées, etc. </td>
                    <td> Génération de texte, complétion automatique, etc. </td>
                </tr>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT vs. GPT</h3>
            <table>
                <tr>
                    <th> Caractéristique </th>
                    <th> BERT </th>
                    <th> GPT </th>
                </tr>
                <tr>
                    <td> Approche du Contexte </td>
                    <td> Bidirectionnelle, prend en compte le contexte avant et après chaque mot </td>
                    <td> Autoregressive, génère du texte séquentiellement en utilisant le contexte précédent </td>
                </tr>
                <tr>
                    <td> Applications Pratiques </td>
                    <td> Classification, extraction d'entités, détection de paraphrases </td>
                    <td> Génération de texte, complétion automatique, conversation naturelle </td>
                </tr>
                <tr>
                    <td> Taille des Modèles </td>
                    <td> Généralement plus petits </td>
                    <td> Souvent plus grands, surtout les versions plus récentes comme GPT-3 </td>
                </tr>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <p>Le système de recommandation est un domaine essentiel de l'informatique qui vise à réduire la surcharge
                d'informations en fournissant des suggestions filtrées et pertinentes aux utilisateurs.</p>
            <p><b>Réduction de la surcharge d'informations</b> : Les systèmes de recommandation visent à aider les
                utilisateurs à naviguer à travers une grande quantité d'informations en fournissant des recommandations
                ciblées et adaptées à leurs préférences.</p>
            <h4>Fonctionnement</h4>
            <ul>
                <li><b>Prédire la préférence de l'utilisateur</b> : Les systèmes de recommandation utilisent des
                    algorithmes pour analyser le comportement passé de l'utilisateur, ses préférences, et d'autres
                    données pertinentes afin de prédire les éléments qui pourraient l'intéresser à l'avenir.</li>
                <li><b>Gestion de la surcharge d'informations</b> : En filtrant et en triant les informations, ces
                    systèmes aident à éviter la surcharge cognitive en présentant à l'utilisateur uniquement ce qui est
                    le plus susceptible de l'intéresser.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <h4>Types de Recommandations</h4>
            <ul>
                <li><b>Recommandations personnalisées</b> : Basées sur l'historique et les préférences individuelles de
                    l'utilisateur.</li>
                <li><b>Recommandations non personnalisées</b> : Générales et applicables à un large groupe
                    d'utilisateurs.</li>
            </ul>
            <h4>Algorithmes couramment utilisés</h4>
            <ul>
                <li><b>Filtrage collaboratif</b> : Basé sur les comportements et les préférences d'utilisateurs
                    similaires.</li>
                <li><b>Filtrage basé sur le contenu</b> : Utilise des caractéristiques du produit ou de l'élément
                    lui-même pour faire des recommandations.</li>
                <li><b>Apprentissage profond</b> : Des techniques telles que les réseaux de neurones profonds peuvent
                    être utilisées pour modéliser des modèles complexes de préférences.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <h4>Applications</h4>
            <ul>
                <li><b>Générateurs de playlists</b> : Pour les services de vidéo et de musique, offrant des
                    recommandations de chansons basées sur le goût musical de l'utilisateur.</li>
                <li><b>Recommandations de produits</b> : Dans les plateformes de commerce électronique, suggérant des
                    articles basés sur les achats antérieurs ou les préférences.</li>
                <li><b>Recommandations de livres</b> : Sur les plateformes de vente de livres en ligne, proposant des
                    ouvrages similaires à ceux déjà appréciés.</li>
                <li><b>Recommandations de contenu sur les réseaux sociaux</b> : Proposant des publications, des amis ou
                    des groupes en fonction de l'activité passée et des préférences de l'utilisateur.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
            <h4>Hypothèse</h4>
            <p>Les individus suivent souvent les recommandations des autres utilisateurs. Cela suppose que les
                préférences et les actions des utilisateurs peuvent être des indicateurs fiables pour recommander des
                articles ou des objets similaires à d'autres utilisateurs.</p>
            <h4>Sources des données : </h4>
            <ul>
                <li><b>Utilisateurs</b> : Les informations relatives aux utilisateurs, y compris leurs préférences,
                    comportements, et actions.</li>
                <li><b>Articles ou objets</b> : Les données concernant les articles ou objets à recommander. Cela peut
                    inclure des détails sur les caractéristiques des articles, leurs catégories, etc.</li>
                <li><b>Transactions</b> : Les interactions et transactions entre les utilisateurs et les articles. Cela
                    peut inclure des achats, des clics, des évaluations, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
            <h4>Collecte des Préférences des Utilisateurs : </h4>
            <ul>
                <li><b>Préférences explicitement exprimées</b> : Les évaluations et les actions des utilisateurs qui
                    sont directement exprimées. Cela peut inclure les avis positifs et négatifs, les évaluations
                    numériques, etc.</li>
                <li><b>Interprétation des actions des utilisateurs</b> : Observation et interprétation des actions des
                    utilisateurs, en particulier dans le contexte de la navigation web. Cela pourrait inclure des
                    comportements tels que les pages visitées, le temps passé sur une page, les articles ajoutés au
                    panier, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation</h3>
            <h4>Méthodes de Collecte des Données : </h4>
            <ul>
                <li><b>Surveillance des activités utilisateurs</b> : Utilisation de technologies de suivi pour observer
                    et enregistrer les actions des utilisateurs, généralement dans le contexte d'une plateforme en
                    ligne.</li>
                <li><b>Systèmes de retour d'information</b> : Encouragement des utilisateurs à fournir des retours
                    d'information explicites sous forme d'évaluations, de commentaires, etc.</li>
                <li><b>Analyse des transactions</b> : Extraction d'informations à partir des transactions entre
                    utilisateurs et articles pour déduire les préférences et les comportements.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
                <a class="next" href="#slide91"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide91">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Fonctions [Ricci 2011]</h3>
            <ul>
                <li><b>Augmenter le nombre d'articles vendus</b> : Les systèmes de recommandation visent à stimuler les
                    ventes en suggérant des articles pertinents aux utilisateurs, augmentant ainsi les opportunités
                    d'achat.</li>
                <li><b>Vendre des articles plus variés</b> : En diversifiant les recommandations, les systèmes cherchent
                    à élargir le choix des utilisateurs et à promouvoir la vente d'une gamme plus large d'articles.</li>
                <li><b>Augmenter la satisfaction des utilisateurs</b> : En fournissant des recommandations pertinentes
                    et adaptées aux préférences individuelles, les systèmes visent à accroître la satisfaction des
                    utilisateurs.</li>
                <li><b>Augmenter la fidélité des utilisateurs</b> : En offrant des expériences personnalisées et en
                    répondant aux besoins des utilisateurs, les systèmes cherchent à fidéliser les clients, les incitant
                    à revenir pour davantage d'achats.</li>
                <li><b>Mieux comprendre ce que veut l'utilisateur</b> : Les systèmes de recommandation sont conçus pour
                    comprendre les préférences des utilisateurs au fil du temps, améliorant ainsi la précision des
                    recommandations et la compréhension des besoins individuels.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">91
                <a class="prev" href="#slide90"></a>
                <a class="next" href="#slide92"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide92">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <ul>
                <li><b>Trouver de bons objets</b> : Fournir des recommandations pour des articles ou des objets qui
                    correspondent aux préférences individuelles de l'utilisateur.</li>
                <li><b>Trouver tous les bons articles</b> : Identifier de manière exhaustive tous les articles
                    pertinents en fonction des préférences spécifiques de l'utilisateur</li>
            </ul>
            <h4>Contexte et Variations de Recommandations</h4>
            <ul>
                <li><b>Annotation dans le contexte</b> : Intégrer des informations contextuelles dans les
                    recommandations pour les rendre plus pertinentes et adaptées à la situation actuelle.</li>
                <li><b>Recommander une séquence</b> : Proposer une séquence d'articles ou de contenus, tels que des
                    livres ou des vidéos, sur un sujet donné pour une expérience d'apprentissage ou de divertissement
                    cohérente.</li>
                <li><b>Recommander une combinaison</b> : Offrir des suggestions de combinaisons d'articles, comme un
                    itinéraire de voyage complet, pour répondre à des besoins complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">92
                <a class="prev" href="#slide91"></a>
                <a class="next" href="#slide93"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide93">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <h4>Navigation et consultation</h4>
            <ul>
                <li><b>Navigation (consultation)</b> : Faciliter la navigation et la consultation d'articles en
                    recommandant des éléments pertinents à mesure que l'utilisateur explore la plateforme.</li>
                <li><b>Trouver un système de recommandation crédible</b> : Identifier des systèmes de recommandation
                    réputés et fiables pour garantir des suggestions de qualité.</li>
                <li><b>Améliorer le profil</b> : Continuellement ajuster et améliorer le profil de l'utilisateur en
                    fonction de ses préférences changeantes.</li>
            </ul>
            <h4>Interaction Sociale</h4>
            <ul>
                <li><b>S'exprimer</b> : Permettre aux utilisateurs de s'exprimer en fournissant des retours et en
                    influençant les recommandations.</li>
                <li><b>Aider les autres</b> : Offrir des mécanismes pour que les utilisateurs puissent recommander des
                    articles à d'autres utilisateurs.</li>
                <li><b>Influencer les autres</b> : Permettre aux utilisateurs d'influencer les préférences d'autres
                    utilisateurs en partageant leurs recommandations.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">93
                <a class="prev" href="#slide92"></a>
                <a class="next" href="#slide94"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide94">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
            <ul>
                <li><b>Filtrage collaboratif</b>
                    <ul>
                        <li>Principe : basé sur les évaluations de plusieurs utilisateurs</li>
                        <li>Fonctionnement : Identifie des utilisateurs similaires à celui pour lequel la recommandation
                            est générée et propose des articles appréciés par ces utilisateurs similaires.</li>
                    </ul>
                </li>
                <li><b>Filtrage basé sur le contenu</b>
                    <ul>
                        <li>Principe : basé sur les profils des utilisateurs</li>
                        <li>Fonctionnement : Recommande des articles similaires à ceux que l'utilisateur a aimés par le
                            passé, en se basant sur les caractéristiques ou le contenu des articles.</li>
                    </ul>
                </li>
                <li><b>Démographiques</b>
                    <ul>
                        <li>Principe : le profil démographique de l'utilisateur, par exemple le lieu et la langue</li>
                        <li>Fonctionnement : Propose des recommandations en fonction des caractéristiques démographiques
                            de l'utilisateur.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">94
                <a class="prev" href="#slide93"></a>
                <a class="next" href="#slide95"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide95">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
            <ul>
                <li><b>Basé sur la connaissance</b>
                    <ul>
                        <li>Principe :des recommandations basées sur la connaissance du domaine </li>
                        <li>Fonctionnement : Utilise une compréhension approfondie du contenu ou du domaine pour
                            recommander des articles pertinents.</li>
                    </ul>
                </li>
                <li><b>Basé sur la communauté</b>
                    <ul>
                        <li>Principe :des recommandations basées sur les préférences des amis des utilisateurs </li>
                        <li>Fonctionnement : Identifie les goûts et les préférences des amis de l'utilisateur pour
                            proposer des recommandations similaires.</li>
                    </ul>
                </li>
                <li><b>Systèmes hybrides de recommandation [Gomez-Uribe 2016]</b>
                    <ul>
                        <li>Principe : Intégration de plusieurs approches.</li>
                        <li>Fonctionnement : Combiner différentes méthodes de recommandation pour tirer parti de leurs
                            avantages respectifs et fournir des suggestions plus précises et diversifiées.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">95
                <a class="prev" href="#slide94"></a>
                <a class="next" href="#slide96"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide96">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage collaboratif</h3>
            <p>Le filtrage collaboratif basé sur les transactions implique l'analyse des transactions entre utilisateurs
                et articles pour générer des recommandations. </p>
            <ul>
                <li><b>Transactions</b> : Les transactions font référence aux interactions ou aux actions effectuées par
                    les utilisateurs lorsqu'ils interagissent avec des articles, tels que des achats, des évaluations,
                    des clics, etc. Les informations sur les transactions sont collectées et utilisées comme base pour
                    comprendre les préférences des utilisateurs et générer des recommandations.</li>
                <li><b>Algorithmes: Règles de l'association</b>
                    <ul>
                        <li><b>Fonctionnement</b> : Les règles de l'association identifient des modèles de co-occurrence
                            dans les transactions. Par exemple, si les utilisateurs qui ont acheté l'article A ont
                            également tendance à acheter l'article B, une règle de l'association peut être établie entre
                            A et B.</li>
                        <li><b>Application</b> : Ces règles peuvent être utilisées pour recommander des articles à un
                            utilisateur en se basant sur les préférences d'autres utilisateurs ayant des transactions
                            similaires.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">96
                <a class="prev" href="#slide95"></a>
                <a class="next" href="#slide97"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide97">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage collaboratif</h3>
            <h4>Avantages</h4>
            <ul>
                <li><b>Personnalisation</b> : Les recommandations sont personnalisées en fonction des comportements
                    d'achat et des transactions passées de l'utilisateur.</li>
                <li><b>Découverte de modèles</b> : Permet de découvrir des modèles de comportement d'achat et
                    d'identifier des associations entre différents articles.</li>
            </ul>
            <h4>Limitations</h4>
            <ul>
                <li><b>Sparsité des données</b> : Si un utilisateur a effectué un nombre limité de transactions, les
                    recommandations peuvent être moins précises en raison de la sparsité des données.</li>
                <li><b>Problème du démarrage à froid</b> : Il peut y avoir des difficultés lorsqu'un nouvel utilisateur
                    effectue peu ou pas de transactions, entraînant des défis dans la génération de recommandations.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">97
                <a class="prev" href="#slide96"></a>
                <a class="next" href="#slide98"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide98">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <p>Le filtrage collaboratif basé sur le contenu repose sur la description des objets et le profil des
                intérêts de l'utilisateur. </p>
            <ul>
                <li><b>Description d'objet </b>
                    <ul>
                        <li>Les caractéristiques ou le contenu des objets (articles, produits, etc.) sont utilisés pour
                            décrire chaque élément de manière détaillée.</li>
                    </ul>
                <li><b>Profil d'intérêts de l'utilisateur</b>
                    <ul>
                        <li><b>Modèle des préférences</b> : Un modèle représentant les préférences de l'utilisateur basé
                            sur la description des objets qu'il a aimés par le passé.</li>
                        <li><b>Historique d'interactions</b> : L'historique des interactions de l'utilisateur avec le
                            système de recommandation est également utilisé pour ajuster le modèle au fil du temps.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">98
                <a class="prev" href="#slide97"></a>
                <a class="next" href="#slide99"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide99">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <h4>Algorithmes</h4>
            <ul>
                <li><b>Arbres de décision</b> : Les arbres de décision sont utilisés pour modéliser les préférences de
                    l'utilisateur en fonction des caractéristiques des objets. L'arbre est construit pour prendre des
                    décisions sur les recommandations en se basant sur ces caractéristiques.</li>
                <li><b>Méthodes du plus proche voisin</b> : Les méthodes du plus proche voisin comparent le profil
                    d'intérêts de l'utilisateur avec ceux d'autres utilisateurs pour trouver les plus similaires. Les
                    objets appréciés par ces utilisateurs similaires sont recommandés.</li>
                <li><b>Classificateurs linéaires</b> : Les classificateurs linéaires modélisent la relation entre les
                    caractéristiques des objets et les préférences de l'utilisateur de manière linéaire. Cela peut
                    inclure des modèles tels que la régression logistique.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">99
                <a class="prev" href="#slide98"></a>
                <a class="next" href="#slide100"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide100">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <h4>Avantages</h4>
            <ul>
                <li><b>Personnalisation</b> : Les recommandations sont personnalisées en fonction du profil d'intérêts
                    de l'utilisateur, ce qui les rend adaptées à ses goûts spécifiques.</li>
                <li><b>Gestion du problème du démarrage à froid</b> : Peut mieux gérer le problème du démarrage à froid
                    pour de nouveaux utilisateurs en se basant sur la description d'objets.</li>
            </ul>
            <h4>Limitations</h4>
            <ul>
                <li><b>Dépendance à la description d'objet</b> : L'efficacité dépend de la qualité de la description des
                    objets, et certaines approches peuvent être sensibles à des descriptions incomplètes ou subjectives.
                </li>
                <li><b>Manque de diversité</b> : Peut avoir tendance à recommander des objets similaires à ceux que
                    l'utilisateur a aimés, limitant la diversité des suggestions.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">100
                <a class="prev" href="#slide99"></a>
                <a class="next" href="#slide101"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide101">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Systèmes hybrides [Gomez-Uribe 2016]</h3>
            <ul>
                <li><b>Filtrage basé sur le contenu et filtrage collaboratif</b> : Combiner ces approches peut compenser
                    les limitations individuelles, offrant des recommandations plus robustes qui tiennent compte à la
                    fois du contenu des objets et des comportements d'autres utilisateurs.</li>
                <li><b>Filtrage basé sur le contenu, filtrage collaboratif et démographique</b> : Cette approche hybride
                    prend en compte non seulement les préférences individuelles de l'utilisateur mais aussi des aspects
                    démographiques pour des recommandations plus contextualisées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">101
                <a class="prev" href="#slide100"></a>
                <a class="next" href="#slide102"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide102">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Systèmes hybrides</h3>
            <h4>Avantages</h4>
            <ul>
                <li><b>Meilleure précision</b> : En combinant différentes approches, les systèmes hybrides peuvent
                    fournir des recommandations plus précises et diversifiées.</li>
                <li><b>Gestion des limitations</b> : Compenser les limitations spécifiques de chaque approche
                    individuelle.</li>
                <li><b>Adaptabilité</b> : S'adapter à différents types d'utilisateurs et de contextes.</li>
                <li><b>Réduction du problème du démarrage à froid</b> : En intégrant des aspects démographiques, par
                    exemple, les systèmes hybrides peuvent mieux gérer le démarrage à froid pour de nouveaux
                    utilisateurs.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">102
                <a class="prev" href="#slide101"></a>
                <a class="next" href="#slide103"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide103">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Mesures de performance [Ziegler 2005, Ricci 2011]</h3>
            <ul>
                <li><b>Précision et efficacité [Beel 2013a]</b> : La capacité du système à fournir des recommandations
                    pertinentes et la rapidité avec laquelle le système génère des recommandations.</li>
                <li><b>Diversité</b> : La variété des recommandations fournies pour éviter la redondance et introduire
                    de nouveaux éléments.</li>
                <li><b>Persistance de la recommandation</b> : La cohérence des recommandations au fil du temps, offrant
                    une expérience utilisateur stable.</li>
                <li><b>Vie privée [Pu 2012]</b> : La protection et le respect de la vie privée des utilisateurs lors de
                    la collecte et de l'utilisation des données.</li>
                <li><b>Démographie des utilisateurs</b> : L'intégration de facteurs démographiques pour des
                    recommandations plus contextuelles.</li>
                <li><b>Robustesse (lutte contre la fraude) [Konstan 2012]</b> : La capacité à résister aux tentatives de
                    manipulation ou de fraude dans le système.</li>
                <li><b>Sérendipité</b> : La capacité à surprendre l'utilisateur en proposant des recommandations
                    inattendues mais appréciées.</li>
                <li><b>Confiance</b> : La fiabilité perçue du système, renforçant la confiance de l'utilisateur dans les
                    recommandations fournies.</li>
                <li><b>Étiquetage (recommandations organiques ou sponsorisées) [Beel 2013b]</b> : La transparence dans
                    la distinction entre les recommandations générées organiquement et celles sponsorisées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">103
                <a class="prev" href="#slide102"></a>
                <a class="next" href="#slide104"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide104">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
            <p>Les recommandations à haut risque se réfèrent à des domaines où les conséquences d'une recommandation
                incorrecte ou inappropriée peuvent être significatives.</p>
            <h4>Exemple</h4>
            <p><b>Assurance</b> : Les recommandations dans le domaine de l'assurance peuvent avoir des implications
                financières importantes. Par exemple, une recommandation inappropriée en matière d'assurance pourrait
                entraîner des conséquences financières négatives pour l'utilisateur.</p>
            <h4>Intégration des Capacités d'Explication</h4>
            <p>Les systèmes de recommandation à haut risque devraient intégrer des capacités d'explication, permettant
                de fournir des justifications claires pour chaque recommandation.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">104
                <a class="prev" href="#slide103"></a>
                <a class="next" href="#slide105"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide105">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
            <h4>Avantages des explications</h4>
            <li><b>Justification</b> : Les explications fournissent une justification transparente du raisonnement
                derrière chaque recommandation.</li>
            <li><b>Participation des utilisateurs</b> : Les explications encouragent la participation des utilisateurs
                en les aidant à comprendre pourquoi une recommandation particulière a été faite.</li>
            <li><b>Éducation</b> : Les explications contribuent à l'éducation des utilisateurs en les informant sur les
                critères pris en compte par le système pour générer des recommandations.</li>
            <li><b>Acceptation</b> : Les explications améliorent l'acceptation des recommandations en fournissant une
                visibilité sur le processus de prise de décision du système.</li>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">105
                <a class="prev" href="#slide104"></a>
                <a class="next" href="#slide106"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide106">
        <div class="header">
            <h1>4.9. Représentation des connaissances et raisonnement</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
            <p>La Représentation des connaissances et raisonnement (KRR) constituent un domaine clé de l'intelligence
                artificielle. La KRR est largement utilisée dans des domaines tels que la planification, la
                représentation du langage naturel, la gestion des connaissances, les systèmes experts, etc.</p>
            <h4>Représentation des Connaissances</h4>
            <p>La représentation des connaissances implique la création d'une représentation lisible par machine de la
                connaissance d'un monde ou d'un domaine particulier. Cela vise à permettre aux systèmes informatiques de
                comprendre, interpréter et raisonner sur l'information.</p>
            <h5>Exemples</h4>
                <ul>
                    <li><b>Réseaux Sémantiques</b> : Utilisent des relations sémantiques pour connecter des entités et
                        représenter la connaissance.</li>
                    <li><b>Ontologies</b> : Définissent des concepts, des propriétés et des relations dans un domaine
                        spécifique.</li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">106
                <a class="prev" href="#slide105"></a>
                <a class="next" href="#slide107"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide107">
        <div class="header">
            <h1>4.9. Représentation des connaissances et raisonnement</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
            <h4>Raisonnement</h4>
            <p>Le raisonnement consiste à déduire de nouvelles informations à partir des connaissances existantes. C'est
                le processus par lequel les systèmes tirent des conclusions logiques.</p>
            <h4>Compromis entre expressivité et praticité</h4>
            <ul>
                <li><b>Expressivité</b> : La capacité à représenter une variété de connaissances de manière détaillée.
                </li>
                <li><b>Praticité</b> : La facilité d'utilisation et de manipulation de la représentation des
                    connaissances.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">107
                <a class="prev" href="#slide106"></a>
                <a class="next" href="#slide108"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide108">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <figure>
                <img src="../../2021/MachineLearning/Semantic_web_stack.svg" height="400vh" />
                <figcaption style="text-align:center">Semantic Web Stack
                    (https://commons.wikimedia.org/wiki/File:Semantic_web_stack.svg)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">108
                <a class="prev" href="#slide107"></a>
                <a class="next" href="#slide109"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide109">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles</h3>
            <ul>
                <li>Constitué de règles et de contraintes</li>
                <li>Vérifie qu'à un moment donné, le système est cohérent</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">109
                <a class="prev" href="#slide108"></a>
                <a class="next" href="#slide110"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide110">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Programmation logique</h3>
            <ul>
                <li>Logique propositionnelle</li>
                <li>Logique du premier ordre (FOL, logique des prédicats)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">110
                <a class="prev" href="#slide109"></a>
                <a class="next" href="#slide111"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide111">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog</h3>
            <ul>
                <li>Langage de programmation déclaratif</li>
                <li>Langage de programmation logique : basé sur la logique du premier ordre</li>
                <li>Développé en 1972 par Alain Colmerauer</li>
                <li>Exprimé en termes de relations : faits et règles</li>
                <li>Utilisé pour la démonstration de théorèmes, les systèmes experts et le traitement du langage naturel
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">111
                <a class="prev" href="#slide110"></a>
                <a class="next" href="#slide112"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide112">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: types de données</h3>
            <ul>
                <li>Atome</li>
                <li>Nombres</li>
                <li>Variables</li>
                <li>Terme composé (par exemple, chaînes, listes)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">112
                <a class="prev" href="#slide111"></a>
                <a class="next" href="#slide113"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide113">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: règles</h3>
            <p>Un programme Prolog contient des clauses de la forme suivante.</p>
            <code>
                          Tête : - Corps.
                        </code>
            <ul>
                <li>Le corps peut contenir un ou plusieurs prédicats utilisant la conjonction et la disjonction</li>
                <li>La tête ne contient aucune conjonction et disjonction</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">113
                <a class="prev" href="#slide112"></a>
                <a class="next" href="#slide114"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide114">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: faits</h3>
            <p>Une clause avec un corps vide est appelée fait.</p>
            <code>
                          cat(bob).</br>
                          cat(alice).</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">114
                <a class="prev" href="#slide113"></a>
                <a class="next" href="#slide115"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide115">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: Installation</h3>
            <p>Sur une machine Ubuntu</p>
            <code>
                          $ sudo apt install gprolog
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">115
                <a class="prev" href="#slide114"></a>
                <a class="next" href="#slide116"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide116">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog</h3>
            <code>
$ prolog</br>
GNU Prolog 1.4.5 (64 bits)</br>
Compiled Feb  5 2017, 10:30:08 with gcc</br>
By Daniel Diaz</br>
Copyright (C) 1999-2016 Daniel Diaz</br>
| ?- [user].</br>
compiling user for byte code...</br>
cat(tom).</br>
cat(alice).</br>
</br>
user compiled, 2 lines read - 241 bytes written, 12239 ms</br>
</br>
(4 ms) yes</br>
| ?- </br>
</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">116
                <a class="prev" href="#slide115"></a>
                <a class="next" href="#slide117"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide117">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
?- cat(X).</br>
</br>
X = tom ? </br>
</br>
yes</br>
| ?- cat(bob).</br>
</br>
no</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">117
                <a class="prev" href="#slide116"></a>
                <a class="next" href="#slide118"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide118">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
| ?- [user].                             </br>
compiling user for byte code...</br>
cat(tom).                           </br>
cat(alice).                         </br>
allcats(L) :- findall(X, cat(X), L).</br>
</br>
user compiled, 3 lines read - 490 bytes written, 10638 ms</br>
</br>
yes</br>
| ?- allcats(L).                         </br>
</br>
L = [tom,alice]</br>
</br>
yes</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">118
                <a class="prev" href="#slide117"></a>
                <a class="next" href="#slide119"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide119">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
| ?- [user].              </br>
compiling user for byte code...</br>
friend(bob, alice).  </br>
friend(alice, kevin).</br>
friend(bob, thomas).                </br>
friend(bob, peter).  </br>
</br>
user compiled, 4 lines read - 486 bytes written, 77256 ms</br>
</br>
(10 ms) yes</br>
| ?- friend(bob, X).      </br>
</br>
X = alice ? a</br>
</br>
X = thomas</br>
</br>
X = peter</br>
</br>
(1 ms) yes</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">119
                <a class="prev" href="#slide118"></a>
                <a class="next" href="#slide120"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide120">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ cat friend.pl
friend(bob, alice).
friend(alice, kevin).
friend(bob, thomas).
friend(bob, peter).
human(X):-friend(X,_).
human(Y):-friend(_,Y).
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">120
                <a class="prev" href="#slide119"></a>
                <a class="next" href="#slide121"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide121">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
compiling /home/user/friend.pl for byte code...
/home/user/friend.pl compiled, 4 lines read - 515 bytes written, 22 ms
| ?- friend(bob,alice).

true ?

yes
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">121
                <a class="prev" href="#slide120"></a>
                <a class="next" href="#slide122"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide122">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
| ?- human(X).
X = bob ? a
X = alice
X = bob
X = bob
X = alice
X = kevin
X = thomas
X = peter

yes
| ?-
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">122
                <a class="prev" href="#slide121"></a>
                <a class="next" href="#slide123"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide123">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Beel 2013a] Beel, Joeran, et al. “A Comparative Analysis of Offline and Online Evaluations and
                    Discussion of Research Paper Recommender System Evaluation.” Proceedings of the International
                    Workshop on Reproducibility and Replication in
                    Recommender Systems Evaluation, Association for Computing Machinery, 2013</li>
                <li>[Beel 2013b] Beel, Joeran, et al. “Sponsored vs. Organic (Research Paper) Recommendations and the
                    Impact of Labeling.” Research and Advanced Technology for Digital Libraries, edited by Trond Aalberg
                    et al., Springer, 2013, pp. 391–95.</li>
                <li>[Chrupała 2006] Chrupała, Grzegorz. Simple Data-Driven Context-Sensitive Lemmatization. 2006.
                    doras.dcu.ie, http://www.unizar.es/departamentos/filologia_inglesa/sepln2006/.</li>
                <li>[Frakes 2003] Frakes, William B., and Christopher J. Fox. “Strength and Similarity of Affix Removal
                    Stemming Algorithms.” ACM SIGIR Forum, vol. 37, no. 1, Apr. 2003, pp. 26–30. Spring 2003</li>
                <li>[Gomez-Uribe 2016] Gomez-Uribe, Carlos A., and Neil Hunt. “The Netflix Recommender System:
                    Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems,
                    vol. 6, no. 4, Dec. 2016, p. 13:1–13:19. January
                    2016
                </li>
                Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers -
                Volume 2, Association for Computational Linguistics, 2012, pp. 368–372.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">123
                <a class="prev" href="#slide122"></a>
                <a class="next" href="#slide124"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide124">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Gesmundo 2012] Gesmundo, Andrea, and Tanja Samardžić. “Lemmatisation as a Tagging Task.”
                <li>[Herlocker 2000] Herlocker, Jonathan L., et al. “Explaining Collaborative Filtering
                    Recommendations.” Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work,
                    Association for Computing Machinery, 2000, pp. 241–250.
                    ACM
                </li>
                <li>[Konstan 2012] Konstan, Joseph A., and John Riedl. “Recommender Systems: From Algorithms to User
                    Experience.” User Modeling and User-Adapted Interaction, vol. 22, no. 1–2, Apr. 2012, pp. 101–123.
                </li>
                <li>[Màrquez 2000] Màrquez, Lluís, et al. “A Machine Learning Approach to POS Tagging.” Machine
                    Learning, vol. 39, no. 1, Apr. 2000, pp. 59–91.</li>
                <li>[Mikolov 2013] Mikolov, Tomas, et al. “Efficient Estimation of Word Representations in Vector
                    Space.” ArXiv:1301.3781 [Cs], Sept. 2013.</li>
                <li>[Miller 1995] Miller, George A. “WordNet: A Lexical Database for English.” Communications of the
                    ACM, vol. 38, no. 11, Nov. 1995, pp. 39–41. Nov. 1995</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">124
                <a class="prev" href="#slide123"></a>
                <a class="next" href="#slide125"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide125">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Pazzani 2007] Pazzani, Michael J., and Daniel Billsus. “Content-Based Recommendation Systems.” The
                    Adaptive Web: Methods and Strategies of Web Personalization, edited by Peter Brusilovsky et al.,
                    Springer, 2007, pp. 325–41. </li>
                <li>[Porter 1980] Porter, M. F. “An Algorithm for Suffix Stripping.” Program, vol. 14, no. 3, Jan. 1980,
                    pp. 130–37. Emerald Insight</li>
                <li>[Pu 2012] Pu, Pearl, et al. “Evaluating Recommender Systems from the User’s Perspective: Survey of
                    the State of the Art.” User Modeling and User-Adapted Interaction, vol. 22, no. 4, Oct. 2012, pp.
                    317–55.
                </li>
                <li>[Ricci 2011] Ricci, Francesco, et al. “Introduction to Recommender Systems Handbook.” Recommender
                    Systems Handbook, edited by Francesco Ricci et al., Springer US, 2011, pp. 1–35. </li>
                <li>[Ziegler 2005] Ziegler, Cai-Nicolas, et al. “Improving Recommendation Lists through Topic
                    Diversification.” Proceedings of the 14th International Conference on World Wide Web, Association
                    for Computing Machinery, 2005, pp. 22–32.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">125
                <a class="prev" href="#slide124"></a>
                <a class="next" href="#slide126"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide126">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web</h3>
            <ul>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation">https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation</a>
                </li>
                <li><a %
                        href="https://fr.wikipedia.org/wiki/Racinisation">https://fr.wikipedia.org/wiki/Racinisation</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Intelligence_artificielle">https://fr.wikipedia.org/wiki/Intelligence_artificielle</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Programmation_logique">https://fr.wikipedia.org/wiki/Programmation_logique</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">https://en.wikipedia.org/wiki/Morphology_(linguistics)</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Word_embedding">https://en.wikipedia.org/wiki/Word_embedding</a>
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
                <li><a href="https://fr.wikipedia.org/wiki/Prolog">https://fr.wikipedia.org/wiki/Prolog</a></li>
                <li><a href="https://www.nltk.org/howto/stem.html">https://www.nltk.org/howto/stem.html</a></li>
                <li><a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></li>
                <li><a href="https://spacy.io/usage/spacy-101">https://spacy.io/usage/spacy-101</a></li>
                <li><a href="https://spacy.io/usage/visualizers">https://spacy.io/usage/visualizers</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">126
                <a class="prev" href="#slide125"></a>
                <a class="next" href="#slide127"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide127">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">127
                <a class="prev" href="#slide126"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 120) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>
