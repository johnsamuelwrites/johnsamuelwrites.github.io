<html>

<head>
    <meta charset="utf-8" />
    <title>Intelligence artificielle (2023-2024): Cours: John Samuel</title>
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            flex-direction: column;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 3vmin;
            flex-wrap: wrap;
        }

        .codeexample {
            background-color: #eeeeee;
        }

        /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
        pre {
            line-height: 125%;
        }

        td.linenos .normal {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        td.linenos .special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos.special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        body .hll {
            background-color: #ffffcc
        }

        body {
            background: #f8f8f8;
        }

        body .c {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment */
        body .err {
            border: 1px solid #FF0000
        }

        /* Error */
        body .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */
        body .o {
            color: #666666
        }

        /* Operator */
        body .ch {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Hashbang */
        body .cm {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Multiline */
        body .cp {
            color: #9C6500
        }

        /* Comment.Preproc */
        body .cpf {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.PreprocFile */
        body .c1 {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Single */
        body .cs {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Special */
        body .gd {
            color: #A00000
        }

        /* Generic.Deleted */
        body .ge {
            font-style: italic
        }

        /* Generic.Emph */
        body .gr {
            color: #E40000
        }

        /* Generic.Error */
        body .gh {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Heading */
        body .gi {
            color: #008400
        }

        /* Generic.Inserted */
        body .go {
            color: #717171
        }

        /* Generic.Output */
        body .gp {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Prompt */
        body .gs {
            font-weight: bold
        }

        /* Generic.Strong */
        body .gu {
            color: #800080;
            font-weight: bold
        }

        /* Generic.Subheading */
        body .gt {
            color: #0044DD
        }

        /* Generic.Traceback */
        body .kc {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Constant */
        body .kd {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Declaration */
        body .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */
        body .kp {
            color: #008000
        }

        /* Keyword.Pseudo */
        body .kr {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Reserved */
        body .kt {
            color: #B00040
        }

        /* Keyword.Type */
        body .m {
            color: #666666
        }

        /* Literal.Number */
        body .s {
            color: #BA2121
        }

        /* Literal.String */
        body .na {
            color: #687822
        }

        /* Name.Attribute */
        body .nb {
            color: #008000
        }

        /* Name.Builtin */
        body .nc {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Class */
        body .no {
            color: #880000
        }

        /* Name.Constant */
        body .nd {
            color: #AA22FF
        }

        /* Name.Decorator */
        body .ni {
            color: #717171;
            font-weight: bold
        }

        /* Name.Entity */
        body .ne {
            color: #CB3F38;
            font-weight: bold
        }

        /* Name.Exception */
        body .nf {
            color: #0000FF
        }

        /* Name.Function */
        body .nl {
            color: #767600
        }

        /* Name.Label */
        body .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */
        body .nt {
            color: #008000;
            font-weight: bold
        }

        /* Name.Tag */
        body .nv {
            color: #19177C
        }

        /* Name.Variable */
        body .ow {
            color: #AA22FF;
            font-weight: bold
        }

        /* Operator.Word */
        body .w {
            color: #bbbbbb
        }

        /* Text.Whitespace */
        body .mb {
            color: #666666
        }

        /* Literal.Number.Bin */
        body .mf {
            color: #666666
        }

        /* Literal.Number.Float */
        body .mh {
            color: #666666
        }

        /* Literal.Number.Hex */
        body .mi {
            color: #666666
        }

        /* Literal.Number.Integer */
        body .mo {
            color: #666666
        }

        /* Literal.Number.Oct */
        body .sa {
            color: #BA2121
        }

        /* Literal.String.Affix */
        body .sb {
            color: #BA2121
        }

        /* Literal.String.Backtick */
        body .sc {
            color: #BA2121
        }

        /* Literal.String.Char */
        body .dl {
            color: #BA2121
        }

        /* Literal.String.Delimiter */
        body .sd {
            color: #BA2121;
            font-style: italic
        }

        /* Literal.String.Doc */
        body .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */
        body .se {
            color: #AA5D1F;
            font-weight: bold
        }

        /* Literal.String.Escape */
        body .sh {
            color: #BA2121
        }

        /* Literal.String.Heredoc */
        body .si {
            color: #A45A77;
            font-weight: bold
        }

        /* Literal.String.Interpol */
        body .sx {
            color: #008000
        }

        /* Literal.String.Other */
        body .sr {
            color: #A45A77
        }

        /* Literal.String.Regex */
        body .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */
        body .ss {
            color: #19177C
        }

        /* Literal.String.Symbol */
        body .bp {
            color: #008000
        }

        /* Name.Builtin.Pseudo */
        body .fm {
            color: #0000FF
        }

        /* Name.Function.Magic */
        body .vc {
            color: #19177C
        }

        /* Name.Variable.Class */
        body .vg {
            color: #19177C
        }

        /* Name.Variable.Global */
        body .vi {
            color: #19177C
        }

        /* Name.Variable.Instance */
        body .vm {
            color: #19177C
        }

        /* Name.Variable.Magic */
        body .il {
            color: #666666
        }

        /* Literal.Number.Integer.Long */


        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 3vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 40%;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 100%;
            max-height: 100%;
        }

        .content figure img {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 4vh;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }


        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                height: 76%;
                width: 90vw;
                display: flex;
                color: #000000;
                text-align: left;
                padding: 5vw;
                font-size: 3vmin;
                flex-wrap: wrap;
            }

            .content figure img {
                max-width: 80%;
                max-height: 80%;
            }

            .content figcaption {
                max-width: 80%;
                max-height: 80%;
            }
        }
    </style>
    <script src="../../2021/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:2.5vw">Intelligence artificielle</h1>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Year</b>: 2023-2024<br />
                <b>Email</b>: john(dot)samuel(at)cpe(dot)fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles (NLP) </h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Intelligence artificielle</h3>
            <figure>
                <img src="../../../../../images/art/courses/deeplearningposition.svg" height="450px" />
                <figcaption>Intelligence artificielle</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <p>Le <b>traitement automatique des langues (TAL)</b> est un domaine interdisciplinaire de la linguistique
                informatique qui se concentre sur l'analyse et la compréhension du <b>langage naturel</b> (celui utilisé
                par les humains). Cette section aborde plusieurs aspects clés du TAL, notamment :</p>
            <ul>
                <li><b>Analyser et comprendre le langage naturel (humain)</b>: Le TAL se consacre à la compréhension du
                    langage naturel dans divers contextes, qu'il s'agisse de textes écrits ou de discours verbal.</li>
                <li>Interaction homme-machine</li>
                <li><b>Syntaxe d'une langue</b>
                    <ul>
                        <li>Parsing : Le parsing consiste à analyser la structure grammaticale des phrases.</li>
                        <li>L'étiquetage en parties du discours (PoS) : L'étiquetage PoS consiste à assigner des
                            catégories grammaticales (comme verbe, nom, adjectif, etc.) aux mots d'une phrase.</li>
                    </ul>
                </li>
                <li><b>Sémantique d'une langue</b>
                    <ul>
                        <li>Traduction automatique</li>
                        <li>Reconnaissance d'entités nommées (NER): La NER consiste à identifier des entités spécifiques
                            (comme des noms de personnes, de lieux ou d'organisations) dans un texte.</li>
                        <li>Analyse des sentiments</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Analyse de systèmes TAL</h3>
            <ul>
                <li><b>Racinisation</b> : La racinisation est le processus de réduction des mots à leur forme de base ou
                    de racine. </li>
                <li><b>Étiquetage morpho-syntaxique</b> : Cette étape consiste à attribuer des balises ou des étiquettes
                    aux mots dans un texte en fonction de leur rôle grammatical et de leur structure. </li>
                <li><b>Lemmatisation</b> : Contrairement à la racinisation, la lemmatisation consiste à ramener les mots
                    à leur forme canonique ou lemmes. </li>
                <li><b>Morphologie</b> : La morphologie concerne l'étude de la structure des mots, notamment comment ils
                    sont formés à partir de morphèmes (unités de sens). </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation [Frakes 2003]</h3>
            <ul>
                <li>grouper les mots en fonction de leur similarité sémantique.</li>
                <li>Algorithmes de suppression des affixes: supprimer les suffixes ou préfixes des mots produisant une
                    racine </li>
                <li>Exemples
                    <ul>
                        <li>engineer: engineer, engineered, engineering</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: mesures d'évaluation [Frakes 2003]</h3>
            <ul>
                <li>La mesure dans laquelle un algorithm modifie des mots qu'elle réduit à ses racines est appelée la
                    <b>force</b> de l'algorithme
                </li>
                <li>Une métrique de <b>similarité</b> des algorithmes met en correspondance les n-tuples d'algorithmes
                    (n au moins 2), avec un nombre indiquant la similarité des algorithmes. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: distance de Hamming [Frakes 2003]</h3>
            <ol>
                <li>La distance de Hamming entre deux chaînes de longueur égale est définie comme le nombre de
                    caractères des deux chaînes qui sont différents à la même position.</li>
                <li>Pour les chaînes de longueur inégale, ajouter la différence de longueur à la distance de Hamming
                    pour obtenir une fonction de distance de Hamming modifiée \(d\)</li>
                <li>Exemples
                    <ul>
                        <li>tri: try, tried, trying</li>
                        <li>\(d\)(tri, try)= 1</li>
                        <li>\(d\)(tri, tried)= 2</li>
                        <li>\(d\)(tri, trying)= 4</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: force [Frakes 2003]</h3>
            <ol>
                <li>Le nombre moyen de mots par classe</li>
                <li>Facteur de compression de l'indice. Soit n est le nombre de mots dans le corpus et s est le nombre
                    de racines. \[\frac{n - s}{n}\]
                </li>
                <li>Le nombre de mots et de racines qui diffèrent</li>
                <li>Le nombre moyen de caractères supprimés lors de la formation des racines</li>
                <li>La médiane et la moyenne de la distance de Hamming modifiée entre les mots et leur racine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: similarité [Frakes 2003]</h3>
            <ol>
                <li>Soit \(A1\) et \(A2\) sont deux algorithmes</li>
                <li>Soit \(W\) une liste de mots et \(n\) le nombre de mots dans \(W\) \[ M(A1,A2,W) = \frac{n}{\Sigma
                    d(x_i, y_i)}\]
                </li>
                <li>pour tous les mots \(w_i\) en W, \(x_i\) est le résultat de l'application de \(A1\) à \(w_i\) et
                    \(y_i\) est le résultat de l'application de \(A2\) à \(w_i\)</li>
                <li>des algorithmes plus similaires auront des valeurs plus élevées de M</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: ntlk</h3>
            <p>L'objectif est de réduire les mots à leur forme de base ou racine, en éliminant les suffixes, ce qui
                permet de regrouper différentes formes d'un mot sous une forme commune. </p>
            <ul>
                <li><b>Porter [Porter 1980]</b> : Le Porter Stemming Algorithm, créé par Martin Porter en 1980, est basé
                    sur un ensemble de règles heuristiques. Il suit une approche itérative en appliquant une série de
                    transformations séquentielles aux mots.</li>
                <li><b>Snowball</b> Le Snowball (anciennement appelé Porter2) est une amélioration du Porter Stemmer. Il
                    suit également une approche basée sur des règles, mais il est plus systématique dans son traitement
                    des différents cas de racination. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Porter</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Snowball</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">snowball</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">snowball</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <ul>
                <li>Part of Speech (PoS) Tagging</li>
                <li>attribution à chaque mot d'un texte de la balise morphosyntaxique appropriée dans son contexte
                    d'apparition
                </li>
                <li>Exemples des balises
                    <ul>
                        <li>noms</li>
                        <li>verbes</li>
                        <li>adjectifs</li>
                        <li>adverbes</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
            <ol>
                <li>approche manuelle
                    <ul>
                        <li>constuction des règles)</li>
                    </ul>
                </li>
                <li>approche statistique
                    <ul>
                        <li>collection de n-grammes (bi-grammes, tri-grammes, ...)</li>
                        <li>ensemble de fréquences de cooccurrence</li>
                        <li>l'estimation de la probabilité d'une séquence de longueur n est calculée en tenant compte de
                            son occurrence dans le corpus d'entraînement</li>
                    </ul>
                </li>
                <li>apprentissage machine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{}</span><span class="s2">-grams&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
1-grams<br/>
('He',) ('went',) ('to',) ('school',) ('yesterday',) ('and',) ('attended',) ('the',) ('classes',) <br/>
2-grams<br/>
('He', 'went') ('went', 'to') ('to', 'school') ('school', 'yesterday') ('yesterday', 'and') ('and', 'attended') ('attended', 'the') ('the', 'classes') <br/>
3-grams<br/>
('He', 'went', 'to') ('went', 'to', 'school') ('to', 'school', 'yesterday') ('school', 'yesterday', 'and') ('yesterday', 'and', 'attended') ('and', 'attended', 'the') ('attended', 'the', 'classes') <br/>
4-grams<br/>
('He', 'went', 'to', 'school') ('went', 'to', 'school', 'yesterday') ('to', 'school', 'yesterday', 'and') ('school', 'yesterday', 'and', 'attended') ('yesterday', 'and', 'attended', 'the') ('and', 'attended', 'the', 'classes')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>

            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He goes to school daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>PRP</td>
                    <td>pronoun, personal</td>
                </tr>
                <tr>
                    <td>VBZ</td>
                    <td>verb, present tense, 3rd person singular</td>
                </tr>
                <tr>
                    <td>TO</td>
                    <td>"to" as preposition</td>
                </tr>
                <tr>
                    <td>NN</td>
                    <td>"noun, common, singular or mass</td>
                </tr>
                <tr>
                    <td>RB</td>
                    <td>adverb</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				 $ pip3 install spacy<br/>
				 $ python3 -m spacy download en_core_web_sm<br/>
                         </code>
            </p>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				  import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("He goes to school daily")<br/>
                                  <br/>
                                  for token in doc:<br/>
                                  &nbsp;&nbsp;print(token.text, token.pos_, token.dep_)<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
				  He PRON nsubj<br/>
                                  goes VERB ROOT<br/>
                                  to ADP prep<br/>
                                  school NOUN pobj<br/>
                                  daily ADV advmod<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("He goes to school daily")<br/>
                                  <br/>
                                  for token in doc:<br/>
				  &nbsp;&nbsp;print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,<br/>
				  &nbsp;&nbsp;&nbsp;&nbsp;token.shape_, token.is_alpha, token.is_stop)<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
				  He -PRON- PRON PRP nsubj Xx True True<br/>
                                  goes go VERB VBZ ROOT xxxx True False<br/>
                                  to to ADP IN prep xx True True<br/>
                                  school school NOUN NN pobj xxxx True False<br/>
                                  daily daily ADV RB advmod xxxx True False<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Gesmundo 2012]</h3>
            <ul>
                <li>regrouper les formes de mots qui appartiennent au même paradigme morphologique flexionnel et
                    attribuer à chaque paradigme son lemme correspondant.</li>
                <li>Exemples
                    <ul>
                        <li>go: go, goes, going, went, gone</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Chrupała 2006, Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation comme une tâche d'étiquetage</li>
                <li>Attribuer un label pour chaque transformation d'un label en lemme</li>
                <li>4 étapes [Gesmundo 2012]
                    <ol>
                        <li>supprimer un suffixe de longueur \(N_s\)</li>
                        <li>ajouter un nouveau suffixe de lemme \(L_s\)</li>
                        <li>supprimer un préfixe de longueur \(N_p\)</li>
                        <li>ajouter un nouveau préfixe lemme, \(L_p\)</li>
                    </ol>
                </li>
                <li>Transformation \(\tau = \langle N_s, L_s, N_p, L_p \rangle\)</li>
                <li>(going, go) = \(\langle 3, \emptyset, 0, \emptyset \rangle \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer</h3>
            <ul>
                <li>WordNet [Miller 1995]</li>
            </ul>
            <p class="codeexample">
                <code>
			  import nltk<br/>
                          nltk.download('punkt')<br/>
                          nltk.download('wordnet')<br/>
                          nltk.download('averaged_perceptron_tagger')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (sans les balises PoS)</h3>
            <p class="codeexample">
                <code>
			  from nltk.stem import WordNetLemmatizer<br/>
<br/>
                          sentence = "He went to school yesterday and attended the classes"<br/>
                          lemmatizer = WordNetLemmatizer()<br/>
                          <br/>
                          for word in sentence.split():<br/>
                          &nbsp;&nbsp;print(lemmatizer.lemmatize(word), end=' ')<br/>
                         </code>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     He went to school yesterday and attended the class
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
                <code>
                          from nltk.stem import WordNetLemmatizer<br/>
                          from nltk import word_tokenize, pos_tag<br/>
                          from nltk.corpus import wordnet as wn<br/>
                          <br/>
                          # Check the complete list of tags http://www.nltk.org/book/ch05.html<br/>
                          def wntag(tag):<br/>
                          &nbsp;&nbsp;if tag.startswith("J"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.ADJ<br/>
                          &nbsp;&nbsp;elif tag.startswith("R"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.ADV<br/>
                          &nbsp;&nbsp;elif tag.startswith("N"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.NOUN<br/>
                          &nbsp;&nbsp;elif tag.startswith("V"):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;return wn.VERB<br/>
                          &nbsp;&nbsp;return None<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
                <code>
                          lemmatizer = WordNetLemmatizer()<br/>
                          <br/>
                          sentence = "I went to school today and he goes daily"<br/>
                          tokens = word_tokenize(sentence)<br/>
                          for token, tag in pos_tag(tokens):<br/>
                          &nbsp;&nbsp;if wntag(tag):<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;print(lemmatizer.lemmatize(token, wntag(tag)), end=' ')<br/>
                          &nbsp;&nbsp;else:<br/>
                          &nbsp;&nbsp;&nbsp;&nbsp;print(lemmatizer.lemmatize(token), end=' ')<br/>
                         </code>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     I go to school today and he go daily
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("I went to school today and he goes daily")<br/>
                                  <br/>
                                  for token in doc:<br/>
				  &nbsp;&nbsp;print(token.lemma_, end=' ')<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
				  -PRON- go to school today and -PRON- go daily<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>4.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Morphologie</h3>
            <ul>
                <li>l'étude des mots, de leurs les paradigmes et de l’organisation des catégories grammaticales</li>
                <li>examine les parties du discours, l'intonation et l'accent, ainsi que la façon dont le contexte peut
                    modifier la prononciation et le sens d'un mot</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>4.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
				 from spacy import displacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("He goes to school daily")<br/>
                                  <br/>
                                  displacy.serve(doc, style="dep")<br/>
                         </code>
            </p>
            <figure>
                <img src="../../2021/MachineLearning/spacy-dep-output.svg" height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Les embeddings de mots sont une technique d'apprentissage de caractéristiques où des mots ou des phrases
                du vocabulaire sont associés à des vecteurs de nombres réels.</p>
            <ul>
                <li>L'idée principale est de représenter chaque mot par un vecteur dense dans un espace continu, de
                    telle sorte que des mots similaires aient des vecteurs similaires, capturant ainsi les relations
                    sémantiques entre les mots.</li>
                <li>Quantifier et catégoriser les similarités sémantiques entre les éléments linguistiques en fonction
                    de leurs propriétés de distribution dans de grands échantillons de données linguistiques.</li>
                <li>En d'autres termes, les mots qui ont des contextes similaires ou qui apparaissent dans des contextes
                    similaires auront des embeddings de mots similaires.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Avantages de Word Embeddings</p>
            <ul>
                <li><b>Représentation Dense</b> : Les embeddings fournissent une représentation dense, contrairement à
                    une représentation creuse où chaque mot serait représenté par un vecteur binaire indiquant sa
                    présence ou son absence.</li>
                <li><b>Capture des Relations Sémantiques</b> : Les embeddings captent les relations sémantiques et les
                    similitudes entre les mots, ce qui les rend utiles dans de nombreuses tâches de traitement du
                    langage naturel.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Applications de Word Embeddings</p>
            <ul>
                <li><b>Similarité Sémantique</b> : Mesurer la similarité sémantique entre les mots.</li>
                <li><b>Traduction Automatique</b> : Améliorer les performances des systèmes de traduction automatique.
                </li>
                <li><b>Analyse des Sentiments</b> : Mieux comprendre le contexte et les relations sémantiques dans
                    l'analyse des sentiments, entre autres applications.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy est une bibliothèque open-source pour le traitement du langage naturel (NLP) en Python. Elle offre
                des outils performants et efficaces pour effectuer diverses tâches de traitement du langage naturel, de
                l'analyse syntaxique à la reconnaissance d'entités nommées. spaCy est conçu pour être rapide, précis et
                facile à utiliser.</p>
            <ul>
                <li><b>Collecte de Données</b> : Les modèles spaCy sont souvent entraînés sur de vastes ensembles de
                    données annotées, qui peuvent inclure des corpus textuels avec des annotations pour l'analyse
                    syntaxique, la reconnaissance d'entités nommées, etc.</li>
                <li><b>Annotation des Données</b> : Les données collectées sont annotées manuellement avec des
                    informations linguistiques spécifiques telles que les parties du discours, les entités nommées, les
                    relations syntaxiques, etc.</li>
                <li><b>Entraînement Initial</b> : Les modèles spaCy sont initialement entraînés sur ces ensembles de
                    données annotées pour apprendre les structures linguistiques. Ce processus peut inclure
                    l'utilisation d'algorithmes d'apprentissage automatique tels que les réseaux de neurones.</li>
                <li><b>Optimisation et Réglage</b> : Les modèles sont ensuite optimisés et réglés pour améliorer leurs
                    performances sur des tâches spécifiques. Cela peut impliquer des itérations sur le processus
                    d'entraînement en ajustant les hyperparamètres du modèle.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>Évaluation</b> : Les modèles sont évalués sur des ensembles de données de test distincts pour
                    mesurer leur précision, leur rappel et d'autres métriques spécifiques à la tâche.</li>
                <li><b>Construction des Modèles Linguistiques Pré-entraînés</b> : Une fois le modèle entraîné et évalué,
                    spaCy construit des modèles linguistiques pré-entraînés qui encapsulent les connaissances acquises
                    sur la structure linguistique.</li>
                <li><b>Téléchargement et Utilisation</b> : Les utilisateurs peuvent télécharger ces modèles
                    pré-entraînés via spaCy et les utiliser dans leurs applications pour effectuer diverses tâches de
                    traitement du langage naturel sans avoir à entraîner un modèle de zéro.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy propose différents modèles linguistiques pré-entraînés pour différentes langues et tâches. Le
                modèle en_core_web_lg est un modèle vectoriel large d'anglais.</p>
            <h4>Installation du Modèle spaCy (en_core_web_lg) :</h4>
            <p class="codeexample">
                <code>
				 $ python3 -m spacy download en_core_web_lg<br/>
                         </code>
            </p>
            <h4>Chargement du Modèle spaCy :</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Avantages de spaCy :</p>
            <ul>
                <li><b>Performance élevée</b> : spaCy est reconnu pour sa rapidité d'exécution, ce qui le rend adapté au
                    traitement de grands volumes de texte en temps réel.</li>
                <li><b>Modèles pré-entraînés</b> : spaCy propose des modèles linguistiques pré-entraînés pour plusieurs
                    langues, ce qui facilite l'analyse de texte sans nécessiter d'entraînement à partir de zéro.</li>
                <li><b>Extraction d'informations linguistiques riches</b> : spaCy fournit des informations linguistiques
                    détaillées telles que les parties du discours, les entités nommées, les relations syntaxiques, et
                    plus encore.</li>
                <li><b>API conviviale</b> : L'API spaCy est conçue pour être intuitive et conviviale. Elle facilite la
                    réalisation de tâches complexes avec des lignes de code concises.</li>
                <li><b>Intégration avec d'autres bibliothèques</b> : spaCy s'intègre bien avec d'autres bibliothèques
                    Python populaires, facilitant son utilisation dans des projets plus larges.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Limites de spaCy :</p>
            <ul>
                <li><b>Dépendance des modèles linguistiques</b> : L'utilisation de modèles pré-entraînés signifie que la
                    qualité des résultats dépend de la qualité du modèle. Dans des domaines de spécialité ou pour des
                    langues moins courantes, les modèles peuvent ne pas être aussi performants.</li>
                <li><b>Gestion des entités nommées</b> : Bien que spaCy excelle dans la reconnaissance d'entités
                    nommées, il peut parfois avoir du mal avec des tâches plus complexes impliquant des variations
                    contextuelles.</li>
                <li><b>Taille des modèles</b> : Les modèles pré-entraînés peuvent être relativement volumineux, ce qui
                    peut être un inconvénient dans des environnements avec des restrictions de mémoire ou pour des
                    applications mobiles.</li>
                <li><b>Personnalisation limitée</b> : Bien que spaCy offre des fonctionnalités de personnalisation,
                    elles peuvent être limitées par rapport à d'autres bibliothèques NLP plus flexibles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>

<span class="c1"># Définir les mots à comparer</span>
<span class="n">words_to_compare</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;apple&quot;</span><span class="p">]</span>

<span class="c1"># Calculer la similarité entre les paires de mots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
        <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="o">=</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">doc1</span><span class="p">,</span> <span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word1</span><span class="p">),</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
        <span class="n">similarity_score</span> <span class="o">=</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité (</span><span class="si">{}</span><span class="s2"> / </span><span class="si">{}</span><span class="s2">): </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">similarity_score</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <h4>Affichage</h4>
            <p class="codeexample">
                <code>
                <pre>
Similarité (dog / cat): ...
Similarité (dog / apple): ...
Similarité (cat / apple): ...
                </pre>

                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: vector</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>

<span class="c1"># Imprimer les vecteurs de chaque jeton sur une seule ligne</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteurs de &#39;</span><span class="si">{}</span><span class="s2">&#39; : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">,</span> <span class="n">vector_list</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>Word2Vec a marqué un tournant significatif dans la <b>représentation des mots</b> dans le domaine de
                l'apprentissage automatique.</p>
            <ul>
                <li>C'est une technique publiée en <b>2013</b> par une équipe de chercheurs dirigée par <b>Tomas
                        Mikolov</b> chez Google.</li>
                <li><b>Représentation vectorielle</b> : Word2Vec représente chaque mot distinct avec un vecteur dans un
                    espace continu. Ces vecteurs captent les relations sémantiques et syntaxiques entre les mots.</li>
                <li><b>Apprentissage basé sur un réseau neuronal</b> : Le modèle utilise un réseau neuronal pour
                    apprendre des associations de mots à partir d'un vaste corpus de texte. Cette approche permet de
                    capturer des nuances complexes dans la signification des mots.</li>
                <li><b>Entrée et sortie</b> : Word2Vec prend en entrée un large corpus de texte et produit un espace
                    vectoriel, généralement de plusieurs centaines de dimensions. Cette représentation vectorielle
                    permet de mesurer la similarité sémantique entre les mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>L'implémentation de Word2Vec se déroule en plusieurs étapes :</p>
            <ol>
                <li><b>Prétraitement des données</b> : Le texte est nettoyé et prétraité pour éliminer les éléments
                    indésirables tels que la ponctuation et les stopwords.</li>
                <li><b>Création d'un vocabulaire</b> : Les mots uniques du corpus sont utilisés pour construire un
                    vocabulaire. Chaque mot est ensuite associé à un index.</li>
                <li><b>Génération de paires mot-contexte</b> : Pour chaque mot du corpus, des paires mot-contexte sont
                    créées en utilisant une fenêtre contextuelle glissante. Ces paires servent d'exemples
                    d'entraînement.</li>
                <li><b>Construction du modèle Word2Vec</b> : Un modèle de réseau neuronal est créé, avec une couche
                    d'entrée représentant les mots, une couche cachée (skip-gram ou CBOW), et une couche de sortie pour
                    prédire le mot suivant dans le contexte.</li>
                <li><b>Entraînement du modèle</b> : Le modèle est entraîné sur les paires mot-contexte générées,
                    ajustant les poids du réseau pour minimiser la différence entre les prédictions et les vrais mots du
                    contexte.</li>
                <li><b>Obtention des embeddings</b> : Les vecteurs de mots appris pendant l'entraînement, appelés
                    embeddings, sont extraits. Chaque mot du vocabulaire est maintenant représenté par un vecteur dense
                    dans l'espace continu.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec</h3>
            <ul>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
                <li>une simple fonction mathématique (par exemple, la similarité cosinus entre les vecteurs) indique le
                    niveau de similarité sémantique entre les mots représentés par ces vecteurs \[\text{similarity} =
                    \cos(\theta) = {\mathbf{A} \cdot \mathbf{B}
                    \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i B_i} }{
                    \sqrt{\sum\limits_{i=1}^{n}{A_i^2}} \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} },\]
                </li>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <p>CBOW est un modèle spécifique de Word2Vec. Dans ce modèle, la prédiction du mot courant se fait en
                utilisant une fenêtre de mots contextuels voisins. L'ordre des mots de contexte n'influence pas la
                prédiction, ce qui en fait une approche robuste.</p>
            <ul>
                <li><b>Modèle prédictif</b> : CBOW prédit le mot cible en se basant sur le contexte qui l'entoure, mais
                    contrairement à d'autres modèles, l'ordre spécifique des mots dans ce contexte n'est pas pris en
                    compte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <ul>
                <li><b>Entrée</b> : La donnée d'entrée du modèle CBOW est une fenêtre de mots contextuels entourant le
                    mot cible. Cette fenêtre est définie par un paramètre appelé la taille de la fenêtre.</li>
                <li><b>Architecture</b> : CBOW utilise une architecture de réseau neuronal à une seule couche cachée. La
                    couche d'entrée représente les mots du contexte, et la couche de sortie représente le mot cible à
                    prédire.</li>
                <li><b>Entraînement</b> : Le modèle est entraîné en ajustant les poids du réseau pour minimiser la
                    différence entre les prédictions du modèle et le mot cible réel. Cela se fait à travers des
                    techniques d'optimisation comme la rétropropagation du gradient.</li>
                <li><b>Sortie</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les relations sémantiques entre les mots, permettant
                    ainsi de représenter chaque mot par un vecteur dans un espace continu.</li>
                <li><b>Avantages</b> : CBOW est souvent plus rapide à entraîner que d'autres modèles comme le Skip-gram
                    (une autre variante de Word2Vec) et peut être plus efficace dans des contextes où l'ordre séquentiel
                    des mots n'est pas critique.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle CBOW avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>

<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des deux mots les plus probables suivant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots suivant &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <p>Le modèle Skip-gram est une autre variante de Word2Vec qui se concentre sur la prédiction de la fenêtre
                voisine des mots de contexte à partir du mot courant. </p>
            <ul>
                <li><b>Objectif</b> : L'objectif principal du modèle Skip-gram est de prendre un mot source (le mot
                    courant) et de prédire les mots qui l'entourent dans une fenêtre de contexte donnée.</li>
                <li><b>Entrée</b> : Le mot source est utilisé comme donnée d'entrée du modèle, et la sortie souhaitée
                    est la distribution des probabilités des mots du contexte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <ul>
                <li><b>Architecture</b> : Skip-gram utilise une architecture de réseau neuronal à une seule couche
                    cachée. La couche d'entrée représente le mot source, et la couche de sortie représente les mots du
                    contexte.</li>
                <li><b>Entraînement</b> : Pendant l'entraînement, les poids du réseau sont ajustés pour minimiser la
                    différence entre les prédictions du modèle et la véritable distribution des mots du contexte. Cela
                    se fait généralement à l'aide de techniques d'optimisation comme la rétropropagation du gradient.
                </li>
                <li><b>Pondération du contexte</b> : Une caractéristique importante du modèle Skip-gram est que
                    l'architecture accorde plus de poids aux mots de contexte proches du mot source que ceux plus
                    éloignés. Cela permet de mieux capturer les relations sémantiques et syntaxiques locales.</li>
                <li><b>Embeddings</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les similitudes sémantiques entre les mots, permettant
                    de représenter chaque mot par un vecteur dans un espace continu.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle Skip-gram avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="c1"># sg: 1 pour skip-gram ; sinon CBOW.</span>
<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des mots les plus probables dans le contexte entourant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots dans le contexte de &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <p>Extraire les entités nommées et les assigner à des catégories spécifiques.</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/datarepresentation.svg"
                    height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("Paris is the capital of France. In 2015, its population was recorded as 2,206,488")<br/>
                                  <br/>
                                  for entity in doc.ents:<br/>
                                  &nbsp;&nbsp;print(entity.text, entity.start_char, entity.end_char, entity.label_)<br/>
                         </code>
            </p>
            <p class="codeexample">
                <code>
                                  Paris 0 5 GPE<br/>
                                  France 24 30 GPE<br/>
                                  2015 35 39 DATE<br/>
                                  2,206,488 72 81 CARDINAL<br/>

                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
                <code>
				 import spacy<br/>
<br/>
                                  nlp = spacy.load("en_core_web_sm")<br/>
                                  doc = nlp("Paris is the capital of France. In 2015, its population was recorded as 2,206,488")<br/>
                                  <br/>
                                  displacy.serve(doc, style="ent")<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>GPE</td>
                    <td>Pays, villes, états.</td>
                </tr>
                <tr>
                    <td>DATE</td>
                    <td>Dates ou périodes absolues ou relatives</td>
                </tr>
                <tr>
                    <td>CARDINAL</td>
                    <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Installation</p>
            <p class="codeexample">
                <code>
			import nltk</br>
                        nltk.download('vader_lexicon')</br>
                         </code>
            </p>
            <p>Usage</p>
            <p class="codeexample">
                <code>
			from nltk.sentiment.vader import SentimentIntensityAnalyzer</br>
                        sia = SentimentIntensityAnalyzer()</br>
                        sentiment = sia.polarity_scores("this movie is good")</br>
                        print(sentiment)</br>
                        sentiment = sia.polarity_scores("this movie is not very good")</br>
                        print(sentiment)</br>
                        sentiment = sia.polarity_scores("this movie is bad")</br>
                        print(sentiment)</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}</br>
                        {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'compound': -0.3865}</br>
                        {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique</h3>
            <ul>
                <li>l'utilisation de logiciels pour traduire un texte ou un discours d'une langue à une autre.</li>
                <li>approches:
                    <ul>
                        <li>approche manuelle (règles)</li>
                        <li>approche statistique</li>
                        <li>approche hybride: règles et approches statistique</li>
                        <li>apprentissage machine</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Transformer</h3>
            <ul>
                <li> BERT (Bidirectional Encoder Representations from Transformers)</li>
                <li> GPT (Generative Pre-trained Transformer)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <ul>
                <li>Objectif principal : réduire la surcharge d'informations en fournissant des informations filtrées et
                    pertinentes
                </li>
                <li>Prévoir la préférence de l'utilisateur</li>
                <li>aide à gérer la surcharge d'informations</li>
                <li>Recommandations personnalisées et non personnalisées</li>
                <li>Applications
                    <ul>
                        <li>les générateurs de playlists pour les services de vidéo et de musique</li>
                        <li>les recommandations de produits</li>
                        <li>les recommandations de livres</li>
                        <li>les recommandations de contenu pour les plateformes de médias sociaux</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
            <ul>
                <li>Hypothèse: les individus suivent souvent les recommandations des autres utilisateurs</li>
                <li>Sources des données:
                    <ul>
                        <li>Utilisateurs</li>
                        <li>Articles ou objets</li>
                        <li>Transactions</li>
                    </ul>

                </li>
                <li>Recueillir les préférences des utilisateurs
                    <ul>
                        <li>les préférences explicitement exprimées: les évaluations et les actions des utilisateurs
                            comme les avis favorables et défavorables</li>
                        <li>l'interprétation des actions des utilisateurs: navigation web</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Fonctions [Ricci 2011]</h3>
            <ul>
                <li>Augmenter le nombre d'articles vendus.</li>
                <li>Vendre des articles plus variés.</li>
                <li>Augmenter la satisfaction des utilisateurs</li>
                <li>Augmenter la fidélité des utilisateurs.</li>
                <li>Mieux comprendre ce que veut l'utilisateur.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <ul>
                <li>Trouver de bons objets</li>
                <li>Trouver tous les bons articles</li>
                <li>Annotation dans le contexte</li>
                <li>Recommander une séquence: des livres, des vidéos sur un sujet donné</li>
                <li>Recommander une combinaison: plan de voyage </li>
                <li>Navigation (consultation)</li>
                <li>Trouver un système de recommandation crédible</li>
                <li>Améliorer le profil</li>
                <li>S'exprimer</li>
                <li>Aider les autres</li>
                <li>Influencer les autres</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
            <ul>
                <li>Filtrage collaboratif : basé sur les évaluations de plusieurs utilisateurs</li>
                <li>Filtrage basé sur le contenu : basé sur les profils des utilisateurs</li>
                <li>Démographiques: le profil démographique de l'utilisateur, par exemple le lieu et la langue</li>
                <li>Basé sur la connaissance: des recommandations basées sur la connaissance du domaine </li>
                <li>Basé sur la communauté: des recommandations basées sur les préférences des amis des utilisateurs
                </li>
                <li>Systèmes hybrides de recommandation [Gomez-Uribe 2016]</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage collaboratif</h3>
            <ul>
                <li>Transactions</li>
                <li>Algorithmes
                    <ul>
                        <li>Règles de l'association</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <ul>
                <li>Recommandation basée sur une description de l'objet et un profil des intérêts de l'utilisateur</li>
                <li>profil utilisateur
                    <ul>
                        <li>Un modèle des préférences de l'utilisateur</li>
                        <li>l'historique des interactions de l'utilisateur avec le système de recommandation.</li>
                    </ul>
                <li>Algorithmes
                    <ul>
                        <li>Arbres de décision</li>
                        <li>Méthodes du plus proche voisin</li>
                        <li>Classificateurs linéaires</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Systèmes hybrides [Gomez-Uribe 2016]</h3>
            <ul>
                <li>Filtrage basé sur le contenu et filtrage collaboratif</li>
                <li>Filtrage basé sur le contenu, filtrage collaboratif et démographique</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Mesures de performance [Ziegler 2005, Ricci 2011]</h3>
            <ul>
                <li>Précision et efficacité [Beel 2013a]</li>
                <li>Diversité</li>
                <li>Persistance de la recommandation</li>
                <li>Vie privée [Pu 2012]</li>
                <li>Démographie des utilisateurs</li>
                <li>Robustesse (lutte contre la fraude) [Konstan 2012]</li>
                <li>Sérendipité</li>
                <li>Confiance</li>
                <li>Étiquetage (recommandations organiques ou sponsorisées) [Beel 2013b]</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
            <ul>
                <li>Par exemple, assurance</li>
                <li>la nécessité de disposer d'indicateurs permettant de faire confiance ou de douter d'une
                    recommandation
                </li>
                <li>Intégrer les capacités d'explication aux systèmes de recommandation</li>
                <li>Avantages des explications
                    <ul>
                        <li>Justification</li>
                        <li>Participation des utilisateurs</li>
                        <li>Éducation</li>
                        <li>Acceptation</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>4.9. Représentation des connaissances et raisonnement</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Représentation des connaissances et raisonnement</h3>
            <ul>
                <li>KRR (Knowledge Representation & Reasoning) : Une forme de représentation lisible par machine de la
                    connaissance d'un monde ou d'un domaine.</li>
                <li>Exemple : réseaux sémantiques, ontologies</li>
                <li>Un compromis entre expressivité et praticité</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <figure>
                <img src="../../2021/MachineLearning/Semantic_web_stack.svg" height="400vh" />
                <figcaption style="text-align:center">Semantic Web Stack
                    (https://commons.wikimedia.org/wiki/File:Semantic_web_stack.svg)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles</h3>
            <ul>
                <li>Constitué de règles et de contraintes</li>
                <li>Vérifie qu'à un moment donné, le système est cohérent</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Programmation logique</h3>
            <ul>
                <li>Logique propositionnelle</li>
                <li>Logique du premier ordre (FOL, logique des prédicats)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog</h3>
            <ul>
                <li>Langage de programmation déclaratif</li>
                <li>Langage de programmation logique : basé sur la logique du premier ordre</li>
                <li>Développé en 1972 par Alain Colmerauer</li>
                <li>Exprimé en termes de relations : faits et règles</li>
                <li>Utilisé pour la démonstration de théorèmes, les systèmes experts et le traitement du langage naturel
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: types de données</h3>
            <ul>
                <li>Atome</li>
                <li>Nombres</li>
                <li>Variables</li>
                <li>Terme composé (par exemple, chaînes, listes)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: règles</h3>
            <p>Un programme Prolog contient des clauses de la forme suivante.</p>
            <code>
                          Tête : - Corps.
                        </code>
            <ul>
                <li>Le corps peut contenir un ou plusieurs prédicats utilisant la conjonction et la disjonction</li>
                <li>La tête ne contient aucune conjonction et disjonction</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: faits</h3>
            <p>Une clause avec un corps vide est appelée fait.</p>
            <code>
                          cat(bob).</br>
                          cat(alice).</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: Installation</h3>
            <p>Sur une machine Ubuntu</p>
            <code>
                          $ sudo apt install gprolog
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog</h3>
            <code>
$ prolog</br>
GNU Prolog 1.4.5 (64 bits)</br>
Compiled Feb  5 2017, 10:30:08 with gcc</br>
By Daniel Diaz</br>
Copyright (C) 1999-2016 Daniel Diaz</br>
| ?- [user].</br>
compiling user for byte code...</br>
cat(tom).</br>
cat(alice).</br>
</br>
user compiled, 2 lines read - 241 bytes written, 12239 ms</br>
</br>
(4 ms) yes</br>
| ?- </br>
</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
?- cat(X).</br>
</br>
X = tom ? </br>
</br>
yes</br>
| ?- cat(bob).</br>
</br>
no</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
| ?- [user].                             </br>
compiling user for byte code...</br>
cat(tom).                           </br>
cat(alice).                         </br>
allcats(L) :- findall(X, cat(X), L).</br>
</br>
user compiled, 3 lines read - 490 bytes written, 10638 ms</br>
</br>
yes</br>
| ?- allcats(L).                         </br>
</br>
L = [tom,alice]</br>
</br>
yes</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
| ?- [user].              </br>
compiling user for byte code...</br>
friend(bob, alice).  </br>
friend(alice, kevin).</br>
friend(bob, thomas).                </br>
friend(bob, peter).  </br>
</br>
user compiled, 4 lines read - 486 bytes written, 77256 ms</br>
</br>
(10 ms) yes</br>
| ?- friend(bob, X).      </br>
</br>
X = alice ? a</br>
</br>
X = thomas</br>
</br>
X = peter</br>
</br>
(1 ms) yes</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ cat friend.pl
friend(bob, alice).
friend(alice, kevin).
friend(bob, thomas).
friend(bob, peter).
human(X):-friend(X,_).
human(Y):-friend(_,Y).
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
compiling /home/user/friend.pl for byte code...
/home/user/friend.pl compiled, 4 lines read - 515 bytes written, 22 ms
| ?- friend(bob,alice).

true ?

yes
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
| ?- human(X).
X = bob ? a
X = alice
X = bob
X = bob
X = alice
X = kevin
X = thomas
X = peter

yes
| ?-
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Beel 2013a] Beel, Joeran, et al. “A Comparative Analysis of Offline and Online Evaluations and
                    Discussion of Research Paper Recommender System Evaluation.” Proceedings of the International
                    Workshop on Reproducibility and Replication in
                    Recommender Systems Evaluation, Association for Computing Machinery, 2013</li>
                <li>[Beel 2013b] Beel, Joeran, et al. “Sponsored vs. Organic (Research Paper) Recommendations and the
                    Impact of Labeling.” Research and Advanced Technology for Digital Libraries, edited by Trond Aalberg
                    et al., Springer, 2013, pp. 391–95.</li>
                <li>[Chrupała 2006] Chrupała, Grzegorz. Simple Data-Driven Context-Sensitive Lemmatization. 2006.
                    doras.dcu.ie, http://www.unizar.es/departamentos/filologia_inglesa/sepln2006/.</li>
                <li>[Frakes 2003] Frakes, William B., and Christopher J. Fox. “Strength and Similarity of Affix Removal
                    Stemming Algorithms.” ACM SIGIR Forum, vol. 37, no. 1, Apr. 2003, pp. 26–30. Spring 2003</li>
                <li>[Gomez-Uribe 2016] Gomez-Uribe, Carlos A., and Neil Hunt. “The Netflix Recommender System:
                    Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems,
                    vol. 6, no. 4, Dec. 2016, p. 13:1–13:19. January
                    2016
                </li>
                Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers -
                Volume 2, Association for Computational Linguistics, 2012, pp. 368–372.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Gesmundo 2012] Gesmundo, Andrea, and Tanja Samardžić. “Lemmatisation as a Tagging Task.”
                <li>[Herlocker 2000] Herlocker, Jonathan L., et al. “Explaining Collaborative Filtering
                    Recommendations.” Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work,
                    Association for Computing Machinery, 2000, pp. 241–250.
                    ACM
                </li>
                <li>[Konstan 2012] Konstan, Joseph A., and John Riedl. “Recommender Systems: From Algorithms to User
                    Experience.” User Modeling and User-Adapted Interaction, vol. 22, no. 1–2, Apr. 2012, pp. 101–123.
                </li>
                <li>[Màrquez 2000] Màrquez, Lluís, et al. “A Machine Learning Approach to POS Tagging.” Machine
                    Learning, vol. 39, no. 1, Apr. 2000, pp. 59–91.</li>
                <li>[Mikolov 2013] Mikolov, Tomas, et al. “Efficient Estimation of Word Representations in Vector
                    Space.” ArXiv:1301.3781 [Cs], Sept. 2013.</li>
                <li>[Miller 1995] Miller, George A. “WordNet: A Lexical Database for English.” Communications of the
                    ACM, vol. 38, no. 11, Nov. 1995, pp. 39–41. Nov. 1995</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Pazzani 2007] Pazzani, Michael J., and Daniel Billsus. “Content-Based Recommendation Systems.” The
                    Adaptive Web: Methods and Strategies of Web Personalization, edited by Peter Brusilovsky et al.,
                    Springer, 2007, pp. 325–41. </li>
                <li>[Porter 1980] Porter, M. F. “An Algorithm for Suffix Stripping.” Program, vol. 14, no. 3, Jan. 1980,
                    pp. 130–37. Emerald Insight</li>
                <li>[Pu 2012] Pu, Pearl, et al. “Evaluating Recommender Systems from the User’s Perspective: Survey of
                    the State of the Art.” User Modeling and User-Adapted Interaction, vol. 22, no. 4, Oct. 2012, pp.
                    317–55.
                </li>
                <li>[Ricci 2011] Ricci, Francesco, et al. “Introduction to Recommender Systems Handbook.” Recommender
                    Systems Handbook, edited by Francesco Ricci et al., Springer US, 2011, pp. 1–35. </li>
                <li>[Ziegler 2005] Ziegler, Cai-Nicolas, et al. “Improving Recommendation Lists through Topic
                    Diversification.” Proceedings of the 14th International Conference on World Wide Web, Association
                    for Computing Machinery, 2005, pp. 22–32.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web</h3>
            <ul>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation">https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation</a>
                </li>
                <li><a %
                        href="https://fr.wikipedia.org/wiki/Racinisation">https://fr.wikipedia.org/wiki/Racinisation</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Intelligence_artificielle">https://fr.wikipedia.org/wiki/Intelligence_artificielle</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Programmation_logique">https://fr.wikipedia.org/wiki/Programmation_logique</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">https://en.wikipedia.org/wiki/Morphology_(linguistics)</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Word_embedding">https://en.wikipedia.org/wiki/Word_embedding</a>
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
                <li><a href="https://fr.wikipedia.org/wiki/Prolog">https://fr.wikipedia.org/wiki/Prolog</a></li>
                <li><a href="https://www.nltk.org/howto/stem.html">https://www.nltk.org/howto/stem.html</a></li>
                <li><a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></li>
                <li><a href="https://spacy.io/usage/spacy-101">https://spacy.io/usage/spacy-101</a></li>
                <li><a href="https://spacy.io/usage/visualizers">https://spacy.io/usage/visualizers</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
                <a class="next" href="#slide91"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide91">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">91
                <a class="prev" href="#slide90"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 90) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>
