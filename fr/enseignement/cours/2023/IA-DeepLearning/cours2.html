<html>

<head>
    <meta charset="utf-8" />
    <title>Intelligence artificielle (2023-2024): Cours: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            line-height: 1.7em;
            flex-direction: column;
            align-items: flex-start;
            margin: 0 auto;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .codeexample {
            background-color: #eeeeee;
        }

        /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
        pre {
            line-height: 125%;
        }

        td.linenos .normal {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        td.linenos .special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos.special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        body .hll {
            background-color: #ffffcc
        }

        body {
            background: #f8f8f8;
        }

        body .c {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment */
        body .err {
            border: 1px solid #FF0000
        }

        /* Error */
        body .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */
        body .o {
            color: #666666
        }

        /* Operator */
        body .ch {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Hashbang */
        body .cm {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Multiline */
        body .cp {
            color: #9C6500
        }

        /* Comment.Preproc */
        body .cpf {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.PreprocFile */
        body .c1 {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Single */
        body .cs {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Special */
        body .gd {
            color: #A00000
        }

        /* Generic.Deleted */
        body .ge {
            font-style: italic
        }

        /* Generic.Emph */
        body .gr {
            color: #E40000
        }

        /* Generic.Error */
        body .gh {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Heading */
        body .gi {
            color: #008400
        }

        /* Generic.Inserted */
        body .go {
            color: #717171
        }

        /* Generic.Output */
        body .gp {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Prompt */
        body .gs {
            font-weight: bold
        }

        /* Generic.Strong */
        body .gu {
            color: #800080;
            font-weight: bold
        }

        /* Generic.Subheading */
        body .gt {
            color: #0044DD
        }

        /* Generic.Traceback */
        body .kc {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Constant */
        body .kd {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Declaration */
        body .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */
        body .kp {
            color: #008000
        }

        /* Keyword.Pseudo */
        body .kr {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Reserved */
        body .kt {
            color: #B00040
        }

        /* Keyword.Type */
        body .m {
            color: #666666
        }

        /* Literal.Number */
        body .s {
            color: #BA2121
        }

        /* Literal.String */
        body .na {
            color: #687822
        }

        /* Name.Attribute */
        body .nb {
            color: #008000
        }

        /* Name.Builtin */
        body .nc {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Class */
        body .no {
            color: #880000
        }

        /* Name.Constant */
        body .nd {
            color: #AA22FF
        }

        /* Name.Decorator */
        body .ni {
            color: #717171;
            font-weight: bold
        }

        /* Name.Entity */
        body .ne {
            color: #CB3F38;
            font-weight: bold
        }

        /* Name.Exception */
        body .nf {
            color: #0000FF
        }

        /* Name.Function */
        body .nl {
            color: #767600
        }

        /* Name.Label */
        body .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */
        body .nt {
            color: #008000;
            font-weight: bold
        }

        /* Name.Tag */
        body .nv {
            color: #19177C
        }

        /* Name.Variable */
        body .ow {
            color: #AA22FF;
            font-weight: bold
        }

        /* Operator.Word */
        body .w {
            color: #bbbbbb
        }

        /* Text.Whitespace */
        body .mb {
            color: #666666
        }

        /* Literal.Number.Bin */
        body .mf {
            color: #666666
        }

        /* Literal.Number.Float */
        body .mh {
            color: #666666
        }

        /* Literal.Number.Hex */
        body .mi {
            color: #666666
        }

        /* Literal.Number.Integer */
        body .mo {
            color: #666666
        }

        /* Literal.Number.Oct */
        body .sa {
            color: #BA2121
        }

        /* Literal.String.Affix */
        body .sb {
            color: #BA2121
        }

        /* Literal.String.Backtick */
        body .sc {
            color: #BA2121
        }

        /* Literal.String.Char */
        body .dl {
            color: #BA2121
        }

        /* Literal.String.Delimiter */
        body .sd {
            color: #BA2121;
            font-style: italic
        }

        /* Literal.String.Doc */
        body .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */
        body .se {
            color: #AA5D1F;
            font-weight: bold
        }

        /* Literal.String.Escape */
        body .sh {
            color: #BA2121
        }

        /* Literal.String.Heredoc */
        body .si {
            color: #A45A77;
            font-weight: bold
        }

        /* Literal.String.Interpol */
        body .sx {
            color: #008000
        }

        /* Literal.String.Other */
        body .sr {
            color: #A45A77
        }

        /* Literal.String.Regex */
        body .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */
        body .ss {
            color: #19177C
        }

        /* Literal.String.Symbol */
        body .bp {
            color: #008000
        }

        /* Name.Builtin.Pseudo */
        body .fm {
            color: #0000FF
        }

        /* Name.Function.Magic */
        body .vc {
            color: #19177C
        }

        /* Name.Variable.Class */
        body .vg {
            color: #19177C
        }

        /* Name.Variable.Global */
        body .vi {
            color: #19177C
        }

        /* Name.Variable.Instance */
        body .vm {
            color: #19177C
        }

        /* Name.Variable.Magic */
        body .il {
            color: #666666
        }

        /* Literal.Number.Integer.Long */


        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 70%;
            margin-bottom: 1vmax;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 90%;
            max-height: 90%;
        }

        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }

        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 1.7em;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }


        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                font-size: 2.8vmin;
            }

            .content .flexcontent {
                font-size: 2.5vmin;
            }
        }
    </style>
    <script src="../../2021/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:2.5vw">Intelligence artificielle</h1>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Year</b>: 2023-2024<br />
                <b>Email</b>: john(dot)samuel(at)cpe(dot)fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles (NLP) </h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Intelligence artificielle</h3>
            <figure>
                <img src="../../../../../images/art/courses/deeplearningposition.svg" height="450px" />
                <figcaption>Intelligence artificielle</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <p>Le <b>traitement automatique des langues (TAL)</b> est un domaine interdisciplinaire de la linguistique
                informatique qui se concentre sur l'analyse et la compréhension du <b>langage naturel</b> (celui utilisé
                par les humains). Cette section aborde plusieurs aspects clés du TAL, notamment :</p>
            <ul>
                <li><b>Analyser et comprendre le langage naturel (humain)</b>: Le TAL se consacre à la compréhension du
                    langage naturel dans divers contextes, qu'il s'agisse de textes écrits ou de discours verbal.</li>
                <li>Interaction homme-machine</li>
                <li><b>Syntaxe d'une langue</b>
                    <ul>
                        <li>Parsing : Le parsing consiste à analyser la structure grammaticale des phrases.</li>
                        <li>L'étiquetage en parties du discours (PoS) : L'étiquetage PoS consiste à assigner des
                            catégories grammaticales (comme verbe, nom, adjectif, etc.) aux mots d'une phrase.</li>
                    </ul>
                </li>
                <li><b>Sémantique d'une langue</b>
                    <ul>
                        <li>Traduction automatique</li>
                        <li>Reconnaissance d'entités nommées (NER): La NER consiste à identifier des entités spécifiques
                            (comme des noms de personnes, de lieux ou d'organisations) dans un texte.</li>
                        <li>Analyse des sentiments</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>4.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Analyse de systèmes TAL</h3>
            <ul>
                <li><b>Racinisation</b> : La racinisation est le processus de réduction des mots à leur forme de base ou
                    de racine. </li>
                <li><b>Étiquetage morpho-syntaxique</b> : Cette étape consiste à attribuer des balises ou des étiquettes
                    aux mots dans un texte en fonction de leur rôle grammatical et de leur structure. </li>
                <li><b>Lemmatisation</b> : Contrairement à la racinisation, la lemmatisation consiste à ramener les mots
                    à leur forme canonique ou lemmes. </li>
                <li><b>Morphologie</b> : La morphologie concerne l'étude de la structure des mots, notamment comment ils
                    sont formés à partir de morphèmes (unités de sens). </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation [Frakes 2003]</h3>
            <ul>
                <li>La racination, souvent appelée <b>stemming</b> en anglais, est un processus de normalisation
                    linguistique visant à réduire les mots à leur forme racine, en ignorant les affixes. Elle est
                    utilisée pour simplifier les variations morphologiques des mots. </li>
                <li>Les <b>algorithmes de racination</b> appliquent généralement des règles heuristiques pour éliminer
                    les préfixes et suffixes courants.
                    <ul>
                        <li><b>Exemples</b> : Porter, Snowball</li>
                        <li><b>Limitations</b> : La racination peut conduire à des résultats non valides, car elle peut
                            produire des racines qui ne sont pas des mots réels.</li>
                    </ul>
                </li>
                <li>Exemples
                    <ul>
                        <li>engineer: engineer, engineered, engineering</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation</h3>
            <p>Quelques exemples d'issues potentiellement <b>non valides</b> :</p>
            <ul>
                <li>Racination excessive :
                    <ul>
                        <li>Mot d'origine : "happily"</li>
                        <li>Racination : "happi" (au lieu de la forme correcte "happy")</li>
                    </ul>
                </li>
                <li>Racination incorrecte :
                    <ul>
                        <li> Mot d'origine : "better"</li>
                        <li> Racination : "bet" (au lieu de la forme correcte "better")</li>
                    </ul>
                <li>Création de faux mots :
                    <ul>
                        <li> Mot d'origine : "unhappiness"</li>
                        <li> Racination : "unhappi" (crée un faux mot au lieu de "unhappy")</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation</h3>
            <ul>
                <li>Ambiguïté des règles :
                    <ul>
                        <li> Mot d'origine : "flies" (verbe)</li>
                        <li> Racination : "fli" (peut être confondu avec le nom "fly")</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: mesures d'évaluation [Frakes 2003]</h3>
            <ul>
                <li>La mesure dans laquelle un algorithm modifie des mots qu'elle réduit à ses racines est appelée la
                    <b>force</b> de l'algorithme
                </li>
                <li>Une métrique de <b>similarité</b> des algorithmes met en correspondance les n-tuples d'algorithmes
                    (n au moins 2), avec un nombre indiquant la similarité des algorithmes. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: distance de Hamming [Frakes 2003]</h3>
            <ol>
                <li>La distance de Hamming entre deux chaînes de longueur égale est définie comme le nombre de
                    caractères des deux chaînes qui sont différents à la même position.</li>
                <li>Pour les chaînes de longueur inégale, ajouter la différence de longueur à la distance de Hamming
                    pour obtenir une fonction de distance de Hamming modifiée \(d\)</li>
                <li>Exemples
                    <ul>
                        <li>tri: try, tried, trying</li>
                        <li>\(d\)(tri, try)= 1</li>
                        <li>\(d\)(tri, tried)= 2</li>
                        <li>\(d\)(tri, trying)= 4</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: force [Frakes 2003]</h3>
            <ol>
                <li>Le nombre moyen de mots par classe</li>
                <li>Facteur de compression de l'indice. Soit n est le nombre de mots dans le corpus et s est le nombre
                    de racines. \[\frac{n - s}{n}\]
                </li>
                <li>Le nombre de mots et de racines qui diffèrent</li>
                <li>Le nombre moyen de caractères supprimés lors de la formation des racines</li>
                <li>La médiane et la moyenne de la distance de Hamming modifiée entre les mots et leur racine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: similarité [Frakes 2003]</h3>
            <ol>
                <li>Soit \(A1\) et \(A2\) sont deux algorithmes</li>
                <li>Soit \(W\) une liste de mots et \(n\) le nombre de mots dans \(W\) \[ M(A1,A2,W) = \frac{n}{\Sigma
                    d(x_i, y_i)}\]
                </li>
                <li>pour tous les mots \(w_i\) en W, \(x_i\) est le résultat de l'application de \(A1\) à \(w_i\) et
                    \(y_i\) est le résultat de l'application de \(A2\) à \(w_i\)</li>
                <li>des algorithmes plus similaires auront des valeurs plus élevées de M</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: nltk</h3>
            <p>L'objectif est de réduire les mots à leur forme de base ou racine, en éliminant les suffixes, ce qui
                permet de regrouper différentes formes d'un mot sous une forme commune. </p>
            <ul>
                <li><b>Porter [Porter 1980]</b> : Le Porter Stemming Algorithm, créé par Martin Porter en 1980, est basé
                    sur un ensemble de règles heuristiques. Il suit une approche itérative en appliquant une série de
                    transformations séquentielles aux mots.</li>
                <li><b>Snowball</b> Le Snowball (anciennement appelé Porter2) est une amélioration du Porter Stemmer. Il
                    suit également une approche basée sur des règles, mais il est plus systématique dans son traitement
                    des différents cas de racination. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <p>L'algorithme de Porter, également connu sous le nom de stemmer de Porter, est un algorithme de racination
                (stemming) développé par Martin Porter en 1980. Son objectif est de réduire les mots à leur forme racine
                ou base en éliminant les suffixes couramment utilisés en anglais.</p>
            <ul>
                <li><b>Prétraitement</b> : Convertir le mot en minuscules. Identifier le préfixe 'y' et le traiter comme
                    une voyelle s'il est en première position, sinon comme une consonne.</li>
                <li><b>Application des règles de racination</b> : L'algorithme de Porter utilise une série de règles
                    pour éliminer les suffixes. Ces règles sont appliquées séquentiellement jusqu'à ce qu'aucune d'entre
                    elles ne s'applique plus. Les règles comprennent des opérations comme la suppression de suffixes
                    spécifiques, la transformation de certains suffixes en d'autres, et la manipulation de la longueur
                    des mots. </li>
                <li><b>Post-traitement</b> : Certains ajustements post-traitement sont effectués pour améliorer la
                    précision de la racination.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <p>L'algorithme de Porter utilise une série de règles de racination pour réduire les mots à leur forme
                racine. Quelques-unes des règles de l'algorithme de Porter :</p>
            <ol>
                <li>Règles de suppression de suffixes :
                    <ul>
                        <li> "s" : Supprimer le suffixe "s" à la fin des mots.</li>
                        <li> "sses" : Remplacer par "ss" si la séquence se termine par "sses".</li>
                    </ul>
                </li>
                <li>Règles de traitement de suffixes spécifiques :
                    <ul>
                        <li> "eed" ou "eedly" : Remplacer par "ee" si la séquence se termine par "eed" ou "eedly".</li>
                        <li> "ed" : Supprimer "ed" à la fin du mot s'il y a une voyelle précédente.</li>
                        <li> "ing" : Supprimer "ing" à la fin du mot s'il y a une voyelle précédente.</li>
                    </ul>
                </li>

                </li>
                <ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <ol start="3">
                <li>Règles de transformation de suffixes en d'autres suffixes :
                    <ul>
                        <li> "at" : Remplacer par "ate" si la séquence se termine par "at".</li>
                        <li> "bl" : Ajouter "e" à la fin si la séquence se termine par "bl".</li>
                    </ul>
                </li>

                <li>Règles de manipulation de la longueur des mots :
                    <ul>
                        <li> Si la séquence se termine par une consonne suivie de "y", remplacer par "i" à la fin.</li>
                        <li> Si la séquence se termine par deux consonnes, supprimer la dernière consonne si la
                            précédente est une voyelle.</li>
                    </ul>
                </li>

                <li>Règles de manipulation des doubles consonnes :
                    <ul>
                        <li> Supprimer une lettre double à la fin du mot.</li>
                    </ul>
                </li>
                <ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Porter</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Snowball</h3>
            <p>L'algorithme de Snowball, également connu sous le nom de Snowball stemmer, est un algorithme de
                racination (stemming) développé par Martin Porter comme une extension de son algorithme de Porter.
                Snowball a été conçu pour être plus modulaire et extensible, permettant aux utilisateurs de créer des
                stemmers pour différentes langues en utilisant un ensemble commun de conventions.</p>
            <p>Les caractéristiques principales de l'algorithme de Snowball :</p>
            <ul>
                <li><b>Modularité</b> : L'algorithme de Snowball est conçu de manière modulaire, permettant la
                    définition de règles spécifiques pour chaque langue. Chaque règle est encapsulée dans une unité
                    appelée "step."</li>
                <li><b>Structure du Langage</b> : L'algorithme de Snowball est souvent utilisé pour différentes langues,
                    et la structure du langage est définie par des fichiers de règles spécifiques à chaque langue. Ces
                    fichiers décrivent comment les suffixes et préfixes doivent être traités.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Snowball</h3>
            <ul>
                <li><b>Extensibilité</b> : Les utilisateurs peuvent étendre l'algorithme de Snowball pour traiter des
                    langues spécifiques en ajoutant des règles appropriées dans un fichier dédié à cette langue.</li>
                <li><b>Étape de Règle</b> : Chaque étape (step) de l'algorithme de Snowball est constituée de règles qui
                    décrivent comment transformer un mot. Chaque règle a une forme similaire à "condition -> action," où
                    la condition spécifie quand appliquer la règle, et l'action définit la transformation à effectuer.
                </li>
                <li><b>Itération</b> : L'algorithme de Snowball applique les étapes de règle itérativement jusqu'à ce
                    qu'aucune d'entre elles ne puisse être appliquée. Cette itération permet de réduire progressivement
                    les mots à leur forme racine.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>4.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Snowball</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">snowball</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">snowball</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <ul>
                <li>L'étiquetage morpho-syntaxique, également appelé <b style="color:#00549d">Part of Speech (PoS)
                        Tagging</b>, est un processus dans lequel chaque mot d'un texte se voit attribuer une balise
                    morpho-syntaxique appropriée en fonction de son rôle grammatical et de son contexte d'apparition.
                    Ces balises indiquent la catégorie grammaticale à laquelle chaque mot appartient. </li>
                <li>Il permet de capturer la <b style="color:#00549d">structure grammaticale</b> d'un texte, facilitant
                    ainsi la compréhension et l'analyse linguistique automatisées.</li>
                <li>Les algorithmes d'étiquetage morpho-syntaxique utilisent généralement des <b
                        style="color:#00549d">modèles statistiques</b> ou des <b style="color:#00549d">règles
                        linguistiques</b> pour assigner ces balises en fonction du contexte entourant chaque mot.</li>
                <li>Exemples des balises
                    <ul>
                        <li><b style="color:#008000">Noms</b> : Indiquent des entités ou objets concrets. Exemple :
                            "chat," "maison," "fleur"</li>
                        <li><b style="color:#008000">Verbes</b> : Indiquent des actions ou des états. Exemple :
                            "marcher," "manger," "être"</li>
                        <li><b style="color:#008000">Adjectifs</b> : Décrivent ou qualifient des noms. Exemple : "beau,"
                            "rapide," "intelligent"</li>
                        <li><b style="color:#008000">Adverbes</b> : Modifient des verbes, des adjectifs ou d'autres
                            adverbes, fournissant des informations sur la manière, le lieu, le temps, etc. Exemple :
                            "rapidement," "bien," "ici"</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
            <ol>
                <li><b>Approche manuelle</b> :
                    <ul>
                        <li>Construction de règles linguistiques manuelles pour analyser la structure linguistique</li>
                        <li>Exemple : Définir des règles pour identifier les parties du discours en fonction de la
                            syntaxe.</li>
                    </ul>
                </li>
                <li><b>Approche statistique</b> :
                    <ul>
                        <li>Utilisation de statistiques et de probabilités pour modéliser les relations linguistiques.
                        </li>
                        <li>Collection de n-grammes (bi-grammes, tri-grammes, ...)</li>
                        <li>Ensemble de fréquences de cooccurrence</li>
                        <li>L'estimation de la probabilité d'une séquence de longueur n est calculée en tenant compte de
                            son occurrence dans le corpus d'entraînement</li>
                    </ul>
                </li>
                <li><b>Apprentissage machine</b> :
                    <ul>
                        <li>Utilisation de techniques d'apprentissage machine pour apprendre automatiquement des modèles
                            linguistiques à partir de données d'entraînement.</li>
                        <li>Les algorithmes peuvent être entraînés à reconnaître des motifs et des structures
                            linguistiques complexes</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{}</span><span class="s2">-grams&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams (affichage)</h3>
            <p class="codeexample">
                <code>
1-grams<br/>
('He',) ('went',) ('to',) ('school',) ('yesterday',) ('and',) ('attended',) ('the',) ('classes',) <br/>
2-grams<br/>
('He', 'went') ('went', 'to') ('to', 'school') ('school', 'yesterday') ('yesterday', 'and') ('and', 'attended') ('attended', 'the') ('the', 'classes') <br/>
3-grams<br/>
('He', 'went', 'to') ('went', 'to', 'school') ('to', 'school', 'yesterday') ('school', 'yesterday', 'and') ('yesterday', 'and', 'attended') ('and', 'attended', 'the') ('attended', 'the', 'classes') <br/>
4-grams<br/>
('He', 'went', 'to', 'school') ('went', 'to', 'school', 'yesterday') ('to', 'school', 'yesterday', 'and') ('school', 'yesterday', 'and', 'attended') ('yesterday', 'and', 'attended', 'the') ('and', 'attended', 'the', 'classes')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>

            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He goes to school daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>PRP</td>
                    <td>pronoun, personal</td>
                </tr>
                <tr>
                    <td>VBZ</td>
                    <td>verb, present tense, 3rd person singular</td>
                </tr>
                <tr>
                    <td>TO</td>
                    <td>"to" as preposition</td>
                </tr>
                <tr>
                    <td>NN</td>
                    <td>"noun, common, singular or mass</td>
                </tr>
                <tr>
                    <td>RB</td>
                    <td>adverb</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				 $ pip3 install spacy<br/>
				 $ python3 -m spacy download en_core_web_sm<br/>
                         </code>
            </p>
            <p>Usage</p>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span></br>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  He PRON nsubj<br/>
                                  goes VERB ROOT<br/>
                                  to ADP prep<br/>
                                  school NOUN pobj<br/>
                                  daily ADV advmod<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">tag_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">,</span>
            <span class="n">token</span><span class="o">.</span><span class="n">shape_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_alpha</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  He -PRON- PRON PRP nsubj Xx True True<br/>
                                  goes go VERB VBZ ROOT xxxx True False<br/>
                                  to to ADP IN prep xx True True<br/>
                                  school school NOUN NN pobj xxxx True False<br/>
                                  daily daily ADV RB advmod xxxx True False<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation, consiste à regrouper les différentes formes d'un mot qui appartiennent au même
                    paradigme morphologique flexionnel et à attribuer à chaque paradigme son lemme correspondant. </li>
                <li>Cette méthode vise à ramener les variations flexionnelles d'un mot à sa <b
                        style="color:#00549d">forme canonique</b> ou à sa racine. </li>
                <li>La lemmatisation permet de simplifier la représentation des mots en les ramenant à leur forme de
                    base, ce qui facilite la recherche, l'analyse et le traitement automatique du langage naturel. </li>
                <li>Exemples
                    <ul>
                        <li>go: go, goes, going, went, gone</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Chrupała 2006, Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation comme une tâche d'étiquetage</li>
                <li>Attribuer un label pour chaque transformation d'un label en lemme</li>
                <li>4 étapes [Gesmundo 2012]
                    <ol>
                        <li>supprimer un suffixe de longueur \(N_s\)</li>
                        <li>ajouter un nouveau suffixe de lemme \(L_s\)</li>
                        <li>supprimer un préfixe de longueur \(N_p\)</li>
                        <li>ajouter un nouveau préfixe lemme, \(L_p\)</li>
                    </ol>
                </li>
                <li>Transformation \(\tau = \langle N_s, L_s, N_p, L_p \rangle\)</li>
                <li>(going, go) = \(\langle 3, \emptyset, 0, \emptyset \rangle \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer</h3>
            <ul>
                <li><b style="color:#00549d">WordNet [Miller 1995]</b> : WordNet est une base de données lexicale de la
                    langue anglaise qui organise les mots en synsets (ensembles de synonymes) et les relie entre eux par
                    des relations lexicales telles que l'hypernymie (relation "est-un") et l'hyponymie (relation "a pour
                    instance").</li>
                <li><b style="color:#00549d">WordNetLemmatizer</b> : Le module WordNetLemmatizer dans NLTK utilise
                    WordNet pour la lemmatisation des mots. Il attribue à chaque mot sa forme canonique ou lemme, en
                    tenant compte des différentes formes flexionnelles. </li>
            </ul>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger&#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (sans les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     He went to school yesterday and attended the class
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">pos_tag</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>

<span class="c1"># Check the complete list of tags http://www.nltk.org/book/ch05.html</span>
<span class="k">def</span> <span class="nf">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;J&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADJ</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADV</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;N&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">NOUN</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">VERB</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;I went to school today and he goes daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">)),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     I go to school today and he go daily
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>4.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I went to school today and he goes daily&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  -PRON- go to school today and -PRON- go daily<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>4.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Morphologie</h3>
            <ul>
                <li>La morphologie lexicale est une branche de la linguistique qui se concentre sur l'étude des <b
                        style="color:#00549d">mots</b>, de leurs <b style="color:#00549d">formes</b>, de leurs <b
                        style="color:#00549d">paradigmes</b> et de l'organisation des <b
                        style="color:#00549d">catégories grammaticales</b>. </li>
                <li>Elle examine de près les parties du discours, l'intonation, l'accentuation, ainsi que la manière
                    dont le contexte peut influencer la prononciation et le sens d'un mot.</li>
                <li>Elle explore la structure interne des mots et comment ils interagissent avec la grammaire et le
                    contexte pour communiquer des significations spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>4.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">,</span> <span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <figure>
                <img src="../../2021/MachineLearning/spacy-dep-output.svg" width="400vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Les embeddings de mots sont une technique d'apprentissage de caractéristiques où des mots ou des phrases
                du vocabulaire sont associés à des vecteurs de nombres réels.</p>
            <ul>
                <li>L'idée principale est de représenter chaque mot par un vecteur dense dans un espace continu, de
                    telle sorte que des mots similaires aient des vecteurs similaires, capturant ainsi les relations
                    sémantiques entre les mots.</li>
                <li>Quantifier et catégoriser les similarités sémantiques entre les éléments linguistiques en fonction
                    de leurs propriétés de distribution dans de grands échantillons de données linguistiques.</li>
                <li>En d'autres termes, les mots qui ont des contextes similaires ou qui apparaissent dans des contextes
                    similaires auront des embeddings de mots similaires.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Avantages de Word Embeddings</p>
            <ul>
                <li><b>Représentation dense</b> : Les embeddings fournissent une représentation dense, contrairement à
                    une représentation creuse où chaque mot serait représenté par un vecteur binaire indiquant sa
                    présence ou son absence.</li>
                <li><b>Capture des relations sémantiques</b> : Les embeddings captent les relations sémantiques et les
                    similitudes entre les mots, ce qui les rend utiles dans de nombreuses tâches de traitement du
                    langage naturel.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Applications de Word Embeddings</p>
            <ul>
                <li><b>Similarité sémantique</b> : Mesurer la similarité sémantique entre les mots.</li>
                <li><b>Traduction automatique</b> : Améliorer les performances des systèmes de traduction automatique.
                </li>
                <li><b>Analyse des sentiments</b> : Mieux comprendre le contexte et les relations sémantiques dans
                    l'analyse des sentiments, entre autres applications.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy est une bibliothèque open-source pour le traitement du langage naturel (NLP) en Python. Elle offre
                des outils performants et efficaces pour effectuer diverses tâches de traitement du langage naturel, de
                l'analyse syntaxique à la reconnaissance d'entités nommées. spaCy est conçu pour être rapide, précis et
                facile à utiliser.</p>
            <ul>
                <li><b>Collecte de données</b> : Les modèles spaCy sont souvent entraînés sur de vastes ensembles de
                    données annotées, qui peuvent inclure des corpus textuels avec des annotations pour l'analyse
                    syntaxique, la reconnaissance d'entités nommées, etc.</li>
                <li><b>Annotation des données</b> : Les données collectées sont annotées manuellement avec des
                    informations linguistiques spécifiques telles que les parties du discours, les entités nommées, les
                    relations syntaxiques, etc.</li>
                <li><b>Entraînement initial</b> : Les modèles spaCy sont initialement entraînés sur ces ensembles de
                    données annotées pour apprendre les structures linguistiques. Ce processus peut inclure
                    l'utilisation d'algorithmes d'apprentissage automatique tels que les réseaux de neurones.</li>
                <li><b>Optimisation et réglage</b> : Les modèles sont ensuite optimisés et réglés pour améliorer leurs
                    performances sur des tâches spécifiques. Cela peut impliquer des itérations sur le processus
                    d'entraînement en ajustant les hyperparamètres du modèle.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>Évaluation</b> : Les modèles sont évalués sur des ensembles de données de test distincts pour
                    mesurer leur précision, leur rappel et d'autres métriques spécifiques à la tâche.</li>
                <li><b>Construction des modèles linguistiques pré-entraînés</b> : Une fois le modèle entraîné et évalué,
                    spaCy construit des modèles linguistiques pré-entraînés qui encapsulent les connaissances acquises
                    sur la structure linguistique.</li>
                <li><b>Téléchargement et utilisation</b> : Les utilisateurs peuvent télécharger ces modèles
                    pré-entraînés via spaCy et les utiliser dans leurs applications pour effectuer diverses tâches de
                    traitement du langage naturel sans avoir à entraîner un modèle de zéro.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy propose différents modèles linguistiques pré-entraînés pour différentes langues et tâches. Le
                modèle en_core_web_lg est un modèle vectoriel large d'anglais.</p>
            <h4>Installation du Modèle spaCy (en_core_web_lg) :</h4>
            <p class="codeexample">
                <code>
				 $ python3 -m spacy download en_core_web_lg<br/>
                         </code>
            </p>
            <h4>Chargement du Modèle spaCy :</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Avantages de spaCy :</p>
            <ul>
                <li><b>Performance élevée</b> : spaCy est reconnu pour sa rapidité d'exécution, ce qui le rend adapté au
                    traitement de grands volumes de texte en temps réel.</li>
                <li><b>Modèles pré-entraînés</b> : spaCy propose des modèles linguistiques pré-entraînés pour plusieurs
                    langues, ce qui facilite l'analyse de texte sans nécessiter d'entraînement à partir de zéro.</li>
                <li><b>Extraction d'informations linguistiques riches</b> : spaCy fournit des informations linguistiques
                    détaillées telles que les parties du discours, les entités nommées, les relations syntaxiques, et
                    plus encore.</li>
                <li><b>API conviviale</b> : L'API spaCy est conçue pour être intuitive et conviviale. Elle facilite la
                    réalisation de tâches complexes avec des lignes de code concises.</li>
                <li><b>Intégration avec d'autres bibliothèques</b> : spaCy s'intègre bien avec d'autres bibliothèques
                    Python populaires, facilitant son utilisation dans des projets plus larges.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Limites de spaCy :</p>
            <ul>
                <li><b>Dépendance des modèles linguistiques</b> : L'utilisation de modèles pré-entraînés signifie que la
                    qualité des résultats dépend de la qualité du modèle. Dans des domaines de spécialité ou pour des
                    langues moins courantes, les modèles peuvent ne pas être aussi performants.</li>
                <li><b>Gestion des entités nommées</b> : Bien que spaCy excelle dans la reconnaissance d'entités
                    nommées, il peut parfois avoir du mal avec des tâches plus complexes impliquant des variations
                    contextuelles.</li>
                <li><b>Taille des modèles</b> : Les modèles pré-entraînés peuvent être relativement volumineux, ce qui
                    peut être un inconvénient dans des environnements avec des restrictions de mémoire ou pour des
                    applications mobiles.</li>
                <li><b>Personnalisation limitée</b> : Bien que spaCy offre des fonctionnalités de personnalisation,
                    elles peuvent être limitées par rapport à d'autres bibliothèques NLP plus flexibles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>

<span class="c1"># Définir les mots à comparer</span>
<span class="n">words_to_compare</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;apple&quot;</span><span class="p">]</span>

<span class="c1"># Calculer la similarité entre les paires de mots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
        <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="o">=</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">doc1</span><span class="p">,</span> <span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word1</span><span class="p">),</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
        <span class="n">similarity_score</span> <span class="o">=</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité (</span><span class="si">{}</span><span class="s2"> / </span><span class="si">{}</span><span class="s2">): </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">similarity_score</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <h4>Affichage</h4>
            <p class="codeexample">
                <code>
                <pre>
Similarité (dog / cat): ...
Similarité (dog / apple): ...
Similarité (cat / apple): ...
                </pre>

                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>4.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: vector</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>

<span class="c1"># Imprimer les vecteurs de chaque jeton sur une seule ligne</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteurs de &#39;</span><span class="si">{}</span><span class="s2">&#39; : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">,</span> <span class="n">vector_list</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>Word2Vec a marqué un tournant significatif dans la <b>représentation des mots</b> dans le domaine de
                l'apprentissage automatique.</p>
            <ul>
                <li>C'est une technique publiée en <b>2013</b> par une équipe de chercheurs dirigée par <b>Tomas
                        Mikolov</b> chez Google.</li>
                <li><b>Représentation vectorielle</b> : Word2Vec représente chaque mot distinct avec un vecteur dans un
                    espace continu. Ces vecteurs captent les relations sémantiques et syntaxiques entre les mots.</li>
                <li><b>Apprentissage basé sur un réseau neuronal</b> : Le modèle utilise un réseau neuronal pour
                    apprendre des associations de mots à partir d'un vaste corpus de texte. Cette approche permet de
                    capturer des nuances complexes dans la signification des mots.</li>
                <li><b>Entrée et sortie</b> : Word2Vec prend en entrée un large corpus de texte et produit un espace
                    vectoriel, généralement de plusieurs centaines de dimensions. Cette représentation vectorielle
                    permet de mesurer la similarité sémantique entre les mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>L'implémentation de Word2Vec se déroule en plusieurs étapes :</p>
            <ol>
                <li><b>Prétraitement des données</b> : Le texte est nettoyé et prétraité pour éliminer les éléments
                    indésirables tels que la ponctuation et les stopwords.</li>
                <li><b>Création d'un vocabulaire</b> : Les mots uniques du corpus sont utilisés pour construire un
                    vocabulaire. Chaque mot est ensuite associé à un index.</li>
                <li><b>Génération de paires mot-contexte</b> : Pour chaque mot du corpus, des paires mot-contexte sont
                    créées en utilisant une fenêtre contextuelle glissante. Ces paires servent d'exemples
                    d'entraînement.</li>
                <li><b>Construction du modèle Word2Vec</b> : Un modèle de réseau neuronal est créé, avec une couche
                    d'entrée représentant les mots, une couche cachée (skip-gram ou CBOW), et une couche de sortie pour
                    prédire le mot suivant dans le contexte.</li>
                <li><b>Entraînement du modèle</b> : Le modèle est entraîné sur les paires mot-contexte générées,
                    ajustant les poids du réseau pour minimiser la différence entre les prédictions et les vrais mots du
                    contexte.</li>
                <li><b>Obtention des embeddings</b> : Les vecteurs de mots appris pendant l'entraînement, appelés
                    embeddings, sont extraits. Chaque mot du vocabulaire est maintenant représenté par un vecteur dense
                    dans l'espace continu.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>4.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec</h3>
            <ul>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
                <li>une simple fonction mathématique (par exemple, la similarité cosinus entre les vecteurs) indique le
                    niveau de similarité sémantique entre les mots représentés par ces vecteurs \[\text{similarity} =
                    \cos(\theta) = {\mathbf{A} \cdot \mathbf{B}
                    \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i B_i} }{
                    \sqrt{\sum\limits_{i=1}^{n}{A_i^2}} \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} },\]
                </li>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <p>CBOW est un modèle spécifique de Word2Vec. Dans ce modèle, la prédiction du mot courant se fait en
                utilisant une fenêtre de mots contextuels voisins. L'ordre des mots de contexte n'influence pas la
                prédiction, ce qui en fait une approche robuste.</p>
            <ul>
                <li><b>Modèle prédictif</b> : CBOW prédit le mot cible en se basant sur le contexte qui l'entoure, mais
                    contrairement à d'autres modèles, l'ordre spécifique des mots dans ce contexte n'est pas pris en
                    compte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <ul>
                <li><b>Entrée</b> : La donnée d'entrée du modèle CBOW est une fenêtre de mots contextuels entourant le
                    mot cible. Cette fenêtre est définie par un paramètre appelé la taille de la fenêtre.</li>
                <li><b>Architecture</b> : CBOW utilise une architecture de réseau neuronal à une seule couche cachée. La
                    couche d'entrée représente les mots du contexte, et la couche de sortie représente le mot cible à
                    prédire.</li>
                <li><b>Entraînement</b> : Le modèle est entraîné en ajustant les poids du réseau pour minimiser la
                    différence entre les prédictions du modèle et le mot cible réel. Cela se fait à travers des
                    techniques d'optimisation comme la rétropropagation du gradient.</li>
                <li><b>Sortie</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les relations sémantiques entre les mots, permettant
                    ainsi de représenter chaque mot par un vecteur dans un espace continu.</li>
                <li><b>Avantages</b> : CBOW est souvent plus rapide à entraîner que d'autres modèles comme le Skip-gram
                    (une autre variante de Word2Vec) et peut être plus efficace dans des contextes où l'ordre séquentiel
                    des mots n'est pas critique.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle CBOW avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>4.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>

<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des deux mots les plus probables suivant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots suivant &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <p>Le modèle Skip-gram est une autre variante de Word2Vec qui se concentre sur la prédiction de la fenêtre
                voisine des mots de contexte à partir du mot courant. </p>
            <ul>
                <li><b>Objectif</b> : L'objectif principal du modèle Skip-gram est de prendre un mot source (le mot
                    courant) et de prédire les mots qui l'entourent dans une fenêtre de contexte donnée.</li>
                <li><b>Entrée</b> : Le mot source est utilisé comme donnée d'entrée du modèle, et la sortie souhaitée
                    est la distribution des probabilités des mots du contexte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <ul>
                <li><b>Architecture</b> : Skip-gram utilise une architecture de réseau neuronal à une seule couche
                    cachée. La couche d'entrée représente le mot source, et la couche de sortie représente les mots du
                    contexte.</li>
                <li><b>Entraînement</b> : Pendant l'entraînement, les poids du réseau sont ajustés pour minimiser la
                    différence entre les prédictions du modèle et la véritable distribution des mots du contexte. Cela
                    se fait généralement à l'aide de techniques d'optimisation comme la rétropropagation du gradient.
                </li>
                <li><b>Pondération du contexte</b> : Une caractéristique importante du modèle Skip-gram est que
                    l'architecture accorde plus de poids aux mots de contexte proches du mot source que ceux plus
                    éloignés. Cela permet de mieux capturer les relations sémantiques et syntaxiques locales.</li>
                <li><b>Embeddings</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les similitudes sémantiques entre les mots, permettant
                    de représenter chaque mot par un vecteur dans un espace continu.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle Skip-gram avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="c1"># sg: 1 pour skip-gram ; sinon CBOW.</span>
<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>4.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des mots les plus probables dans le contexte entourant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots dans le contexte de &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <p>La Reconnaissance d'Entités Nommées (NER) consiste à identifier et classer des entités spécifiques dans
                un texte. Ces entités peuvent inclure des personnes, des lieux, des organisations, des dates, des
                montants monétaires, etc. Le but est d'extraire des informations structurées à partir de données
                textuelles non structurées.</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/datarepresentation.svg"
                    height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <ul>
                <li><b>Identification d'entités</b> : La première étape de la NER consiste à identifier les mots ou
                    groupes de mots qui représentent des entités dans le texte. Ces entités peuvent être des noms de
                    personnes, des noms de lieux, des noms d'organisations, etc.</li>
                <li><b>Classification des entités</b> : Une fois les entités identifiées, elles sont classifiées dans
                    des catégories spécifiques. Par exemple, une entité peut être classée comme "PERSON" si elle
                    représente une personne, "LOCATION" si elle représente un lieu, "ORGANIZATION" si elle représente
                    une organisation, et ainsi de suite.</li>
                <li><b>Contextualisation</b> : La NER tient compte du contexte dans lequel une entité apparaît. Par
                    exemple, le mot "banc" peut être classé comme une entité financière dans le contexte d'une
                    discussion sur l'économie, mais comme une entité physique dans le contexte d'un parc.</li>
                <li><b>Relations entre entités</b> : Dans certains cas, la NER peut également inclure la détection des
                    relations entre différentes entités dans le texte. Par exemple, la relation entre une personne et
                    l'organisation qu'elle travaille.</li>
                <li><b>Applications pratiques</b> : Les résultats de la NER peuvent être utilisés dans diverses
                    applications, telles que l'amélioration de la recherche d'informations, l'extraction de relations,
                    la catégorisation de documents, la création de résumés automatiques, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <p>La Reconnaissance d'Entités Nommées (NER) est souvent réalisée à l'aide de modèles d'apprentissage
                automatique, et plusieurs algorithmes peuvent être utilisés dans ce contexte. Quelques-uns des
                algorithmes couramment employés :</p>
            <ul>
                <li><b>Modèles de markov cachés (HMM - Hidden Markov Models)</b> : Les HMM ont été utilisés pour la NER,
                    où l'idée est de modéliser la séquence des étiquettes d'entités en tant que séquence cachée derrière
                    la séquence observable de mots.</li>
                <li><b>Réseaux de neurones</b> : Les architectures de réseaux de neurones, y compris les réseaux de
                    neurones récurrents (RNN), les réseaux de neurones récurrents bidirectionnels (BiRNN), et les
                    réseaux de neurones récurrents à mémoire à court terme (LSTM), ont montré des performances
                    significatives dans la NER.</li>
                <li><b>Transformers</b> : Les modèles basés sur les transformers, tels que BERT (Bidirectional Encoder
                    Representations from Transformers) et ses variantes, ont considérablement amélioré les performances
                    en NER. Ces modèles sont pré-entraînés sur de grandes quantités de données textuelles et captent des
                    représentations contextuelles riches.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <ul>
                <li><b>Modèles statistiques traditionnels</b> : Des approches statistiques plus traditionnelles, comme
                    les modèles de séquence et les classificateurs basés sur des caractéristiques, ont également été
                    utilisées dans des scénarios où des quantités limitées de données annotées sont disponibles.</li>
                <li><b>Règles et expressions régulières</b> : Dans certains cas, des règles manuelles ou des expressions
                    régulières peuvent être utilisées pour extraire des entités spécifiques, surtout lorsque des motifs
                    clairs et récurrents peuvent être définis.</li>
                <li><b>Entraînement supervisé</b> : Les méthodes d'entraînement supervisé consistent à annoter
                    manuellement un ensemble de données avec des entités nommées, puis à entraîner un modèle sur ces
                    données annotées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Analyser le texte</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Afficher les informations sur les entités</span>
<span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">:</span>
    <span class="n">entity_text</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">text</span>
    <span class="n">start_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">start_char</span>
    <span class="n">end_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">end_char</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">label_</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entité: </span><span class="si">{}</span><span class="s2">, Début: </span><span class="si">{}</span><span class="s2">, Fin: </span><span class="si">{}</span><span class="s2">, Catégorie: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entity_text</span><span class="p">,
                 </span> <span class="n">start_char</span><span class="p">,</span> <span class="n">end_char</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
                <code>
                <pre>
Entité: Paris, Début: 0, Fin: 5, Catégorie: GPE
Entité: France, Début: 24, Fin: 30, Catégorie: GPE
Entité: 2015, Début: 35, Fin: 39, Catégorie: DATE
Entité: 2,206,488, Début: 72, Fin: 81, Catégorie: CARDINAL
                         </pre>
                </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">serve</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">, </span><span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>GPE</td>
                    <td>Pays, villes, états.</td>
                </tr>
                <tr>
                    <td>DATE</td>
                    <td>Dates ou périodes absolues ou relatives</td>
                </tr>
                <tr>
                    <td>CARDINAL</td>
                    <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p> Le lexique <b>VADER</b> (Valence Aware Dictionary and sEntiment Reasoner) est spécifiquement conçu pour
                analyser les sentiments dans du texte en attribuant des scores de positivité, négativité et neutralité
                aux mots ainsi qu'aux expressions.</p>
            <h4>Installation</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;vader_lexicon&#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER</h3>
            <p>VADER est une bibliothèque d'analyse de sentiment conçue pour évaluer le sentiment d'un morceau de texte,
                généralement une phrase ou un paragraphe.</p>
            <ul>
                <li><b>Dictionnaire et Scores :</b> VADER utilise un dictionnaire pré-annoté avec des scores de
                    positivité, négativité et neutralité pour des milliers de mots et expressions. Chaque mot est
                    associé à un score qui indique dans quelle mesure il est perçu comme positif ou négatif.</li>
                <li><b>Polarité des Mots :</b> Pour chaque mot dans le texte, VADER examine son score dans le
                    dictionnaire. Certains mots ont des scores forts, indiquant une polarité positive ou négative,
                    tandis que d'autres ont des scores plus neutres.</li>
                <li><b>Modificateurs et Emphase :</b> VADER prend en compte les modificateurs, tels que les adverbes,
                    qui peuvent influencer la polarité d'un mot. Il reconnaît également l'emphase en attribuant des
                    poids différents aux mots en majuscules.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER</h3>
            <ul>
                <li><b>Calcul du Score Composé :</b> VADER agrège les scores des mots en utilisant une formule qui prend
                    en compte la distribution des polarités dans le texte. Le score composé résultant est une mesure
                    globale du sentiment de la phrase.</li>
                <li><b>Résultats :</b> Le résultat final de l'analyse est un ensemble de scores qui indiquent la
                    positivité, la négativité, la neutralité et un score composé global. Ces scores sont normalisés dans
                    une échelle de -1 à 1, où -1 représente un sentiment extrêmement négatif, 1 représente un sentiment
                    extrêmement positif, et 0 représente la neutralité.</li>
            </ul>
            <p>VADER est souvent utilisé pour l'analyse de sentiment rapide et basée sur des règles. Bien qu'il soit
                efficace dans de nombreux cas, il peut ne pas être aussi précis que des méthodes plus complexes basées
                sur l'apprentissage automatique, notamment dans des contextes où l'analyse nécessite une compréhension
                plus profonde du langage et de la syntaxe.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER: Usage</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.sentiment.vader</span> <span class="kn">import</span> <span class="n">SentimentIntensityAnalyzer</span>
<span class="n">sia</span> <span class="o">=</span> <span class="n">SentimentIntensityAnalyzer</span><span class="p">()</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is not very good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is bad&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Les scores renvoyés par VADER représentent différentes mesures du sentiment dans un texte. Une
                explication de chaque score :</p>
            <ul>
                <li><b>Positivité (Positive Score)</b> : Ce score mesure la positivité relative du texte. Il indique
                    dans quelle mesure le texte contient des éléments positifs. Plus le score est élevé, plus le texte
                    est perçu comme positif.</li>
                <li><b>Négativité (Negative Score)</b> : - Ce score mesure la négativité relative du texte. Il indique
                    dans quelle mesure le texte contient des éléments négatifs. Plus le score est élevé, plus le texte
                    est perçu comme négatif.</li>
                <li><b>Neutralité (Neutral Score)</b> : Ce score mesure la neutralité relative du texte. Il indique dans
                    quelle mesure le texte est neutre, c'est-à-dire dépourvu d'éléments fortement positifs ou négatifs.
                    Plus le score est élevé, plus le texte est perçu comme neutre.</li>
                <li><b>Score Composé (Compound Score)</b> : Le score composé est une mesure agrégée du sentiment qui
                    prend en compte à la fois la positivité et la négativité du texte. Il combine les scores positif,
                    négatif et neutre en une seule valeur. Le score composé est souvent utilisé pour évaluer le
                    sentiment global du texte. Un score composé élevé indique un sentiment fort, qu'il soit positif ou
                    négatif, tandis qu'un score proche de zéro indique un texte neutre.</li>
            </ul>
            <p>Les scores sont normalisés dans une échelle de -1 à 1, où -1 représente un sentiment extrêmement négatif,
                1 représente un sentiment extrêmement positif, et 0 représente la neutralité. Les scores peuvent être
                interprétés individuellement ou conjointement pour obtenir une compréhension complète du sentiment dans
                le texte analysé.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}</br>
                        {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'compound': -0.3865}</br>
                        {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique</h3>
            <p>La traduction automatique est le processus d'utilisation de logiciels pour traduire un texte ou un
                discours d'une langue à une autre. Il existe plusieurs approches pour aborder ce problème complexe :</p>
            <ul>
                <li><b>Approche manuelle (règles)</b> : Cette approche repose sur des règles linguistiques définies
                    manuellement par des linguistes ou des experts en langues. Les règles peuvent inclure des
                    correspondances mot à mot, des règles de grammaire, et d'autres connaissances linguistiques
                    spécifiques.</li>
                <li><b>Approche statistique</b> : Dans cette approche, les modèles statistiques sont utilisés pour
                    apprendre les relations entre les phrases dans différentes langues à partir de grands ensembles de
                    données parallèles. Ces modèles peuvent être basés sur des probabilités conditionnelles et des
                    méthodes statistiques avancées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique</h3>
            <ul>
                <li><b>Approche hybride (règles et statistique)</b> : L'approche hybride combine des éléments des
                    approches manuelles et statistiques. Elle peut utiliser des règles pour des aspects spécifiques de
                    la traduction tout en tirant parti de l'apprentissage statistique pour d'autres parties du
                    processus.</li>
                <li><b>Apprentissage machine</b> : L'apprentissage machine, en particulier les modèles neuronaux, a
                    considérablement amélioré les performances de la traduction automatique. Les réseaux de neurones,
                    tels que les réseaux de neurones récurrents (RNN) et les transformers, ont montré des résultats
                    impressionnants en capturant des structures linguistiques complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche manuelle (règles)</h3>
            <p>L'approche manuelle dans la traduction automatique repose sur l'utilisation de règles linguistiques
                préalablement définies par des linguistes ou des experts en langues. Ces règles spécifient comment
                traduire des éléments spécifiques d'une langue source vers une langue cible.</p>
            <h4>Exemple : Traduction de phrases simples (anglais vers français)</h4>
            <p>Supposons que nous avons une règle manuelle qui dit que le mot "hello" en anglais doit être traduit par
                "bonjour" en français. De plus, nous avons une règle indiquant que la phrase "I love" doit être traduite
                par "j'aime". Ces règles sont simples et sont utilisées mot à mot.</p>
            <ul>
                <li><b>Application des règles manuelles</b> : "Hello" est traduit en "bonjour." "I love" est traduit en
                    "j'aime." "programming" n'a pas de règle spécifique, donc nous le laissons tel quel.</li>
                <li><b>Traduction résultante</b> : La phrase anglaise "Hello, I love programming" serait traduite en
                    français par "Bonjour, j'aime programming."</li>
            </ul>
            <p> Dans des situations réelles, les règles peuvent devenir extrêmement complexes, impliquant des contextes
                grammaticaux, des variations lexicales, des idiomes, etc.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
            <p>L'approche statistique de la traduction automatique repose sur l'utilisation de modèles statistiques pour
                apprendre les relations entre les phrases dans différentes langues à partir de grands ensembles de
                données parallèles. Contrairement à l'approche manuelle basée sur des règles linguistiques définies par
                des experts, l'approche statistique utilise des statistiques et des probabilités pour estimer la
                probabilité d'une traduction donnée.</p>
            <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
            <ul>
                <li><b>Ensembles de données parallèles</b> : Pour entraîner un modèle statistique, des ensembles de
                    données parallèles sont nécessaires. Ces ensembles contiennent des phrases dans la langue source et
                    leurs traductions correspondantes dans la langue cible.</li>
                <li><b>Alignement de phrases</b> : Les phrases équivalentes dans les deux langues sont alignées dans
                    l'ensemble de données parallèles. Cela crée des paires de phrases qui serviront de données
                    d'entraînement pour le modèle.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
            <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
            <ul>
                <li><b>Extraction de caractéristiques</b> : Des caractéristiques pertinentes sont extraites à partir des
                    paires de phrases alignées. Ces caractéristiques peuvent inclure des n-grammes (groupes de mots
                    consécutifs), des séquences de mots, des informations sur la syntaxe, etc.</li>
                <li><b>Entraînement du modèle</b> : Un modèle statistique, souvent basé sur des méthodes probabilistes
                    comme les modèles de langue conditionnels, est entraîné sur ces caractéristiques extraites. Le
                    modèle apprend les probabilités conditionnelles des traductions données les phrases sources.</li>
                <li><b>Estimation des probabilités</b> : Lors de la traduction d'une nouvelle phrase, le modèle estime
                    les probabilités des différentes traductions possibles en fonction des caractéristiques de cette
                    phrase.</li>
                <li><b>Sélection de la meilleure traduction</b> : La traduction avec la probabilité la plus élevée est
                    sélectionnée comme la traduction finale de la phrase source.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
            <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
            <ul>
                <li><b>Évaluation et ajustement</b> : Le modèle est évalué sur des ensembles de données de test pour
                    mesurer sa performance. Si nécessaire, le modèle peut être ajusté pour améliorer ses performances.
                </li>
            </ul>
            <p>Un exemple concret de cette approche pourrait être l'utilisation de modèles de langues conditionnels (par
                exemple, les modèles de Markov conditionnels) pour estimer la probabilité d'une traduction donnée une
                phrase source.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche Hybride (Règles et Statistique)</h3>
            <p>L'approche hybride dans la traduction automatique combine des éléments des approches manuelles (basées
                sur des règles) et des approches statistiques. L'idée est d'utiliser des règles linguistiques pour
                certaines parties de la traduction tout en tirant parti de modèles statistiques pour d'autres parties du
                processus. Cette combinaison vise à exploiter les avantages des deux approches pour obtenir des
                traductions de meilleure qualité.</p>
            <h4>Les étapes clés :</h4>
            <li><b>Règles linguistiques</b> : Des règles linguistiques sont définies manuellement pour certaines
                constructions grammaticales, expressions idiomatiques, ou autres aspects linguistiques spécifiques. Ces
                règles peuvent être élaborées par des experts linguistes.</li>
            <li><b>Ensembles de données parallèles</b> : Comme dans l'approche statistique, des ensembles de données
                parallèles contenant des phrases dans la langue source et leurs traductions dans la langue cible sont
                nécessaires.</li>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Approche Hybride (Règles et Statistique)</h3>
            <h4>Les étapes clés :</h4>
            <li><b>Apprentissage statistique</b> : Un modèle statistique est entraîné sur ces ensembles de données
                parallèles, en utilisant des méthodes statistiques pour apprendre les relations entre les phrases dans
                les deux langues.</li>
            <li><b>Application des règles</b> : Lors du processus de traduction, les règles linguistiques sont
                appliquées en priorité. Si une partie du texte correspond à une règle prédéfinie, la traduction basée
                sur la règle est utilisée.</li>
            <li><b>Utilisation du modèle statistique</b> : Pour les parties du texte qui ne correspondent pas à des
                règles prédéfinies, le modèle statistique est utilisé pour générer la traduction en se basant sur les
                relations apprises à partir des ensembles de données parallèles.</li>
            <li><b>Intégration des traductions partielles</b> : Les traductions générées par les règles et celles
                générées par le modèle statistique sont intégrées pour former la traduction finale du texte.</li>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
            <p>L'approche machine dans la traduction automatique fait référence à l'utilisation de techniques
                d'apprentissage machine, en particulier d'algorithmes d'apprentissage automatique, pour automatiser le
                processus de traduction entre langues. Cette approche a considérablement évolué avec le développement de
                modèles plus avancés basés sur l'apprentissage profond, notamment les modèles de séquence à séquence.
            </p>
            <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
            <ul>
                <li><b>Ensembles de données parallèles</b> : Des ensembles de données parallèles sont nécessaires,
                    contenant des phrases dans la langue source et leurs traductions correspondantes dans la langue
                    cible. Ces ensembles serviront de données d'entraînement pour le modèle.</li>
                <li><b>Modèle de séquence à séquence (Seq2Seq)</b> : Le modèle central dans cette approche est
                    généralement un modèle de séquence à séquence (Seq2Seq). Ce type de modèle utilise un réseau
                    neuronal récurrent (RNN) ou une architecture de transformer pour traiter des séquences de données.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
            <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
            <ul>
                <li><b>Encodage de la phrase source</b> : La phrase source est encodée en une représentation vectorielle
                    par le réseau neuronal. Cela capture les informations sémantiques et syntaxiques de la phrase
                    source.</li>
                <li><b>Décodage vers la langue cible</b> : Le modèle décode ensuite la représentation encodée pour
                    générer la séquence de mots dans la langue cible. C'est le processus de génération de la traduction.
                </li>
                <li><b>Entraînement supervisé</b> : Le modèle est entraîné sur les ensembles de données parallèles en
                    utilisant une approche supervisée. Il apprend à minimiser la différence entre la séquence générée et
                    la traduction attendue dans la langue cible.</li>
                <li><b>Optimisation</b> : Des techniques d'optimisation, telles que la descente de gradient stochastique
                    (SGD) ou des optimiseurs plus avancés comme Adam, sont utilisées pour ajuster les poids du modèle
                    pendant l'entraînement.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>4.6. Traduction automatique (Machine Translation)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
            <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
            <ul>
                <li><b>Évaluation et ajustement</b> : Le modèle est évalué sur des ensembles de données de test
                    indépendants pour mesurer sa performance. Des ajustements peuvent être effectués pour améliorer les
                    résultats.</li>
                <li><b>Incorporation de méthodes plus avancées</b> : Les modèles de traduction automatique basés sur
                    l'apprentissage machine peuvent être améliorés en incorporant des techniques plus avancées telles
                    que l'attention, qui permettent au modèle de se concentrer sur des parties spécifiques de la phrase
                    source lors de la génération de la traduction.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Transformer</h3>
            <p>Le Transformer est une architecture de réseau de neurones qui a révolutionné le domaine du traitement du
                langage naturel depuis son introduction par Vaswani et al. en 2017. Deux des modèles les plus célèbres
                basés sur l'architecture Transformer sont : </p>
            <ul>
                <li> BERT (Bidirectional Encoder Representations from Transformers)</li>
                <li> GPT (Generative Pre-trained Transformer)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT (Bidirectional Encoder Representations from Transformers) :</h3>
            <p> BERT, développé par Google, est un modèle pré-entraîné qui a été formé sur de vastes corpus de texte. Ce
                qui distingue BERT, c'est son approche bidirectionnelle pour la représentation des mots. Contrairement à
                des modèles précédents qui utilisaient une compréhension unidirectionnelle du contexte, BERT prend en
                compte le contexte à la fois avant et après un mot dans une phrase, améliorant ainsi la compréhension du
                sens.</p>
            <ul>
                <li><b>Utilisation</b> : BERT a été pré-entraîné sur des tâches telles que la prédiction de mots masqués
                    dans une phrase (Masked Language Model) et la prédiction de la relation entre deux phrases (Next
                    Sentence Prediction). Ces pré-entraînements permettent à BERT d'acquérir une compréhension profonde
                    du langage.</li>
                <li><b>Applications</b> : BERT est souvent utilisé comme base pour des tâches spécifiques telles que la
                    classification de texte, l'extraction d'entités nommées, la compréhension de texte, etc. Des
                    versions pré-entraînées de BERT sont disponibles, et les modèles peuvent être fine-tunés pour des
                    tâches spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : composants</h3>
            <p> Un modèle BERT (Bidirectional Encoder Representations from Transformers) est composé de plusieurs
                éléments clés, reflétant l'architecture Transformer. Les composants principaux d'un modèle BERT :
            </p>
            <ul>
                <li><b>Embedding token</b> : Cette couche transforme les tokens (mots ou sous-mots) d'une séquence en
                    vecteurs d'embedding. Chaque token est représenté par un vecteur qui capture son sens sémantique.
                    Ces embeddings peuvent également inclure des informations de position pour indiquer la position de
                    chaque token dans la séquence.</li>
                <li><b>Embedding de segment</b> : BERT prend en compte le contexte global d'une séquence, même lorsque
                    celle-ci contient plusieurs phrases. Pour ce faire, une couche d'embedding de segment est utilisée.
                    Elle attribue un segment spécifique à chaque token pour indiquer à quel segment il appartient. Cela
                    permet au modèle de distinguer différentes parties de la séquence.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : composants</h3>
            <ul>
                <li><b>Encoder BERT (Transformateur)</b> : L'élément central de BERT est l'encodeur, qui suit
                    l'architecture du Transformer. L'encodeur est composé de plusieurs couches d'attention multi-têtes.
                    Chaque couche prend en compte les relations entre les tokens et utilise l'attention pour attribuer
                    des poids aux différents tokens en fonction de leur importance contextuelle.</li>
                <li><b>Attention multi-tête</b> : Chaque couche d'attention contient plusieurs "têtes" d'attention.
                    Chaque tête capture des aspects différents des relations entre les tokens. L'utilisation de
                    plusieurs têtes permet au modèle de capturer des relations complexes dans le contexte.</li>
                <li><b>Couche de pooling</b> : BERT utilise souvent une couche de pooling pour agréger les
                    représentations de tous les tokens en une seule représentation. Cette représentation agrégée peut
                    être utilisée pour des tâches spécifiques, comme la classification de texte.</li>
                <li><b>Couche de classification</b> : Pour les tâches de classification, une couche de classification
                    est ajoutée au-dessus du modèle BERT. Cette couche peut consister en une ou plusieurs couches denses
                    qui projettent la représentation agrégée sur l'espace de sortie de la tâche.</li>
                <li><b>Fine-tuning et couches spécifiques à la tâche</b> : Lorsque BERT est fine-tuné pour une tâche
                    spécifique, des couches supplémentaires peuvent être ajoutées pour adapter le modèle à la tâche en
                    question. Cela peut inclure des couches de classification, des couches de régression, ou d'autres
                    couches spécifiques à la sortie souhaitée.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
                <a class="next" href="#slide91"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide91">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : fonctionnement</h3>
            <ul>
                <li><b>Prétraitement des données</b> : Avant d'entraîner ou d'utiliser BERT, les données doivent être
                    prétraitées. Cela inclut la tokenisation, où les phrases sont divisées en tokens (mots ou
                    sous-mots), et l'ajout de tokens spéciaux, tels que [CLS] (pour le début de la phrase) et [SEP]
                    (pour la séparation entre les phrases). De plus, des embeddings de segment peuvent être ajoutés pour
                    indiquer à quel segment appartient chaque token, ce qui est utile pour les tâches de compréhension
                    de texte.</li>
                <li><b>Architecture du modèle BERT</b> : BERT utilise une architecture Transformer avec un encodeur
                    bidirectionnel. La partie "bidirectionnelle" signifie que le modèle prend en compte le contexte à la
                    fois avant et après chaque mot dans une phrase, ce qui améliore la compréhension du sens.</li>
                <li><b>Pré-entraînement</b> : BERT est pré-entraîné sur de grandes quantités de données textuelles non
                    annotées. Il est formé à prédire les mots masqués dans une séquence (Masked Language Model, MLM) et
                    à prédire la relation entre deux phrases (Next Sentence Prediction, NSP). Le modèle apprend ainsi
                    une représentation profonde et contextuelle du langage.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">91
                <a class="prev" href="#slide90"></a>
                <a class="next" href="#slide92"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide92">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT : fonctionnement</h3>
            <ul>
                <li><b>Fine-tuning</b> : Après le pré-entraînement, BERT peut être fine-tuné sur des tâches spécifiques.
                    Par exemple, pour la classification de texte, une couche de classification peut être ajoutée
                    au-dessus de la représentation BERT, et le modèle peut être entraîné sur des données annotées pour
                    la tâche spécifique.</li>
                <li><b>Utilisation du modèle fine-tuné</b> : Une fois fine-tuné, le modèle BERT peut être utilisé pour
                    effectuer des tâches spécifiques, telles que la classification de texte, la reconnaissance d'entités
                    nommées, ou d'autres tâches de traitement du langage naturel.</li>
                <li><b>Gestion de la longueur des séquences</b> : BERT a une limitation sur la longueur maximale des
                    séquences qu'il peut traiter. Pour les textes plus longs, des techniques comme le fractionnement du
                    texte ou le choix d'une sous-séquence significative peuvent être utilisées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">92
                <a class="prev" href="#slide91"></a>
                <a class="next" href="#slide93"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide93">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer)</h3>
            <p>GPT, développé par OpenAI, est un modèle basé sur l'architecture Transformer, mais avec une approche de
                génération de texte. GPT utilise un modèle de langage pré-entraîné qui a appris à prédire le mot suivant
                dans une séquence de mots. Il est capable de générer du texte cohérent et contextuellement approprié.
            </p>
            <ul>
                <li><b>Utilisation</b> : GPT est pré-entraîné sur un large corpus de texte, et la pré-entraînement vise
                    à enseigner au modèle la structure et les motifs du langage. Il peut ensuite être fine-tuné sur des
                    tâches spécifiques selon les besoins.</li>
                <li><b>Applications</b> : GPT est souvent utilisé pour des tâches de génération de texte, comme la
                    rédaction automatique de contenu, la complétion de texte, et d'autres applications où la création de
                    texte naturel est requise.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">93
                <a class="prev" href="#slide92"></a>
                <a class="next" href="#slide94"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide94">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : composants</h3>
            <p> Le modèle GPT (Generative Pre-trained Transformer) est composé de plusieurs éléments clés qui suivent
                l'architecture Transformer. Les composants principaux de GPT : </p>
            <ul>
                <li><b>Embedding token</b> : Tout comme dans BERT, GPT utilise une couche d'embedding pour convertir les
                    tokens (mots ou sous-mots) en vecteurs d'embedding. Chaque token est représenté par un vecteur qui
                    capture son sens sémantique.</li>
                <li><b>Positional encoding</b> : GPT prend en compte la position des mots dans une séquence. Pour ce
                    faire, une couche de Positional Encoding est ajoutée aux embeddings de token pour introduire des
                    informations de position.</li>
                <li><b>Encodeur-décodeur transformer</b> : Contrairement à BERT, GPT utilise une architecture
                    encodeur-décodeur basée sur le Transformer. L'encodeur capture les relations entre les tokens dans
                    une séquence, tandis que le décodeur est utilisé pour générer la séquence de sortie.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">94
                <a class="prev" href="#slide93"></a>
                <a class="next" href="#slide95"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide95">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : composants</h3>
            <ul>
                <li><b>Attention multi-tête</b> : Comme dans BERT, GPT utilise une attention multi-tête pour capturer
                    des relations complexes entre les mots. Chaque tête d'attention se concentre sur différents aspects
                    des relations entre les tokens.</li>
                <li><b>Couche de pooling (ou moyenne)</b> : GPT utilise souvent une couche de pooling pour agréger les
                    représentations de tous les tokens en une seule représentation. Cette représentation agrégée est
                    ensuite utilisée comme entrée pour le décodeur.</li>
                <li><b>Décodeur GPT</b> : Le décodeur est la partie du modèle qui génère la séquence de sortie. Il prend
                    la représentation agrégée en entrée et génère séquentiellement les tokens de sortie un par un.</li>
                <li><b>Fine-tuning</b> : Après le pré-entraînement, GPT peut être fine-tuné pour des tâches spécifiques
                    en ajoutant des couches spécifiques à la tâche. Ces couches supplémentaires sont souvent des couches
                    de classification ou de régression, selon la nature de la tâche.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">95
                <a class="prev" href="#slide94"></a>
                <a class="next" href="#slide96"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide96">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : fonctionnement</h3>
            <ul>
                <li><b>Pré-entraînement</b> : GPT est pré-entraîné sur un vaste corpus de texte non annoté. L'objectif
                    du pré-entraînement est d'apprendre une représentation riche et contextuelle du langage. Pendant
                    cette phase, le modèle apprend à prédire le mot suivant dans une séquence de mots. Il utilise une
                    architecture encodeur-décodeur basée sur le Transformer.</li>
                <li><b>Embedding et positional encoding</b> : Chaque mot dans une séquence est représenté par un vecteur
                    d'embedding. GPT prend également en compte la position de chaque mot dans la séquence en utilisant
                    une couche de Positional Encoding.</li>
                <li><b>Encodeur-décodeur transformer</b> : GPT utilise une architecture de Transformer où l'encodeur
                    capture les relations entre les mots dans une séquence, et le décodeur est utilisé pour générer la
                    séquence de sortie. Cependant, GPT est principalement utilisé de manière autoregressive, ce qui
                    signifie qu'il génère séquentiellement un mot à la fois en utilisant les mots précédemment générés
                    comme contexte.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">96
                <a class="prev" href="#slide95"></a>
                <a class="next" href="#slide97"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide97">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : fonctionnement</h3>
            <ul>
                <li><b>Attention multi-tête</b> : GPT utilise l'attention multi-tête pour capturer des relations
                    complexes entre les mots. Chaque tête d'attention se concentre sur différents aspects du contexte,
                    permettant au modèle de saisir des dépendances à long terme et des relations subtiles.</li>
                <li><b>Génération de texte</b> : Lors de la génération de texte, le modèle prend une séquence initiale
                    en entrée et génère séquentiellement les mots suivants. À chaque étape, le modèle utilise les mots
                    générés précédemment comme contexte pour prédire le mot suivant. Ce processus se répète jusqu'à ce
                    qu'une séquence complète soit générée.</li>
                <li><b>Fine-tuning pour des tâches spécifiques</b> : Après le pré-entraînement, GPT peut être fine-tuné
                    pour des tâches spécifiques en ajoutant des couches supplémentaires spécifiques à la tâche. Par
                    exemple, pour la classification de texte, des couches de classification peuvent être ajoutées
                    au-dessus du modèle pré-entraîné.</li>
                <li><b>Gestion de la longueur des séquences</b> : GPT peut gérer des séquences de longueur variable,
                    mais il a une limite pratique sur la longueur maximale de la séquence qu'il peut générer.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">97
                <a class="prev" href="#slide96"></a>
                <a class="next" href="#slide98"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide98">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT vs. GPT</h3>
            <table>
                <tr>
                    <th> Caractéristique </th>
                    <th> BERT </th>
                    <th> GPT </th>
                </tr>
                <tr>
                    <td> Objectif du Pré-entraînement </td>
                    <td> Prédiction bidirectionnelle des mots (MLM) et prédiction de relation entre deux phrases (NSP)
                    </td>
                    <td> Génération de texte autoregressive </td>
                </tr>
                <tr>
                    <td> Architecture </td>
                    <td> Encodeur bidirectionnel </td>
                    <td> Encodeur-décodeur avec orientation autoregressive </td>
                </tr>
                <tr>
                    <td> Utilisation en Fine-Tuning </td>
                    <td> Classification de texte, extraction d'entités nommées, etc. </td>
                    <td> Génération de texte, complétion automatique, etc. </td>
                </tr>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">98
                <a class="prev" href="#slide97"></a>
                <a class="next" href="#slide99"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide99">
        <div class="header">
            <h1>4.7. Modèles de langage Transformer</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">BERT vs. GPT</h3>
            <table>
                <tr>
                    <th> Caractéristique </th>
                    <th> BERT </th>
                    <th> GPT </th>
                </tr>
                <tr>
                    <td> Approche du Contexte </td>
                    <td> Bidirectionnelle, prend en compte le contexte avant et après chaque mot </td>
                    <td> Autoregressive, génère du texte séquentiellement en utilisant le contexte précédent </td>
                </tr>
                <tr>
                    <td> Applications Pratiques </td>
                    <td> Classification, extraction d'entités, détection de paraphrases </td>
                    <td> Génération de texte, complétion automatique, conversation naturelle </td>
                </tr>
                <tr>
                    <td> Taille des Modèles </td>
                    <td> Généralement plus petits </td>
                    <td> Souvent plus grands, surtout les versions plus récentes comme GPT-3 </td>
                </tr>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">99
                <a class="prev" href="#slide98"></a>
                <a class="next" href="#slide100"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide100">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <p>Le système de recommandation est un domaine essentiel de l'informatique qui vise à réduire la surcharge
                d'informations en fournissant des suggestions filtrées et pertinentes aux utilisateurs.</p>
            <p><b>Réduction de la surcharge d'informations</b> : Les systèmes de recommandation visent à aider les
                utilisateurs à naviguer à travers une grande quantité d'informations en fournissant des recommandations
                ciblées et adaptées à leurs préférences.</p>
            <h4>Fonctionnement</h4>
            <ul>
                <li><b>Prédire la préférence de l'utilisateur</b> : Les systèmes de recommandation utilisent des
                    algorithmes pour analyser le comportement passé de l'utilisateur, ses préférences, et d'autres
                    données pertinentes afin de prédire les éléments qui pourraient l'intéresser à l'avenir.</li>
                <li><b>Gestion de la surcharge d'informations</b> : En filtrant et en triant les informations, ces
                    systèmes aident à éviter la surcharge cognitive en présentant à l'utilisateur uniquement ce qui est
                    le plus susceptible de l'intéresser.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">100
                <a class="prev" href="#slide99"></a>
                <a class="next" href="#slide101"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide101">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <h4>Types de Recommandations</h4>
            <ul>
                <li><b>Recommandations personnalisées</b> : Basées sur l'historique et les préférences individuelles de
                    l'utilisateur.</li>
                <li><b>Recommandations non personnalisées</b> : Générales et applicables à un large groupe
                    d'utilisateurs.</li>
            </ul>
            <h4>Algorithmes couramment utilisés</h4>
            <ul>
                <li><b>Filtrage collaboratif</b> : Basé sur les comportements et les préférences d'utilisateurs
                    similaires.</li>
                <li><b>Filtrage basé sur le contenu</b> : Utilise des caractéristiques du produit ou de l'élément
                    lui-même pour faire des recommandations.</li>
                <li><b>Apprentissage profond</b> : Des techniques telles que les réseaux de neurones profonds peuvent
                    être utilisées pour modéliser des modèles complexes de préférences.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">101
                <a class="prev" href="#slide100"></a>
                <a class="next" href="#slide102"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide102">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Système de recommandation</h3>
            <h4>Applications</h4>
            <ul>
                <li><b>Générateurs de playlists</b> : Pour les services de vidéo et de musique, offrant des
                    recommandations de chansons basées sur le goût musical de l'utilisateur.</li>
                <li><b>Recommandations de produits</b> : Dans les plateformes de commerce électronique, suggérant des
                    articles basés sur les achats antérieurs ou les préférences.</li>
                <li><b>Recommandations de livres</b> : Sur les plateformes de vente de livres en ligne, proposant des
                    ouvrages similaires à ceux déjà appréciés.</li>
                <li><b>Recommandations de contenu sur les réseaux sociaux</b> : Proposant des publications, des amis ou
                    des groupes en fonction de l'activité passée et des préférences de l'utilisateur.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">102
                <a class="prev" href="#slide101"></a>
                <a class="next" href="#slide103"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide103">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
            <h4>Hypothèse</h4>
            <p>Les individus suivent souvent les recommandations des autres utilisateurs. Cela suppose que les
                préférences et les actions des utilisateurs peuvent être des indicateurs fiables pour recommander des
                articles ou des objets similaires à d'autres utilisateurs.</p>
            <h4>Sources des données : </h4>
            <ul>
                <li><b>Utilisateurs</b> : Les informations relatives aux utilisateurs, y compris leurs préférences,
                    comportements, et actions.</li>
                <li><b>Articles ou objets</b> : Les données concernant les articles ou objets à recommander. Cela peut
                    inclure des détails sur les caractéristiques des articles, leurs catégories, etc.</li>
                <li><b>Transactions</b> : Les interactions et transactions entre les utilisateurs et les articles. Cela
                    peut inclure des achats, des clics, des évaluations, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">103
                <a class="prev" href="#slide102"></a>
                <a class="next" href="#slide104"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide104">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
            <h4>Collecte des Préférences des Utilisateurs : </h4>
            <ul>
                <li><b>Préférences explicitement exprimées</b> : Les évaluations et les actions des utilisateurs qui
                    sont directement exprimées. Cela peut inclure les avis positifs et négatifs, les évaluations
                    numériques, etc.</li>
                <li><b>Interprétation des actions des utilisateurs</b> : Observation et interprétation des actions des
                    utilisateurs, en particulier dans le contexte de la navigation web. Cela pourrait inclure des
                    comportements tels que les pages visitées, le temps passé sur une page, les articles ajoutés au
                    panier, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">104
                <a class="prev" href="#slide103"></a>
                <a class="next" href="#slide105"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide105">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réalisation</h3>
            <h4>Méthodes de Collecte des Données : </h4>
            <ul>
                <li><b>Surveillance des activités utilisateurs</b> : Utilisation de technologies de suivi pour observer
                    et enregistrer les actions des utilisateurs, généralement dans le contexte d'une plateforme en
                    ligne.</li>
                <li><b>Systèmes de retour d'information</b> : Encouragement des utilisateurs à fournir des retours
                    d'information explicites sous forme d'évaluations, de commentaires, etc.</li>
                <li><b>Analyse des transactions</b> : Extraction d'informations à partir des transactions entre
                    utilisateurs et articles pour déduire les préférences et les comportements.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">105
                <a class="prev" href="#slide104"></a>
                <a class="next" href="#slide106"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide106">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Fonctions [Ricci 2011]</h3>
            <ul>
                <li><b>Augmenter le nombre d'articles vendus</b> : Les systèmes de recommandation visent à stimuler les
                    ventes en suggérant des articles pertinents aux utilisateurs, augmentant ainsi les opportunités
                    d'achat.</li>
                <li><b>Vendre des articles plus variés</b> : En diversifiant les recommandations, les systèmes cherchent
                    à élargir le choix des utilisateurs et à promouvoir la vente d'une gamme plus large d'articles.</li>
                <li><b>Augmenter la satisfaction des utilisateurs</b> : En fournissant des recommandations pertinentes
                    et adaptées aux préférences individuelles, les systèmes visent à accroître la satisfaction des
                    utilisateurs.</li>
                <li><b>Augmenter la fidélité des utilisateurs</b> : En offrant des expériences personnalisées et en
                    répondant aux besoins des utilisateurs, les systèmes cherchent à fidéliser les clients, les incitant
                    à revenir pour davantage d'achats.</li>
                <li><b>Mieux comprendre ce que veut l'utilisateur</b> : Les systèmes de recommandation sont conçus pour
                    comprendre les préférences des utilisateurs au fil du temps, améliorant ainsi la précision des
                    recommandations et la compréhension des besoins individuels.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">106
                <a class="prev" href="#slide105"></a>
                <a class="next" href="#slide107"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide107">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <ul>
                <li><b>Trouver de bons objets</b> : Fournir des recommandations pour des articles ou des objets qui
                    correspondent aux préférences individuelles de l'utilisateur.</li>
                <li><b>Trouver tous les bons articles</b> : Identifier de manière exhaustive tous les articles
                    pertinents en fonction des préférences spécifiques de l'utilisateur</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">107
                <a class="prev" href="#slide106"></a>
                <a class="next" href="#slide108"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide108">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <h4>Contexte et Variations de Recommandations</h4>
            <ul>
                <li><b>Annotation dans le contexte</b> : Intégrer des informations contextuelles dans les
                    recommandations pour les rendre plus pertinentes et adaptées à la situation actuelle.</li>
                <li><b>Recommander une séquence</b> : Proposer une séquence d'articles ou de contenus, tels que des
                    livres ou des vidéos, sur un sujet donné pour une expérience d'apprentissage ou de divertissement
                    cohérente.</li>
                <li><b>Recommander une combinaison</b> : Offrir des suggestions de combinaisons d'articles, comme un
                    itinéraire de voyage complet, pour répondre à des besoins complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">108
                <a class="prev" href="#slide107"></a>
                <a class="next" href="#slide109"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide109">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <h4>Navigation et consultation</h4>
            <ul>
                <li><b>Navigation (consultation)</b> : Faciliter la navigation et la consultation d'articles en
                    recommandant des éléments pertinents à mesure que l'utilisateur explore la plateforme.</li>
                <li><b>Trouver un système de recommandation crédible</b> : Identifier des systèmes de recommandation
                    réputés et fiables pour garantir des suggestions de qualité.</li>
                <li><b>Améliorer le profil</b> : Continuellement ajuster et améliorer le profil de l'utilisateur en
                    fonction de ses préférences changeantes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">109
                <a class="prev" href="#slide108"></a>
                <a class="next" href="#slide110"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide110">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
            <h4>Interaction Sociale</h4>
            <ul>
                <li><b>S'exprimer</b> : Permettre aux utilisateurs de s'exprimer en fournissant des retours et en
                    influençant les recommandations.</li>
                <li><b>Aider les autres</b> : Offrir des mécanismes pour que les utilisateurs puissent recommander des
                    articles à d'autres utilisateurs.</li>
                <li><b>Influencer les autres</b> : Permettre aux utilisateurs d'influencer les préférences d'autres
                    utilisateurs en partageant leurs recommandations.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">110
                <a class="prev" href="#slide109"></a>
                <a class="next" href="#slide111"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide111">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
            <ul>
                <li><b>Filtrage collaboratif</b>
                    <ul>
                        <li>Principe : basé sur les évaluations de plusieurs utilisateurs</li>
                        <li>Fonctionnement : Identifie des utilisateurs similaires à celui pour lequel la recommandation
                            est générée et propose des articles appréciés par ces utilisateurs similaires.</li>
                    </ul>
                </li>
                <li><b>Filtrage basé sur le contenu</b>
                    <ul>
                        <li>Principe : basé sur les profils des utilisateurs</li>
                        <li>Fonctionnement : Recommande des articles similaires à ceux que l'utilisateur a aimés par le
                            passé, en se basant sur les caractéristiques ou le contenu des articles.</li>
                    </ul>
                </li>
                <li><b>Démographiques</b>
                    <ul>
                        <li>Principe : le profil démographique de l'utilisateur, par exemple le lieu et la langue</li>
                        <li>Fonctionnement : Propose des recommandations en fonction des caractéristiques démographiques
                            de l'utilisateur.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">111
                <a class="prev" href="#slide110"></a>
                <a class="next" href="#slide112"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide112">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
            <ul>
                <li><b>Basé sur la connaissance</b>
                    <ul>
                        <li>Principe :des recommandations basées sur la connaissance du domaine </li>
                        <li>Fonctionnement : Utilise une compréhension approfondie du contenu ou du domaine pour
                            recommander des articles pertinents.</li>
                    </ul>
                </li>
                <li><b>Basé sur la communauté</b>
                    <ul>
                        <li>Principe :des recommandations basées sur les préférences des amis des utilisateurs </li>
                        <li>Fonctionnement : Identifie les goûts et les préférences des amis de l'utilisateur pour
                            proposer des recommandations similaires.</li>
                    </ul>
                </li>
                <li><b>Systèmes hybrides de recommandation [Gomez-Uribe 2016]</b>
                    <ul>
                        <li>Principe : Intégration de plusieurs approches.</li>
                        <li>Fonctionnement : Combiner différentes méthodes de recommandation pour tirer parti de leurs
                            avantages respectifs et fournir des suggestions plus précises et diversifiées.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">112
                <a class="prev" href="#slide111"></a>
                <a class="next" href="#slide113"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide113">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage collaboratif</h3>
            <p>Le filtrage collaboratif basé sur les transactions implique l'analyse des transactions entre utilisateurs
                et articles pour générer des recommandations. </p>
            <ul>
                <li><b>Transactions</b> : Les transactions font référence aux interactions ou aux actions effectuées par
                    les utilisateurs lorsqu'ils interagissent avec des articles, tels que des achats, des évaluations,
                    des clics, etc. Les informations sur les transactions sont collectées et utilisées comme base pour
                    comprendre les préférences des utilisateurs et générer des recommandations.</li>
                <li><b>Algorithmes: Règles de l'association</b>
                    <ul>
                        <li><b>Fonctionnement</b> : Les règles de l'association identifient des modèles de co-occurrence
                            dans les transactions. Par exemple, si les utilisateurs qui ont acheté l'article A ont
                            également tendance à acheter l'article B, une règle de l'association peut être établie entre
                            A et B.</li>
                        <li><b>Application</b> : Ces règles peuvent être utilisées pour recommander des articles à un
                            utilisateur en se basant sur les préférences d'autres utilisateurs ayant des transactions
                            similaires.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">113
                <a class="prev" href="#slide112"></a>
                <a class="next" href="#slide114"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide114">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage collaboratif</h3>
            <h4>Avantages</h4>
            <ul>
                <li><b>Personnalisation</b> : Les recommandations sont personnalisées en fonction des comportements
                    d'achat et des transactions passées de l'utilisateur.</li>
                <li><b>Découverte de modèles</b> : Permet de découvrir des modèles de comportement d'achat et
                    d'identifier des associations entre différents articles.</li>
            </ul>
            <h4>Limitations</h4>
            <ul>
                <li><b>Sparsité des données</b> : Si un utilisateur a effectué un nombre limité de transactions, les
                    recommandations peuvent être moins précises en raison de la sparsité des données.</li>
                <li><b>Problème du démarrage à froid</b> : Il peut y avoir des difficultés lorsqu'un nouvel utilisateur
                    effectue peu ou pas de transactions, entraînant des défis dans la génération de recommandations.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">114
                <a class="prev" href="#slide113"></a>
                <a class="next" href="#slide115"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide115">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <p>Le filtrage collaboratif basé sur le contenu repose sur la description des objets et le profil des
                intérêts de l'utilisateur. </p>
            <ul>
                <li><b>Description d'objet </b>
                    <ul>
                        <li>Les caractéristiques ou le contenu des objets (articles, produits, etc.) sont utilisés pour
                            décrire chaque élément de manière détaillée.</li>
                    </ul>
                <li><b>Profil d'intérêts de l'utilisateur</b>
                    <ul>
                        <li><b>Modèle des préférences</b> : Un modèle représentant les préférences de l'utilisateur basé
                            sur la description des objets qu'il a aimés par le passé.</li>
                        <li><b>Historique d'interactions</b> : L'historique des interactions de l'utilisateur avec le
                            système de recommandation est également utilisé pour ajuster le modèle au fil du temps.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">115
                <a class="prev" href="#slide114"></a>
                <a class="next" href="#slide116"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide116">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <h4>Algorithmes</h4>
            <ul>
                <li><b>Arbres de décision</b> : Les arbres de décision sont utilisés pour modéliser les préférences de
                    l'utilisateur en fonction des caractéristiques des objets. L'arbre est construit pour prendre des
                    décisions sur les recommandations en se basant sur ces caractéristiques.</li>
                <li><b>Méthodes du plus proche voisin</b> : Les méthodes du plus proche voisin comparent le profil
                    d'intérêts de l'utilisateur avec ceux d'autres utilisateurs pour trouver les plus similaires. Les
                    objets appréciés par ces utilisateurs similaires sont recommandés.</li>
                <li><b>Classificateurs linéaires</b> : Les classificateurs linéaires modélisent la relation entre les
                    caractéristiques des objets et les préférences de l'utilisateur de manière linéaire. Cela peut
                    inclure des modèles tels que la régression logistique.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">116
                <a class="prev" href="#slide115"></a>
                <a class="next" href="#slide117"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide117">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <h4>Avantages</h4>
            <ul>
                <li><b>Personnalisation</b> : Les recommandations sont personnalisées en fonction du profil d'intérêts
                    de l'utilisateur, ce qui les rend adaptées à ses goûts spécifiques.</li>
                <li><b>Gestion du problème du démarrage à froid</b> : Peut mieux gérer le problème du démarrage à froid
                    pour de nouveaux utilisateurs en se basant sur la description d'objets.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">117
                <a class="prev" href="#slide116"></a>
                <a class="next" href="#slide118"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide118">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
            <h4>Limitations</h4>
            <ul>
                <li><b>Dépendance à la description d'objet</b> : L'efficacité dépend de la qualité de la description des
                    objets, et certaines approches peuvent être sensibles à des descriptions incomplètes ou subjectives.
                </li>
                <li><b>Manque de diversité</b> : Peut avoir tendance à recommander des objets similaires à ceux que
                    l'utilisateur a aimés, limitant la diversité des suggestions.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">118
                <a class="prev" href="#slide117"></a>
                <a class="next" href="#slide119"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide119">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Systèmes hybrides [Gomez-Uribe 2016]</h3>
            <ul>
                <li><b>Filtrage basé sur le contenu et filtrage collaboratif</b> : Combiner ces approches peut compenser
                    les limitations individuelles, offrant des recommandations plus robustes qui tiennent compte à la
                    fois du contenu des objets et des comportements d'autres utilisateurs.</li>
                <li><b>Filtrage basé sur le contenu, filtrage collaboratif et démographique</b> : Cette approche hybride
                    prend en compte non seulement les préférences individuelles de l'utilisateur mais aussi des aspects
                    démographiques pour des recommandations plus contextualisées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">119
                <a class="prev" href="#slide118"></a>
                <a class="next" href="#slide120"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide120">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Systèmes hybrides</h3>
            <h4>Avantages</h4>
            <ul>
                <li><b>Meilleure précision</b> : En combinant différentes approches, les systèmes hybrides peuvent
                    fournir des recommandations plus précises et diversifiées.</li>
                <li><b>Gestion des limitations</b> : Compenser les limitations spécifiques de chaque approche
                    individuelle.</li>
                <li><b>Adaptabilité</b> : S'adapter à différents types d'utilisateurs et de contextes.</li>
                <li><b>Réduction du problème du démarrage à froid</b> : En intégrant des aspects démographiques, par
                    exemple, les systèmes hybrides peuvent mieux gérer le démarrage à froid pour de nouveaux
                    utilisateurs.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">120
                <a class="prev" href="#slide119"></a>
                <a class="next" href="#slide121"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide121">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Mesures de performance [Ziegler 2005, Ricci 2011]</h3>
            <ul>
                <li><b>Précision et efficacité [Beel 2013a]</b> : La capacité du système à fournir des recommandations
                    pertinentes et la rapidité avec laquelle le système génère des recommandations.</li>
                <li><b>Diversité</b> : La variété des recommandations fournies pour éviter la redondance et introduire
                    de nouveaux éléments.</li>
                <li><b>Persistance de la recommandation</b> : La cohérence des recommandations au fil du temps, offrant
                    une expérience utilisateur stable.</li>
                <li><b>Vie privée [Pu 2012]</b> : La protection et le respect de la vie privée des utilisateurs lors de
                    la collecte et de l'utilisation des données.</li>
                <li><b>Démographie des utilisateurs</b> : L'intégration de facteurs démographiques pour des
                    recommandations plus contextuelles.</li>
                <li><b>Robustesse (lutte contre la fraude) [Konstan 2012]</b> : La capacité à résister aux tentatives de
                    manipulation ou de fraude dans le système.</li>
                <li><b>Sérendipité</b> : La capacité à surprendre l'utilisateur en proposant des recommandations
                    inattendues mais appréciées.</li>
                <li><b>Confiance</b> : La fiabilité perçue du système, renforçant la confiance de l'utilisateur dans les
                    recommandations fournies.</li>
                <li><b>Étiquetage (recommandations organiques ou sponsorisées) [Beel 2013b]</b> : La transparence dans
                    la distinction entre les recommandations générées organiquement et celles sponsorisées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">121
                <a class="prev" href="#slide120"></a>
                <a class="next" href="#slide122"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide122">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
            <p>Les recommandations à haut risque se réfèrent à des domaines où les conséquences d'une recommandation
                incorrecte ou inappropriée peuvent être significatives.</p>
            <h4>Exemple</h4>
            <p><b>Assurance</b> : Les recommandations dans le domaine de l'assurance peuvent avoir des implications
                financières importantes. Par exemple, une recommandation inappropriée en matière d'assurance pourrait
                entraîner des conséquences financières négatives pour l'utilisateur.</p>
            <h4>Intégration des Capacités d'Explication</h4>
            <p>Les systèmes de recommandation à haut risque devraient intégrer des capacités d'explication, permettant
                de fournir des justifications claires pour chaque recommandation.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">122
                <a class="prev" href="#slide121"></a>
                <a class="next" href="#slide123"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide123">
        <div class="header">
            <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
            <h4>Avantages des explications</h4>
            <li><b>Justification</b> : Les explications fournissent une justification transparente du raisonnement
                derrière chaque recommandation.</li>
            <li><b>Participation des utilisateurs</b> : Les explications encouragent la participation des utilisateurs
                en les aidant à comprendre pourquoi une recommandation particulière a été faite.</li>
            <li><b>Éducation</b> : Les explications contribuent à l'éducation des utilisateurs en les informant sur les
                critères pris en compte par le système pour générer des recommandations.</li>
            <li><b>Acceptation</b> : Les explications améliorent l'acceptation des recommandations en fournissant une
                visibilité sur le processus de prise de décision du système.</li>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">123
                <a class="prev" href="#slide122"></a>
                <a class="next" href="#slide124"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide124">
        <div class="header">
            <h1>4.9. Représentation des connaissances et raisonnement</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
            <p>La Représentation des connaissances et raisonnement (KRR) constituent un domaine clé de l'intelligence
                artificielle. La KRR est largement utilisée dans des domaines tels que la planification, la
                représentation du langage naturel, la gestion des connaissances, les systèmes experts, etc.</p>
            <h4>Représentation des Connaissances</h4>
            <p>La représentation des connaissances implique la création d'une représentation lisible par machine de la
                connaissance d'un monde ou d'un domaine particulier. Cela vise à permettre aux systèmes informatiques de
                comprendre, interpréter et raisonner sur l'information.</p>
            <h5>Exemples</h4>
                <ul>
                    <li><b>Réseaux Sémantiques</b> : Utilisent des relations sémantiques pour connecter des entités et
                        représenter la connaissance.</li>
                    <li><b>Ontologies</b> : Définissent des concepts, des propriétés et des relations dans un domaine
                        spécifique.</li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">124
                <a class="prev" href="#slide123"></a>
                <a class="next" href="#slide125"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide125">
        <div class="header">
            <h1>4.9. Représentation des connaissances et raisonnement</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
            <h4>Raisonnement</h4>
            <p>Le raisonnement consiste à déduire de nouvelles informations à partir des connaissances existantes. C'est
                le processus par lequel les systèmes tirent des conclusions logiques.</p>
            <h4>Compromis entre expressivité et praticité</h4>
            <ul>
                <li><b>Expressivité</b> : La capacité à représenter une variété de connaissances de manière détaillée.
                </li>
                <li><b>Praticité</b> : La facilité d'utilisation et de manipulation de la représentation des
                    connaissances.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">125
                <a class="prev" href="#slide124"></a>
                <a class="next" href="#slide126"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide126">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <figure>
                <img src="../../2021/MachineLearning/Semantic_web_stack.svg" height="400vh" />
                <figcaption style="text-align:center">Semantic Web Stack
                    (https://commons.wikimedia.org/wiki/File:Semantic_web_stack.svg)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">126
                <a class="prev" href="#slide125"></a>
                <a class="next" href="#slide127"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide127">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <p>Le Semantic Web Stack, également connu sous le nom de pile sémantique, représente une série de
                technologies et de standards interconnectés qui ont été développés pour réaliser la vision du World Wide
                Web sémantique. Cette vision, initiée par Tim Berners-Lee, vise à rendre le contenu du web plus
                compréhensible par les machines en ajoutant une sémantique aux données, permettant ainsi une meilleure
                interopérabilité et une utilisation plus avancée des informations sur le web.</p>
            <p>Les principales couches de la pile sémantique (du bas vers le haut) :</p>
            <ul>
                <li><b>XML (eXtensible Markup Language)</b> : La base de la pile sémantique est constituée par XML, un
                    langage de balisage extensible qui permet de structurer et de représenter des données de manière
                    lisible par les machines.</li>
                <li><b>XML Namespaces</b> : Les espaces de noms XML sont utilisés pour éviter les conflits de noms entre
                    les éléments XML provenant de différentes sources.</li>
                <ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">127
                <a class="prev" href="#slide126"></a>
                <a class="next" href="#slide128"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide128">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <ul>
                <li><b>RDF (Resource Description Framework)</b> : RDF est un langage standardisé permettant de décrire
                    des ressources du web de manière à faciliter le partage et la réutilisation d'informations entre
                    applications. Il repose sur un modèle de graphe pour représenter les relations entre les entités.
                </li>
                <li><b>RDF Schema (RDFS)</b> : RDFS est une extension de RDF qui fournit des classes et des propriétés
                    permettant de définir la structure des données RDF, facilitant ainsi la création de schémas et de
                    taxonomies.</li>
                <li><b>OWL (Web Ontology Language)</b> : OWL est une extension de RDF qui permet de définir des
                    ontologies, c'est-à-dire des modèles de connaissances formels décrivant les relations entre les
                    concepts. Il offre une expressivité accrue par rapport à RDFS.</li>
                <li><b>SPARQL (SPARQL Protocol and RDF Query Language)</b> : - SPARQL est un langage de requête
                    permettant d'interroger des données RDF. Il offre une puissante capacité de recherche et
                    d'interrogation des informations sémantiques.</li>
                <ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">128
                <a class="prev" href="#slide127"></a>
                <a class="next" href="#slide129"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide129">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <ul>
                <li><b>RDFa et Microformats</b> : RDFa (RDF in attributes) et Microformats sont des approches pour
                    intégrer des données sémantiques dans des documents HTML, permettant aux agents intelligents
                    d'extraire des informations à partir de pages web.</li>
                <li><b>Linked Data</b> : Le concept de Linked Data encourage la publication de données structurées sur
                    le web, avec des liens entre les ensembles de données, facilitant la découverte et l'intégration des
                    informations.</li>
            </ul>
            <p>En utilisant cette pile sémantique, le World Wide Web sémantique vise à créer une infrastructure où les
                machines peuvent comprendre, interpréter et exploiter le contenu du web de manière plus avancée, ouvrant
                ainsi la porte à de nombreuses applications intelligentes et à une meilleure interconnexion des données.
            </p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">129
                <a class="prev" href="#slide128"></a>
                <a class="next" href="#slide130"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide130">
        <div class="header">
            <h1>4.10. Web sémantique (Semantic Web)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web sémantique</h3>
            <ul>
                <li><b>RIF (Rule Interchange Format)</b> : RIF fournit un cadre standard pour l'échange de règles entre
                    différentes applications. Il s'agit d'un langage formel permettant de spécifier des règles logiques,
                    ce qui est crucial pour l'automatisation de l'inférence et du raisonnement sur les données
                    sémantiques.</li>
                <li><b>Unifying Logic</b> : Cette couche englobe les différentes logiques formelles utilisées dans le
                    World Wide Web sémantique, y compris les logiques de description telles que OWL et les logiques de
                    règles comme celles représentées par RIF. L'objectif est de fournir un cadre logique unifié pour
                    décrire les connaissances.</li>
                <li><b>Proof and Trust (Preuve et Confiance)</b> : Cette couche implique la capacité de fournir des
                    preuves formelles pour les déclarations sémantiques. Elle intègre également des mécanismes de
                    confiance, permettant d'évaluer la fiabilité des sources d'informations et d'inférer la crédibilité
                    des données sémantiques.</li>
                <li><b>User Applications (Applications Utilisateur)</b> : Au sommet de la pile, nous avons les
                    applications utilisateur qui tirent parti des données sémantiques pour offrir des fonctionnalités
                    avancées. Cela pourrait inclure des applications d'analyse des sentiments, de recherche sémantique,
                    de recommandation personnalisée, etc.</li>
            </ul>
            <p></p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">130
                <a class="prev" href="#slide129"></a>
                <a class="next" href="#slide131"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide131">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles</h3>
            <p>Un moteur de règles, également connu sous le nom de moteur d'inférence, est un composant logiciel qui
                traite et applique un ensemble de règles et de contraintes définies.</p>
            <ul>
                <li><b>Règles :</b> Les règles sont des instructions conditionnelles qui décrivent les relations entre
                    différentes entités ou éléments dans un système. Chaque règle a une condition qui, si elle est
                    remplie, déclenche une action spécifique.</li>
                <li><b>Contraintes :</b> Les contraintes définissent des limitations ou des exigences spécifiques sur
                    les données ou les actions dans un système. Elles contribuent à garantir la cohérence et la
                    conformité des opérations.</li>
                <li><b>Traitement des données :</b> Le moteur de règles traite les données en fonction des règles
                    définies. Il examine l'état actuel du système et évalue si les conditions des règles sont
                    satisfaites.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">131
                <a class="prev" href="#slide130"></a>
                <a class="next" href="#slide132"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide132">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles</h3>
            <ul>
                <li><b>Évaluation des conditions :</b> Le moteur de règles évalue les conditions de chaque règle pour
                    déterminer si elles sont vraies ou fausses. Si une condition est vraie, la règle associée est
                    déclenchée.</li>
                <li><b>Action :</b> Lorsqu'une règle est déclenchée, une action spécifique est exécutée. Cette action
                    peut inclure la modification de données, la génération de notifications, ou d'autres opérations
                    définies dans la logique métier.</li>
                <li><b>Cohérence du système :</b> Le moteur de règles garantit la cohérence du système en appliquant les
                    règles de manière séquentielle et en veillant à ce que les modifications apportées respectent
                    l'ensemble des contraintes.</li>
                <li><b>Cycle d'évaluation :</b> Le moteur de règles peut fonctionner de manière répétée, évaluant en
                    continu l'état du système en fonction des règles. Cela permet une gestion dynamique et adaptative
                    des opérations en réponse aux changements dans le système.</li>
            </ul>
            <p>Les moteurs de règles sont largement utilisés dans divers domaines, tels que la gestion des workflows, la
                logique métier, les systèmes d'alerte, et d'autres applications où des règles spécifiques doivent être
                appliquées pour maintenir la cohérence et automatiser les processus. Ils offrent une approche
                déclarative pour spécifier la logique métier sans nécessiter une programmation explicite.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">132
                <a class="prev" href="#slide131"></a>
                <a class="next" href="#slide133"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide133">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles : Applications</h3>
            <ul>
                <li><b>Systèmes de gestion de workflow :</b> Un moteur de règles peut être utilisé pour orchestrer le
                    flux de travail dans une entreprise. Par exemple, il peut déclencher des étapes spécifiques en
                    fonction de certaines conditions, comme l'approbation d'une demande ou la disponibilité de
                    ressources.</li>
                <li><b>Systèmes d'alerte :</b> Dans le domaine des systèmes de surveillance, un moteur de règles peut
                    être employé pour déclencher des alertes en cas de dépassement de seuils prédéfinis. Cela pourrait
                    inclure des alertes de performance, de sécurité, ou d'autres métriques.</li>
                <li><b>Logique métier dans les applications :</b> Les applications métier complexes peuvent utiliser des
                    moteurs de règles pour gérer la logique métier. Par exemple, dans un système bancaire, des règles
                    peuvent être définies pour gérer les conditions d'octroi de prêts.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">133
                <a class="prev" href="#slide132"></a>
                <a class="next" href="#slide134"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide134">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles : Applications</h3>
            <ul>
                <li><b>Systèmes de gestion de la chaîne d'approvisionnement :</b> Les moteurs de règles peuvent être
                    intégrés aux systèmes de gestion de la chaîne d'approvisionnement pour automatiser des décisions
                    telles que la sélection des fournisseurs en fonction de critères prédéfinis.</li>
                <li><b>Systèmes de tarification dynamique :</b> Dans le commerce électronique, un moteur de règles peut
                    être utilisé pour ajuster dynamiquement les prix en fonction de divers facteurs tels que la demande
                    du marché, la disponibilité des produits, ou d'autres conditions commerciales.</li>
                <li><b>Systèmes de gestion des fraudes :</b> Les moteurs de règles sont souvent utilisés dans les
                    systèmes de détection de fraude. Ils peuvent déclencher des alertes lorsque des modèles de
                    comportement suspects sont détectés, aidant ainsi à prévenir les activités frauduleuses.</li>
                <li><b>Systèmes de traitement des demandes :</b> Les moteurs de règles peuvent être intégrés aux
                    systèmes de traitement des demandes pour automatiser la prise de décision en fonction de critères
                    prédéfinis, accélérant ainsi le processus global.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">134
                <a class="prev" href="#slide133"></a>
                <a class="next" href="#slide135"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide135">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles : Langages</h3>
            <ul>
                <li><b>Drools (langage Drools Rule Language - DRL) :</b> - Drools est un moteur de règles open source
                    basé sur le langage DRL. Il utilise une syntaxe déclarative pour définir des règles métier.</li>
                <li><b>RuleML (Rule Markup Language) :</b> - RuleML est un langage de balisage spécialement conçu pour
                    représenter des règles métier sous une forme lisible par machine.</li>
                <li><b>Jess (Java Expert System Shell) :</b> - Jess est un moteur de règles pour Java qui utilise son
                    propre langage basé sur le Lisp pour définir les règles.</li>
                <li><b>CLIPS (C Language Integrated Production System) :</b> - CLIPS est un système expert qui inclut un
                    moteur de règles. Il utilise un langage de programmation déclaratif pour spécifier des règles et des
                    faits.</li>
                <li><b>Rete :</b> - Rete est un algorithme utilisé dans plusieurs moteurs de règles, y compris CLIPS et
                    Drools. Il est conçu pour optimiser l'évaluation des règles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">135
                <a class="prev" href="#slide134"></a>
                <a class="next" href="#slide136"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide136">
        <div class="header">
            <h1>4.11. Moteur de règles (Rule Engines)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Moteur de règles : Logiciels</h3>
            <ul>
                <li><b>Drools :</b> Drools, mentionné ci-dessus, est un moteur de règles open source développé par Red
                    Hat. Il offre des fonctionnalités avancées pour la gestion de règles métier.</li>
                <li><b>IBM Operational Decision Manager (ODM) :</b> ODM est une solution d'IBM qui fournit un moteur de
                    règles pour automatiser et gérer les décisions métier dans les applications.</li>
                <li><b>Corticon :</b> Progress Corticon est une plateforme de gestion de règles métier qui permet aux
                    utilisateurs de modéliser, exécuter et optimiser des règles métier.</li>
                <li><b>InRule :</b> InRule est une plateforme de gestion de règles métier qui permet aux entreprises de
                    définir, gérer et automatiser les règles métier.</li>
                <li><b>Camunda :</b> Camunda propose un moteur de règles dans le cadre de son ensemble de solutions BPMN
                    (Business Process Model and Notation) open source.</li>
                <li><b>JBoss Rules (anciennement JRules) :</b> JBoss Rules, maintenant intégré à Drools, était un moteur
                    de règles développé par IBM avant d'être open source.</li>
                <li><b>Microsoft BizTalk Rules Engine :</b> Microsoft BizTalk Server inclut un moteur de règles qui
                    permet aux entreprises de définir et de gérer les règles métier dans leurs processus.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">136
                <a class="prev" href="#slide135"></a>
                <a class="next" href="#slide137"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide137">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Programmation logique</h3>
            <p>La programmation logique, en particulier la logique propositionnelle et la logique du premier ordre
                (FOL), joue un rôle fondamental dans le domaine de l'intelligence artificielle. </p>
            <ul>
                <li><b>Logique propositionnelle</b> : En IA, la logique propositionnelle est souvent utilisée pour
                    modéliser des systèmes de connaissance simples. Elle est adaptée pour représenter des faits, des
                    règles et des relations de manière formelle.</li>
                <li><b>Logique du premier ordre (FOL, logique des prédicats)</b> : La logique du premier ordre est
                    largement utilisée en IA pour modéliser des connaissances plus riches et des raisonnements plus
                    sophistiqués. Elle permet de décrire des entités, leurs attributs et les relations entre elles de
                    manière plus expressive.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">137
                <a class="prev" href="#slide136"></a>
                <a class="next" href="#slide138"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide138">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog</h3>
            <p>Prolog (Programming in Logic) est un langage de programmation déclaratif, basé sur la logique du premier
                ordre.</p>
            <ul>
                <li>Prolog est souvent qualifié de <b>langage de programmation déclaratif</b>, car les programmes Prolog
                    décrivent les relations entre les entités plutôt que de spécifier explicitement les étapes à suivre
                    pour atteindre un résultat.</li>
                <li>Prolog repose sur la <b>logique du premier ordre</b>, ce qui signifie qu'il permet de décrire des
                    relations logiques complexes à l'aide de prédicats, de variables et de quantificateurs. Les
                    programmes Prolog sont composés de faits (des déclarations vraies) et de règles (des relations
                    logiques).</li>
                <li>Prolog a été développé par <b>Alain Colmerauer</b> dans les années 1970 à l'Université de Marseille.
                </li>
                <li>Les programmes Prolog sont exprimés en termes de <b>relations</b>. Les faits décrivent des relations
                    qui sont toujours vraies, tandis que les règles décrivent des relations qui sont vraies dans
                    certaines conditions. L'exécution d'une requête en Prolog revient à chercher des solutions aux
                    relations spécifiées.</li>
                <li>Prolog a été largement utilisé dans des domaines tels que la <b>démonstration de théorèmes</b>, les
                    systèmes experts et le traitement du langage naturel. En raison de sa nature déclarative et de son
                    approche logique, Prolog est bien adapté pour représenter et résoudre des problèmes impliquant des
                    relations complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">138
                <a class="prev" href="#slide137"></a>
                <a class="next" href="#slide139"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide139">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: types de données</h3>
            <p>En Prolog, les types de données fondamentaux incluent les atomes, les nombres, les variables et les
                termes composés.</p>
            <h4>Atome :</h4>
            <p>Un atome est une chaîne de caractères qui représente un nom. Il commence généralement par une lettre
                minuscule et peut contenir des lettres, des chiffres et des caractères de soulignement. Les atomes sont
                utilisés pour représenter des constantes et des noms dans Prolog.</p>
            <div class="highlight">
                <pre><span></span><span class="nf">animal</span><span class="p">(</span><span class="s s-Atom">chien</span><span class="p">).</span>
<span class="nf">couleur</span><span class="p">(</span><span class="s s-Atom">rouge</span><span class="p">).</span>
</pre>
            </div>

        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">139
                <a class="prev" href="#slide138"></a>
                <a class="next" href="#slide140"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide140">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: types de données</h3>
            <h4>Nombres :</h4>
            <p>Prolog prend en charge les entiers et les nombres à virgule flottante comme types de données numériques.
            </p>
            <div class="highlight">
                <pre><span></span><span class="nf">age</span><span class="p">(</span><span class="s s-Atom">personne1</span><span class="p">,</span> <span class="mi">23</span><span class="p">).</span>
<span class="nf">prix</span><span class="p">(</span><span class="s s-Atom">livre1</span><span class="p">,</span> <span class="mf">19.99</span><span class="p">).</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">140
                <a class="prev" href="#slide139"></a>
                <a class="next" href="#slide141"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide141">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: types de données</h3>
            <h4>Variables :</h4>
            <p>Les variables sont des symboles qui représentent des valeurs inconnues. Elles commencent généralement par
                une lettre majuscule ou un soulignement.</p>
            <div class="highlight">
                <pre><span></span><span class="nf">personne</span><span class="p">(</span><span class="nv">X</span><span class="p">).</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">141
                <a class="prev" href="#slide140"></a>
                <a class="next" href="#slide142"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide142">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: types de données</h3>
            <h4>Terme Composé :</h4>
            <p>Les termes composés sont des structures de données complexes créées à partir de foncteurs (noms de
                termes) et d'arguments. Ils sont utilisés pour représenter des entités composées.</p>
            <div class="highlight">
                <pre><span></span><span class="nf">point</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">).</span>
<span class="nf">livre</span><span class="p">(</span><span class="nf">titre</span><span class="p">(</span><span class="s s-Atom">&#39;Introduction à Prolog&#39;</span><span class="p">),</span> <span class="nf">auteur</span><span class="p">(</span><span class="s s-Atom">&#39;John Doe&#39;</span><span class="p">)).</span>
</pre>
            </div>

            <p>Les termes composés peuvent également être utilisés pour représenter des listes, des arbres et d'autres
                structures de données complexes.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">142
                <a class="prev" href="#slide141"></a>
                <a class="next" href="#slide143"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide143">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: règles</h3>
            <p>En Prolog, les règles sont des clauses qui définissent des relations entre différents termes. La
                structure générale d'une règle est la suivante :</p>
            <code>
                          Tête : - Corps.
                        </code>
            <ul>
                <li><b>Tête</b> : La tête de la règle spécifie la relation que nous voulons définir. Elle est
                    généralement composée d'un seul prédicat.</li>
                <li><b>Corps</b> : Le corps de la règle contient une séquence de prédicats liés par des conjonctions (,
                    pour ET) et des disjonctions (; pour OU). Le corps de la règle spécifie les conditions sous
                    lesquelles la relation spécifiée dans la tête est vraie.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">143
                <a class="prev" href="#slide142"></a>
                <a class="next" href="#slide144"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide144">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: faits</h3>
            <p>En Prolog, une clause avec un corps vide est en effet appelée un fait.</p>
            <div class="highlight">
                <pre><span></span><span class="nf">personne</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">).</span>
<span class="nf">personne</span><span class="p">(</span><span class="s s-Atom">alice</span><span class="p">).</span>
</pre>
            </div>
            <p>Les faits sont des affirmations qui sont considérées comme vraies dans le monde que vous décrivez. Ces
                faits peuvent ensuite être utilisés dans des requêtes ou d'autres règles pour déduire des informations
                supplémentaires.</p>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">144
                <a class="prev" href="#slide143"></a>
                <a class="next" href="#slide145"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide145">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: Installation</h3>
            <p>GNU Prolog, également connu sous le nom de gprolog, est un compilateur Prolog basé sur le standard ISO
                Prolog.
                Il est conçu pour être utilisé sur différentes plates-formes, y compris Linux, Windows et d'autres
                systèmes d'exploitation.
            </p>
            <p>L'installation de gprolog sur une machine Ubuntu</p>
            <code>
                          $ sudo apt install gprolog
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">145
                <a class="prev" href="#slide144"></a>
                <a class="next" href="#slide146"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide146">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog</h3>
            <code>
$ prolog</br>
GNU Prolog 1.4.5 (64 bits)</br>
Compiled Feb  5 2017, 10:30:08 with gcc</br>
By Daniel Diaz</br>
Copyright (C) 1999-2016 Daniel Diaz</br>
| ?- [user].</br>
compiling user for byte code...</br>
personne(tom).</br>
personne(alice).</br>
</br>
user compiled, 2 lines read - 241 bytes written, 12239 ms</br>
(4 ms) yes</br>
| ?- </br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">146
                <a class="prev" href="#slide145"></a>
                <a class="next" href="#slide147"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide147">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
?- personne(X).</br>
</br>
X = tom ? </br>
</br>
yes</br>
| ?- cat(bob).</br>
</br>
no</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">147
                <a class="prev" href="#slide146"></a>
                <a class="next" href="#slide148"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide148">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
| ?- [user].                             </br>
compiling user for byte code...</br>
personne(tom).                           </br>
personne(alice).                         </br>
personnes(L) :- findall(X, personne(X), L).</br>
</br>
user compiled, 3 lines read - 490 bytes written, 10638 ms</br>
</br>
yes</br>
| ?- personnes(L).                         </br>
</br>
L = [tom,alice]</br>
</br>
yes</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">148
                <a class="prev" href="#slide147"></a>
                <a class="next" href="#slide149"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide149">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <code>
| ?- [user].              </br>
compiling user for byte code...</br>
friend(bob, alice).  </br>
friend(alice, kevin).</br>
friend(bob, thomas).                </br>
friend(bob, peter).  </br>
user compiled, 4 lines read - 486 bytes written, 77256 ms</br>
(10 ms) yes</br>
| ?- friend(bob, X).      </br>
</br>
X = alice ? a</br>
X = thomas</br>
X = peter</br>
(1 ms) yes</br>
                        </code>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">149
                <a class="prev" href="#slide148"></a>
                <a class="next" href="#slide150"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide150">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ cat friend.pl
<div class="highlight"><pre><span></span><span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">alice</span><span class="p">).</span>
<span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">alice</span><span class="p">,</span> <span class="s s-Atom">kevin</span><span class="p">).</span>
<span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">thomas</span><span class="p">).</span>
<span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">peter</span><span class="p">).</span>
<span class="nf">human</span><span class="p">(</span><span class="nv">X</span><span class="p">):-</span><span class="nf">friend</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="k">_</span><span class="p">).</span>
<span class="nf">human</span><span class="p">(</span><span class="nv">Y</span><span class="p">):-</span><span class="nf">friend</span><span class="p">(</span><span class="k">_</span><span class="p">,</span><span class="nv">Y</span><span class="p">).</span>
</pre>
        </div>
        </code>
        </pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">150
                <a class="prev" href="#slide149"></a>
                <a class="next" href="#slide151"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide151">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
compiling /home/user/friend.pl for byte code...
/home/user/friend.pl compiled, 4 lines read - 515 bytes written, 22 ms
| ?- friend(bob,alice).

true ?

yes
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">151
                <a class="prev" href="#slide150"></a>
                <a class="next" href="#slide152"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide152">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
| ?- human(X).
X = bob ? a
X = alice
X = bob
X = bob
X = alice
X = kevin
X = thomas
X = peter

yes
| ?-
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">152
                <a class="prev" href="#slide151"></a>
                <a class="next" href="#slide153"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide153">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ cat ancetre.pl
<div class="highlight"><pre><span></span><span class="cm">/* Faits : Déclaration des relations parents */</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">jane</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">jim</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">ann</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">bob</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">jim</span><span class="p">,</span> <span class="s s-Atom">pat</span><span class="p">).</span>

<span class="cm">/* Règle : X est l&#39;ancêtre de Y si X est le parent de Y ou si X est l&#39;ancêtre d&#39;un parent de Y. */</span>
<span class="nf">ancetre</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Y</span><span class="p">)</span> <span class="p">:-</span> <span class="nf">parent</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Y</span><span class="p">).</span>
<span class="nf">ancetre</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Y</span><span class="p">)</span> <span class="p">:-</span> <span class="nf">parent</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Z</span><span class="p">),</span> <span class="nf">ancetre</span><span class="p">(</span><span class="nv">Z</span><span class="p">,</span> <span class="nv">Y</span><span class="p">).</span>
</pre>
        </div>
        </code>
        </pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">153
                <a class="prev" href="#slide152"></a>
                <a class="next" href="#slide154"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide154">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file friend.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
/home/user/ancetre.pl compiled, 11 lines read - 1119 bytes written, 14 ms
| ?- ancetre(kevin,X).
X = jane ? a
X = jim
X = ann
X = bob
X = pat

no
| ?-
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">154
                <a class="prev" href="#slide153"></a>
                <a class="next" href="#slide155"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide155">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <div class="highlight">
                <pre><span></span><span class="cm">/* Faits : Déclaration des élèves et de leurs notes */</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">math</span><span class="p">,</span> <span class="mi">85</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">anglais</span><span class="p">,</span> <span class="mi">90</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">math</span><span class="p">,</span> <span class="mi">92</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">anglais</span><span class="p">,</span> <span class="mi">88</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">math</span><span class="p">,</span> <span class="mi">78</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">anglais</span><span class="p">,</span> <span class="mi">85</span><span class="p">).</span>

<span class="cm">/* Règle : Calcul de la moyenne des notes d&#39;un élève */</span>
<span class="nf">moyenne</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Moyenne</span><span class="p">)</span> <span class="p">:-</span>
    <span class="nf">findall</span><span class="p">(</span><span class="nv">Note</span><span class="p">,</span> <span class="nf">note</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="k">_</span><span class="p">,</span> <span class="nv">Note</span><span class="p">),</span> <span class="nv">Notes</span><span class="p">),</span>
    <span class="nf">length</span><span class="p">(</span><span class="nv">Notes</span><span class="p">,</span> <span class="nv">N</span><span class="p">),</span>
    <span class="nf">somme_liste</span><span class="p">(</span><span class="nv">Notes</span><span class="p">,</span> <span class="nv">Sum</span><span class="p">),</span>
    <span class="nv">Moyenne</span> <span class="o">is</span> <span class="nv">Sum</span> <span class="o">/</span> <span class="nv">N</span><span class="p">.</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">155
                <a class="prev" href="#slide154"></a>
                <a class="next" href="#slide156"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide156">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <div class="highlight">
                <pre><span></span>
<span class="cm">/* Prédicat auxiliaire : Calcul de la somme d&#39;une liste */</span>
<span class="nf">somme_liste</span><span class="p">([],</span> <span class="mi">0</span><span class="p">).</span>
<span class="nf">somme_liste</span><span class="p">([</span><span class="nv">X</span><span class="p">|</span><span class="nv">Xs</span><span class="p">],</span> <span class="nv">Sum</span><span class="p">)</span> <span class="p">:-</span>
    <span class="nf">somme_liste</span><span class="p">(</span><span class="nv">Xs</span><span class="p">,</span> <span class="nv">Reste</span><span class="p">),</span>
    <span class="nv">Sum</span> <span class="o">is</span> <span class="nv">X</span> <span class="o">+</span> <span class="nv">Reste</span><span class="p">.</span>

<span class="cm">/* Prédicat auxiliaire : Calcul de la somme d&#39;une liste */</span>
<span class="nf">somme_liste</span><span class="p">([],</span> <span class="mi">0</span><span class="p">).</span>
<span class="nf">somme_liste</span><span class="p">([</span><span class="nv">X</span><span class="p">|</span><span class="nv">Xs</span><span class="p">],</span> <span class="nv">Sum</span><span class="p">)</span> <span class="p">:-</span>
    <span class="nf">somme_liste</span><span class="p">(</span><span class="nv">Xs</span><span class="p">,</span> <span class="nv">Reste</span><span class="p">),</span>
    <span class="nv">Sum</span> <span class="o">is</span> <span class="nv">X</span> <span class="o">+</span> <span class="nv">Reste</span><span class="p">.</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">156
                <a class="prev" href="#slide155"></a>
                <a class="next" href="#slide157"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide157">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <div class="highlight">
                <pre><span></span>
<span class="cm">/* Règle : Affichage des élèves avec leur moyenne */</span>
<span class="nf">afficher_moyennes</span> <span class="o">:-</span>
    <span class="nf">setof</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Matiere</span><span class="s s-Atom">^</span><span class="nv">Note</span><span class="s s-Atom">^</span><span class="p">(</span><span class="nf">note</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Matiere</span><span class="p">,</span> <span class="nv">Note</span><span class="p">)),</span> <span class="nv">Eleves</span><span class="p">),</span>
    <span class="nf">member</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Eleves</span><span class="p">),</span>
    <span class="nf">moyenne</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Moyenne</span><span class="p">),</span>
    <span class="nf">format</span><span class="p">(</span><span class="s s-Atom">&#39;Élève: ~w, Moyenne: ~2f~n&#39;</span><span class="p">,</span> <span class="p">[</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Moyenne</span><span class="p">]),</span>
    <span class="s s-Atom">fail</span><span class="p">.</span>
<span class="s s-Atom">afficher_moyennes</span><span class="p">.</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">157
                <a class="prev" href="#slide156"></a>
                <a class="next" href="#slide158"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide158">
        <div class="header">
            <h1>4.12. Programmation logique et IA</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
            <pre>
			<code>
$ prolog --consult-file notes.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
/home/user/notes.pl compiled, 30 lines read - 3086 bytes written, 3 ms 
| ?- afficher_moyennes.
Élève: bob, Moyenne: 81.50
Élève: jane, Moyenne: 90.00
Élève: kevin, Moyenne: 87.50

(1 ms) no
| ?-
                        </code>
			</pre>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">158
                <a class="prev" href="#slide157"></a>
                <a class="next" href="#slide159"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide159">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Beel 2013a] Beel, Joeran, et al. “A Comparative Analysis of Offline and Online Evaluations and
                    Discussion of Research Paper Recommender System Evaluation.” Proceedings of the International
                    Workshop on Reproducibility and Replication in
                    Recommender Systems Evaluation, Association for Computing Machinery, 2013</li>
                <li>[Beel 2013b] Beel, Joeran, et al. “Sponsored vs. Organic (Research Paper) Recommendations and the
                    Impact of Labeling.” Research and Advanced Technology for Digital Libraries, edited by Trond Aalberg
                    et al., Springer, 2013, pp. 391–95.</li>
                <li>[Chrupała 2006] Chrupała, Grzegorz. Simple Data-Driven Context-Sensitive Lemmatization. 2006.
                    doras.dcu.ie, http://www.unizar.es/departamentos/filologia_inglesa/sepln2006/.</li>
                <li>[Frakes 2003] Frakes, William B., and Christopher J. Fox. “Strength and Similarity of Affix Removal
                    Stemming Algorithms.” ACM SIGIR Forum, vol. 37, no. 1, Apr. 2003, pp. 26–30. Spring 2003</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">159
                <a class="prev" href="#slide158"></a>
                <a class="next" href="#slide160"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide160">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Gomez-Uribe 2016] Gomez-Uribe, Carlos A., and Neil Hunt. “The Netflix Recommender System:
                    Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems,
                    vol. 6, no. 4, Dec. 2016, p. 13:1–13:19. January
                    2016 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short
                    Papers -
                    Volume 2, Association for Computational Linguistics, 2012, pp. 368–372.</li>
                <li>[Gesmundo 2012] Gesmundo, Andrea, and Tanja Samardžić. “Lemmatisation as a Tagging Task.”
                <li>[Herlocker 2000] Herlocker, Jonathan L., et al. “Explaining Collaborative Filtering
                    Recommendations.” Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work,
                    Association for Computing Machinery, 2000, pp. 241–250.
                    ACM
                </li>
                <li>[Konstan 2012] Konstan, Joseph A., and John Riedl. “Recommender Systems: From Algorithms to User
                    Experience.” User Modeling and User-Adapted Interaction, vol. 22, no. 1–2, Apr. 2012, pp. 101–123.
                </li>
                <li>[Màrquez 2000] Màrquez, Lluís, et al. “A Machine Learning Approach to POS Tagging.” Machine
                    Learning, vol. 39, no. 1, Apr. 2000, pp. 59–91.</li>
                <li>[Mikolov 2013] Mikolov, Tomas, et al. “Efficient Estimation of Word Representations in Vector
                    Space.” ArXiv:1301.3781 [Cs], Sept. 2013.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">160
                <a class="prev" href="#slide159"></a>
                <a class="next" href="#slide161"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide161">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Miller 1995] Miller, George A. “WordNet: A Lexical Database for English.” Communications of the
                    ACM, vol. 38, no. 11, Nov. 1995, pp. 39–41. Nov. 1995</li>
                <li>[Pazzani 2007] Pazzani, Michael J., and Daniel Billsus. “Content-Based Recommendation Systems.” The
                    Adaptive Web: Methods and Strategies of Web Personalization, edited by Peter Brusilovsky et al.,
                    Springer, 2007, pp. 325–41. </li>
                <li>[Porter 1980] Porter, M. F. “An Algorithm for Suffix Stripping.” Program, vol. 14, no. 3, Jan. 1980,
                    pp. 130–37. Emerald Insight</li>
                <li>[Pu 2012] Pu, Pearl, et al. “Evaluating Recommender Systems from the User’s Perspective: Survey of
                    the State of the Art.” User Modeling and User-Adapted Interaction, vol. 22, no. 4, Oct. 2012, pp.
                    317–55.
                </li>
                <li>[Ricci 2011] Ricci, Francesco, et al. “Introduction to Recommender Systems Handbook.” Recommender
                    Systems Handbook, edited by Francesco Ricci et al., Springer US, 2011, pp. 1–35. </li>
                <li>[Ziegler 2005] Ziegler, Cai-Nicolas, et al. “Improving Recommendation Lists through Topic
                    Diversification.” Proceedings of the 14th International Conference on World Wide Web, Association
                    for Computing Machinery, 2005, pp. 22–32.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">161
                <a class="prev" href="#slide160"></a>
                <a class="next" href="#slide162"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide162">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web</h3>
            <ul>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation">https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation</a>
                </li>
                <li><a href="https://fr.wikipedia.org/wiki/Racinisation">https://fr.wikipedia.org/wiki/Racinisation</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Intelligence_artificielle">https://fr.wikipedia.org/wiki/Intelligence_artificielle</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Programmation_logique">https://fr.wikipedia.org/wiki/Programmation_logique</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">https://en.wikipedia.org/wiki/Morphology_(linguistics)</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Word_embedding">https://en.wikipedia.org/wiki/Word_embedding</a>
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
                <li><a href="https://fr.wikipedia.org/wiki/Prolog">https://fr.wikipedia.org/wiki/Prolog</a></li>
                <li><a href="https://www.nltk.org/howto/stem.html">https://www.nltk.org/howto/stem.html</a></li>
                <li><a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></li>
                <li><a href="https://spacy.io/usage/spacy-101">https://spacy.io/usage/spacy-101</a></li>
                <li><a href="https://spacy.io/usage/visualizers">https://spacy.io/usage/visualizers</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">162
                <a class="prev" href="#slide161"></a>
                <a class="next" href="#slide163"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide163">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
            <div class="navigation">163
                <a class="prev" href="#slide162"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 162) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>