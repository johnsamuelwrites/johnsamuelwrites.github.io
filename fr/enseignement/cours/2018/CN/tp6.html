<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <meta http-equiv="Content-Language" content="en"/>
    <link rel="shortcut icon" href="../.../../2017/DataMining/images/logo/favicon.png"/>
    <title>Chimie et Numérique: John Samuel</title>
    <style type="text/css">
    body{
      background-color: #FFFFFF;
    }
    #sidebar {
      position: fixed;
      background-color: #1B80CF;
      top: 0;
      left: 0;
      bottom: 0;
      width:25vw;
    }
    #sidebar .title {
      position:relative;
      text-align: center;
      line-height: 5vmax;
      font-size: 1.4vmax;
      font-family: 'Arial';
      margin-top: 30vh;
    }
    #sidebar .title a:link,
    #sidebar .title a:visited{
     color: #FFFFFF;
     text-decoration:none;
    }
    .subtitle {
      top: 50vh;
      text-align: center;
      line-height: 1.3vmax;
      font-family: 'Arial';
      font-size: 1.5vmax;
      color: #FFFFFF;
    }
    a:link, a:visited {
     color:#1B80CF;
    }
    .subtitle a:link,
    .subtitle a:visited{
     color: #FFFFFF;
     text-decoration:none;
    }
    .licence {
      position:fixed;
      text-align: right;
      bottom:0;
      right:0;
    }
    .home {
     position:fixed;
     text-align: left;
     font-family: 'Arial';
     color: #D3D3D3;
     z-index:100;
     width:100%;
     background-color:#FFFFFF;
     top:0px;
     margin-bottom:10px;
     padding-bottom:10px;
    }
    .codeexample {
      background-color:#eeeeee;
    }
    .home ul{
      margin: 0;
      padding: 0;
      text-align: left;
      list-style:none;
    }
    .home li{
     position: relative;
     float: left;
     padding-top:15px;
     margin-right: 1em;
     font-family: 'Arial';
    }
    .home li:hover {
      display:block;
    }
    .home a:link,
    .home a:visited{
     color: #D3D3D3;
    }
    .home li:hover a:link,
    .home li:hover a:visited{
      text-decoration:none;
      padding:15px;
      color:#FFFFFF;
      background-color: #1B80CF;
    }
    .content {
     line-height: 1.8vmax;
     font-size: 1.2vmax;
     font-family: 'Arial';
     margin-top: 15vh;
     width:90%;
    }
    .content h2, h3, h4{
     color:#1B80CF;
    }
    .content a:link,
    .content a:visited{
     color: #1B80CF;
    }
    .content h2::before,
    .content h3::before{
       display: block;
       content : " ";
       visibility:hidden;
       height:50px;
       margin-top:-50px;
       pointer-events: none;
       background-color:#FFFFFF;
    }
    .content a:link,
    .content a:visited{
     color:#1B80CF;
    }
    .content li {
      margin:5px;
    }
    .exercise {
     margin-left:2vw;
    }
    .exercise p{
     margin-left:1vw;
    }
    .exercise img{
     width:100%;
    }
    .content a:link,
    .content a:visited{
     color:#1B80CF;
    }
    .home a:link,
    .home a:visited{
     color: #D3D3D3;
    }
    .page {
      width:65vw;
      height:100%;
      margin-left:25vw;
      overflow: auto;
      padding: 0 1em;
    }
    img {
     max-width:100%;
     max-height:100%;
    }
    /* Using same Jupyter CSS
     */
    .highlight  { background: #f8f8f8; }
    .highlight .c { color: #408080; font-style: italic } /* Comment */
    .highlight .err { border: 1px solid #FF0000 } /* Error */
    .highlight .k { color: #008000; font-weight: bold } /* Keyword */
    .highlight .o { color: #666666 } /* Operator */
    .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
    .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
    .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
    .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
    .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
    .highlight .k { color: #008000; font-weight: bold } /* Keyword */
    .highlight .s2 { color: #BA2121 } /* Literal.String.Double */
    .highlight .s1 { color: #BA2121 } /* Literal.String.Single */
    .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
    .highlight .nb { color: #008000 } /* Name.Builtin */
    .highlight .mb { color: #666666 } /* Literal.Number.Bin */
    .highlight .mf { color: #666666 } /* Literal.Number.Float */
    .highlight .mh { color: #666666 } /* Literal.Number.Hex */
    .highlight .mi { color: #666666 } /* Literal.Number.Integer */
    .highlight .mo { color: #666666 } /* Literal.Number.Oct */
    @media (max-width: 640px), screen and (orientation: portrait) {
      body {
        max-width:100%;
        max-height:100%;
      }
      #sidebar {
        position: fixed;
        background-color: #1B80CF;
        top: 0;
        left: 0;
        bottom: 80vh;
        width:100vw;
      }
      #sidebar .title {
        text-align: center;
        position: fixed;
        margin-top: 6vh;
        left:0px;
        right:0px;
        line-height: 3.5vmax;
        font-size: 1.5vmax;
        font-family: 'Arial';
      }
      #sidebar .subtitle {
        text-align:center;
        top: 5vh;
        left:0px;
        right:0px;
        position: fixed;
        margin-top: 10vh;
        font-size: 1.5vmax;
      }
      #sidebar .title a:link,
      #sidebar .title a:visited{
        text-align:center;
        color:#FFFFFF;
      }
      #sidebar .subtitle a:link,
      #sidebar .subtitle a:visited{
        text-align:center;
        color:#FFFFFF;
      }
      .home{
        z-index:100;
        width:100%;
        background-color:#1B80CF;
        font-size:1.5vmax;
      }
      .home a:link,
      .home a:visited{
        text-decoration:none;
        color:#FFFFFF;
      }
      .content {
        line-height: 3.8vmax;
        font-size: 1.8vmax;
        font-family: 'Arial';
        margin-top:22vh;
      }
      .content a:link,
      .content a:visited{
        color:#1B80CF;
      }
      .page {
        top: 40vh;
        width:95%;
        margin-left:0vw;
      }
      .page img {
        max-width:100%;
        max-height:100%;
        border:0;
      }
    }

    </style>
  </head>
  <body vocab="http://schema.org/">
    <div id="sidebar">
     <div class="title">
      <h1><a href="./index.html">Travaux Pratiques</a></h1>
     </div>
     <div class="subtitle">
      <h3><a href="../../../../about.html">John Samuel</a></h3>
     </div>
    </div>
    <div class="licence"><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="../../../../../images/license.png"/></a>
    </div>
    <div class="page">
      <div class="home">
       <ul typeof="BreadcrumbList">
        <li property="itemListElement" typeof="ListItem">
          <a property="item" typeof="WebPage" href="../../../../index.html">
            <span property="name">Accueil</span>
          </a>
        </li>
        <li property="itemListElement" typeof="ListItem">
         <a property="item" typeof="WebPage" href="index.html">
          <span property="name">Chimie et Numérique</span>
         </a>
        </li>
        <li property="itemListElement" typeof="ListItem">
         <a property="item" typeof="WebPage" href="../../../index.html">
          <span property="name">Enseignement</span>
         </a>
        </li>
       </ul>
      </div>
      <div class="content">
        <h3>Les fondamentaux des sciences du numérique pour les chimistes</h3>
        <h3>Objectifs</h3>
        <ol>
          <li>Continue working with <a href="http://scikit-learn.org/stable/modules/clustering.html">clustering</a> and <a href="http://scikit-learn.org/stable/modules/svm.html">classification</a> algorithms</li>
          <li>Work on <a href="http://scikit-learn.org/stable/modules/linear_model.html">linear regression models</a></li>
          <li>Start working on <a href="http://scikit-learn.org/stable/modules/neural_networks_supervised.html">neural network models</a> including single and multilayered perceptrons.</li>
          <li>Continue working on the <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender system</a></li>
        </ol>
        <h3>Evaluation</h3>
        <p>Chaque exercice a un niveau de difficulté. Les exercices faciles et de difficulté moyenne vous aident pour comprendre les fondamentaux. Il est recommandé de finir ces exercices avant de commencer les exercices difficiles. Le niveau de difficulté de l'exercice:</p>
        <ol>
          <li><span style="color:red">&#9733;</span>: Facile</li>
          <li><span style="color:red">&#9733;&#9733;</span>: Moyen</li>
          <li><span style="color:red">&#9733;&#9733;&#9733;</span>: Difficile</li>
        </ol>
        <h4>Installation</h4>
        <div class="exercise">
          <p>If needed, please refer <a href="./installation.html">installation</a> page. For today's exercise, we will continue with the libraries you have already installed.</p>
        </div>
        <h4>Exercice 6.1 <span style="color:red">&#9733;</span></h4>
        <div class="exercise">
          <p>During <a href="./practicals.html">practical session 2</a>, we saw a clustering algorithm called KMeans. We will try to get more clusters and also check the time taken by each of these algorithms.</p>
          <p>Let's start with some very simple exercises to experiment with the KMeans algorithm. Consider the following data and visualize it on a using a scatter plot.</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

          <p>Visually, it is quite evident that there are two clusters. But let's use KMeans algorithm to obtain the 2 clusters. We will first see the labels of our clustered data.</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">clusters</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clusters</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</pre></div>

          <p>Now, we will visualize the clusters using a scatter plot. We will use two colors for visually distinguishing them.</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">clusters</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;#ff0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#00ff00&quot;</span><span class="p">])</span>
    
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">clusters</span><span class="o">.</span><span class="n">labels_</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
          <p>What if we tried to obtain 4 clusters? Try running the following code, multiple times. Any observation? Try changing the value of <i>n_init</i> with higher values.</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="k">import</span> <span class="n">KMeans</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> 
              <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">],[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>

<span class="n">clusters</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">clusters</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;#ff0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#00ff00&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000ff&quot;</span><span class="p">,</span> <span class="s2">&quot;#ffff00&quot;</span><span class="p">])</span>
    
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">clusters</span><span class="o">.</span><span class="n">labels_</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
          <p>Now we will try obtaining clusters with some real data (reference: <a href="./citypopulation.json">citypopulation.json</a>, Source: Wikidata). It contains information concerning different cities of the world: city name, year of its foundation and its population in the year 2010. In the following code, we want to cluster population data and to observe whether there is any correlation between age and recent population (2010) statistics. In the following code, there is a commented line. You can un-comment it to try with different population numbers. Any observation? Why did we use LabelEncoder? What is its purpose?</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">pandas.io.json</span> <span class="k">import</span> <span class="n">json_normalize</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">LabelEncoder</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s1">&#39;citypopulation.json&#39;</span><span class="p">))</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">json_normalize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">le</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">dataframe</span><span class="p">[</span><span class="s1">&#39;cityLabel&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">dataframe</span><span class="p">[</span><span class="s1">&#39;cityLabel&#39;</span><span class="p">])</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span> <span class="p">{</span><span class="s2">&quot;year&quot;</span><span class="p">:</span><span class="s2">&quot;&lt;i4&quot;</span><span class="p">,</span> <span class="s2">&quot;cityLabel&quot;</span><span class="p">:</span><span class="s2">&quot;&lt;U200&quot;</span><span class="p">,</span> <span class="s2">&quot;population&quot;</span><span class="p">:</span><span class="s2">&quot;i&quot;</span><span class="p">})</span>
<span class="n">dataframe</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">dataframe</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1500</span><span class="p">]</span>
<span class="c1">#dataframe = dataframe.loc[dataframe[&#39;population&#39;] &lt; 700000]</span>
<span class="n">yearPopulation</span> <span class="o">=</span> <span class="n">dataframe</span><span class="p">[[</span><span class="s1">&#39;year&#39;</span><span class="p">,</span> <span class="s1">&#39;population&#39;</span><span class="p">]]</span>


<span class="n">clusters</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">clusters</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">yearPopulation</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;#ff0000&quot;</span><span class="p">,</span> <span class="s2">&quot;#00ff00&quot;</span><span class="p">,</span> <span class="s2">&quot;#0000ff&quot;</span><span class="p">,</span> <span class="s2">&quot;#ffff00&quot;</span><span class="p">])</span>
   

<span class="n">plot</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yearPopulation</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">],</span> <span class="n">yearPopulation</span><span class="p">[</span><span class="s1">&#39;population&#39;</span><span class="p">],
     </span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">clusters</span><span class="o">.</span><span class="n">labels_</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
          <p>Now let's continue working with <a href="../../2017/DataMining/images/flower.jpg">flower.jpg</a>. Let's start once again with <b>KMeans</b> and try to get clusters of size between 2 and 11.</p> 
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import KMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             fig, axes = plot.subplots(nrows=5, ncols=2, figsize=(20,25))<br/>
             <br/>
             xaxis = 0<br/>
             yaxis = 0<br/>
             for x in range(2, 12):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = KMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;npbins = numpy.arange(0, cluster_count + 1)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;histogram = numpy.histogram(clusters.labels_, bins=npbins)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;labels = numpy.unique(clusters.labels_)<br/>
             <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;barlist = axes[xaxis, yaxis].bar(labels, histogram[0])<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;if(yaxis == 0):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;else:<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  xaxis = xaxis + 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 0<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;for i in range(cluster_count):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  barlist[i].set_color('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[i][0]),<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      math.ceil(clusters.cluster_centers_[i][1]), math.ceil(clusters.cluster_centers_[i][2])))<br/>
             <br/>
             <br/>
             plot.show()<br/>
            </code>
          </p>
          <p>Your next goal is to test the above code for cluster sizes between 2 and 21 which will give you the figure given below.</p> 
          <p><span style='color:"red"'>Note:</span> The following image was generated after 6 minutes. Optionally, you can add <i>print</i> statements to test whether your code is working fine.</p>
          <p>
            <img style="width:50%" src="../../2017/DataMining/images/kmeans.png"></img>
          </p>
          <p>Now we modify the above algorithm to use <b>MiniBatchKMeans</b> clustering algorithm (refer <a href="http://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans">here</a>). Observe the changes.</p>
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import MiniBatchKMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             fig, axes = plot.subplots(nrows=5, ncols=2, figsize=(20,25))<br/>
             <br/>
             xaxis = 0<br/>
             yaxis = 0<br/>
             for x in range(2, 12):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;npbins = numpy.arange(0, cluster_count + 1)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;histogram = numpy.histogram(clusters.labels_, bins=npbins)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;labels = numpy.unique(clusters.labels_)<br/>
             <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;barlist = axes[xaxis, yaxis].bar(labels, histogram[0])<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;if(yaxis == 0):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;else:<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  xaxis = xaxis + 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 0<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;for i in range(cluster_count):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  barlist[i].set_color('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[i][0]),<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      math.ceil(clusters.cluster_centers_[i][1]), math.ceil(clusters.cluster_centers_[i][2])))<br/>
             <br/>
             <br/>
             plot.show()<br/>
            </code>
          </p>
          <p>What did you observe? Your next goal is to test the above code for cluster sizes between 2 and 21 which will give you the figure given below.</p> 
          <p>What are your conclusions?</p> 
          <p>
            <img style="width:50%" src="../../2017/DataMining/images/minibatchkmeans.png"></img>
          </p>
          <p>In order to compare the two algorithms, we consider the time taken by each of these algorithms. We will repeat the above experiment, but this time we will plot the time taken to obtain clusters of different sizes.</p> 
          <p>We start with <b>KMeans</b>.</p>
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import time<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import KMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             for x in range(1, 20):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;start_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = KMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;end_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;total_time = end_time - start_time<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;print("Total time: ", x, ":", total_time)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;X.append(x)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;Y.append(total_time)<br/>
             <br/>
             plot.bar(X, Y)<br/>
             plot.show()<br/>
            </code>
          </p>
          <p>You may get a graph similar to the following.</p>
          <p>
            <img style="width:50%" src="../../2017/DataMining/images/kmeanstime.png"></img>
          </p>
          <p>We now use <b>MiniBatchKMeans</b>.</p>
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import time<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import MiniBatchKMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             for x in range(1, 20):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;start_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;end_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;total_time = end_time - start_time<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;print("Total time: ", x, ":", total_time)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;X.append(x)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;Y.append(total_time)<br/>
             <br/>
             plot.bar(X, Y)<br/>
             plot.show()<br/>
            </code>
          </p>
          <p>You may get a graph similar to the following.</p>
          <p>
            <img style="width:40%" src="../../2017/DataMining/images/minibatchkmeanstime.png"></img>
          </p>
          <p>Now test the above code using <b>MiniBatchKMeans</b> algorithm with cluster sizes between 2 and 50. What are your observations? </p>
          <p>Finally we want to see whether we get the same cluster centers from both the algorithms. Run the following program to see the cluster centers produced by the two algorithms. We use two different colors (red and black) to distinguish the cluster centers from the two algorithms.</p>
          <p>
          <p class="codeexample">
            <code>
              from PIL import Image<br/>
              import numpy<br/>
              import math<br/>
              import matplotlib.pyplot as plot<br/>
              from sklearn.cluster import KMeans<br/>
              from sklearn.cluster import MiniBatchKMeans<br/>
              <br/>
              imgfile = Image.open("flower.jpg")<br/>
              numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
              <br/>
              cluster_count = 10<br/>
              <br/>
              clusters = KMeans(n_clusters = cluster_count)<br/>
              clusters.fit(numarray)<br/>
              <br/>
              mclusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
              mclusters.fit(numarray)<br/>
              <br/>
              fig, axes = plot.subplots(nrows=3, ncols=1, figsize=(20,25))<br/>
              #Scatter plot for RG (RGB)<br/>
              axes[0].scatter(numarray[:,0],numarray[:,1])<br/>
              axes[0].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,1], c='red')<br/>
              axes[0].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,1], c='black')<br/>
              <br/>
              #Scatter plot of RB (RGB)<br/>
              axes[1].scatter(numarray[:,0],numarray[:,2])<br/>
              axes[1].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,2], c='red')<br/>
              axes[1].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,2], c='black')<br/>
              <br/>
              #Scatter plot of GB (RGB)<br/>
              axes[2].scatter(numarray[:,1],numarray[:,2])<br/>
              axes[2].scatter(clusters.cluster_centers_[:,1], clusters.cluster_centers_[:,2], c='red')<br/>
              axes[2].scatter(mclusters.cluster_centers_[:,1], mclusters.cluster_centers_[:,2], c='black')<br/>
            </code>
          </p>
          <p>
            <img style="width:40%" src="../../2017/DataMining/images/scatterplots.png"></img>
          </p>
          <p>We would like to see how the individual pixel values have been clustered. Run the following program a couple of times.</p>
          <p class="codeexample">
            <code>
              from PIL import Image<br/>
              import numpy<br/>
              import math<br/>
              import time<br/>
              import matplotlib.pyplot as plot<br/>
              from sklearn.cluster import KMeans<br/>
              from sklearn.cluster import MiniBatchKMeans<br/>
              <br/>
              imgfile = Image.open("flower.jpg")<br/>
              numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
              <br/>
              cluster_count = 10<br/>
              <br/>
              mclusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
              mclusters.fit(numarray)<br/>
              <br/>
              npbins = numpy.arange(0, cluster_count + 1)<br/>
              histogram = numpy.histogram(mclusters.labels_, bins=npbins)<br/>
              labels = numpy.unique(mclusters.labels_)<br/>
              <br/>
              fig, axes = plot.subplots(nrows=3, ncols=2, figsize=(20,25))<br/>
              <br/>
              #Scatter plot for RG (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = mclusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(mclusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       math.ceil(mclusters.cluster_centers_[j][1]), 0))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[0,0].scatter(numarray[:,0],numarray[:,1], c=colors)<br/>
              axes[0,0].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,1], marker="+", c='red')<br/>
              <br/>
              #Scatter plot for RB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = mclusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(mclusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       0, math.ceil(mclusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[1,0].scatter(numarray[:,0],numarray[:,2], c=colors)<br/>
              axes[1,0].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,2], marker="+", c='white')<br/>
              <br/>
              #Scatter plot for GB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = mclusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (0, math.ceil(mclusters.cluster_centers_[j][1]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;        math.ceil(mclusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[2,0].scatter(numarray[:,1],numarray[:,2], c=colors)<br/>
              axes[2,0].scatter(mclusters.cluster_centers_[:,1], mclusters.cluster_centers_[:,2], marker="+", c='yellow')<br/>
              <br/>
              clusters = KMeans(n_clusters = cluster_count)<br/>
              clusters.fit(numarray)<br/>
              <br/>
              npbins = numpy.arange(0, cluster_count + 1)<br/>
              histogram = numpy.histogram(clusters.labels_, bins=npbins)<br/>
              labels = numpy.unique(clusters.labels_)<br/>
              <br/>
              #Scatter plot for RG (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = clusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       math.ceil(clusters.cluster_centers_[j][1]), 0))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[0,1].scatter(numarray[:,0],numarray[:,1], c=colors)<br/>
              axes[0,1].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,1], marker="+", c='red')<br/>
              <br/>
              #Scatter plot for RB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = clusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       0, math.ceil(clusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[1,1].scatter(numarray[:,0],numarray[:,2], c=colors)<br/>
              axes[1,1].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,2], marker="+", c='white')<br/>
              <br/>
              #Scatter plot for GB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = clusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (0, math.ceil(clusters.cluster_centers_[j][1]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;        math.ceil(clusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[2,1].scatter(numarray[:,1],numarray[:,2], c=colors)<br/>
              axes[2,1].scatter(clusters.cluster_centers_[:,1], clusters.cluster_centers_[:,2], marker="+", c='yellow')<br/>
              plot.show()<br/>
            </code>
          </p>
          <p>
            <img style="width:40%" src="../../2017/DataMining/images/kmeansminibatchcomparison.png"></img>
          </p>
          <p>What are your conclusions? </p>
        </div>
        <h4>Exercice 6.2 <span style="color:red">&#9733;</span></h4>
        <div class="exercise">
         <p>We will now work with <b>linear regression</b> (refer <a href="http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares">here</a>). </p>
         <p>Let's see some simple programs. In the following data, where we have some sample data for the equation: <i>y = x</i>. We will first train our Linear Regression model with a very small subset and test whether it is able to predict <i>y-</i>values for new <i>x-</i>values. </p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">]])</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1">#printing coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>

          <p>Next, we have some sample data for the equation: <i>y = x + 1</i>. We will train our Linear Regression model test whether it is able to predict <i>y-</i>values for new <i>x-</i>values. </p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1">#printing coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>
          <p>But, what if we tried the Linear Regression model for the equation: <i>y = x<sup>2</sup></i>. What did you observe with the following code? Did it predict the <i>y-</i>values correctly?</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">16</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">25</span><span class="p">]])</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1">#printing coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>

         <p>Now, Let's repeat the above experiment by making use of Polynomial features. Try changing the value of <i>degree</i> in the code given below. </p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">16</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">25</span><span class="p">]])</span>

<span class="c1">#using polynomial features</span>
<span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">x_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_poly</span><span class="p">,</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1">#printing coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_predict</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>
 
         <p>Now let's try with third order polynomial equation (cubic equation).</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="k">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">27</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">64</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span><span class="mi">125</span><span class="p">]])</span>

<span class="c1">#using polynomial features</span>
<span class="n">pf</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">x_poly</span> <span class="o">=</span> <span class="n">pf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">numarray</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_poly</span><span class="p">,</span> <span class="n">numarray</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1">#printing coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">pf</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_predict</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>
         <p>Download the file <a href="./population.csv">population.csv</a> (source: query given in <a href="./references.html">references</a>). We will first plot this multi-annual population.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset)<br/>
           df.plot(x='year', y='population', kind='scatter')<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="../../2017/DataMining/images/populationscatterplot.png"></img>
         </p>
         <p>We will focus on data starting from 1960 (why?). Our goal is to use regression techniques to predict population. But we don't know how to verify. So with the available data, we create two categories: training data and test data.</p>
         <p>Now continuing with the population data (from TP1 and TP2), we split it into two: training data and test data. We will plot the actual population values and the predicted values.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           from sklearn.linear_model import LinearRegression<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset[4:])<br/>
           <br/>
           #training data<br/>
           x_train = df['year'][:40].values.reshape(-1, 1)<br/>
           y_train = df['population'][:40].values.reshape(-1, 1)<br/>
           <br/>
           #training<br/>
           lr = LinearRegression()<br/>
           lr.fit(x_train, y_train)<br/>
           <br/>
           #printing coefficients<br/>
           print(lr.intercept_, lr.coef_)<br/>
           <br/>
           #prediction<br/>
           x_predict = x_train = df['year'][41:].values.reshape(-1, 1)<br/>
           y_actual = df['population'][41:].values.reshape(-1, 1) <br/>
           y_predict = lr.predict(x_predict)<br/>
           <br/>
           plot.scatter(x_predict, y_actual)<br/>
           plot.plot(x_predict, y_predict, color='red', linewidth=2)<br/>
           plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="../../2017/DataMining/images/populationlinearregression.png"></img>
         </p>
         <p>Now test the above program including the data before 1960. What did you notice? You may have got the following graph.</p>
         <p>
           <img style="width:40%" src="../../2017/DataMining/images/allpopulationlinearregression.png"></img>
         </p>
         <p>What are your observations? So the above program using linear regression perfectly fit for a subset of data. Let's now try with <b>polynomial features</b> with degree 2 (refer <a href="http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions">Polynomial Regression: Extending linear models</a>).</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           from sklearn.linear_model import LinearRegression<br/>
           from sklearn.preprocessing import PolynomialFeatures<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset[4:])<br/>
           <br/>
           #training data<br/>
           <br/>
           x_train = df['year'][:50].values.reshape(-1, 1)<br/>
           y_train = df['population'][:50].values.reshape(-1, 1)<br/>
           <br/>
           pf = PolynomialFeatures(degree=2)<br/>
           x_poly = pf.fit_transform(x_train)<br/>
           <br/>
           #training<br/>
           lr = LinearRegression()<br/>
           lr.fit(x_poly, y_train)<br/>
           <br/>
           #printing coefficients<br/>
           print(lr.intercept_, lr.coef_)<br/>
           <br/>
           #prediction<br/>
           x_predict = x_train = df['year'][41:].values.reshape(-1, 1)<br/>
           y_actual = df['population'][41:].values.reshape(-1, 1) <br/>
           y_predict = lr.predict(pf.fit_transform(x_predict))<br/>
           <br/>
           plot.scatter(x_predict, y_actual)<br/>
           plot.plot(x_predict, y_predict, color='red', linewidth=2)<br/>
           plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="../../2017/DataMining/images/populationpolynomialregression.png"></img>
         </p>
         <p>Before jumping into a conclusion, let's consider the entire data and see.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           from sklearn.linear_model import LinearRegression<br/>
           from sklearn.preprocessing import PolynomialFeatures<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset)<br/>
           <br/>
           #training data<br/>
           <br/>
           x_train = df['year'][:40].values.reshape(-1, 1)<br/>
           y_train = df['population'][:40].values.reshape(-1, 1)<br/>
           <br/>
           pf = PolynomialFeatures(degree=2)<br/>
           x_poly = pf.fit_transform(x_train)<br/>
           <br/>
           #training<br/>
           lr = LinearRegression()<br/>
           lr.fit(x_poly, y_train)<br/>
           <br/>
           #printing coefficients<br/>
           print(lr.intercept_, lr.coef_)<br/>
           <br/>
           #prediction<br/>
           x_predict = x_train = df['year'][41:].values.reshape(-1, 1)<br/>
           <br/>
           # Let's add some more years<br/>
           x_predict = np.append(range(1900, 1959), x_predict)<br/>
           x_predict = x_predict.reshape(-1, 1)<br/>
           <br/>
           y_actual = df['population'][41:].values.reshape(-1, 1) <br/>
           y_predict = lr.predict(pf.fit_transform(x_predict))<br/>
           <br/>
           plot.scatter(df['year'], df['population'])<br/>
           plot.plot(x_predict, y_predict, color='red', linewidth=2)<br/>
           plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="../../2017/DataMining/images/allpopulationpolynomialregression.png"></img>
         </p>
         <p>What do you think? Can we use this program to predict the missing data (especially in the absence of other external source of information)? Try the above program with different degrees.</p>
        </div>
        <h4>Exercice 6.3 <span style="color:red">&#9733;&#9733;</span></h4>
        <div class="exercise">
         <p>Classifiers are helpful to classify our dataset into one or more classes. But unlike clustering algorithms, it's the user who has to classify the data in a classification algorithm. In the following exercises, we see different classifiers, starting with perceptron. Look at the input data (<i>numarray</i>) and the associated labels (<i>result</i>). Here, we will label the complete dataset and see whether our model (Perceptron) did work well.</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Perceptron</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
                     <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
                     <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="p">])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">perceptron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>


         <p>Now we will remove some labeled/classified data and see the predicted results. </p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">Perceptron</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> 
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                     <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                     <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">perceptron</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>

         <p>Now, we will try another classifer: MLPClassifier</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="k">import</span> <span class="n">MLPClassifier</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> 
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                   <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                   <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                   <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">mlpclassifier</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">mlpclassifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">mlpclassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>


         <p>Now, we will try another classifer using support vector machines.</p>
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plot</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="k">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">svm</span><span class="p">,</span> <span class="n">metrics</span>

<span class="n">numarray</span> <span class="o">=</span> <span class="n">numarray</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
                     <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> 
                     <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                   <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> 
                   <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span>
                   <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> 

<span class="n">svcclassifier</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">100.</span><span class="p">)</span>
<span class="n">svcclassifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">numarray</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

<span class="n">x_predict</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>

<span class="n">y_predict</span> <span class="o">=</span> <span class="n">svcclassifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_predict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_predict</span><span class="p">)</span>
</pre></div>

         <p>We will now use scikit-learn and the classifiers seen above to recognize handwriting. Scikit-learn has a lot of datasets. We will use one such dataset called digits dataset, which consists of labeled handwriting images of digits. The following program will show the labels.</p>
         <p class="codeexample">
          <code>
             from sklearn import datasets<br/>
             import numpy as np<br/>
             <br/>
             digits = datasets.load_digits()<br/>
             <br/>
             print(np.unique(digits.target))<br/>
          </code>
         </p>
         <p>We will now see the total number of images and the contents of one test image.</p>
         <p class="codeexample">
          <code>
             from sklearn import datasets<br/>
             import numpy as np<br/>
             import matplotlib.pyplot as plot<br/>
             <br/>
             digits = datasets.load_digits()<br/>
             <br/>
             print("Number of images: ", digits.images.size)<br/>
             print("Input data: ", digits.images[0])<br/>
             print("Label:", digits.target[0])<br/>
             <br/>
             plot.imshow(digits.images[0], cmap=plot.cm.gray_r)<br/>
             plot.show()<br/>
          </code>
         </p>
         <p>We will now use a support vector classifier to train the data. We will split our data into two: training data and test data. Remember that we already have labels for the entire dataset.</p>
         <p class="codeexample">
          <code>
             from sklearn import datasets, svm<br/>
             import numpy as np<br/>
             import matplotlib.pyplot as plot<br/>
             <br/>
             digits = datasets.load_digits()<br/>
             <br/>
             training_images = digits.images[:int(digits.images.shape[0]/2)]<br/>
             training_images = training_images.reshape((training_images.shape[0], -1))<br/>
             <br/>
             training_target = digits.target[0:int(digits.target.shape[0]/2)]<br/>
             <br/>
             classifier = svm.SVC(gamma=0.001, C=100.)<br/>
             #training<br/>
             classifier.fit(training_images, training_target)<br/>
             <br/>
             #prediction<br/>
             predict_image = digits.images[int(digits.images.shape[0]/2)+2]<br/>
             print("Predicted value: ", classifier.predict(predict_image.reshape(1,-1)))<br/>
             <br/>
             plot.imshow(predict_image, cmap=plot.cm.gray_r)<br/>
             plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="../../2017/DataMining/images/scikitlearnfour.png"></img>
         </p>
         <p>Now let's try predicting the remaining labels and use the classifcation report to get the precision of prediction.</p>
         <p class="codeexample">
          <code>
             from sklearn import datasets, svm, metrics<br/>
             import numpy as np<br/>
             import matplotlib.pyplot as plot<br/>
             <br/>
             digits = datasets.load_digits()<br/>
             <br/>
             training_images = digits.images[:int(digits.images.shape[0]/2)]<br/>
             training_images = training_images.reshape((training_images.shape[0], -1))<br/>
             <br/>
             training_target = digits.target[0:int(digits.target.shape[0]/2)]<br/>
             <br/>
             classifier = svm.SVC(gamma=0.001, C=100.)<br/>
             #training<br/>
             classifier.fit(training_images, training_target)<br/>
             <br/>
             #prediction<br/>
             predict_images = digits.images[int(digits.images.shape[0]/2)+1:]<br/>
             actual_labels = digits.target[int(digits.target.shape[0]/2)+1:]<br/>
             predicted_labels = classifier.predict(predict_images.reshape((predict_images.shape[0], -1)))<br/>
             <br/>
             #classification report<br/>
             print(metrics.classification_report(actual_labels,predicted_labels))<br/>
          </code>
         </p>
         <p>There are other classifiers available. We will now work with Perceptron (refer <a href="http://scikit-learn.org/stable/modules/linear_model.html#perceptron">here</a>) and see its performance.</p>
         <p class="codeexample">
          <code>
             from sklearn import datasets, metrics<br/>
             from sklearn.linear_model import Perceptron<br/>
             import numpy as np<br/>
             import matplotlib.pyplot as plot<br/>
             <br/>
             digits = datasets.load_digits()<br/>
             <br/>
             training_images = digits.images[:int(digits.images.shape[0]/2)]<br/>
             training_images = training_images.reshape((training_images.shape[0], -1))<br/>
             <br/>
             training_target = digits.target[0:int(digits.target.shape[0]/2)]<br/>
             <br/>
             classifier = Perceptron(max_iter=1000)<br/>
             #training<br/>
             classifier.fit(training_images, training_target)<br/>
             <br/>
             #prediction<br/>
             predict_images = digits.images[int(digits.images.shape[0]/2)+1:]<br/>
             actual_labels = digits.target[int(digits.target.shape[0]/2)+1:]<br/>
             predicted_labels = classifier.predict(predict_images.reshape((predict_images.shape[0], -1)))<br/>
             <br/>
             #classification report<br/>
             print(metrics.classification_report(actual_labels,predicted_labels))<br/>
          </code>
         </p>
         <p>Finally, we will finish the test with Multilayer Perceptron (refer <a href="http://scikit-learn.org/stable/modules/neural_networks_supervised.html#multi-layer-perceptron">here</a>).</p>
         <p class="codeexample">
          <code>
             from sklearn import datasets, metrics<br/>
             from sklearn.neural_network import MLPClassifier<br/>
             import numpy as np<br/>
             import matplotlib.pyplot as plot<br/>
             <br/>
             digits = datasets.load_digits()<br/>
             <br/>
             training_images = digits.images[:int(digits.images.shape[0]/2)]<br/>
             training_images = training_images.reshape((training_images.shape[0], -1))<br/>
             <br/>
             training_target = digits.target[0:int(digits.target.shape[0]/2)]<br/>
             <br/>
             classifier = MLPClassifier(alpha=2, max_iter=1000)<br/>
             #training<br/>
             classifier.fit(training_images, training_target)<br/>
             <br/>
             #prediction<br/>
             predict_images = digits.images[int(digits.images.shape[0]/2)+1:]<br/>
             actual_labels = digits.target[int(digits.target.shape[0]/2)+1:]<br/>
             predicted_labels = classifier.predict(predict_images.reshape((predict_images.shape[0], -1)))<br/>
             <br/>
             #classification report<br/>
             print(metrics.classification_report(actual_labels,predicted_labels))<br/>
          </code>
         </p>
         <p>Did you try changing the number of hidden layers?</p>
         <p>What are your observations after trying the different classifiers? </p>
        </div>
        <h4>Exercice 6.4 <span style="color:red">&#9733;&#9733;</span></h4>
        <div class="exercise">
         <p>In this exercise, we will label different parts of images and train a classifier. Then we will predict the labels. Let's start by splitting an image into sub-images in the following manner.</p>
         <p class="codeexample">
          <code>
             import os,sys<br/>
             from PIL import Image<br/>
             import matplotlib.pyplot as plot<br/>
             import numpy<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             print(imgfile.size)<br/>
             <br/>
             figure, axes = plot.subplots(nrows=5, ncols=5, figsize=(20,25))<br/>
             <br/>
             xaxis = 0<br/>
             for i in range(0, 640, 128):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;yaxis=0<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;for j in range(0,480, 96):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#print(i, j)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bbox = (i, j, i+128, j+96)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;axes[xaxis, yaxis].imshow(imgfile.crop(bbox))<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yaxis = yaxis + 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;xaxis = xaxis + 1<br/>
             <br/>
             plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:50%" src="../../2017/DataMining/images/subimages.png"></img>
         </p>
         <p>Your next goal is to first reorder the above sub-images so that it looks similar to the original image (like a jig-saw puzzle). For this purpose, you have to modify the above code. Then you label these subimages based on the predominant colors. You can ask the user to label the subimages into colors, like green, yellow, etc. For every subimage, we will have <b>only one</b> color. Then train and test your classifiers using the following methods.</p>
         <ul>
           <li>Support vector classifier (SVC)</li>
           <li>Perceptron</li>
           <li>Multilayer Perceptron</li>
         </ul>
         <p>Do not forget to print the classification report for every classifier. How was the precision of every classifier that you considered? Can you increase the number of subimages and test again?</p>
        </div> 
      </div>
  </body>
</html>
