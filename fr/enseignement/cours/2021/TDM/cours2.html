<html>

<head>
    <meta charset="utf-8" />
    <title>Traitement de données massives (2021-2022): Traitement de données: John Samuel</title>
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }
        
        .slide {
            height: 100%;
            width: 100%;
        }
        
        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            flex-direction: column;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 3vmin;
            flex-wrap: wrap;
        }
        
        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }
        
        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }
        
        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }
        
        .content .flexcontent {
            height: 79%;
            width: 95vw;
            display: flex;
            font-size: 3vmin;
            flex-direction: row;
            flex-wrap: wrap;
            margin: 0;
        }
        
        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }
        
        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 40%;
            margin-bottom: 1vmax;
        }
        
        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }
        
        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }
        
        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }
        
        .content figure {
            max-width: 90%;
            max-height: 90%;
        }
        
        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }
        
        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            vertical-align: center;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
        
        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }
        
        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }
        
        .content li {
            line-height: 4vh;
        }
        
        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }
        
        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }
        
        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }
        
        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }
        
        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }
        
        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }
        
        .footer .navigation .next::after {
            content: "| >";
        }
        
        .footer .navigation .prev::after {
            content: "< ";
        }
        /* Using same Jupyter CSS
     */
        
        .highlight {
            background: #f8f8f8;
        }
        
        .highlight .c {
            color: #408080;
            font-style: italic
        }
        /* Comment */
        
        .highlight .err {
            border: 1px solid #FF0000
        }
        /* Error */
        
        .highlight .k {
            color: #008000;
            font-weight: bold
        }
        /* Keyword */
        
        .highlight .o {
            color: #666666
        }
        /* Operator */
        
        .highlight .ch {
            color: #408080;
            font-style: italic
        }
        /* Comment.Hashbang */
        
        .highlight .c1 {
            color: #408080;
            font-style: italic
        }
        /* Comment.Single */
        
        .highlight .cs {
            color: #408080;
            font-style: italic
        }
        /* Comment.Special */
        
        .highlight .cm {
            color: #408080;
            font-style: italic
        }
        /* Comment.Multiline */
        
        .highlight .nn {
            color: #0000FF;
            font-weight: bold
        }
        /* Name.Namespace */
        
        .highlight .k {
            color: #008000;
            font-weight: bold
        }
        /* Keyword */
        
        .highlight .s2 {
            color: #BA2121
        }
        /* Literal.String.Double */
        
        .highlight .s1 {
            color: #BA2121
        }
        /* Literal.String.Single */
        
        .highlight .kn {
            color: #008000;
            font-weight: bold
        }
        /* Keyword.Namespace */
        
        .highlight .nb {
            color: #008000
        }
        /* Name.Builtin */
        
        .highlight .mb {
            color: #666666
        }
        /* Literal.Number.Bin */
        
        .highlight .mf {
            color: #666666
        }
        /* Literal.Number.Float */
        
        .highlight .mh {
            color: #666666
        }
        /* Literal.Number.Hex */
        
        .highlight .mi {
            color: #666666
        }
        /* Literal.Number.Integer */
        
        .highlight .mo {
            color: #666666
        }
        /* Literal.Number.Oct */
        
        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }
            .slide {
                height: 100%;
                width: 100%;
            }
            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }
            .content .topicheading {
                width: 90%;
            }
            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }
            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }
            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }
        
        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }
            .content {
                height: 76%;
                width: 90vw;
                display: flex;
                color: #000000;
                text-align: left;
                padding: 5vw;
                font-size: 3vmin;
                flex-wrap: wrap;
            }
            .content figure img {
                max-width: 80%;
                max-height: 80%;
            }
            .content figcaption {
                max-width: 80%;
                max-height: 80%;
            }
        }
    </style>
    <script src="../../../../../fr/enseignement/cours/2020/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:3.5vw">Traitement de données massives</h1>
            <p><b>John Samuel</b><br/> CPE Lyon<br/><br/>
                <b>Year</b>: 2021-2022<br/>
                <b>Email</b>: john(dot)samuel(at)cpe(dot)fr<br/><br/>
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Objectifs</h1>
            <ol>
                <li>Régularités</li>
                <li>Exploration des données</li>
                <li>Algorithmes</li>
                <li>Sélection de caractéristiques</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <figure class="flexcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="300px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg" height="300px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="300px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG" height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Régularités naturelles</h1>
            <ul>
                <li>Symétrie</li>
                <li>Arbres, fractales</li>
                <li>Spirales</li>
                <li>Chaos</li>
                <li>Ondes</li>
                <li>Bulles, mousse</li>
                <li>Pavages</li>
                <li>Ruptures</li>
                <li>Taches, bandes</li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Kittyply_edit1.jpg" height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG" height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Rio_Negro_meanders.JPG" height="150px" width="150px" />
            </figure>
            <Régularitésfigure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Apis_florea_nest_closeup2.jpg" height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg" height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Equus_grevyi_(aka).jpg" height="150px" width="150px" />
                </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Créations humaines</h1>
            <ul>
                <li>Bâtiments (Symétrie)</li>
                <li>Villes</li>
                <li>Environnement virtuel (e.g., jeux de vidéo)</li>
                <li>Les artefacts humains</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Roman_geometric_mosaic.jpg" height="300px" width="600px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Création</h1>
            <ul>
                <li>Répétition</li>
                <li>Fractales
                    <ul>
                        <li>Ensemble de Julia: <i>f(z) = z<sup>2</sup> + c</i></li>
                    </ul>
                </li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/400px-Tiling_Dual_Semiregular_V3-3-3-3-6_Floret_Pentagonal.svg.png" height="200px" width="200px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Finite_subdivision_of_a_radial_link.png" height="200px" width="200px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Julia_set_(indigo).png" height="200px" width="200px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Synonymes</h1>
            <ul>
                <li>Fouille de données</li>
                <li>Forage de données</li>
                <li>Extraction de connaissances à partir de données</li>
                <li>Data mining</li>
                <li>Machine learning</li>
                <li>Apprentissage automatique</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Usine 4.0</h1>
            <figure>
                <img src="../../2017/CN/578px-Industry_4.0.png" height="350vh" />
                <figcaption style="text-align:center">Industrie 4.0</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Reconnaissance de formes</h1>
            <ul>
                <li>Identifier des motifs informatiques à partir de données brutes</li>
                <li>Approches
                    <ol>
                        <li><b>Apprentissage supervisé</b>: Apprentissage automatique qui utilise un ensemble de données étiquetées</li>
                        <li><b>Apprentissage non-supervisé</b>: Apprentissage automatique qui utilise un ensemble de données non-étiquetées</li>
                        <li><b>Apprentissage semi-supervisé</b>: Apprentissage automatique qui utilise un ensemble de données étiquetées et non-étiquetées</li>
                    </ol>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Formalisation</h1>
            <ul>
                <li><b>Vecteur euclidien</b>: objet géométrique avec magnitude et direction</li>
                <li><b>Espace vectoriel</b>: collection de vecteurs qui peuvent être additionnés et multipliés par des nombres</li>
                <li><b>Vecteur de caractéristiques</b>: vecteur n-dimensionnel</li>
                <li><b>Espace de caractéristiques</b>: Espace vectoriel associé aux vecteurs</li>
            </ul>
            <h3>Exemples de caractéristiques</h3>
            <ul>
                <li><b>Images</b>: les valeurs des pixels.</li>
                <li><b>Textes</b>: Fréquence d'apparition des phrases textuelles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Formalisation</h1>
            <ul>
                <li><b>Construction de caractéristiques<sup>1</sup></b>: construction of new features from already available features</li>
                <li><b>Opérateurs de construction pour les caractéristiques</b>
                    <ul>
                        <li>Opérateurs d'égalité, opérateurs arithmétiques, opérateurs de tableau (min, max, moyenne, etc.)...</li>
                    </ul>
                </li>
            </ul>
            <h3>Exemple</h3>
            <ul>
                <li>Soit <b>Année de naissance</b> et <b>Année de décès</b> deux caractéristiques existantes.</li>
                <li>Une nouvelle caractéristique appelée <b>âge</b> est créée. <b>âge</b> = <b>Année de décès</b> - <b>Année de naissance</b></li>
            </ul>
            <ol style="font-size:2vh">
                <li>https://en.wikipedia.org/wiki/Feature_vector</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Formalisation: Supervised learning</h1>
            <ul>
                <li>Soit \(N\) le nombre d'exemples d'entraînement</li>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>Soit \({(x_1, y_1),...,(x_N, y_N)}\) les \(N\) exemples d'entraînement, où
                    <ul>
                        <li>\(x_i\) est le vecteur de caractéristiques de <i>i<sup>ème</sup></i> exemple d'entraînement.</li>
                        <li>\(y_i\) est son label.</li>
                    </ul>
                </li>
                <li>L'objectif de l'algorithme d'apprentissage supervisé est de trouver \(g: X &#8594; Y\), où
                    <ul>
                        <li><i>g</i> est l'une des fonctions de l'ensemble des fonctions possibles <i>G</i> (espace des hypothèses)</li>
                    </ul>
                </li>
                <li><b>Fonction d'évaluation <i>F</i></b> indiquent l'espace des fonctions d'évaluation, où
                    <ul>
                        <li>\(f: X &#215; Y &#8594; R\) telle que <i>g</i> renvoie la fonction d'évaluation la plus élevée.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Formalisation: Apprentissage non supervisé</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>L'objectif de l'algorithme d'apprentissage non supervisé est
                    <ul>
                        <li>trouver la mise en correspondance \(X &#8594; Y\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>1. Régularités</h1>
        </div>
        <div class="content">
            <h1>Formalisation: Apprentissage semi-supervisé</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>Soit \({(x_1, y_1),...,(x_l, y_l)}\) l'ensemble d'exemples d'exercices étiquetés</li>
                <li>Soit \({x_{l+1},...,x_{l+u}}\) sont les \(u\) ensembles des vecteurs de caractéristiques non étiquetées de \(X\).</li>
                <li>L'objectif de l'algorithme d'apprentissage semi-supervisé est de faire
                    <ul>
                        <li><b>l'apprentissage transductif</b>, c'est-à-dire trouver des étiquettes correctes pour \({x_{l+1},...,x_{l+u}}\).</li>
                        <li><b>l'apprentissage inductif</b>, c'est-à-dire trouver la bonne mise en correspondance \(X &#8594; Y\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>2. Data Mining</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Activités</h1>
            <ol>
                <li>Classification</li>
                <li>Partitionnement de données (Clustering)</li>
                <li>Régression</li>
                <li>Étiquetage des séquences</li>
                <li>Règles d'association</li>
                <li>Détection d'anomalies</li>
                <li>Récapitulation</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">2.1.1 Introduction</h1>
            <ul>
                <li>Catégorisation algorithmique d'objets.</li>
                <li>Attribuer une classe ou catégorie à chaque objet (ou individu)</li>
                <li>Classification binaire ou classification en classes multiples </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Filtrage de contenu (e.g., spam/pourriel)</li>
                <li>Classification de documents</li>
                <li>Reconnaissance de l'écriture manuscrite</li>
                <li>Reconnaissance automatique de la parole</li>
                <li>Moteurs de recherche</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">2.1.2 Définition formelle</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>L'objectif de l'algorithme de classification (ou classificateur) est de trouver \({(x_1, y_1),...,(x_l, y_k)}\), c'est-à-dire l'attribution d'une étiquette connue à chaque vecteur de caractéristique d'entrée, où
                    <ul>
                        <li>\(x_i &#8712; X \)</li>
                        <li>\(y_i &#8712; Y \)</li>
                        <li>\(|X| = l \)</li>
                        <li>\(|Y| = k \)</li>
                        <li>\(l &gt;= k\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">2.1.3. Classificateurs</h3>
            <ul>
                <li>Algorithme de classification</li>
                <li>Deux types de classificateurs:
                    <ul>
                        <li><b>Classificateurs binaires</b> attribue un objet à l'une des deux classes</li>
                        <li><b>Classificateurs multiclasses</b> attribue un objet à une ou plusieurs classes</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">2.1.4 Linear Classificateurs</h3>
            <ul>
                <li>Fonction linéaire attribuant un score à chaque catégorie possible en combinant le vecteur de caractéristiques d'une instance avec un vecteur de poids, en utilisant un produit de points.</li>
                <li>Formalisation :
                    <ul>
                        <li>Soit \(X\) être l'espace de saisie des caractéristiques et \(x_i &#8712; X\)</li>
                        <li>Soit \(&#946_k\) un vecteur de poids pour la catégorie <i>k</i></li>
                        <li>score(\(x_i, k) = x_i.&#946;_k\), score pour l'attribution de la catégorie \(k\) à l'instance \(x_i\). La catégorie qui donne le score le plus élevé est attribuée à la catégorie de l'instance.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">2.1.5. Précision et rappel</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/positivenegative.svg" height="400px" />
                <figcaption>Les vrais positifs et les vrais négatifs</figcaption>
            </figure>
        </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">2.1.5. Précision et rappel</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/Precisionrecall.svg" height="400px" />
                <figcaption>Précision et rappel</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">2.1.5. Précision et rappel</h2>
            <p>Soit</p>
            <ul>
                <li><i>tp</i>: nombre de vrais postifs</li>
                <li><i>fp</i>: nombre de faux positifs</li>
                <li><i>fn</i>: nombre de faux négatifs</li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2018/DataMining/Precisionrecall.svg" height="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">2.1.5. Précision et rappel</h2>
            <p>Alors</p>
            <ul>
                <li>Précision \[p = \frac{tp}{(tp + fp)}\]</li>
                <li>Rappel (Recall) \[r = \frac{tp}{(tp + fn)}\]</i>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">2.1.5. Précision et rappel</h2>
            <ul>
                <li>score F1 est la moyenne harmonique de la précision et du rappel : </li>
                <li>F1-score \[f1 = 2 * \frac{(p * r)}{(p + r)}\]</li>
                <li>F1-score: meilleure valeur à 1 (précision et rappel parfaits) et pire à 0.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">2.1.5. Précision et rappel</h2>
            <ul>
                <li>\(F_\beta\)-score utilise un facteur réel positif β, où β est choisi de telle sorte que le rappel est considéré comme β fois plus important que la précision, est : </li>
                <li>\(F_\beta\)-score \[F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{p} \cdot \mathrm{r}}{(\beta^2 \cdot \mathrm{p}) + \mathrm{r}}\]</li>
                <li>Exemple: \(F_2\) score</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Matrice de confusion</h2>
            <ul>
                <li>une matrice qui mesure la qualité d'un système de classification</li>
                <li>chaque ligne de la matrice représente les instances d'une classe prédite</li>
                <li>chaque colonne représente les instances d'une classe réelle</li>
                <li>Toutes les prédictions correctes sont situées dans la diagonale du tableau</li>
                <li>Les erreurs de prédiction seront représentées par des valeurs situées en dehors de la diagonale.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/positivenegative.svg" height="200px" />
                <figcaption>Les vrais positifs et les vrais négatifs</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Matrice de confusion</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/DataMining/confusionmatrix.png" height="400px" />
                <figcaption>Matrice de confusion pour un classificateur SVM pour les chiffres manuscrits (MNIST)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Matrice de confusion</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/DataMining/confusionmatrix1.png" height="400px" />
                <figcaption>Matrice de confusion pour un perceptron pour les chiffres manuscrits (MNIST)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Classification binaire</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/binaryclassifier.svg" height="400px" />
                <figcaption>Classification binaire</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Classification multiclasse</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/multiclassclassifier.svg" height="400px" />
                <figcaption>Classification multiclasse</figcaption>
            </figure>
        </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Classification multiclasse [Aly 2005]</h3>
            <ul>
                <li>Transformation en classification binaire
                    <ul>
                        <li>L'approche un contre le reste (Un contre tous)</li>
                        <li>L'approche un-contre-un</li>
                    </ul>
                </li>
                <li>Extension de la classification binaire
                    <ul>
                        <li>Réseaux de neurones</li>
                        <li>k-voisins les plus proches</li>
                    </ul>
                </li>
                <li>la classification hiérarchique.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest (One-vs.-all) strategy</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsall.svg" height="400px" />
                <figcaption>La strategie un-contre le rest pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest (One-vs.-all) strategy</h3>
            <ul>
                <li>Entraîner un seul classificateur par classe, avec les échantillons de cette classe comme échantillons positifs et tous les autres comme négatifs. </li>
                <li>Chaque classificateur produit un score de confiance réel pour sa décision</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsall.svg" height="300px" />
                <figcaption>La strategie un-contre le rest pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest or One-vs.-all (OvR, OvA) strategy</h3>
            <ul>
                <li>Entrées :
                    <ul>
                        <li>\(L\), un apprenant (algorithme d'entraînement pour les classificateurs binaires)</li>
                        <li>échantillons \(X\)</li>
                        <li>étiquettes \(y\), où \(y_i ∈ \{1,..,K \} \) est l'étiquette de l'échantillon \(X_i\)
                    </ul>
                    </li>
                    <li>Sortie :
                        <ul>
                            <li>une liste de classificateurs \(f_k\), où \(k ∈ \{1,..,K \} \)
                        </ul>
                        </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest or One-vs.-all (OvR, OvA) strategy</h3>
            <p>Prendre des décisions signifie appliquer tous les classificateurs à un échantillon invisible x et prédire l'étiquette k pour laquelle le classificateur correspondant rapporte le score de confiance le plus élevé : \[\hat{y} = \underset{k \in
                \{1 \ldots K\}}{\arg\!\max}\; f_k(x)\]</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-one strategy</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsone.svg" height="400px" />
                <figcaption>La strategie un-contre-un pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-one strategy</h3>
            <li>nécessite l'entraînement des \(\frac{K (K - 1)}{2}\) classificateurs binaires</li>
            <li>chaque classificateur reçoit les échantillons d'une paire de classes du jeu de formation original, et doit apprendre à distinguer ces deux classes.</li>
            <li>Au moment de la prédiction, un système de vote est appliqué : tous les \(\frac{K (K - 1)}{2}\) classificateurs sont appliqués à un échantillon non vu et la classe qui a obtenu le plus grand nombre de prédictions est prédite par le classificateur
                combiné.
            </li>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsone.svg" height="200px" />
                <figcaption>La strategie un-contre-un pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h1>2.2.1. Introduction</h1>
            <ul>
                <li>Diviser un ensemble de données en différents « paquets » homogènes,</li>
                <li>Les données de chaque sous-ensemble partagent des caractéristiques communes</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cluster-2.svg.png" height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Analyse des réseaux sociaux</li>
                <li>Segmentation d'image</li>
                <li>Systèmes de recommandation</li>
            </ul>
            <figure class="gridconten">
                <img src="../../2017/CN/Social_Network_Analysis_Visualization.png" height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Soit \(X\) être l'espace de saisie des caractéristiques</li>
                <li>L'objectif du regroupement est de trouver \(k\) des sous-ensembles de \(X\), de façon à ce que</li>
                <p>\[ C_1.. &#8746; ..C_k &#8746; C_{outliers} = X \] </i> et</p>
                <p> \[ C_i &#8745; C_j = &#981;, i &#8800; j; 1 &lt;i,j &lt;k \]</i>
                </p>
                <p>\(C_{outliers}\) peut consister en des cas extrêmes (anomalie de données)</p>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h1>Modèles de regroupement</h1>
            <ul>
                <li><b>Modèles de centroïdes</b> : groupe représenté par un seul vecteur moyen</li>
                <li><b>Modèles de connectivité</b> : proximité de la connectivité</li>
                <li>Modèles de distribution : regroupements modélisées à l'aide de distributions statistiques</li>
                <li>Modèles de densité : regroupements de régions denses connectées dans l'espace de données</li>
                <li>Modèles de sous-espace</li>
                <li>Modèles de groupes</li>
                <li>Modèles graphiques</li>
                <li>Modèles neuronaux</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png" height="200px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Iris_dendrogram.png" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>2.3 Régression</h1>
        </div>
        <div class="content">
            <h1>2.3 Régression</h1>
            <ul>
                <li>Trouver une fonction qui modélise les données</li>
                <li>Estimer les relations entre les variables</li>
                <li>Analyser la relation d'une variable par rapport à une ou plusieurs autres.</li>
                <li>Attribuer une valeur réelle à chaque entrée</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>2.3 Régression</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Applications</h1>
            <ul>
                <li>Prévisions météorologiques</li>
                <li>Prévisions de ventes</li>
                <li>Apprentissage machine</li>
                <li>Finance</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Linear_regression.svg" height="400px" width="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>2.3 Régression</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Une fonction qui associe un élément de données à une variable de prédiction</li>
                <li>Soit \(X\) les variables indépendantes</li>
                <li>Soit \(Y\) les variables dépendantes</li>
                <li>Soit \(&#946;\) les paramètres inconnus (scalaires ou vectoriels)</li>
                <li>Le but du modèle de régression est d'approximer \(Y\) avec \(X, &#946;\), c'est à dire,
            </ul>
            <p>\[ Y &#8773; f(X,&#946;) \]</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>2.3 Régression</h1>
        </div>
        <div class="content">
            <h1>Régression linéaire</h1>
            <ul>
                <li>ligne droite: \(y_i = &#946;_0 + &#946;_1x_i + &#949;_i\) OR</li>
                <li>parabole: \(y_i = &#946;_0 + &#946;_1x_i + &#946;_1x_i^2 +&#949;_i\)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>2.3 Régression</h1>
        </div>
        <div class="content">
            <h1>Régression linéaire</h1>
            <ul>
                <li>ligne droite: \(y_i = &#946;_0 + &#946;_1x_i + &#949;_i\) OR</li>
                <li> \( ŷ_i = &#946;_0 + &#946;_{1_i} \) OR</li>
                <li>Résiduels: \(e_i = ŷ_i - y_i\)</li>
                <li>Somme des carrés des résidus, \(SSE = &#931; e_i\), where \(1 &lt; i &lt; n\)</li>
                <li>L'objectif est de minimiser l'SSE</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ul>
                <li>Attribuer une classe à chaque membre d'une séquence de valeurs</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Etiquetage de la partie du discours</li>
                <li>Traduction linguistique</li>
                <li>Analyse vidéo</li>
                <li>Reconnaissance de l'écriture manuscrite</li>
                <li>Extraction d'informations</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>Soit \(&#12296;x_1,...,x_T&#12297;\) une séquence de longueur \(T\).</li>
                <li>L'objectif de l'étiquetage des séquences est de générer une séquence correspondante
                    <ul>
                        <li>\(&#12296;y_1,...,y_T&#12297;\) des étiquettes</li>
                        <li>\(x_i &#8712; X\) </li>
                        <li>\(y_j &#8712; Y\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h1>Association Rules</h1>
            <ul>
                <li>Recherche de relations entre les variables</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Exploitation de l'utilisation du web</li>
                <li>Détection d'intrusion</li>
                <li>Analyse d'affinité</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Soit \(I\) un ensemble de \(n\) attributs binaires appelés items</li>
                <li>Soit \(T\) un ensemble de \(m\) transactions appelé base de données</li>
                <li>Soit \(I\) = \(\{(i_1,...,i_n)\}\) et \(T\) = \({(t_1,...,t_m)}\)</li>
                <li>L'objectif de l'apprentissage des règles d'association est de trouver
                    <ul>
                        <li>\(X &#8658; Y\), where \(X &#8658; Y &#8838; I\)</li>
                        <li>\(X\) est l'antécédent</li>
                        <li>\(Y\) est la conséquence</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Support: how frequently an itemset appears in the database
                    <ul>
                        <li>\[supp(X) = \frac{|t &#8712;T; X &#8838; t|}{ |T|}\]</i>
                        </li>
                    </ul>
                </li>
                <li>Confidence: how frequently the rule has been found to be true.
                    <ul>
                        <li>\[conf(X &#8658; Y) = \frac{supp(X &#8746; Y)}{supp(X)}\]</i>
                        </li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Lift: the ratio of the observed support to that of the expected if X and Y were independent
                    <ul>
                        <li>\[lift(X &#8658; Y) = \frac{supp(X &#8746; Y)}{(supp(X) &#10761; supp(Y))}\]</i>
                        </li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h1>Example</h1>
            <ul>
                <li>{<b>bread</b>, <b>butter</b>} &#8658; {<b>milk</b>}</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/associationruletable.png" height="400px" width="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <ul>
                <li>Identification de données inhabituelles</li>
                <li>Approches
                    <ol>
                        <li>Détection supervisé</li>
                        <li>Détection non-supervisé</li>
                        <li>Détection semi-supervisé</li>
                    </ol>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Applications</h1>
            <ul>
                <li>Détection d'intrusion</li>
                <li>Détection de fraude</li>
                <li>System health monitoring</li>
                <li>Détection d'événements dans les réseaux de capteurs</li>
                <li>Détection d'abus dans un système d'information</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <h1>Characteristics</h1>
            <ul>
                <li>Des sursauts inattendus</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <h1>Formalisation</h1>
            <ul>
                <li>Soit \(Y\) un ensemble de mesures</li>
                <li>Soit \(P_Y(y)\) un modèle statistique pour la distribution des \(Y\) dans des conditions "normales"..</li>
                <li>Soit \(T\) un seuil défini par l'utilisateur..</li>
                <li>Une mesure \(x\) est une valeur isolée si \(P_Y(x) &lt; T\)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading"></h1>
            <ul>
                <li>Synthèse courte d'un ensemble de données</li>
                <li>Génération de rapports</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Applications</h1>
            <ul>
                <li>Extraction des mots-clès</li>
                <li>Récapitulation de documents</li>
                <li>Moteurs de recherche</li>
                <li>Récapitulation d'images</li>
                <li>Récapitulation de vidéos: découvrir des événements principaux dans une vidéo</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1>Formalisation: Synthèse multi-documents</h1>
            <ul>
                <li>Soit \(\{D = D_1, ..., D_k\}\) une collection de \(k\) documents </li>
                <li>Un document \(\{D = t_1, ..., t_m\}\) se compose de m unités textuelles (mots, phrases, paragraphes, etc.) </li>
                <li>Soit \(\{D = t_1, ..., t_n\}\) être l'ensemble complet de toutes les unités textuelles de tous les documents, où
                    <ul>
                        <li>\(t_i &#8712; D\), si et seulement si \(&#8707; D_j\) de sorte que \(t_i &#8712; D_j\)</li>
                    </ul>
                </li>
                <li>\(S &#8838; D\) constitutes a summary</li>
                <li> Deux fonctions de scoring
                    <ul>
                        <li>\(Rel(i)\): pertinence de l'unité textuelle \(i\) dans le résumé</li>
                        <li>\(Red(i,j)\): Redondance entre deux unités textuelles \(t_i, t_j\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1>Formalisation: Multidocument summarization</h1>
            <ul>
                <li>La note pour un résumé \(S\)
                    <ul>
                        <li>\(s(S)\) note pour un résumé S</li>
                        <li>\(l(i)\) est la longueur de l'unité textuelle \(i\)</li>
                        <li>\(K\) est la longueur maximale fixée du résumé</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/scoringfunction.png" height="200px" width="500px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ul>
                <li>Trouver un sous-ensemble à partir de l'ensemble du sous-ensemble</li>
                <li>Approches
                    <ol>
                        <li><b>Extraction</b>: Sélection d'un sous-ensemble de mots, de phrases ou d'expressions existants dans le texte original sans aucune modification</li>
                        <li><b>Abstraction</b>: construire une représentation sémantique interne et utiliser ensuite les techniques de génération du langage naturel</li>
                    </ol>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1>Résumé extractif</h1>
            <ul>
                <li>Approches
                    <ol>
                        <li><b>Résumé générique</b>: Obtenir un résumé générique</li>
                        <li><b>Résumé pertinent pour la recherche</b></li>
                    </ol>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>3. Algorithms</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ol>
                <li>Support Vector Machines (SVM)</li>
                <li>Descente du gradient stochastique</li>
                <li>Voisins proches</li>
                <li>Bayes naïfs</li>
                <li>Arbres de décision</li>
                <li>Ensemble Methods (Forêt d'arbres décisionnels)</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Introduction</h1>
            <ul>
                <li>Approche d'apprentissage supervisé</li>
                <li>Algorithme de classification binaire</li>
                <li>Construit un hyperplan assurant la séparation maximale entre deux classes</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/SVM Separating Hyperplanes.svg" height="350px" width="350px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Hyperplane</h1>
            <ul>
                <li>L'hyperplan de l'espace n-dimensionnel est un sous-espace de dimension <i>n-1</i></li>
                <li>Exemples
                    <ul>
                        <li>L'hyperplan d'un espace à deux dimensions est une ligne à une dimension</li>
                        <li>L'hyperplan d'un espace tridimensionnel est un plan bidimensionnel</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Le but d'un SVM est d'estimer une fonction \(f: R^N &#10761; {+1,-1}\), c'est à dire,
                    <ul>
                        <li>Si \(x_1,...,x_l\) &#8712; \(R^N\) sont les \(N\) points de données d'entrée,</li>
                        <li>L'objectif est de trouver \((x_1,y_1),...,(x_l,y_l)\) &#8712; \(R^N &#10761; {+1,-1}\)</li>
                    </ul>
                </li>
                <li>Tout hyperplan peut être écrit par l'équation en utilisant un ensemble de points d'entrée \(x\)
                    <ul>
                        <li>\(w.x - b = 0\), où</li>
                        <li>\(w &#8712; R^N\), un vecteur normal à la plane </li>
                        <li>\(b &#8712; R\)</li>
                    </ul>
                </li>
                <li> Une fonction de décision est donnée par \(f(x) = sign(w.x - b )\)
                </li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Surface_normal_illustration.svg" height="400px" width="500px" />
                <figcaption>Normal vector</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Si les données de formation sont séparables linéairement, deux hyperplans peuvent être sélectionnés</li>
                <li>Ils séparent les deux classes de données, <br> afin que la distance entre elles soit la plus grande possible.</li>
                <li>Les hyperplans peuvent être donnés par les équations
                    <ul>
                        <li>\(w.x - b = 1\)</li>
                        <li>\(w.x - b = -1\)</li>
                    </ul>
                </li>
                <li>La distance entre les deux hyperplans peut être donnée par \( \frac{2}{||w||} \)</i>
                </li>
                <li>La région située entre ces deux hyperplans est appelée marge.</li>
                <li>L'hyperplan à marge maximale est l'hyperplan <br> qui se trouve à mi-chemin entre eux.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png" height="500px" width="500px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Définition formelle</h1>
            <ul>
                <li>Afin d'éviter que les points de données ne tombent dans la marge, les contraintes suivantes sont ajoutées
                    <ul>
                        <li>\(w.x_i - b &gt;= 1\), si \(y_i = 1\)</li>
                        <li>\(w.x_i - b &lt;= -1\), si \(y_i = -1\)</li>
                    </ul>
                    <li>\(y_i(w.x_i - b) &gt;= 1\), \(1&lt;= i &lt;= n\)</li>
                </li>
                <li>L'objectif est de minimiser ||w|| sous réserve de \(y_i(w.x_i - b) &gt;= 1\), \(1&lt;= i &lt;= n\)</li>
                <li>Une solution pour les deux \(w\) et \(b\) donne le classificateur \(f(x) = sign(w.x - b)\)</li>
                <li>L'hyperplan à marge maximale est entièrement déterminé par les points qui en sont les plus proches, appelés vecteurs de soutien</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Data mining</h1>
            <ul>
                <li>Classification (classification multi-classes)</li>
                <li>Régression</li>
                <li>Détection des anomalies</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Catégorisation des textes et des hypertextes</li>
                <li>Classification des images</li>
                <li>Reconnaissance de l'écriture manuscrite</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ul>
                <li>Une approximation stochastique de l'optimisation de la descente du gradient</li>
                <li>Méthode itérative pour minimiser une fonction objective qui s'écrit comme une somme de fonctions différenciables.</li>
                <li>Trouve des minima ou des maxima par itération</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1>Gradient</h1>
            <ul>
                <li>Généralisation multi-variable du dérivé. </li>
                <li>Donne la pente de la tangente du graphe d'une fonction</li>
                <li>Le gradient pointe dans la direction du plus grand taux d'augmentation d'une fonction</li>
                <li>L'amplitude du gradient est la pente du graphique dans cette direction</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1>Gradient ou dérivé</h1>
            <ul>
                <li>Dérivés définis sur des fonctions d'une seule variable</li>
                <li>Gradient défini sur des fonctions de variables multiples</li>
                <li>Le gradient est une fonction à valeur vectorielle (la plage est un vecteur)</li>
                <li>Le dérivé est une fonction à valeur scalaire</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1>Algorithme du gradient</h1>
            <ul>
                <li>Algorithme d'optimisation itératif du premier ordre pour trouver le minimum d'une fonction.</li>
                <li>Trouver un minimum local implique de prendre des mesures proportionnelles à</br> le négatif du gradient de la fonction au point courant.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg" height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1>Méthode standard de descente de gradient</h1>
            <ul>
                <li>Prenons le problème de la minimisation d'une fonction objective
                    <ul>
                        <li>\(Q(w) = \frac{1}{n} (&#931;Q_i(w)), 1&lt;=i&lt;n\)</li>
                        <li>\(Q_i(w)\) est la valeur de la fonction objectif pour le \(i\)-ème exemple.</li>
                        <li>\(Q(w)\) est le risque empirique.</li>
                    </ul>
                </li>
                <li>\(w = w - &#951;.&#8711; Q(w)\)</li>
                <li>\(w = w - \frac{\eta}{n} \sum_{i=1}^n \nabla Q_i(w)\), \(\eta\) est le pas de l'itération </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1>Méthode itérative</h1>
            <ul>
                <li>Choisissez un vecteur initial de paramètres \(w\) et le taux d'apprentissage &#951;.</li>
                <li>Répétez l'opération jusqu'à l'obtention d'un minimum approximatif :
                    <ul>
                        <li>Mélangez aléatoirement les exemples dans le jeu de formation.</li>
                        <li>\(w = w - &#951;.&#8711; Q_i(w)\), \(i=1...n\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Classification</li>
                <li>Régression</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>partitionnement en k-moyennes (k-means clustering)</h1>
            <ul>
                <li>méthode de partitionnement de données</li>
                <li>L'entrée est un ensemble de points et un nombre k et l'objectif est de diviser ces points en k groupes</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png" height="200px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>partitionnement en k-moyennes (k-means clustering)</h1>
            <h2 class="topicsubheading">Étape 1 (Initialisation)</h2>
            <ul>
                <li>k "moyens" initiaux (dans ce cas k=3) sont générés de manière aléatoire</li>
            </ul>
            <figure>
                <img src="../../../../../fr/enseignement/cours/2020/DataMining/K_Means_Example_Step_1.svg" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>partitionnement en k-moyennes (k-means clustering)</h1>
            <h2 class="topicsubheading">Étape 2 (Étape d'affectation)</h2>
            <ul>
                <li>k clusters sont créés en associant chaque observation à la moyenne la plus proche. Les partitions représentent ici le diagramme de Voronoï généré par les moyennes.
                </li>
            </ul>
            <figure>
                <img src="../../../../../fr/enseignement/cours/2020/DataMining/K_Means_Example_Step_2.svg" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>partitionnement en k-moyennes (k-means clustering)</h1>
            <h2 class="topicsubheading">Étape 3 (Étape de mise à jour et calcul du centroïde)</h2>
            <ul>
                <li>Le centroïde de chacun des k agrégats devient la nouvelle moyenne.</li>
            </ul>
            <figure>
                <img src="../../../../../fr/enseignement/cours/2020/DataMining/K_Means_Example_Step_3.svg" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>partitionnement en k-moyennes (k-means clustering)</h1>
            <h2 class="topicsubheading">Étape 4 (Répéter jusqu'à la convergence)</h2>
            <ul>
                <li>Les étapes 2 et 3 sont répétées jusqu'à ce que la convergence soit atteinte. </li>
                <li>L'algorithme a convergé lorsque les affectations ne changent plus.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>Méthode des k plus proches voisins</h1>
            <ul>
                <li>Classification k-NN : la sortie est une appartenance à une classe
                    <br/> (l'objet est classé par un vote majoritaire de ses voisins).</li>
                <li>Régression k-NN : la sortie est la valeur de propriété de l'objet</br>
                    (valeurs moyennes de ses k plus proches voisins)</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/KnnClassification.svg" height="500px" width="500px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Régression</li>
                <li>Détection des anomalies</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ul>
                <li>Collection de classificateurs probabilistes simples basés sur l'application du théorème de Bayes avec une forte hypothèse d'indépendance entre les caractéristiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Classification des documents (spam/non-spam)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
                <a class="next" href="#slide91"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide91">
        <div class="header">
            <h1>3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h1>Théorème de Bayes</h1>
            <ul>
                <li>\(P(A), P(B)\) sont des probabilités d'observer A et B indépendamment l'un de l'autre.</li>
                <li>\(P(A|B)\) est une probabilité conditionnelle, la probabilité que l'événement \(A\) se produise étant donné que \(B\) est vrai</li>
                <li>\(P(B|A)\) est une probabilité conditionnelle, la probabilité que l'événement \(B\) se produise étant donné que \(A\) est vrai</li>
                <li> \(P(B) &#8800; 0\)</li>
            </ul>
            <p>\[P(A|B) = \frac{(P(B|A).P(A))}{P(B)}\]</i>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">91
                <a class="prev" href="#slide90"></a>
                <a class="next" href="#slide92"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide92">
        <div class="header">
            <h1>3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h1>Théorème de Bayes: Classification d'un message</h1>
            <ul>
                <li>\(P(S)\) est la probabilité globale qu'un message donné soit un spam.</li>
                <li>\(P(H)\) est la probabilité globale qu'un message donné ne soit pas du spam.</li>
                <li>\(P(S|W)\) est la probabilité qu'un message soit un spam, sachant que le mot s'y trouve ;</li>
                <li>\(P(W|S)\) est la probabilité que le mot apparaisse dans les messages de spam ;</li>
                <li>\(P(W|H)\) est la probabilité que le mot "réplique" apparaisse dans les messages ham.</li>
            </ul>
            <p>\[P(S|W) = \frac{P(W|S) \cdot P(S)}{P(W|S) \cdot P(S) + P(W|H) \cdot P(H)}\]</i>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">92
                <a class="prev" href="#slide91"></a>
                <a class="next" href="#slide93"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide93">
        <div class="header">
            <h1>3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ul>
                <li>Outil d'aide à la décision</li>
                <li>Modèle arborescent des décisions et de leurs conséquences possibles</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Decision_tree_model.png" height="400px" width="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">93
                <a class="prev" href="#slide92"></a>
                <a class="next" href="#slide94"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide94">
        <div class="header">
            <h1>3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ul>
                <li> Les données sont disponibles sous la forme \[(\textbf{x},Y) = (x_1, x_2, x_3, ..., x_k, Y)\]</li>
                <li>Le vecteur \(\textbf{x}\) est composé des caractéristiques suivantes \(x_1, x_2, x_3, ...\)</li>
                <li>\(Y\) est la variable dépendante qui peut dépendre de \(\textbf{x}\)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">94
                <a class="prev" href="#slide93"></a>
                <a class="next" href="#slide95"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide95">
        <div class="header">
            <h1>3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Classification</li>
                <li>Régression</li>
                <li>Analyse de la décision : identifier les stratégies pour atteindre un objectif</li>
                <li>Recherche opérationnelle</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">95
                <a class="prev" href="#slide94"></a>
                <a class="next" href="#slide96"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide96">
        <div class="header">
            <h1>3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <h1>Définition</h1>
            <ul>
                <li>Collecte de plusieurs algorithmes d'apprentissage pour obtenir de meilleures performances prédictives qu'un seul des algorithmes constitutifs</li>
                <li>Les forêts aléatoires sont obtenues en construisant des arbres de décision multiples au moment de la formation</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">96
                <a class="prev" href="#slide95"></a>
                <a class="next" href="#slide97"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide97">
        <div class="header">
            <h1>3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <h1>Algorithme</h1>
            <ul>
                <li>Soit \(X = x_1,x_2,..x_n\) un ensemble de données avec des réponses \(Y = y_1,y_2,..y_n\)</li>
                <li> Soit \(b = 1, 2,..B\)
                    <ul>
                        <li>Échantillon, avec remplacement (un élément peut apparaître plusieurs fois dans un même échantillon), \(n\) exemples de formation de \(X, Y\) ; appelez-les \(X_b, Y_b\).</li>
                        <li>Former un arbre de classification ou de régression \(f_b\) sur \(X_b, Y_b\).</li>
                    </ul>
                </li>
                <li>Après entraînement, les prédictions pour les échantillons non vus x' peuvent être faites en faisant la moyenne des prédictions de tous les arbres de régression individuels sur x' \[\hat{f} = \frac{1}{B} \sum_{b=1}^Bf_b (x')\] ou par un
                    vote à la majorité dans le cas des arbres de classification.
                </li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">97
                <a class="prev" href="#slide96"></a>
                <a class="next" href="#slide98"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide98">
        <div class="header">
            <h1>3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Classification multiclasse</li>
                <li>Classification multilabel (problème de l'attribution d'un ou plusieurs labels à chaque instance. Il n'y a pas de limite au nombre de classes auxquelles une instance peut être assignée).</li>
                <li>Régression</li>
                <li>Détection des anomalies</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">98
                <a class="prev" href="#slide97"></a>
                <a class="next" href="#slide99"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide99">
        <div class="header">
            <h1>4. Sélection de caractéristique</h1>
        </div>
        <div class="content">
            <h1>Définition</h1>
            <ul>
                <li>Processus de sélection d'un sous-ensemble de caractéristiques pertinentes</li>
                <li>Utilisé dans des domaines présentant un grand nombre de caractéristiques et relativement peu de points d'échantillonnage</li>
                <li>une méthode de réduction de la dimensionnalité</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">99
                <a class="prev" href="#slide98"></a>
                <a class="next" href="#slide100"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide100">
        <div class="header">
            <h1>4. Sélection de caractéristique</h1>
        </div>
        <div class="content">
            <h1>Applications</h1>
            <ul>
                <li>Analyse des textes écrits</li>
                <li>Analyse des données des puces à ADN</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">100
                <a class="prev" href="#slide99"></a>
                <a class="next" href="#slide101"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide101">
        <div class="header">
            <h1>4. Sélection de caractéristique</h1>
        </div>
        <div class="content">
            <h1>Définition formelle[8]</h1>
            <ul>
                <li>Soit \(X\) l'ensemble original de \(n\) caractéristiques, c'est-à-dire, \(|X| = n\)</li>
                <li>Soit \(w_i\) le poids attribué à l'élément \(x_i &#8712; X\)</li>
                <li>La sélection binaire attribue des poids binaires tandis que la sélection continue attribue des poids en préservant l'ordre de sa pertinence.</li>
                <li>Soit \(J(X')\) soit une mesure d'évaluation, définie comme \(J: X' &#8838; X &#8594; R\)</li>
                <li> Le problème de la sélection des caractéristiques peut être défini de trois façons
                    <ol>
                        <li>\(|X'| = m &lt; n\). Trouver \(X' &#8834; X\) tel que \(J(X')\) est le maximum</li>
                        <li>Choisir \(J_0\), Trouver \(X' &#8838; X\), tel que \(J(X') &gt;= J_0\)</li>
                        <li>Trouver un compromis entre la minimisation de \(|X'|\) et la maximisation du \(J(X')\)</li>
                    </ol>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">101
                <a class="prev" href="#slide100"></a>
                <a class="next" href="#slide102"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide102">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ol>
                <li>From data mining to knowledge discovery in databases, Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth, AI Magazine Volume 17 Number 3 (1996)</li>
                <li>Survey of Clustering Data Mining Techniques, Pavel Berkhin</li>
                <li>Mining association rules between sets of items in large databases, Agrawal, Rakesh, Tomasz Imieliński, and Arun Swami. Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD 1993. p. 207. </li>
                <li>Comparisons of Sequence Labeling Algorithms and Extensions, Nguyen, Nam, and Yunsong Guo. Proceedings of the 24th international conference on Machine learning. ACM, 2007. </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">102
                <a class="prev" href="#slide101"></a>
                <a class="next" href="#slide103"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide103">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ol start="5">
                <li>An Analysis of Active Learning Strategies for Sequence Labeling Tasks, Settles, Burr, and Mark Craven. Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2008.</li>
                <li>Anomaly detection in crowded scenes, Mahadevan; Vijay et al. Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010</li>
                <li>A Study of Global Inference Algorithms in Multi-Document Summarization. McDonald, Ryan. European Conference on Information Retrieval. Springer, Berlin, Heidelberg, 2007.</li>
                <li>Feature selection algorithms: A survey and experimental evaluation., Molina, Luis Carlos, Lluís Belanche, and Àngela Nebot. Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002.</li>
                <li>Support vector machines, Hearst, Marti A., et al. IEEE Intelligent Systems and their applications 13.4 (1998): 18-28.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">103
                <a class="prev" href="#slide102"></a>
                <a class="next" href="#slide104"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide104">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Ressources en ligne</h1>
            <ul>
                <li><a href="https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124">Accuracy, Recall, Precision, F-Score & Specificity, which to optimize on?</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Patterns_in_nature">Patterns in Nature</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Statistical_classification">Statistical classification</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Association_rule_learning">Association rule learning</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Anomaly_detection">Anomaly detection</a></li>
            </ul>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/Sequence_labeling">Sequence labeling</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Automatic_summarization">Automatic summarization</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Pattern_recognition">Pattern recognition</a></li>
                <li><a href="http://scikit-learn.org/stable/">Scikit-learn</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">104
                <a class="prev" href="#slide103"></a>
                <a class="next" href="#slide105"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide105">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Ressources en ligne</h1>
            <ul>
                <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient descent</a></li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">105
                <a class="prev" href="#slide104"></a>
                <a class="next" href="#slide106"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide106">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">106
                <a class="prev" href="#slide105"></a>
                <a class="next" href="#slide107"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 106) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function(event) {

            event.preventDefault();
            /* This will ensure the default behavior of
													        page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function(event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
    <script src="MathJax.js"></script>
</body>

</html>