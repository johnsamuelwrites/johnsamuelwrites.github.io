<html>

<head>
    <meta charset="utf-8" />
    <title>Traitement de données massives (2025-2026): Construction des modèles de traitement: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            line-height: 1.7em;
            flex-direction: column;
            align-items: flex-start;
            margin: 0 auto;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 90%;
            max-height: 90%;
        }

        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }

        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 1.7em;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }

        /* Using same Jupyter CSS
     */

        .highlight {
            background: #f8f8f8;
        }

        .highlight .c {
            color: #408080;
            font-style: italic
        }

        /* Comment */

        .highlight .err {
            border: 1px solid #FF0000
        }

        /* Error */

        .highlight .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */

        .highlight .o {
            color: #666666
        }

        /* Operator */

        .highlight .ch {
            color: #408080;
            font-style: italic
        }

        /* Comment.Hashbang */

        .highlight .c1 {
            color: #408080;
            font-style: italic
        }

        /* Comment.Single */

        .highlight .cs {
            color: #408080;
            font-style: italic
        }

        /* Comment.Special */

        .highlight .cm {
            color: #408080;
            font-style: italic
        }

        /* Comment.Multiline */

        .highlight .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */

        .highlight .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */

        .highlight .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */

        .highlight .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */

        .highlight .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */

        .highlight .nb {
            color: #008000
        }

        /* Name.Builtin */

        .highlight .mb {
            color: #666666
        }

        /* Literal.Number.Bin */

        .highlight .mf {
            color: #666666
        }

        /* Literal.Number.Float */

        .highlight .mh {
            color: #666666
        }

        /* Literal.Number.Hex */

        .highlight .mi {
            color: #666666
        }

        /* Literal.Number.Integer */

        .highlight .mo {
            color: #666666
        }

        /* Literal.Number.Oct */

        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                font-size: 2.8vmin;
            }

            .content .flexcontent {
                font-size: 2.5vmin;
            }
        }
    </style>
    <script src="../../../../../fr/enseignement/cours/2020/MachineLearning/tex-mml-chtml.js"
        id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:3.5vw">Traitement de données massives</h1>
            <p><b>John Samuel</b></br>
                CPE Lyon<br /><br />
                <b>Année</b>: 2025-2026<br />
                <b>Courriel</b>: john.samuel@cpe.fr</br>
                </br>
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>Data Mining</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs</h3>
            <ol>
                <li>Apprentissage machine</li>
                <li>Apprentissage profond</li>
                <li>Apprentissage par renforcement</li>
                <li>Licences de données, éthique et vie privée</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Neurones biologiques</h2>
            <figure>
                <img src="../../2021/MachineLearning/Neuron3.png" height="350px" />
                <figcaption>Neurone biologique<sup>1</sup></figcaption>
            </figure>
            <ol style="font-size:2vh">
                <li>https://en.wikipedia.org/wiki/File:Neuron3.png</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Introduction</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Colored_neural_network.svg" />
                <figcaption>Réseaux de neurones artificiels</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Réseau de neurones</h2>
            <p>Les réseaux de neurones sont couramment utilisés dans le domaine de l'apprentissage machine, en
                particulier dans des tâches telles que la classification, la régression, la reconnaissance d'images, le
                traitement du langage naturel, et bien d'autres. Un réseau de neurones artificiels est une collection
                d'unités interconnectées appelées neurones artificiels. Ces réseaux sont inspirés de la structure du
                cerveau biologique</p>
            <ul>
                <li><b>Connexions</b> : Chaque connexion entre les neurones, similaire aux synapses dans le cerveau
                    biologique, peut transmettre un signal aux autres neurones.</li>
                <li><b>Transmission de signal</b> : Un neurone artificiel reçoit un signal, le traite à l'aide d'une
                    fonction non linéaire, et peut ensuite transmettre un signal aux neurones qui lui sont connectés.
                </li>
                <li><b>Fonction d'activation</b> : La sortie de chaque neurone est calculée par une fonction non
                    linéaire appliquée à la somme pondérée de ses entrées. Cette fonction d'activation introduit une
                    non-linéarité dans le réseau, permettant de modéliser des relations complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Réseau de neurones</h2>
            <ul>
                <li><b>Poids ajustables</b> : Les neurones et les connexions ont généralement des poids qui sont ajustés
                    au fur et à mesure de l'apprentissage. Ces poids déterminent l'importance relative des différentes
                    entrées pour chaque neurone.</li>
                <li><b>Ajustement des poids</b> : Les poids peuvent être ajustés pour augmenter ou diminuer la force du
                    signal au niveau d'une connexion, influençant ainsi la contribution de cette connexion aux calculs
                    du réseau.</li>
                <li><b>Seuil</b> : Les neurones peuvent avoir un seuil, de sorte qu'un signal n'est envoyé que si la
                    somme pondérée de ses entrées dépasse ce seuil. Cela permet au réseau de moduler sa sensibilité aux
                    entrées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Les couches</h2>
            <p>Les neurones sont organisés en couches. Il existe généralement trois types de couches dans un réseau de
                neurones :</p>
            <ul>
                <li><b>Couche d'Entrée (Input Layer)</b> : Cette couche reçoit les signaux initiaux ou les données en
                    entrée. Chaque neurone dans cette couche représente une caractéristique ou une variable d'entrée.
                </li>
                <li><b>Couches Cachées (Hidden Layers)</b> : Ces couches effectuent des transformations non linéaires
                    sur les entrées. Elles sont responsables de l'extraction et de la représentation des
                    caractéristiques importantes des données. Un réseau de neurones peut avoir une ou plusieurs couches
                    cachées.</li>
                <li><b>Couche de Sortie (Output Layer)</b> : Cette couche génère la sortie du réseau. Le nombre de
                    neurones dans cette couche dépend de la nature de la tâche, par exemple, une classification binaire
                    aurait un neurone de sortie, tandis qu'une classification multi-classes en aurait plusieurs.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Les couches</h2>
            <ul>
                <li><b>Transformations</b> : Chaque couche, y compris la couche d'entrée, effectue des transformations
                    sur les signaux qu'elle reçoit. Ces transformations sont déterminées par les poids des connexions
                    entre les neurones.</li>
                <li><b>Propagation des signaux</b> : Les signaux passent de la première couche (l'entrée) à la dernière
                    couche (la sortie) à travers les connexions pondérées entre les neurones. Ce processus est souvent
                    appelé la propagation avant (forward propagation). Pendant l'apprentissage, la rétropropagation
                    (backpropagation) est utilisée pour ajuster les poids afin de minimiser l'erreur de prédiction.</li>
                <li><b>Architecture</b> : La manière dont les couches sont organisées et connectées dans le réseau
                    constitue son architecture. Les réseaux de neurones peuvent avoir des architectures diverses, y
                    compris des réseaux profonds (avec de nombreuses couches cachées) ou des architectures plus simples.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">L'entraînement</h2>
            <p>L'objectif global de l'entraînement est d'ajuster les poids du réseau de manière à ce qu'il puisse
                généraliser à de nouvelles données, produisant des résultats précis pour des exemples qu'il n'a pas vu
                pendant l'entraînement.</p>
            <ul>
                <li><b>Données d'entraînement</b> : Les réseaux neuronaux apprennent à partir d'exemples. Chaque exemple
                    se compose d'une "entrée" (les caractéristiques) et d'un "résultat" connu (l'étiquette ou la sortie
                    attendue).</li>
                <li><b>Calcul de l'erreur</b> : Lorsque le réseau produit une sortie pour une entrée donnée, l'erreur
                    est calculée en comparant cette sortie à la sortie cible (le résultat connu). Il existe différentes
                    mesures d'erreur, mais la somme des carrés des différences (Mean Squared Error, MSE) est couramment
                    utilisée.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">L'entraînement</h2>
            <ul>
                <li><b>Rétropropagation (Backpropagation)</b> : Le réseau ajuste ses poids en utilisant la
                    rétropropagation. Cette technique minimise l'erreur en modifiant les poids à partir de la couche de
                    sortie jusqu'à la couche d'entrée. La règle de la chaîne du calcul différentiel est appliquée pour
                    propager l'erreur à travers le réseau.</li>
                <li><b>Descente de gradient</b> : La règle d'apprentissage souvent utilisée pour ajuster les poids est
                    la descente de gradient. Elle utilise le gradient de l'erreur par rapport aux poids pour mettre à
                    jour les poids dans la direction qui minimise l'erreur.</li>
                <li><b>Itérations</b> : Le processus d'ajustement des poids en fonction de l'erreur est répété pour de
                    nombreux exemples du jeu de données d'entraînement. Chaque itération est appelée une "époque".
                    Plusieurs époques peuvent être nécessaires pour que le réseau converge vers un état où l'erreur est
                    suffisamment basse.</li>
                <li><b>Optimisation</b> : Différentes techniques d'optimisation peuvent être utilisées pour améliorer la
                    convergence du réseau, telles que l'ajustement adaptatif du taux d'apprentissage.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <ul>
                <li><b>Neurones</b> : Les neurones artificiels sont les unités de base d'un réseau de neurones. Chaque
                    neurone reçoit des signaux d'entrée, effectue un calcul sur ces signaux à l'aide d'une fonction
                    d'activation, et produit une sortie. Les neurones sont organisés en couches, à savoir la couche
                    d'entrée, les couches cachées, et la couche de sortie.</li>
                <li><b>Connexions et Poids</b> : Les connexions entre les neurones sont représentées par des poids.
                    Chaque connexion a un poids associé, qui détermine l'importance relative de cette connexion dans le
                    calcul du neurone de sortie. Pendant l'entraînement, ces poids sont ajustés pour minimiser l'erreur
                    de prédiction du réseau.</li>
                <li><b>Fonction de Propagation (Propagation avant)</b> : La fonction de propagation, également appelée
                    propagation avant, décrit le processus par lequel les signaux se propagent à travers le réseau
                    depuis la couche d'entrée jusqu'à la couche de sortie. Chaque neurone effectue une transformation
                    sur les signaux qu'il reçoit, et ces signaux modifiés sont transmis aux neurones de la couche
                    suivante.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Neurones</h2>
            <p>Chaque neurone artificiel a des entrées, qui peuvent être les valeurs caractéristiques d'un échantillon
                de données externe, et produit une seule sortie. Cette sortie peut être envoyée à plusieurs autres
                neurones, formant ainsi la structure interconnectée du réseau neuronal. La <b>fonction d'activation</b>
                joue un rôle crucial dans le calcul de la sortie d'un neurone. Le processus comprend les étapes
                suivantes :</p>
            <ul>
                <li><b>Somme pondérée</b> : Pour trouver la sortie du neurone, on prend la somme pondérée de tous les
                    intrants (entrées). Chaque entrée est multipliée par le poids correspondant à la connexion.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Neurones</h2>
            <ul>
                <li><b>Ajout d'un terme de biais</b> : Un terme de biais est ajouté à la somme pondérée. Le terme de
                    biais est un paramètre supplémentaire qui permet au modèle d'apprendre un décalage ou une
                    translation.</li>
                <li><b>Activation</b> : La somme pondérée, parfois appelée activation, est ensuite passée par une
                    fonction d'activation. Cette fonction est généralement non linéaire et introduit de la complexité
                    dans le modèle, permettant au réseau de capturer des relations non linéaires dans les données</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <p><b>Connexions et poids</b>: Le réseau de neurones est constitué de connexions, où chaque connexion
                transmet la sortie d'un neurone
                comme entrée à un autre neurone. Chaque connexion possède un poids qui représente son importance
                relative dans la transmission du signal.</p>
            <ul>
                <li>Un neurone donné peut avoir <b>plusieurs connexions d'entrée</b>, recevant des signaux de différents
                    neurones, et plusieurs connexions de sortie, transmettant des signaux à d'autres neurones. Les poids
                    associés à ces connexions permettent au réseau de moduler l'influence de chaque neurone sur les
                    autres, ajustant ainsi la force et la direction des signaux transmis à travers le réseau.</li>
                <li>Cette structure de connexion et de pondération est fondamentale dans le fonctionnement des réseaux
                    de neurones, car elle permet au réseau d'apprendre des représentations complexes des données et
                    d'ajuster ses paramètres pendant l'entraînement pour accomplir des tâches spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Fonction de propagation</h2>
            <p><b>Calcul de l'entrée d'un neurone</b> : La fonction de propagation calcule l'entrée d'un neurone en
                prenant la somme pondérée des sorties de ses prédécesseurs, où chaque sortie est multipliée par le poids
                de la connexion correspondante. Cela peut être représenté mathématiquement comme suit :</p>
            <p>\[ \text{Entrée du Neurone} = \sum_{i=1}^{n} (\text{Sortie du Prédécesseur}_i \times \text{Poids}_i) \]
                où \(n\) est le nombre de connexions d'entrée.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Fonction de propagation</h2>
            <p><b>Ajout d'un terme de biais</b> : Un terme de biais peut être ajouté au résultat de la propagation. Le
                terme de biais est un paramètre supplémentaire, souvent représenté par \(b\) dans les équations, qui
                permet au modèle d'apprendre un décalage ou une translation. Cela donne la forme finale de l'entrée du
                neurone :</p>

            <p>\[ \text{Entrée du Neurone} = \sum_{i=1}^{n} (\text{Sortie du Prédécesseur}_i \times \text{Poids}_i) +
                \text{Biais} \]</p>

        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Fonction de propagation</h2>
            <p><b>Fonction d'Activation</b> : Après avoir calculé l'entrée du neurone, celle-ci est passée à travers une
                fonction d'activation. Cette fonction introduit une non-linéarité dans le modèle, permettant au réseau
                de neurones de capturer des relations complexes et d'apprendre des modèles non linéaires. Certaines des
                fonctions d'activation couramment utilisées comprennent :</p>
            <ul>
                <li><b>Sigmoïde</b> : \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
                <li><b>Tangente hyperbolique (tanh)</b> : \( \text{tanh}(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \)
                </li>
                <li><b>ReLU (Rectified Linear Unit)</b> : \( \text{ReLU}(x) = \max(0, x) \)</li>
                <li><b>Softmax</b> (pour la couche de sortie dans la classification) : \( \text{Softmax}(x)_i =
                    \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Perceptron</h3>
            <p>Le perceptron est un <b>algorithme d'apprentissage supervisé</b> utilisé pour la <b>classification
                    binaire</b>. Il est conçu pour résoudre des problèmes où l'objectif est de déterminer si une entrée
                donnée appartient ou non à une classe particulière.</p>
            <ul>
                <li>Le perceptron a été inventé par <b>Frank Rosenblatt</b> en 1958. L'idée était de créer un modèle
                    simple de neurone artificiel inspiré du fonctionnement des neurones biologiques. Rosenblatt a
                    formulé un algorithme d'apprentissage qui permet au perceptron d'ajuster ses poids en fonction des
                    erreurs de classification, améliorant ainsi ses performances au fil du temps.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Perceptron</h3>
            <ul>
                <li><b>Fonctionnement</b> : Le perceptron prend plusieurs entrées pondérées et les combine en une somme.
                    Ensuite, cette somme est soumise à une fonction d'activation, généralement une fonction échelon
                    (step function), qui produit la sortie binaire du perceptron.</li>
                <li><b>Limitations</b> : Le perceptron a des limitations, notamment sa capacité à résoudre des problèmes
                    non linéaires et son incapacité à apprendre des modèles complexes. Cependant, il a jeté les bases
                    pour le développement de réseaux de neurones plus avancés, en particulier les réseaux multicouches
                    qui peuvent apprendre des représentations hiérarchiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Perceptron</h1>
            <figure>
                <img src="../../2021/MachineLearning/Perceptron_example.svg" height="350px" />
                <figcaption>Perceptron en mettant à jour sa limite linéaire à mesure que d'autres exemples de formation
                    sont ajoutés.<sup>1</sup></figcaption>
            </figure>
            <ol style="font-size:2vh">
                <li>Source: https://en.wikipedia.org/wiki/File:Perceptron_example.svg</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Perceptron</h1>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Perceptron.svg" height="400px" />
                <figcaption>Perceptron</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Perceptron: Définition formelle</h1>
            <ul>
                <li>Soit \(y = f(z)\) la sortie du perceptron pour un vecteur d'entrée <i>z</i></li>
                <li>Soit \(N\) le nombre d'exemples d'entraînement</li>
                <li>Soit <i><b>X</b></i> l'espace de saisie des caractéristiques</li>
                <li>Soit \({(x_{1}, d_{1}),...,(x_{N}, d_{N})}\) be the <i><b>N</b></i> training examples, where
                    <ul>
                        <li>\(x_i\) est le vecteur caractéristique de <i>i<sup>ème</sup></i> exemple d'entraînement.
                        </li>
                        <li>\(d_i\) est la valeur de sortie souhaitée</li>
                        <li>\(x_{j,i}\) est la <i>i<sup>ème</sup></i> caractéristique de <i>j<sup>ème</sup></i> exemple
                            d'entraînement.</li>
                        <li>\(x_{j,0} = 1\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Perceptron: Définition formelle</h3>
            <ul>
                <li>Les poids sont représentés de la manière suivante:
                    <ul>
                        <li>\(w_i\) est la <i>i<sup>ème</sup></i> valeur du vecteur de poids.</li>
                        <li>\(w_i(t)\) est la <i>i<sup>ème</sup></i> valeur du vecteur de poids à un moment donné t.
                        </li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Perceptron : Étapes</h3>
            <ol>
                <li>Initialiser les poids et les seuils</li>
                <li>Pour chaque exemple, \((x_j, d_j)\) dans l'ensemble d'entraînement<i></i>
                    <ul>
                        <li>Calculer la sortie actuelle : \[y_j(t)= f[w(t).x_j]\] \[= f[w_0(t)x_{j,0} + w_1(t)x_{j,1} +
                            w_2(t)x_{j,2} + \dotsb + w_n(t)x_{j,n}]\]</li>
                        <li>Calculer le poids: \[w_i(t + 1) = w_i(t) + r. (d_j-y_j(t))x_{j,i}\]</li>
                    </ul> \(r\) est le taux d'apprentissage.
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Perceptron : Étapes</h3>
            <ol start="3">
                <li>Répétez l'étape 2 jusqu'à l'erreur d'itération \[\frac{1}{s} (&#931; |d_j - y_j(t)|)\] est inférieur
                    au seuil spécifié par l'utilisateur \(\gamma\), ou un nombre prédéterminé d'itérations ont été
                    effectuées, où \(s\) est à nouveau la taille
                    de l'ensemble de l'échantillon.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'Échelon (Step Function)</h2>
            <p>Le perceptron utilise généralement une fonction d'activation simple, et la fonction d'échelon (step
                function) est fréquemment choisie pour cette tâche. </p>
            <h4>Définition</h4>
            <p>La fonction d'échelon attribue une sortie de 1 si la somme pondérée des entrées dépasse un certain seuil,
                et 0 sinon.</p>
            <p>\( f(x) = \begin{cases} 1 & \text{si } x \geq \text{seuil} \\ 0 & \text{sinon} \end{cases} \)</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: fonction d'identité</h2>
            <h4>Équation</h4>
            <p>\[f(x)=x\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=1\]</p>
            <h3></h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_identity.svg"
                    height="380px" />
                <figcaption>Fonction d'identité</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Fonction d'activation: pas binaire</h2>
                <h4>Équation</h4>
                <p>\[f(x) = \begin{cases} 0 & \text{for } x
                    < 0\\ 1 & \text{for } x \ge 0 \end{cases} \]</p>
                        <h4>Dérivée</h4>
                        <p>\[f'(x) = \begin{cases} 0 & \text{for } x \ne 0\\ ? & \text{for } x = 0\end{cases}\]</p>
                        <figure>
                            <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_binary_step.svg"
                                height="380px" />
                            <figcaption>Pas binaire</figcaption>
                        </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: fonction sigmoïde</h2>
            <h4>Équation</h4>
            <p>\[f(x)=\sigma(x)=\frac{1}{1+e^{-x}}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=f(x)(1-f(x))\]</p>
            <figure>
                <img src="../../2021/MachineLearning/Logistic-curve.svg" height="380px" />
                <figcaption>La fonction sigmoïde</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: TanH</h2>
            <h4>Équation</h4>
            <p>\[f(x)=\tanh(x)=\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=1-f(x)^2\]</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_tanh.svg" height="380px" />
                <figcaption>TanH</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: Rectified linear unit: ReLU</h2>
            <h4>Équation</h4>
            <p>\[f(x) = \begin{cases} 0 & \text{for } x \le 0\\ x & \text{for } x > 0\end{cases} = \max\{0,x\}= x
                \textbf{1}_{x>0}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x) = \begin{cases} 0 & \text{for } x \le 0\\ 1 & \text{for } x > 0\end{cases}\]</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_rectified_linear.svg"
                    height="380px" />
                <figcaption>Unité linéaire rectifiée (ReLU)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: Gaussien</h2>
            <h4>Équation</h4>
            <p>\[f(x)=e^{-x^2}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=-2xe^{-x^2}\]</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_gaussian.svg"
                    height="380px" />
                <figcaption>Gaussien</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>3.1. Apprentissage machine</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Perceptron multiclasse</h2>
            <ul>
                <li>Perceptron peut être généralisé à la classification multiclasse. </li>
                <li>Une fonction de représentation d'élément \(f( x , y )\) fait correspondre chaque paire
                    d'entrée/sortie possible à un vecteur d'élément à valeur réelle en dimension finie.</li>
                <li>le vecteur de caractéristique est multiplié par un vecteur de poids \(w\), mais le score obtenu est
                    maintenant utilisé pour choisir parmi de nombreux résultats possibles : \[\hat y =
                    \operatorname{argmax}_y f(x,y) \cdot w.\]</li>
                <li>La réapprentissage se fait par itération sur les exemples, en prédisant un résultat pour chacun, en
                    laissant les poids inchangés lorsque le résultat prédit correspond à l'objectif, et en les modifiant
                    lorsqu'il ne correspond pas. La mise
                    à jour devient : \[w_{t+1} = w_t + f(x, y) - f(x,\hat y)\].</li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <p>Un <b>réseau de neurones profond</b>, également connu sous le nom de réseau de neurones profondément
                hiérarchisé ou réseau neuronal profond (DNN pour Deep Neural Network en anglais), est un type de réseau
                de neurones artificiels qui comprend plusieurs couches de traitement, généralement plus de deux. Ces
                réseaux sont appelés "profonds" en raison de leur architecture empilée de couches, permettant la
                création de représentations hiérarchiques complexes des données.</p>
            <p><b>Architecture en couches</b> : Les réseaux de neurones profonds sont composés de multiples couches,
                généralement divisées en trois types principaux :</p>
            <ul>
                <li><b>Couche d'Entrée</b> : Reçoit les données brutes ou caractéristiques en entrée.</li>
                <li><b>Couches Cachées</b> : Effectuent des transformations non linéaires et apprennent des
                    représentations hiérarchiques des données.</li>
                <li><b>Couche de Sortie</b> : Produit la sortie du réseau, adaptée à la tâche spécifique
                    (classification, régression, etc.).</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Apprentissage Hiérarchique</b> : Les couches cachées d'un réseau de neurones profond apprennent
                    des caractéristiques de plus en plus abstraites et complexes à mesure que l'on progresse en
                    profondeur. Chaque couche représente une abstraction des caractéristiques extraites par les couches
                    précédentes.</li>
                <li><b>Fonctions d'Activation</b> : Des fonctions d'activation non linéaires, telles que ReLU (Rectified
                    Linear Unit) ou ses variantes, sont couramment utilisées dans les couches cachées pour permettre au
                    réseau d'apprendre des relations non linéaires.</li>
                <li><b>Apprentissage Profond</b> : L'apprentissage profond implique l'ajustement simultané des poids de
                    toutes les couches du réseau pour minimiser l'erreur de prédiction. Cela est généralement réalisé en
                    utilisant des techniques de rétropropagation et de descente de gradient.</li>
                <li><b>Utilisations</b> : Les réseaux de neurones profonds sont utilisés dans une variété de tâches,
                    notamment la vision par ordinateur, la reconnaissance vocale, le traitement du langage naturel, la
                    traduction automatique, la recommandation de contenu, et bien d'autres. Leur capacité à apprendre
                    des représentations complexes a conduit à des avancées significatives dans de nombreux domaines de
                    l'intelligence artificielle.</li>
            </ul>
            <p>L'entraînement de réseaux de neurones profonds peut nécessiter des volumes importants de données et de
                puissance de calcul. </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Apprentissage profond</h3>
            <ul>
                <li>Le terme "profond" se réfère à un réseau qui a un grand nombre de couches, généralement plus de
                    trois.</li>
                <li>Ces réseaux sont également appelés "réseaux de neurones profonds" ou "réseaux neuronaux profonds".
                </li>
                <li>Les réseaux de neurones profonds ont été rendus populaires par leurs capacités à apprendre des
                    représentations hiérarchiques complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Exemple: Tensorflow</h2>
            <div class="highlight">
                <pre><span></span><span class="c1"># Importation des bibliothèques nécessaires de TensorFlow</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="c1"># Étape 1: Création d&#39;un modèle séquentiel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># Étape 2: Ajout d&#39;une couche dense avec une fonction d&#39;activation ReLU</span>
<span class="c1"># La couche a 4 neurones, une fonction d&#39;activation &#39;relu&#39;, et prend une entrée de forme (3,)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)))</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Exemple: Tensorflow</h2>
            <div class="highlight">
                <pre><span class="c1"># Étape 3: Ajout d&#39;une couche dense de sortie avec une fonction d&#39;activation softmax</span>
<span class="c1"># La couche a 2 neurones pour une tâche de classification binaire, et softmax est utilisé</span>
<span class="c1"># pour obtenir des probabilités</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="c1"># Étape 4: Compilation du modèle</span>
<span class="c1"># Utilisation de la descente de gradient stochastique (SGD) comme optimiseur avec un taux d&#39;apprentissage de 0.01</span>
<span class="c1"># La fonction de perte est &#39;mean_squared_error&#39; pour un problème de régression</span>
<span class="c1"># Les performances du modèle seront mesurées en termes de &#39;accuracy&#39; (précision)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Étape 1</b>: On crée un modèle séquentiel, qui est une pile linéaire de couches.</li>
                <li><b>Étape 2</b>: On ajoute une couche dense avec 4 neurones utilisant la fonction d'activation ReLU.
                    La couche prend une entrée de forme (3,) - cela signifie que chaque exemple d'entraînement a trois
                    caractéristiques.</li>
                <li><b>Étape 3</b>: On ajoute une couche dense de sortie avec 2 neurones utilisant la fonction
                    d'activation softmax. Cela est couramment utilisé pour les tâches de classification binaire,
                    fournissant des probabilités pour chaque classe.</li>
                <li><b>Étape 4</b>: On compile le modèle en spécifiant l'optimiseur (SGD avec un taux d'apprentissage de
                    0.01), la fonction de perte ('mean_squared_error' pour une tâche de régression), et les métriques de
                    performance ('accuracy' pour mesurer la précision du modèle).</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <figure>
                <img src="../../2021/MachineLearning/Screenshot_2020-10-20 Tensorflow — Neural Network Playground.png"
                    height="450px" />
                <figcaption>Source: https://playground.tensorflow.org/</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <figure>
                <img src="../../2021/MachineLearning/Screenshot_2020-10-20 Tensorflow 2 — Neural Network Playground.png"
                    height="450px" />
                <figcaption>Source: https://playground.tensorflow.org/</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Organisation</h2>
            <p>Un réseau de neurones profond est une architecture complexe où l'information circule de la couche
                d'entrée à travers les couches cachées jusqu'à la couche de sortie. Chaque connexion entre les neurones
                est associée à un poids qui est ajusté pendant le processus d'apprentissage pour optimiser les
                performances du modèle sur la tâche spécifique. L'utilisation de plusieurs couches cachées permet au
                réseau d'apprendre des représentations de plus en plus abstraites et complexes des données.</p>
            <ul>
                <li><b>Organisation en plusieurs couches</b> : Un réseau de neurones profond est structuré en plusieurs
                    couches, généralement composées d'une couche d'entrée, de plusieurs couches cachées et d'une couche
                    de sortie. Chaque couche est composée de neurones, également appelés nœuds ou unités.</li>
                <li><b>Connexions entre les neurones</b> : Les neurones d'une couche sont connectés aux neurones de la
                    couche immédiatement précédente et de la couche immédiatement suivante. Chaque connexion est
                    associée à un poids qui est ajusté pendant l'apprentissage.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Organisation</h2>
            <ul>
                <li><b>Couche d'entrée</b> : La couche d'entrée est la première couche du réseau. Elle reçoit les
                    données externes, souvent représentées par des caractéristiques d'un ensemble de données. Chaque
                    neurone dans la couche d'entrée correspond à une caractéristique spécifique.</li>
                <li><b>Couche de sortie</b> : La couche de sortie est la dernière couche du réseau. Elle produit le
                    résultat final du modèle en fonction de la tâche spécifique, telle que la classification d'une
                    image, la prédiction d'une valeur, etc. Le nombre de neurones dans cette couche dépend du type de
                    problème (par exemple, un neurone pour chaque classe dans une tâche de classification).</li>
                <li><b>Couches cachées</b> : Entre la couche d'entrée et la couche de sortie, il peut y avoir zéro ou
                    plusieurs couches cachées. Ces couches sont responsables de l'extraction de caractéristiques
                    complexes à partir des données d'entrée. Chaque neurone dans une couche cachée combine les
                    informations des neurones de la couche précédente pour apprendre des représentations hiérarchiques.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Organisation et connectivité</h2>
            <p><b>Connectivité entièrement connectée</b>: Dans une connectivité entièrement connectée, chaque neurone
                d'une couche est connecté à chaque neurone de la couche suivante. Cela signifie que toutes les
                informations de la couche précédente sont transmises à chaque neurone de la couche suivante. C'est la
                configuration la plus courante dans les couches totalement connectées, généralement présentes dans les
                parties du réseau proches de la sortie.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
            <div class="fullwidth">
                <figure>
                    <img src="../../2021/MachineLearning/Deep_Learning.jpg" height="400px" />
                    <figcaption>Source: https://en.wikipedia.org/wiki/File:Deep_Learning.jpg</figcaption>
                </figure>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
            <figure class="fullwidth">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Typical_cnn.png" height="400vh" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
            <p>Les réseaux de neurones convolutionnels (CNN) sont une classe d'architectures de réseaux neuronaux
                conçues principalement pour l'analyse des images. Ils ont été particulièrement efficaces dans des tâches
                telles que la classification d'images, la détection d'objets, et la segmentation d'images. </p>
            <ul>
                <li><b>Analyse des Images</b> : Les CNN sont spécifiquement conçus pour travailler avec des données
                    structurées en grilles, comme les images. Ils sont capables de capturer des motifs et des
                    caractéristiques spatiales importantes dans les images.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
            <ul>
                <li><b>Utilise la convolution</b> La convolution est une opération mathématique linéaire utilisée pour
                    extraire des caractéristiques locales à partir de l'image. Les filtres de convolution sont appliqués
                    à l'image pour détecter des motifs tels que des bords, des textures, ou des formes.</li>
                <li><b>Architecture en couches</b> : Les CNN suivent généralement une architecture en couches. Ils ont
                    une couche d'entrée pour recevoir l'image, une ou plusieurs couches cachées composées principalement
                    de couches convolutives, et une couche de sortie pour produire les résultats finaux.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
            <ul>
                <li><b>Couches convolutives</b> : Les couches convolutives sont responsables de l'extraction des
                    caractéristiques de l'image. Chaque couche peut avoir plusieurs filtres de convolution qui
                    apprennent à détecter des motifs spécifiques. Ces couches sont souvent suivies de couches de pooling
                    pour réduire la dimensionnalité de la représentation tout en préservant les caractéristiques
                    importantes.</li>
                <li><b>Applications</b> : Les CNN sont largement utilisés dans des applications telles que la
                    classification d'images (par exemple, reconnaître des animaux dans des photos), la détection
                    d'objets (localiser et identifier des objets spécifiques), et la segmentation d'images (diviser une
                    image en régions sémantiquement significatives)..</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels: architecture</h3>
            <ul>
                <li><b>Modèle hiérarchique des données</b> : Les réseaux neuronaux convolutifs (CNN) sont en effet
                    conçus pour capturer des caractéristiques hiérarchiques dans les données, en particulier dans le
                    contexte de l'analyse d'images. Cela signifie qu'ils peuvent apprendre des motifs simples dans les
                    premières couches, puis combiner ces motifs pour former des caractéristiques plus complexes dans les
                    couches suivantes.</li>
                <li><b>Architecture d'un CNN</b> : Un réseau neuronal convolutif est généralement composé d'une couche
                    d'entrée, de plusieurs couches cachées et d'une couche de sortie. Les couches cachées consistent
                    principalement en couches convolutionnelles, mais peuvent également inclure d'autres types de
                    couches telles que des couches de regroupement (pooling), des couches entièrement connectées, et des
                    couches de normalisation.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Typical_cnn.png" height="180px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Réseaux de neurones convolutionnels: architecture</h3>
            <ul>
                <li><b>Couches convolutionnelles et fonction d'activation</b> : Les couches convolutionnelles appliquent
                    des filtres pour extraire des caractéristiques de l'image. La multiplication est effectuée par la
                    convolution. La fonction d'activation la plus couramment utilisée est ReLU (Rectified Linear Unit),
                    qui introduit une non-linéarité dans le modèle. Cette non-linéarité est importante pour permettre au
                    réseau d'apprendre des relations complexes dans les données.</li>
                <li><b>Couches supplémentaires</b> : Après les couches de convolution, on peut avoir des couches de
                    regroupement pour réduire la dimensionnalité, des couches entièrement connectées pour combiner des
                    caractéristiques globales, et des couches de normalisation pour améliorer la stabilité de
                    l'apprentissage.</li>
            </ul>
            <p>En résumé, les CNN suivent une architecture hiérarchique, où les couches convolutives apprennent des
                caractéristiques locales, et ces caractéristiques sont ensuite combinées dans les couches suivantes pour
                former des représentations plus complexes. La non-linéarité introduite par la fonction d'activation ReLU
                est cruciale pour permettre au modèle d'apprendre des relations non linéaires dans les données.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Noyau (traitement d'image)</h2>
            <p>Un noyau dans le contexte du traitement d'images, également appelé filtre ou masque, est une petite
                matrice qui est appliquée sur une image à l'aide d'une opération de convolution. L'objectif de
                l'application de ces noyaux est de réaliser diverses opérations de filtrage sur l'image, telles que la
                détection de contours, l'amélioration des détails, la mise en évidence de certaines caractéristiques,
                etc.</p>
            <ul>
                <li><b>Convolution dans les CNN</b> : Dans les CNN, la convolution est une opération clé qui consiste à
                    appliquer un ensemble de filtres (noyaux) à une image d'entrée. Chaque filtre est conçu pour
                    extraire des caractéristiques spécifiques de l'image, comme des bords, des textures, ou d'autres
                    motifs.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Noyau (traitement d'image)</h2>
            <ul>
                <li><b>Apprentissage des noyaux</b> : L'une des caractéristiques importantes des CNN est la capacité
                    d'apprendre les filtres (noyaux) de manière automatique pendant l'entraînement. Au lieu de définir
                    manuellement les filtres comme dans le traitement d'images traditionnel, les CNN ajustent les poids
                    des filtres pendant la phase d'apprentissage en fonction des caractéristiques qui sont importantes
                    pour la tâche à accomplir.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>3.2. Apprentissage profond</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Noyau (traitement d'image)</h2>
            <ul>
                <li><b>Rôle dans la hiérarchie des caractéristiques</b> : Les premières couches d'un CNN apprennent
                    généralement des filtres simples qui détectent des contours ou des textures de base. À mesure que
                    l'on progresse dans les couches du réseau, les filtres deviennent plus complexes, capturant des
                    caractéristiques de niveau supérieur, jusqu'à ce que la sortie finale représente des
                    caractéristiques abstraites de l'image d'entrée.</li>
                <li><b>Réduction de dimension avec le pooling</b> : Après la convolution, les CNN utilisent souvent des
                    couches de pooling pour réduire la dimension de la représentation, tout en préservant les
                    caractéristiques importantes extraites par les filtres. Cela permet d'économiser des ressources
                    computationnelles tout en maintenant les informations cruciales.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>3.3. Apprentissage par renforcement</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Apprentissage par renforcement</h1>
            <div class="flexcontent">
                <ul style="width:50%">
                    <li>L'apprentissage par renforcement (Reinforcement Learning - RL) est une branche de
                        l'apprentissage
                        automatique inspirée des théories de la psychologie animale.</li>
                    <li><b>Agent autonome</b> : RL implique un agent autonome interagissant avec un environnement. </li>
                    <li><b>Prise de décision</b> : L'agent prend des décisions en fonction de son état actuel. </li>
                    <li><b>Récompenses et pénalités</b> : L'environnement fournit à l'agent des récompenses, qui peuvent
                        être positives ou négatives. </li>
                    <li><b>Objectif</b> : L'objectif est de maximiser la somme des récompenses cumulatives au fil du
                        temps.
                    </li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Reinforcement_learning_diagram.svg"
                        height="300vh" />
                </figure>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>3.4. Licences, Ethiques et la vie privé</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Licences, Éthique et la vie privé</h1>
            <ul>
                <li>Droits d'utilisation des données</li>
                <li>Confidentialité et vie privée</li>
                <li>Éthique</li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../fr/enseignement/cours/2017/BigData/images/Open_Definition_logo.png"
                    height="100vh" />
                <img src="../../../../../en/teaching/courses/2017/ArchitectureInformationSystems/images/Privacy_written_in_tiles.jpg"
                    height="300vh" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>3.4. Licences, Ethiques et la vie privé</h1>
        </div>
        <div class="content">
            <div class="flexcontent">
                <figure class="fullwidth">
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-NC-ND.svg"
                        height="100vh" />
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-NC-SA.svg"
                        height="100vh" />
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-NC.svg" height="100vh" />
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-ND.svg" height="100vh" />
                </figure>
                <figure class="fullwidth">
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-SA.svg" height="100vh" />
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY.svg" height="100vh" />
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/Cc-zero.svg" height="100vh" />
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/Cc-public_domain_mark_white.svg"
                        height="100vh" />
                    <figcaption>Exemples: Creative Commons (CC)</figcaption>
                </figure>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>3.4. Licences, Ethiques et la vie privé</h1>
        </div>
        <div class="content">
            <figure>
                <img src="../../../../../fr/enseignement/cours/2018/BigData/images/Creative_commons_license_spectrum.svg"
                    height="500vh" />
                <figcaption>Exemples: Creative Commons (CC)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>3.4. Licences, Ethiques et la vie privé</h1>
        </div>
        <div class="content">
            <figure>

                <img src="../../2018/BigData/images/Wikimedia_logo_family_complete-2013.svg" height="400vh" />
                <figcaption>Données ouvertes</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>3.4. Licences, Ethiques et la vie privé</h1>
        </div>
        <div class="content">
            <figure>
                <img src="../../../../../fr/enseignement/cours/2017/BigData/images/LOD_Cloud_2014.svg.png"
                    height="400vh" />
                <figcaption>Données ouvertes liées (Linked Open data: LOD) </figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>3.4. Licences, Ethiques et la vie privé</h1>
        </div>
        <div class="content">
            <figure>
                <img src="../../../../../fr/enseignement/cours/2017/BigData/images/Internet_Archive_logo_and_wordmark.svg.png"
                    height="400vh" />
                <figcaption>Données archivées</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Ressources en ligne</h3>
            <ul>
                <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial Neural Network</a>
                    </li>
                    <li><a href="https://fr.wikipedia.org/wiki/Noyau_(traitement_d%27image)">Noyau (traitement
                            d'image)</a></li>
                    <li><a href="https://fr.wikipedia.org/wiki/R%C3%A9seau_neuronal_convolutif">Réseau neuronal
                            convolutif</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Perceptron">Perceptron</a></li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Couleurs</h3>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h3 class="topicsubheading">Images</h3>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
            </div>
        </div>
    </section>


    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 63) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>
