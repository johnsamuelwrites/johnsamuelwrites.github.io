<html>

<head>
    <meta charset="utf-8" />
    <title>Traitement de donn√©es massives (2025-2026): Cours: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            line-height: 1.7em;
            flex-direction: column;
            align-items: flex-start;
            margin: 0 auto;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .codeexample {
            background-color: #eeeeee;
        }

        /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
        pre {
            line-height: 125%;
        }

        td.linenos .normal {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        td.linenos .special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos.special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        body .hll {
            background-color: #ffffcc
        }

        body {
            background: #f8f8f8;
        }

        body .c {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment */
        body .err {
            border: 1px solid #FF0000
        }

        /* Error */
        body .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */
        body .o {
            color: #666666
        }

        /* Operator */
        body .ch {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Hashbang */
        body .cm {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Multiline */
        body .cp {
            color: #9C6500
        }

        /* Comment.Preproc */
        body .cpf {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.PreprocFile */
        body .c1 {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Single */
        body .cs {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Special */
        body .gd {
            color: #A00000
        }

        /* Generic.Deleted */
        body .ge {
            font-style: italic
        }

        /* Generic.Emph */
        body .gr {
            color: #E40000
        }

        /* Generic.Error */
        body .gh {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Heading */
        body .gi {
            color: #008400
        }

        /* Generic.Inserted */
        body .go {
            color: #717171
        }

        /* Generic.Output */
        body .gp {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Prompt */
        body .gs {
            font-weight: bold
        }

        /* Generic.Strong */
        body .gu {
            color: #800080;
            font-weight: bold
        }

        /* Generic.Subheading */
        body .gt {
            color: #0044DD
        }

        /* Generic.Traceback */
        body .kc {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Constant */
        body .kd {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Declaration */
        body .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */
        body .kp {
            color: #008000
        }

        /* Keyword.Pseudo */
        body .kr {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Reserved */
        body .kt {
            color: #B00040
        }

        /* Keyword.Type */
        body .m {
            color: #666666
        }

        /* Literal.Number */
        body .s {
            color: #BA2121
        }

        /* Literal.String */
        body .na {
            color: #687822
        }

        /* Name.Attribute */
        body .nb {
            color: #008000
        }

        /* Name.Builtin */
        body .nc {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Class */
        body .no {
            color: #880000
        }

        /* Name.Constant */
        body .nd {
            color: #AA22FF
        }

        /* Name.Decorator */
        body .ni {
            color: #717171;
            font-weight: bold
        }

        /* Name.Entity */
        body .ne {
            color: #CB3F38;
            font-weight: bold
        }

        /* Name.Exception */
        body .nf {
            color: #0000FF
        }

        /* Name.Function */
        body .nl {
            color: #767600
        }

        /* Name.Label */
        body .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */
        body .nt {
            color: #008000;
            font-weight: bold
        }

        /* Name.Tag */
        body .nv {
            color: #19177C
        }

        /* Name.Variable */
        body .ow {
            color: #AA22FF;
            font-weight: bold
        }

        /* Operator.Word */
        body .w {
            color: #bbbbbb
        }

        /* Text.Whitespace */
        body .mb {
            color: #666666
        }

        /* Literal.Number.Bin */
        body .mf {
            color: #666666
        }

        /* Literal.Number.Float */
        body .mh {
            color: #666666
        }

        /* Literal.Number.Hex */
        body .mi {
            color: #666666
        }

        /* Literal.Number.Integer */
        body .mo {
            color: #666666
        }

        /* Literal.Number.Oct */
        body .sa {
            color: #BA2121
        }

        /* Literal.String.Affix */
        body .sb {
            color: #BA2121
        }

        /* Literal.String.Backtick */
        body .sc {
            color: #BA2121
        }

        /* Literal.String.Char */
        body .dl {
            color: #BA2121
        }

        /* Literal.String.Delimiter */
        body .sd {
            color: #BA2121;
            font-style: italic
        }

        /* Literal.String.Doc */
        body .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */
        body .se {
            color: #AA5D1F;
            font-weight: bold
        }

        /* Literal.String.Escape */
        body .sh {
            color: #BA2121
        }

        /* Literal.String.Heredoc */
        body .si {
            color: #A45A77;
            font-weight: bold
        }

        /* Literal.String.Interpol */
        body .sx {
            color: #008000
        }

        /* Literal.String.Other */
        body .sr {
            color: #A45A77
        }

        /* Literal.String.Regex */
        body .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */
        body .ss {
            color: #19177C
        }

        /* Literal.String.Symbol */
        body .bp {
            color: #008000
        }

        /* Name.Builtin.Pseudo */
        body .fm {
            color: #0000FF
        }

        /* Name.Function.Magic */
        body .vc {
            color: #19177C
        }

        /* Name.Variable.Class */
        body .vg {
            color: #19177C
        }

        /* Name.Variable.Global */
        body .vi {
            color: #19177C
        }

        /* Name.Variable.Instance */
        body .vm {
            color: #19177C
        }

        /* Name.Variable.Magic */
        body .il {
            color: #666666
        }

        /* Literal.Number.Integer.Long */


        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 70%;
            margin-bottom: 1vmax;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 90%;
            max-height: 90%;
        }

        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }

        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 1.7em;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }


        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                font-size: 2.8vmin;
            }

            .content .flexcontent {
                font-size: 2.5vmin;
            }
        }
    </style>
    <script src="../../2021/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:2.5vw">Traitement de donn√©es massives</h1>
            <h3>Traitement automatique des langues naturelles (TAL)</h3>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Ann√©e</b>: 2025-2026<br />
                <b>Courriel</b>: john.samuel@cpe.fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles (TAL/NLP) </h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Intelligence artificielle</h3>
            <figure>
                <img src="../../../../../images/art/courses/deeplearningposition.svg" height="450px" />
                <figcaption>Intelligence artificielle</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <p>Le <b>traitement automatique des langues (TAL)</b> est un domaine interdisciplinaire de la linguistique
                informatique qui se concentre sur l'analyse et la compr√©hension du <b>langage naturel</b> (celui utilis√©
                par les humains). Cette section aborde plusieurs aspects cl√©s du TAL, notamment :</p>
            <ul>
                <li><b>Analyser et comprendre le langage naturel (humain)</b>: Le TAL se consacre √† la compr√©hension du
                    langage naturel dans divers contextes, qu'il s'agisse de textes √©crits ou de discours verbal.</li>
                <li>Interaction homme-machine</li>
                <li><b>Syntaxe d'une langue</b>
                    <ul>
                        <li>Parsing : Le parsing consiste √† analyser la structure grammaticale des phrases.</li>
                        <li>L'√©tiquetage en parties du discours (PoS) : L'√©tiquetage PoS consiste √† assigner des
                            cat√©gories grammaticales (comme verbe, nom, adjectif, etc.) aux mots d'une phrase.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>S√©mantique d'une langue</b>
                    <ul>
                        <li>Traduction automatique</li>
                        <li>Reconnaissance d'entit√©s nomm√©es (NER): La NER consiste √† identifier des entit√©s sp√©cifiques
                            (comme des noms de personnes, de lieux ou d'organisations) dans un texte.</li>
                        <li>Analyse des sentiments</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Analyse de syst√®mes TAL</h3>
            <ul>
                <li><b>Racinisation</b> : La racinisation est le processus de r√©duction des mots √† leur forme de base ou
                    de racine. </li>
                <li><b>√âtiquetage morpho-syntaxique</b> : Cette √©tape consiste √† attribuer des balises ou des √©tiquettes
                    aux mots dans un texte en fonction de leur r√¥le grammatical et de leur structure. </li>
                <li><b>Lemmatisation</b> : Contrairement √† la racinisation, la lemmatisation consiste √† ramener les mots
                    √† leur forme canonique ou lemmes. </li>
                <li><b>Morphologie</b> : La morphologie concerne l'√©tude de la structure des mots, notamment comment ils
                    sont form√©s √† partir de morph√®mes (unit√©s de sens). </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation [Frakes 2003]</h3>
            <ul>
                <li>La racination, souvent appel√©e <b>stemming</b> en anglais, est un processus de normalisation
                    linguistique visant √† r√©duire les mots √† leur forme racine, en ignorant les affixes. Elle est
                    utilis√©e pour simplifier les variations morphologiques des mots. </li>
                <li>Les <b>algorithmes de racination</b> appliquent g√©n√©ralement des r√®gles heuristiques pour √©liminer
                    les pr√©fixes et suffixes courants.
                    <ul>
                        <li><b>Exemples</b> : Porter, Snowball</li>
                        <li><b>Limitations</b> : La racination peut conduire √† des r√©sultats non valides, car elle peut
                            produire des racines qui ne sont pas des mots r√©els.</li>
                    </ul>
                </li>
                <li>Exemples
                    <ul>
                        <li>engineer: engineer, engineered, engineering</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation</h3>
            <p>Quelques exemples d'issues potentiellement <b>non valides</b> :</p>
            <ul>
                <li>Racination excessive :
                    <ul>
                        <li>Mot d'origine : "happily"</li>
                        <li>Racination : "happi" (au lieu de la forme correcte "happy")</li>
                    </ul>
                </li>
                <li>Racination incorrecte :
                    <ul>
                        <li> Mot d'origine : "better"</li>
                        <li> Racination : "bet" (au lieu de la forme correcte "better")</li>
                    </ul>
                <li>Cr√©ation de faux mots :
                    <ul>
                        <li> Mot d'origine : "unhappiness"</li>
                        <li> Racination : "unhappi" (cr√©e un faux mot au lieu de "unhappy")</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation</h3>
            <ul>
                <li>Ambigu√Øt√© des r√®gles :
                    <ul>
                        <li> Mot d'origine : "flies" (verbe)</li>
                        <li> Racination : "fli" (peut √™tre confondu avec le nom "fly")</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: mesures d'√©valuation [Frakes 2003]</h3>
            <ul>
                <li>La mesure dans laquelle un algorithm modifie des mots qu'elle r√©duit √† ses racines est appel√©e la
                    <b>force</b> de l'algorithme
                </li>
                <li>Une m√©trique de <b>similarit√©</b> des algorithmes met en correspondance les n-tuples d'algorithmes
                    (n au moins 2), avec un nombre indiquant la similarit√© des algorithmes. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: distance de Hamming [Frakes 2003]</h3>
            <ol>
                <li>La distance de Hamming entre deux cha√Ænes de longueur √©gale est d√©finie comme le nombre de
                    caract√®res des deux cha√Ænes qui sont diff√©rents √† la m√™me position.</li>
                <li>Pour les cha√Ænes de longueur in√©gale, ajouter la diff√©rence de longueur √† la distance de Hamming
                    pour obtenir une fonction de distance de Hamming modifi√©e \(d\)</li>
                <li>Exemples
                    <ul>
                        <li>tri: try, tried, trying</li>
                        <li>\(d\)(tri, try)= 1</li>
                        <li>\(d\)(tri, tried)= 2</li>
                        <li>\(d\)(tri, trying)= 4</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: force [Frakes 2003]</h3>
            <ol>
                <li>Le nombre moyen de mots par classe</li>
                <li>Facteur de compression de l'indice. Soit n est le nombre de mots dans le corpus et s est le nombre
                    de racines. \[\frac{n - s}{n}\]
                </li>
                <li>Le nombre de mots et de racines qui diff√®rent</li>
                <li>Le nombre moyen de caract√®res supprim√©s lors de la formation des racines</li>
                <li>La m√©diane et la moyenne de la distance de Hamming modifi√©e entre les mots et leur racine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: similarit√© [Frakes 2003]</h3>
            <ol>
                <li>Soit \(A1\) et \(A2\) sont deux algorithmes</li>
                <li>Soit \(W\) une liste de mots et \(n\) le nombre de mots dans \(W\) \[ M(A1,A2,W) = \frac{n}{\Sigma
                    d(x_i, y_i)}\]
                </li>
                <li>pour tous les mots \(w_i\) en W, \(x_i\) est le r√©sultat de l'application de \(A1\) √† \(w_i\) et
                    \(y_i\) est le r√©sultat de l'application de \(A2\) √† \(w_i\)</li>
                <li>des algorithmes plus similaires auront des valeurs plus √©lev√©es de M</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: nltk</h3>
            <p>L'objectif est de r√©duire les mots √† leur forme de base ou racine, en √©liminant les suffixes, ce qui
                permet de regrouper diff√©rentes formes d'un mot sous une forme commune. </p>
            <ul>
                <li><b>Porter [Porter 1980]</b> : Le Porter Stemming Algorithm, cr√©√© par Martin Porter en 1980, est bas√©
                    sur un ensemble de r√®gles heuristiques. Il suit une approche it√©rative en appliquant une s√©rie de
                    transformations s√©quentielles aux mots.</li>
                <li><b>Snowball</b> Le Snowball (anciennement appel√© Porter2) est une am√©lioration du Porter Stemmer. Il
                    suit √©galement une approche bas√©e sur des r√®gles, mais il est plus syst√©matique dans son traitement
                    des diff√©rents cas de racination. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <p>L'algorithme de Porter, √©galement connu sous le nom de stemmer de Porter, est un algorithme de racination
                (stemming) d√©velopp√© par Martin Porter en 1980. Son objectif est de r√©duire les mots √† leur forme racine
                ou base en √©liminant les suffixes couramment utilis√©s en anglais.</p>
            <ul>
                <li><b>Pr√©traitement</b> : Convertir le mot en minuscules. Identifier le pr√©fixe 'y' et le traiter comme
                    une voyelle s'il est en premi√®re position, sinon comme une consonne.</li>
                <li><b>Application des r√®gles de racination</b> : L'algorithme de Porter utilise une s√©rie de r√®gles
                    pour √©liminer les suffixes. Ces r√®gles sont appliqu√©es s√©quentiellement jusqu'√† ce qu'aucune d'entre
                    elles ne s'applique plus. Les r√®gles comprennent des op√©rations comme la suppression de suffixes
                    sp√©cifiques, la transformation de certains suffixes en d'autres, et la manipulation de la longueur
                    des mots. </li>
                <li><b>Post-traitement</b> : Certains ajustements post-traitement sont effectu√©s pour am√©liorer la
                    pr√©cision de la racination.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <p>L'algorithme de Porter utilise une s√©rie de r√®gles de racination pour r√©duire les mots √† leur forme
                racine. Quelques-unes des r√®gles de l'algorithme de Porter :</p>
            <ol>
                <li>R√®gles de suppression de suffixes :
                    <ul>
                        <li> "s" : Supprimer le suffixe "s" √† la fin des mots.</li>
                        <li> "sses" : Remplacer par "ss" si la s√©quence se termine par "sses".</li>
                    </ul>
                </li>
                <li>R√®gles de traitement de suffixes sp√©cifiques :
                    <ul>
                        <li> "eed" ou "eedly" : Remplacer par "ee" si la s√©quence se termine par "eed" ou "eedly".</li>
                        <li> "ed" : Supprimer "ed" √† la fin du mot s'il y a une voyelle pr√©c√©dente.</li>
                        <li> "ing" : Supprimer "ing" √† la fin du mot s'il y a une voyelle pr√©c√©dente.</li>
                    </ul>
                </li>

                </li>
                <ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <ol start="3">
                <li>R√®gles de transformation de suffixes en d'autres suffixes :
                    <ul>
                        <li> "at" : Remplacer par "ate" si la s√©quence se termine par "at".</li>
                        <li> "bl" : Ajouter "e" √† la fin si la s√©quence se termine par "bl".</li>
                    </ul>
                </li>

                <li>R√®gles de manipulation de la longueur des mots :
                    <ul>
                        <li> Si la s√©quence se termine par une consonne suivie de "y", remplacer par "i" √† la fin.</li>
                        <li> Si la s√©quence se termine par deux consonnes, supprimer la derni√®re consonne si la
                            pr√©c√©dente est une voyelle.</li>
                    </ul>
                </li>

                <li>R√®gles de manipulation des doubles consonnes :
                    <ul>
                        <li> Supprimer une lettre double √† la fin du mot.</li>
                    </ul>
                </li>
                <ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Porter</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Snowball</h3>
            <p>L'algorithme de Snowball, √©galement connu sous le nom de Snowball stemmer, est un algorithme de
                racination (stemming) d√©velopp√© par Martin Porter comme une extension de son algorithme de Porter.
                Snowball a √©t√© con√ßu pour √™tre plus modulaire et extensible, permettant aux utilisateurs de cr√©er des
                stemmers pour diff√©rentes langues en utilisant un ensemble commun de conventions.</p>
            <p>Les caract√©ristiques principales de l'algorithme de Snowball :</p>
            <ul>
                <li><b>Modularit√©</b> : L'algorithme de Snowball est con√ßu de mani√®re modulaire, permettant la
                    d√©finition de r√®gles sp√©cifiques pour chaque langue. Chaque r√®gle est encapsul√©e dans une unit√©
                    appel√©e "step."</li>
                <li><b>Structure du Langage</b> : L'algorithme de Snowball est souvent utilis√© pour diff√©rentes langues,
                    et la structure du langage est d√©finie par des fichiers de r√®gles sp√©cifiques √† chaque langue. Ces
                    fichiers d√©crivent comment les suffixes et pr√©fixes doivent √™tre trait√©s.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Snowball</h3>
            <ul>
                <li><b>Extensibilit√©</b> : Les utilisateurs peuvent √©tendre l'algorithme de Snowball pour traiter des
                    langues sp√©cifiques en ajoutant des r√®gles appropri√©es dans un fichier d√©di√© √† cette langue.</li>
                <li><b>√âtape de R√®gle</b> : Chaque √©tape (step) de l'algorithme de Snowball est constitu√©e de r√®gles qui
                    d√©crivent comment transformer un mot. Chaque r√®gle a une forme similaire √† "condition -> action," o√π
                    la condition sp√©cifie quand appliquer la r√®gle, et l'action d√©finit la transformation √† effectuer.
                </li>
                <li><b>It√©ration</b> : L'algorithme de Snowball applique les √©tapes de r√®gle it√©rativement jusqu'√† ce
                    qu'aucune d'entre elles ne puisse √™tre appliqu√©e. Cette it√©ration permet de r√©duire progressivement
                    les mots √† leur forme racine.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Snowball</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">snowball</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">snowball</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">√âtiquetage morpho-syntaxique [M√†rquez 2000]</h3>
            <ul>
                <li>L'√©tiquetage morpho-syntaxique, √©galement appel√© <b style="color:#00549d">Part of Speech (PoS)
                        Tagging</b>, est un processus dans lequel chaque mot d'un texte se voit attribuer une balise
                    morpho-syntaxique appropri√©e en fonction de son r√¥le grammatical et de son contexte d'apparition.
                    Ces balises indiquent la cat√©gorie grammaticale √† laquelle chaque mot appartient. </li>
                <li>Il permet de capturer la <b style="color:#00549d">structure grammaticale</b> d'un texte, facilitant
                    ainsi la compr√©hension et l'analyse linguistique automatis√©es.</li>
                <li>Les algorithmes d'√©tiquetage morpho-syntaxique utilisent g√©n√©ralement des <b
                        style="color:#00549d">mod√®les statistiques</b> ou des <b style="color:#00549d">r√®gles
                        linguistiques</b> pour assigner ces balises en fonction du contexte entourant chaque mot.</li>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">√âtiquetage morpho-syntaxique [M√†rquez 2000]</h3>
            <ul>
                <li>Exemples des balises
                    <ul>
                        <li><b style="color:#008000">Noms</b> : Indiquent des entit√©s ou objets concrets. Exemple :
                            "chat," "maison," "fleur"</li>
                        <li><b style="color:#008000">Verbes</b> : Indiquent des actions ou des √©tats. Exemple :
                            "marcher," "manger," "√™tre"</li>
                        <li><b style="color:#008000">Adjectifs</b> : D√©crivent ou qualifient des noms. Exemple : "beau,"
                            "rapide," "intelligent"</li>
                        <li><b style="color:#008000">Adverbes</b> : Modifient des verbes, des adjectifs ou d'autres
                            adverbes, fournissant des informations sur la mani√®re, le lieu, le temps, etc. Exemple :
                            "rapidement," "bien," "ici"</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">√âtiquetage morpho-syntaxique [M√†rquez 2000]</h3>
            <h3 class="topicsubheading">Construction de mod√®les linguistiques</h3>
            <ol>
                <li><b>Approche manuelle</b> :
                    <ul>
                        <li>Construction de r√®gles linguistiques manuelles pour analyser la structure linguistique</li>
                        <li>Exemple : D√©finir des r√®gles pour identifier les parties du discours en fonction de la
                            syntaxe.</li>
                    </ul>
                </li>
                <li><b>Approche statistique</b> :
                    <ul>
                        <li>Utilisation de statistiques et de probabilit√©s pour mod√©liser les relations linguistiques.
                        </li>
                        <li>Collection de n-grammes (bi-grammes, tri-grammes, ...)</li>
                        <li>Ensemble de fr√©quences de cooccurrence</li>
                        <li>L'estimation de la probabilit√© d'une s√©quence de longueur n est calcul√©e en tenant compte de
                            son occurrence dans le corpus d'entra√Ænement</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">√âtiquetage morpho-syntaxique [M√†rquez 2000]</h3>
            <h3 class="topicsubheading">Construction de mod√®les linguistiques</h3>
            <ol>
                <li><b>Apprentissage machine</b> :
                    <ul>
                        <li>Utilisation de techniques d'apprentissage machine pour apprendre automatiquement des mod√®les
                            linguistiques √† partir de donn√©es d'entra√Ænement.</li>
                        <li>Les algorithmes peuvent √™tre entra√Æn√©s √† reconna√Ætre des motifs et des structures
                            linguistiques complexes</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{}</span><span class="s2">-grams&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams (affichage)</h3>
            <p class="codeexample">
                <code>
1-grams<br/>
('He',) ('went',) ('to',) ('school',) ('yesterday',) ('and',) ('attended',) ('the',) ('classes',) <br/>
2-grams<br/>
('He', 'went') ('went', 'to') ('to', 'school') ('school', 'yesterday') ('yesterday', 'and') ('and', 'attended') ('attended', 'the') ('the', 'classes') <br/>
3-grams<br/>
('He', 'went', 'to') ('went', 'to', 'school') ('to', 'school', 'yesterday') ('school', 'yesterday', 'and') ('yesterday', 'and', 'attended') ('and', 'attended', 'the') ('attended', 'the', 'classes') <br/>
4-grams<br/>
('He', 'went', 'to', 'school') ('went', 'to', 'school', 'yesterday') ('to', 'school', 'yesterday', 'and') ('school', 'yesterday', 'and', 'attended') ('yesterday', 'and', 'attended', 'the') ('and', 'attended', 'the', 'classes')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>

            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He goes to school daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>PRP</td>
                    <td>pronoun, personal</td>
                </tr>
                <tr>
                    <td>VBZ</td>
                    <td>verb, present tense, 3rd person singular</td>
                </tr>
                <tr>
                    <td>TO</td>
                    <td>"to" as preposition</td>
                </tr>
                <tr>
                    <td>NN</td>
                    <td>"noun, common, singular or mass</td>
                </tr>
                <tr>
                    <td>RB</td>
                    <td>adverb</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				 $ pip3 install spacy<br/>
				 $ python3 -m spacy download en_core_web_sm<br/>
                         </code>
            </p>
            <p>Usage</p>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span></br>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  He PRON nsubj<br/>
                                  goes VERB ROOT<br/>
                                  to ADP prep<br/>
                                  school NOUN pobj<br/>
                                  daily ADV advmod<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>5.1.2. √âtiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">tag_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">,</span>
            <span class="n">token</span><span class="o">.</span><span class="n">shape_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_alpha</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  He -PRON- PRON PRP nsubj Xx True True<br/>
                                  goes go VERB VBZ ROOT xxxx True False<br/>
                                  to to ADP IN prep xx True True<br/>
                                  school school NOUN NN pobj xxxx True False<br/>
                                  daily daily ADV RB advmod xxxx True False<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation, consiste √† regrouper les diff√©rentes formes d'un mot qui appartiennent au m√™me
                    paradigme morphologique flexionnel et √† attribuer √† chaque paradigme son lemme correspondant. </li>
                <li>Cette m√©thode vise √† ramener les variations flexionnelles d'un mot √† sa <b
                        style="color:#00549d">forme canonique</b> ou √† sa racine. </li>
                <li>La lemmatisation permet de simplifier la repr√©sentation des mots en les ramenant √† leur forme de
                    base, ce qui facilite la recherche, l'analyse et le traitement automatique du langage naturel. </li>
                <li>Exemples
                    <ul>
                        <li>go: go, goes, going, went, gone</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Chrupa≈Ça 2006, Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation comme une t√¢che d'√©tiquetage</li>
                <li>Attribuer un label pour chaque transformation d'un label en lemme</li>
                <li>4 √©tapes [Gesmundo 2012]
                    <ol>
                        <li>supprimer un suffixe de longueur \(N_s\)</li>
                        <li>ajouter un nouveau suffixe de lemme \(L_s\)</li>
                        <li>supprimer un pr√©fixe de longueur \(N_p\)</li>
                        <li>ajouter un nouveau pr√©fixe lemme, \(L_p\)</li>
                    </ol>
                </li>
                <li>Transformation \(\tau = \langle N_s, L_s, N_p, L_p \rangle\)</li>
                <li>(going, go) = \(\langle 3, \emptyset, 0, \emptyset \rangle \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer</h3>
            <ul>
                <li><b style="color:#00549d">WordNet [Miller 1995]</b> : WordNet est une base de donn√©es lexicale de la
                    langue anglaise qui organise les mots en synsets (ensembles de synonymes) et les relie entre eux par
                    des relations lexicales telles que l'hypernymie (relation "est-un") et l'hyponymie (relation "a pour
                    instance").</li>
                <li><b style="color:#00549d">WordNetLemmatizer</b> : Le module WordNetLemmatizer dans NLTK utilise
                    WordNet pour la lemmatisation des mots. Il attribue √† chaque mot sa forme canonique ou lemme, en
                    tenant compte des diff√©rentes formes flexionnelles. </li>
            </ul>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger&#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (sans les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     He went to school yesterday and attended the class
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">pos_tag</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>

<span class="c1"># Check the complete list of tags http://www.nltk.org/book/ch05.html</span>
<span class="k">def</span> <span class="nf">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;J&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADJ</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADV</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;N&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">NOUN</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">VERB</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;I went to school today and he goes daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">)),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     I go to school today and he go daily
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I went to school today and he goes daily&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  -PRON- go to school today and -PRON- go daily<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>5.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Morphologie</h3>
            <ul>
                <li>La morphologie lexicale est une branche de la linguistique qui se concentre sur l'√©tude des <b
                        style="color:#00549d">mots</b>, de leurs <b style="color:#00549d">formes</b>, de leurs <b
                        style="color:#00549d">paradigmes</b> et de l'organisation des <b
                        style="color:#00549d">cat√©gories grammaticales</b>. </li>
                <li>Elle examine de pr√®s les parties du discours, l'intonation, l'accentuation, ainsi que la mani√®re
                    dont le contexte peut influencer la prononciation et le sens d'un mot.</li>
                <li>Elle explore la structure interne des mots et comment ils interagissent avec la grammaire et le
                    contexte pour communiquer des significations sp√©cifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>5.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">,</span> <span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <figure>
                <img src="../../2021/MachineLearning/spacy-dep-output.svg" width="400vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Les embeddings de mots sont une technique d'apprentissage de caract√©ristiques o√π des mots ou des phrases
                du vocabulaire sont associ√©s √† des vecteurs de nombres r√©els.</p>
            <ul>
                <li>L'id√©e principale est de repr√©senter chaque mot par un vecteur dense dans un espace continu, de
                    telle sorte que des mots similaires aient des vecteurs similaires, capturant ainsi les relations
                    s√©mantiques entre les mots.</li>
                <li>Quantifier et cat√©goriser les similarit√©s s√©mantiques entre les √©l√©ments linguistiques en fonction
                    de leurs propri√©t√©s de distribution dans de grands √©chantillons de donn√©es linguistiques.</li>
                <li>En d'autres termes, les mots qui ont des contextes similaires ou qui apparaissent dans des contextes
                    similaires auront des embeddings de mots similaires.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Avantages de Word Embeddings</p>
            <ul>
                <li><b>Repr√©sentation dense</b> : Les embeddings fournissent une repr√©sentation dense, contrairement √†
                    une repr√©sentation creuse o√π chaque mot serait repr√©sent√© par un vecteur binaire indiquant sa
                    pr√©sence ou son absence.</li>
                <li><b>Capture des relations s√©mantiques</b> : Les embeddings captent les relations s√©mantiques et les
                    similitudes entre les mots, ce qui les rend utiles dans de nombreuses t√¢ches de traitement du
                    langage naturel.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Applications de Word Embeddings</p>
            <ul>
                <li><b>Similarit√© s√©mantique</b> : Mesurer la similarit√© s√©mantique entre les mots.</li>
                <li><b>Traduction automatique</b> : Am√©liorer les performances des syst√®mes de traduction automatique.
                </li>
                <li><b>Analyse des sentiments</b> : Mieux comprendre le contexte et les relations s√©mantiques dans
                    l'analyse des sentiments, entre autres applications.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy est une biblioth√®que open-source pour le traitement du langage naturel (NLP) en Python. Elle offre
                des outils performants et efficaces pour effectuer diverses t√¢ches de traitement du langage naturel, de
                l'analyse syntaxique √† la reconnaissance d'entit√©s nomm√©es. spaCy est con√ßu pour √™tre rapide, pr√©cis et
                facile √† utiliser.</p>
            <ul>
                <li><b>Collecte de donn√©es</b> : Les mod√®les spaCy sont souvent entra√Æn√©s sur de vastes ensembles de
                    donn√©es annot√©es, qui peuvent inclure des corpus textuels avec des annotations pour l'analyse
                    syntaxique, la reconnaissance d'entit√©s nomm√©es, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>Annotation des donn√©es</b> : Les donn√©es collect√©es sont annot√©es manuellement avec des
                    informations linguistiques sp√©cifiques telles que les parties du discours, les entit√©s nomm√©es, les
                    relations syntaxiques, etc.</li>
                <li><b>Entra√Ænement initial</b> : Les mod√®les spaCy sont initialement entra√Æn√©s sur ces ensembles de
                    donn√©es annot√©es pour apprendre les structures linguistiques. Ce processus peut inclure
                    l'utilisation d'algorithmes d'apprentissage automatique tels que les r√©seaux de neurones.</li>
                <li><b>Optimisation et r√©glage</b> : Les mod√®les sont ensuite optimis√©s et r√©gl√©s pour am√©liorer leurs
                    performances sur des t√¢ches sp√©cifiques. Cela peut impliquer des it√©rations sur le processus
                    d'entra√Ænement en ajustant les hyperparam√®tres du mod√®le.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>√âvaluation</b> : Les mod√®les sont √©valu√©s sur des ensembles de donn√©es de test distincts pour
                    mesurer leur pr√©cision, leur rappel et d'autres m√©triques sp√©cifiques √† la t√¢che.</li>
                <li><b>Construction des mod√®les linguistiques pr√©-entra√Æn√©s</b> : Une fois le mod√®le entra√Æn√© et √©valu√©,
                    spaCy construit des mod√®les linguistiques pr√©-entra√Æn√©s qui encapsulent les connaissances acquises
                    sur la structure linguistique.</li>
                <li><b>T√©l√©chargement et utilisation</b> : Les utilisateurs peuvent t√©l√©charger ces mod√®les
                    pr√©-entra√Æn√©s via spaCy et les utiliser dans leurs applications pour effectuer diverses t√¢ches de
                    traitement du langage naturel sans avoir √† entra√Æner un mod√®le de z√©ro.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy propose diff√©rents mod√®les linguistiques pr√©-entra√Æn√©s pour diff√©rentes langues et t√¢ches. Le
                mod√®le en_core_web_lg est un mod√®le vectoriel large d'anglais.</p>
            <h4>Installation du Mod√®le spaCy (en_core_web_lg) :</h4>
            <p class="codeexample">
                <code>
				 $ python3 -m spacy download en_core_web_lg<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <h4>Chargement du Mod√®le spaCy :</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le mod√®le spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Avantages de spaCy :</p>
            <ul>
                <li><b>Performance √©lev√©e</b> : spaCy est reconnu pour sa rapidit√© d'ex√©cution, ce qui le rend adapt√© au
                    traitement de grands volumes de texte en temps r√©el.</li>
                <li><b>Mod√®les pr√©-entra√Æn√©s</b> : spaCy propose des mod√®les linguistiques pr√©-entra√Æn√©s pour plusieurs
                    langues, ce qui facilite l'analyse de texte sans n√©cessiter d'entra√Ænement √† partir de z√©ro.</li>
                <li><b>Extraction d'informations linguistiques riches</b> : spaCy fournit des informations linguistiques
                    d√©taill√©es telles que les parties du discours, les entit√©s nomm√©es, les relations syntaxiques, et
                    plus encore.</li>
                <li><b>API conviviale</b> : L'API spaCy est con√ßue pour √™tre intuitive et conviviale. Elle facilite la
                    r√©alisation de t√¢ches complexes avec des lignes de code concises.</li>
                <li><b>Int√©gration avec d'autres biblioth√®ques</b> : spaCy s'int√®gre bien avec d'autres biblioth√®ques
                    Python populaires, facilitant son utilisation dans des projets plus larges.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Limites de spaCy :</p>
            <ul>
                <li><b>D√©pendance des mod√®les linguistiques</b> : L'utilisation de mod√®les pr√©-entra√Æn√©s signifie que la
                    qualit√© des r√©sultats d√©pend de la qualit√© du mod√®le. Dans des domaines de sp√©cialit√© ou pour des
                    langues moins courantes, les mod√®les peuvent ne pas √™tre aussi performants.</li>
                <li><b>Gestion des entit√©s nomm√©es</b> : Bien que spaCy excelle dans la reconnaissance d'entit√©s
                    nomm√©es, il peut parfois avoir du mal avec des t√¢ches plus complexes impliquant des variations
                    contextuelles.</li>
                <li><b>Taille des mod√®les</b> : Les mod√®les pr√©-entra√Æn√©s peuvent √™tre relativement volumineux, ce qui
                    peut √™tre un inconv√©nient dans des environnements avec des restrictions de m√©moire ou pour des
                    applications mobiles.</li>
                <li><b>Personnalisation limit√©e</b> : Bien que spaCy offre des fonctionnalit√©s de personnalisation,
                    elles peuvent √™tre limit√©es par rapport √† d'autres biblioth√®ques NLP plus flexibles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le mod√®le spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>

<span class="c1"># D√©finir les mots √† comparer</span>
<span class="n">words_to_compare</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;apple&quot;</span><span class="p">]</span>

<span class="c1"># Calculer la similarit√© entre les paires de mots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
        <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="o">=</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">doc1</span><span class="p">,</span> <span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word1</span><span class="p">),</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
        <span class="n">similarity_score</span> <span class="o">=</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarit√© (</span><span class="si">{}</span><span class="s2"> / </span><span class="si">{}</span><span class="s2">): </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span>
	    <span class="n">similarity_score</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <h4>Affichage</h4>
            <p class="codeexample">
                <code>
                <pre>
Similarit√© (dog / cat): ...
Similarit√© (dog / apple): ...
Similarit√© (cat / apple): ...
                </pre>

                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: vector</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le mod√®le spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte √† analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>

<span class="c1"># Imprimer les vecteurs de chaque jeton sur une seule ligne</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteurs de &#39;</span><span class="si">{}</span><span class="s2">&#39; : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">,</span> <span class="n">vector_list</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>Word2Vec a marqu√© un tournant significatif dans la <b>repr√©sentation des mots</b> dans le domaine de
                l'apprentissage automatique.</p>
            <ul>
                <li>C'est une technique publi√©e en <b>2013</b> par une √©quipe de chercheurs dirig√©e par <b>Tomas
                        Mikolov</b> chez Google.</li>
                <li><b>Repr√©sentation vectorielle</b> : Word2Vec repr√©sente chaque mot distinct avec un vecteur dans un
                    espace continu. Ces vecteurs captent les relations s√©mantiques et syntaxiques entre les mots.</li>
                <li><b>Apprentissage bas√© sur un r√©seau neuronal</b> : Le mod√®le utilise un r√©seau neuronal pour
                    apprendre des associations de mots √† partir d'un vaste corpus de texte. Cette approche permet de
                    capturer des nuances complexes dans la signification des mots.</li>
                <li><b>Entr√©e et sortie</b> : Word2Vec prend en entr√©e un large corpus de texte et produit un espace
                    vectoriel, g√©n√©ralement de plusieurs centaines de dimensions. Cette repr√©sentation vectorielle
                    permet de mesurer la similarit√© s√©mantique entre les mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>L'impl√©mentation de Word2Vec se d√©roule en plusieurs √©tapes :</p>
            <ol>
                <li><b>Pr√©traitement des donn√©es</b> : Le texte est nettoy√© et pr√©trait√© pour √©liminer les √©l√©ments
                    ind√©sirables tels que la ponctuation et les stopwords.</li>
                <li><b>Cr√©ation d'un vocabulaire</b> : Les mots uniques du corpus sont utilis√©s pour construire un
                    vocabulaire. Chaque mot est ensuite associ√© √† un index.</li>
                <li><b>G√©n√©ration de paires mot-contexte</b> : Pour chaque mot du corpus, des paires mot-contexte sont
                    cr√©√©es en utilisant une fen√™tre contextuelle glissante. Ces paires servent d'exemples
                    d'entra√Ænement.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <ol>
                <li><b>Construction du mod√®le Word2Vec</b> : Un mod√®le de r√©seau neuronal est cr√©√©, avec une couche
                    d'entr√©e repr√©sentant les mots, une couche cach√©e (skip-gram ou CBOW), et une couche de sortie pour
                    pr√©dire le mot suivant dans le contexte.</li>
                <li><b>Entra√Ænement du mod√®le</b> : Le mod√®le est entra√Æn√© sur les paires mot-contexte g√©n√©r√©es,
                    ajustant les poids du r√©seau pour minimiser la diff√©rence entre les pr√©dictions et les vrais mots du
                    contexte.</li>
                <li><b>Obtention des embeddings</b> : Les vecteurs de mots appris pendant l'entra√Ænement, appel√©s
                    embeddings, sont extraits. Chaque mot du vocabulaire est maintenant repr√©sent√© par un vecteur dense
                    dans l'espace continu.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec</h3>
            <ul>
                <li>les vecteurs de mots sont positionn√©s dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situ√©s √† proximit√© les uns des autres dans
                    l'espace
                </li>
                <li>une simple fonction math√©matique (par exemple, la similarit√© cosinus entre les vecteurs) indique le
                    niveau de similarit√© s√©mantique entre les mots repr√©sent√©s par ces vecteurs \[\text{similarity} =
                    \cos(\theta) = {\mathbf{A} \cdot \mathbf{B}
                    \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i B_i} }{
                    \sqrt{\sum\limits_{i=1}^{n}{A_i^2}} \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} },\]
                </li>
                <li>les vecteurs de mots sont positionn√©s dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situ√©s √† proximit√© les uns des autres dans
                    l'espace
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <p>CBOW est un mod√®le sp√©cifique de Word2Vec. Dans ce mod√®le, la pr√©diction du mot courant se fait en
                utilisant une fen√™tre de mots contextuels voisins. L'ordre des mots de contexte n'influence pas la
                pr√©diction, ce qui en fait une approche robuste.</p>
            <ul>
                <li><b>Mod√®le pr√©dictif</b> : CBOW pr√©dit le mot cible en se basant sur le contexte qui l'entoure, mais
                    contrairement √† d'autres mod√®les, l'ordre sp√©cifique des mots dans ce contexte n'est pas pris en
                    compte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <ul>
                <li><b>Entr√©e</b> : La donn√©e d'entr√©e du mod√®le CBOW est une fen√™tre de mots contextuels entourant le
                    mot cible. Cette fen√™tre est d√©finie par un param√®tre appel√© la taille de la fen√™tre.</li>
                <li><b>Architecture</b> : CBOW utilise une architecture de r√©seau neuronal √† une seule couche cach√©e. La
                    couche d'entr√©e repr√©sente les mots du contexte, et la couche de sortie repr√©sente le mot cible √†
                    pr√©dire.</li>
                <li><b>Entra√Ænement</b> : Le mod√®le est entra√Æn√© en ajustant les poids du r√©seau pour minimiser la
                    diff√©rence entre les pr√©dictions du mod√®le et le mot cible r√©el. Cela se fait √† travers des
                    techniques d'optimisation comme la r√©tropropagation du gradient.</li>
                <li><b>Sortie</b> : Une fois le mod√®le entra√Æn√©, les poids de la couche d'entr√©e sont utilis√©s comme
                    embeddings de mots. Ces embeddings capturent les relations s√©mantiques entre les mots, permettant
                    ainsi de repr√©senter chaque mot par un vecteur dans un espace continu.</li>
                <li><b>Avantages</b> : CBOW est souvent plus rapide √† entra√Æner que d'autres mod√®les comme le Skip-gram
                    (une autre variante de Word2Vec) et peut √™tre plus efficace dans des contextes o√π l'ordre s√©quentiel
                    des mots n'est pas critique.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Donn√©es d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Pr√©traitement des donn√©es en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du mod√®le CBOW avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fr√©quence totale est inf√©rieure √† cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot pr√©dit dans une phrase</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
       <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>

<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarit√© entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarit√© entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,
                  </span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Pr√©diction des deux mots les plus probables suivant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pr√©diction des mots suivant &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <p>Le mod√®le Skip-gram est une autre variante de Word2Vec qui se concentre sur la pr√©diction de la fen√™tre
                voisine des mots de contexte √† partir du mot courant. </p>
            <ul>
                <li><b>Objectif</b> : L'objectif principal du mod√®le Skip-gram est de prendre un mot source (le mot
                    courant) et de pr√©dire les mots qui l'entourent dans une fen√™tre de contexte donn√©e.</li>
                <li><b>Entr√©e</b> : Le mot source est utilis√© comme donn√©e d'entr√©e du mod√®le, et la sortie souhait√©e
                    est la distribution des probabilit√©s des mots du contexte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <ul>
                <li><b>Architecture</b> : Skip-gram utilise une architecture de r√©seau neuronal √† une seule couche
                    cach√©e. La couche d'entr√©e repr√©sente le mot source, et la couche de sortie repr√©sente les mots du
                    contexte.</li>
                <li><b>Entra√Ænement</b> : Pendant l'entra√Ænement, les poids du r√©seau sont ajust√©s pour minimiser la
                    diff√©rence entre les pr√©dictions du mod√®le et la v√©ritable distribution des mots du contexte. Cela
                    se fait g√©n√©ralement √† l'aide de techniques d'optimisation comme la r√©tropropagation du gradient.
                </li>
                <li><b>Pond√©ration du contexte</b> : Une caract√©ristique importante du mod√®le Skip-gram est que
                    l'architecture accorde plus de poids aux mots de contexte proches du mot source que ceux plus
                    √©loign√©s. Cela permet de mieux capturer les relations s√©mantiques et syntaxiques locales.</li>
                <li><b>Embeddings</b> : Une fois le mod√®le entra√Æn√©, les poids de la couche d'entr√©e sont utilis√©s comme
                    embeddings de mots. Ces embeddings capturent les similitudes s√©mantiques entre les mots, permettant
                    de repr√©senter chaque mot par un vecteur dans un espace continu.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Donn√©es d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Pr√©traitement des donn√©es en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du mod√®le Skip-gram avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fr√©quence totale est inf√©rieure √† cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot pr√©dit dans une phrase</span>
<span class="c1"># sg: 1 pour skip-gram ; sinon CBOW.</span>
<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,
                </span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarit√© entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarit√© entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Pr√©diction des mots les plus probables dans le contexte entourant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pr√©diction des mots dans le contexte de &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entit√©s nomm√©es</h3>
            <p>La Reconnaissance d'Entit√©s Nomm√©es (NER) consiste √† identifier et classer des entit√©s sp√©cifiques dans
                un texte. Ces entit√©s peuvent inclure des personnes, des lieux, des organisations, des dates, des
                montants mon√©taires, etc. Le but est d'extraire des informations structur√©es √† partir de donn√©es
                textuelles non structur√©es.</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/datarepresentation.svg"
                    height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entit√©s nomm√©es</h3>
            <ul>
                <li><b>Identification d'entit√©s</b> : La premi√®re √©tape de la NER consiste √† identifier les mots ou
                    groupes de mots qui repr√©sentent des entit√©s dans le texte. Ces entit√©s peuvent √™tre des noms de
                    personnes, des noms de lieux, des noms d'organisations, etc.</li>
                <li><b>Classification des entit√©s</b> : Une fois les entit√©s identifi√©es, elles sont classifi√©es dans
                    des cat√©gories sp√©cifiques. Par exemple, une entit√© peut √™tre class√©e comme "PERSON" si elle
                    repr√©sente une personne, "LOCATION" si elle repr√©sente un lieu, "ORGANIZATION" si elle repr√©sente
                    une organisation, et ainsi de suite.</li>
                <li><b>Contextualisation</b> : La NER tient compte du contexte dans lequel une entit√© appara√Æt. Par
                    exemple, le mot "banc" peut √™tre class√© comme une entit√© financi√®re dans le contexte d'une
                    discussion sur l'√©conomie, mais comme une entit√© physique dans le contexte d'un parc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entit√©s nomm√©es</h3>
            <ul>
                <li><b>Relations entre entit√©s</b> : Dans certains cas, la NER peut √©galement inclure la d√©tection des
                    relations entre diff√©rentes entit√©s dans le texte. Par exemple, la relation entre une personne et
                    l'organisation qu'elle travaille.</li>
                <li><b>Applications pratiques</b> : Les r√©sultats de la NER peuvent √™tre utilis√©s dans diverses
                    applications, telles que l'am√©lioration de la recherche d'informations, l'extraction de relations,
                    la cat√©gorisation de documents, la cr√©ation de r√©sum√©s automatiques, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entit√©s nomm√©es : Algorithmes</h3>
            <p>La Reconnaissance d'Entit√©s Nomm√©es (NER) est souvent r√©alis√©e √† l'aide de mod√®les d'apprentissage
                automatique, et plusieurs algorithmes peuvent √™tre utilis√©s dans ce contexte. Quelques-uns des
                algorithmes couramment employ√©s :</p>
            <ul>
                <li><b>Mod√®les de markov cach√©s (HMM - Hidden Markov Models)</b> : Les HMM ont √©t√© utilis√©s pour la NER,
                    o√π l'id√©e est de mod√©liser la s√©quence des √©tiquettes d'entit√©s en tant que s√©quence cach√©e derri√®re
                    la s√©quence observable de mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entit√©s nomm√©es : Algorithmes</h3>
            <ul>
                <li><b>R√©seaux de neurones</b> : Les architectures de r√©seaux de neurones, y compris les r√©seaux de
                    neurones r√©currents (RNN), les r√©seaux de neurones r√©currents bidirectionnels (BiRNN), et les
                    r√©seaux de neurones r√©currents √† m√©moire √† court terme (LSTM), ont montr√© des performances
                    significatives dans la NER.</li>
                <li><b>Transformers</b> : Les mod√®les bas√©s sur les transformers, tels que BERT (Bidirectional Encoder
                    Representations from Transformers) et ses variantes, ont consid√©rablement am√©lior√© les performances
                    en NER. Ces mod√®les sont pr√©-entra√Æn√©s sur de grandes quantit√©s de donn√©es textuelles et captent des
                    repr√©sentations contextuelles riches.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entit√©s nomm√©es : Algorithmes</h3>
            <ul>
                <li><b>Mod√®les statistiques traditionnels</b> : Des approches statistiques plus traditionnelles, comme
                    les mod√®les de s√©quence et les classificateurs bas√©s sur des caract√©ristiques, ont √©galement √©t√©
                    utilis√©es dans des sc√©narios o√π des quantit√©s limit√©es de donn√©es annot√©es sont disponibles.</li>
                <li><b>R√®gles et expressions r√©guli√®res</b> : Dans certains cas, des r√®gles manuelles ou des expressions
                    r√©guli√®res peuvent √™tre utilis√©es pour extraire des entit√©s sp√©cifiques, surtout lorsque des motifs
                    clairs et r√©currents peuvent √™tre d√©finis.</li>
                <li><b>Entra√Ænement supervis√©</b> : Les m√©thodes d'entra√Ænement supervis√© consistent √† annoter
                    manuellement un ensemble de donn√©es avec des entit√©s nomm√©es, puis √† entra√Æner un mod√®le sur ces
                    donn√©es annot√©es.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entit√©s nomm√©es</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le mod√®le spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte √† analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France.&quot;</span> + <span class="s2">&quot;In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Analyser le texte</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entit√©s nomm√©es</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Afficher les informations sur les entit√©s</span>
<span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">:</span>
    <span class="n">entity_text</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">text</span>
    <span class="n">start_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">start_char</span>
    <span class="n">end_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">end_char</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">label_</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entit√©: </span><span class="si">{}</span><span class="s2">, D√©but: </span><span class="si">{}</span><span class="s2">, Fin: </span><span class="si">{}</span><span class="s2">, Cat√©gorie: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entity_text</span><span class="p">,
                 </span> <span class="n">start_char</span><span class="p">,</span> <span class="n">end_char</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entit√©s nomm√©es</h3>
            <p class="codeexample">
                <code>
                <pre>
Entit√©: Paris, D√©but: 0, Fin: 5, Cat√©gorie: GPE
Entit√©: France, D√©but: 24, Fin: 30, Cat√©gorie: GPE
Entit√©: 2015, D√©but: 35, Fin: 39, Cat√©gorie: DATE
Entit√©: 2,206,488, D√©but: 72, Fin: 81, Cat√©gorie: CARDINAL
                         </pre>
                </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entit√©s nomm√©es</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le mod√®le spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entit√©s nomm√©es avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">serve</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>

<span class="c1"># Texte √† analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France.&quot;</span> + <span class="s2">&quot;In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entit√©s</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entit√©s nomm√©es</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le mod√®le spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entit√©s nomm√©es avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">, </span><span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Texte √† analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entit√©s</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>5.4. Reconnaissance d'entit√©s nomm√©es (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entit√©s nomm√©es</h3>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>GPE</td>
                    <td>Pays, villes, √©tats.</td>
                </tr>
                <tr>
                    <td>DATE</td>
                    <td>Dates ou p√©riodes absolues ou relatives</td>
                </tr>
                <tr>
                    <td>CARDINAL</td>
                    <td>Les chiffres qui ne correspondent √† aucun autre type.</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p> Le lexique <b>VADER</b> (Valence Aware Dictionary and sEntiment Reasoner) est sp√©cifiquement con√ßu pour
                analyser les sentiments dans du texte en attribuant des scores de positivit√©, n√©gativit√© et neutralit√©
                aux mots ainsi qu'aux expressions.</p>
            <h4>Installation</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;vader_lexicon&#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER</h3>
            <p>VADER est une biblioth√®que d'analyse de sentiment con√ßue pour √©valuer le sentiment d'un morceau de texte,
                g√©n√©ralement une phrase ou un paragraphe.</p>
            <ul>
                <li><b>Dictionnaire et Scores :</b> VADER utilise un dictionnaire pr√©-annot√© avec des scores de
                    positivit√©, n√©gativit√© et neutralit√© pour des milliers de mots et expressions. Chaque mot est
                    associ√© √† un score qui indique dans quelle mesure il est per√ßu comme positif ou n√©gatif.</li>
                <li><b>Polarit√© des Mots :</b> Pour chaque mot dans le texte, VADER examine son score dans le
                    dictionnaire. Certains mots ont des scores forts, indiquant une polarit√© positive ou n√©gative,
                    tandis que d'autres ont des scores plus neutres.</li>
                <li><b>Modificateurs et Emphase :</b> VADER prend en compte les modificateurs, tels que les adverbes,
                    qui peuvent influencer la polarit√© d'un mot. Il reconna√Æt √©galement l'emphase en attribuant des
                    poids diff√©rents aux mots en majuscules.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER</h3>
            <ul>
                <li><b>Calcul du Score Compos√© :</b> VADER agr√®ge les scores des mots en utilisant une formule qui prend
                    en compte la distribution des polarit√©s dans le texte. Le score compos√© r√©sultant est une mesure
                    globale du sentiment de la phrase.</li>
                <li><b>R√©sultats :</b> Le r√©sultat final de l'analyse est un ensemble de scores qui indiquent la
                    positivit√©, la n√©gativit√©, la neutralit√© et un score compos√© global. Ces scores sont normalis√©s dans
                    une √©chelle de -1 √† 1, o√π -1 repr√©sente un sentiment extr√™mement n√©gatif, 1 repr√©sente un sentiment
                    extr√™mement positif, et 0 repr√©sente la neutralit√©.</li>
            </ul>
            <p>VADER est souvent utilis√© pour l'analyse de sentiment rapide et bas√©e sur des r√®gles. Bien qu'il soit
                efficace dans de nombreux cas, il peut ne pas √™tre aussi pr√©cis que des m√©thodes plus complexes bas√©es
                sur l'apprentissage automatique, notamment dans des contextes o√π l'analyse n√©cessite une compr√©hension
                plus profonde du langage et de la syntaxe.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER: Usage</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.sentiment.vader</span> <span class="kn">import</span> <span class="n">SentimentIntensityAnalyzer</span>
<span class="n">sia</span> <span class="o">=</span> <span class="n">SentimentIntensityAnalyzer</span><span class="p">()</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is not very good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is bad&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Les scores renvoy√©s par VADER repr√©sentent diff√©rentes mesures du sentiment dans un texte. Une
                explication de chaque score :</p>
            <ul>
                <li><b>Positivit√© (Positive Score)</b> : Ce score mesure la positivit√© relative du texte. Il indique
                    dans quelle mesure le texte contient des √©l√©ments positifs. Plus le score est √©lev√©, plus le texte
                    est per√ßu comme positif.</li>
                <li><b>N√©gativit√© (Negative Score)</b> : - Ce score mesure la n√©gativit√© relative du texte. Il indique
                    dans quelle mesure le texte contient des √©l√©ments n√©gatifs. Plus le score est √©lev√©, plus le texte
                    est per√ßu comme n√©gatif.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Neutralit√© (Neutral Score)</b> : Ce score mesure la neutralit√© relative du texte. Il indique dans
                    quelle mesure le texte est neutre, c'est-√†-dire d√©pourvu d'√©l√©ments fortement positifs ou n√©gatifs.
                    Plus le score est √©lev√©, plus le texte est per√ßu comme neutre.</li>
                <li><b>Score Compos√© (Compound Score)</b> : Le score compos√© est une mesure agr√©g√©e du sentiment qui
                    prend en compte √† la fois la positivit√© et la n√©gativit√© du texte. Il combine les scores positif,
                    n√©gatif et neutre en une seule valeur. Le score compos√© est souvent utilis√© pour √©valuer le
                    sentiment global du texte. Un score compos√© √©lev√© indique un sentiment fort, qu'il soit positif ou
                    n√©gatif, tandis qu'un score proche de z√©ro indique un texte neutre.</li>
            </ul>
            <p>Les scores sont normalis√©s dans une √©chelle de -1 √† 1, o√π -1 repr√©sente un sentiment extr√™mement n√©gatif,
                1 repr√©sente un sentiment extr√™mement positif, et 0 repr√©sente la neutralit√©. Les scores peuvent √™tre
                interpr√©t√©s individuellement ou conjointement pour obtenir une compr√©hension compl√®te du sentiment dans
                le texte analys√©.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}</br>
                        {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'compound': -0.3865}</br>
                        {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>R√©f√©rences</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Beel 2013a] Beel, Joeran, et al. ‚ÄúA Comparative Analysis of Offline and Online Evaluations and
                    Discussion of Research Paper Recommender System Evaluation.‚Äù Proceedings of the International
                    Workshop on Reproducibility and Replication in
                    Recommender Systems Evaluation, Association for Computing Machinery, 2013</li>
                <li>[Beel 2013b] Beel, Joeran, et al. ‚ÄúSponsored vs. Organic (Research Paper) Recommendations and the
                    Impact of Labeling.‚Äù Research and Advanced Technology for Digital Libraries, edited by Trond Aalberg
                    et al., Springer, 2013, pp. 391‚Äì95.</li>
                <li>[Chrupa≈Ça 2006] Chrupa≈Ça, Grzegorz. Simple Data-Driven Context-Sensitive Lemmatization. 2006.
                    doras.dcu.ie, http://www.unizar.es/departamentos/filologia_inglesa/sepln2006/.</li>
                <li>[Frakes 2003] Frakes, William B., and Christopher J. Fox. ‚ÄúStrength and Similarity of Affix Removal
                    Stemming Algorithms.‚Äù ACM SIGIR Forum, vol. 37, no. 1, Apr. 2003, pp. 26‚Äì30. Spring 2003</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>R√©f√©rences</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Gomez-Uribe 2016] Gomez-Uribe, Carlos A., and Neil Hunt. ‚ÄúThe Netflix Recommender System:
                    Algorithms, Business Value, and Innovation.‚Äù ACM Transactions on Management Information Systems,
                    vol. 6, no. 4, Dec. 2016, p. 13:1‚Äì13:19. January
                    2016 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short
                    Papers -
                    Volume 2, Association for Computational Linguistics, 2012, pp. 368‚Äì372.</li>
                <li>[Gesmundo 2012] Gesmundo, Andrea, and Tanja Samard≈æiƒá. ‚ÄúLemmatisation as a Tagging Task.‚Äù
                <li>[Herlocker 2000] Herlocker, Jonathan L., et al. ‚ÄúExplaining Collaborative Filtering
                    Recommendations.‚Äù Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work,
                    Association for Computing Machinery, 2000, pp. 241‚Äì250.
                    ACM
                </li>
                <li>[Konstan 2012] Konstan, Joseph A., and John Riedl. ‚ÄúRecommender Systems: From Algorithms to User
                    Experience.‚Äù User Modeling and User-Adapted Interaction, vol. 22, no. 1‚Äì2, Apr. 2012, pp. 101‚Äì123.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>R√©f√©rences</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[M√†rquez 2000] M√†rquez, Llu√≠s, et al. ‚ÄúA Machine Learning Approach to POS Tagging.‚Äù Machine
                    Learning, vol. 39, no. 1, Apr. 2000, pp. 59‚Äì91.</li>
                <li>[Mikolov 2013] Mikolov, Tomas, et al. ‚ÄúEfficient Estimation of Word Representations in Vector
                    Space.‚Äù ArXiv:1301.3781 [Cs], Sept. 2013.</li>
                <li>[Miller 1995] Miller, George A. ‚ÄúWordNet: A Lexical Database for English.‚Äù Communications of the
                    ACM, vol. 38, no. 11, Nov. 1995, pp. 39‚Äì41. Nov. 1995</li>
                <li>[Pazzani 2007] Pazzani, Michael J., and Daniel Billsus. ‚ÄúContent-Based Recommendation Systems.‚Äù The
                    Adaptive Web: Methods and Strategies of Web Personalization, edited by Peter Brusilovsky et al.,
                    Springer, 2007, pp. 325‚Äì41. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>R√©f√©rences</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Porter 1980] Porter, M. F. ‚ÄúAn Algorithm for Suffix Stripping.‚Äù Program, vol. 14, no. 3, Jan. 1980,
                    pp. 130‚Äì37. Emerald Insight</li>
                <li>[Pu 2012] Pu, Pearl, et al. ‚ÄúEvaluating Recommender Systems from the User‚Äôs Perspective: Survey of
                    the State of the Art.‚Äù User Modeling and User-Adapted Interaction, vol. 22, no. 4, Oct. 2012, pp.
                    317‚Äì55.
                </li>
                <li>[Ricci 2011] Ricci, Francesco, et al. ‚ÄúIntroduction to Recommender Systems Handbook.‚Äù Recommender
                    Systems Handbook, edited by Francesco Ricci et al., Springer US, 2011, pp. 1‚Äì35. </li>
                <li>[Ziegler 2005] Ziegler, Cai-Nicolas, et al. ‚ÄúImproving Recommendation Lists through Topic
                    Diversification.‚Äù Proceedings of the 14th International Conference on World Wide Web, Association
                    for Computing Machinery, 2005, pp. 22‚Äì32.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>R√©f√©rences</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web</h3>
            <ul>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation">https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation</a>
                </li>
                <li><a href="https://fr.wikipedia.org/wiki/Racinisation">https://fr.wikipedia.org/wiki/Racinisation</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Intelligence_artificielle">https://fr.wikipedia.org/wiki/Intelligence_artificielle</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Programmation_logique">https://fr.wikipedia.org/wiki/Programmation_logique</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">https://en.wikipedia.org/wiki/Morphology_(linguistics)</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Word_embedding">https://en.wikipedia.org/wiki/Word_embedding</a>
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
                <li><a href="https://www.nltk.org/howto/stem.html">https://www.nltk.org/howto/stem.html</a></li>
                <li><a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></li>
                <li><a href="https://spacy.io/usage/spacy-101">https://spacy.io/usage/spacy-101</a></li>
                <li><a href="https://spacy.io/usage/visualizers">https://spacy.io/usage/visualizers</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>R√©f√©rences:</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de donn√©es massives | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 185) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>
