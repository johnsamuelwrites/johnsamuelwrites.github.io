<html>

<head>
    <meta charset="utf-8" />
    <title>Traitement de données massives (2025-2026): Cours: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            line-height: 1.7em;
            flex-direction: column;
            align-items: flex-start;
            margin: 0 auto;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .codeexample {
            background-color: #eeeeee;
        }

        /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
        pre {
            line-height: 125%;
        }

        td.linenos .normal {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        td.linenos .special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos.special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        body .hll {
            background-color: #ffffcc
        }

        body {
            background: #f8f8f8;
        }

        body .c {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment */
        body .err {
            border: 1px solid #FF0000
        }

        /* Error */
        body .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */
        body .o {
            color: #666666
        }

        /* Operator */
        body .ch {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Hashbang */
        body .cm {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Multiline */
        body .cp {
            color: #9C6500
        }

        /* Comment.Preproc */
        body .cpf {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.PreprocFile */
        body .c1 {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Single */
        body .cs {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Special */
        body .gd {
            color: #A00000
        }

        /* Generic.Deleted */
        body .ge {
            font-style: italic
        }

        /* Generic.Emph */
        body .gr {
            color: #E40000
        }

        /* Generic.Error */
        body .gh {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Heading */
        body .gi {
            color: #008400
        }

        /* Generic.Inserted */
        body .go {
            color: #717171
        }

        /* Generic.Output */
        body .gp {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Prompt */
        body .gs {
            font-weight: bold
        }

        /* Generic.Strong */
        body .gu {
            color: #800080;
            font-weight: bold
        }

        /* Generic.Subheading */
        body .gt {
            color: #0044DD
        }

        /* Generic.Traceback */
        body .kc {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Constant */
        body .kd {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Declaration */
        body .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */
        body .kp {
            color: #008000
        }

        /* Keyword.Pseudo */
        body .kr {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Reserved */
        body .kt {
            color: #B00040
        }

        /* Keyword.Type */
        body .m {
            color: #666666
        }

        /* Literal.Number */
        body .s {
            color: #BA2121
        }

        /* Literal.String */
        body .na {
            color: #687822
        }

        /* Name.Attribute */
        body .nb {
            color: #008000
        }

        /* Name.Builtin */
        body .nc {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Class */
        body .no {
            color: #880000
        }

        /* Name.Constant */
        body .nd {
            color: #AA22FF
        }

        /* Name.Decorator */
        body .ni {
            color: #717171;
            font-weight: bold
        }

        /* Name.Entity */
        body .ne {
            color: #CB3F38;
            font-weight: bold
        }

        /* Name.Exception */
        body .nf {
            color: #0000FF
        }

        /* Name.Function */
        body .nl {
            color: #767600
        }

        /* Name.Label */
        body .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */
        body .nt {
            color: #008000;
            font-weight: bold
        }

        /* Name.Tag */
        body .nv {
            color: #19177C
        }

        /* Name.Variable */
        body .ow {
            color: #AA22FF;
            font-weight: bold
        }

        /* Operator.Word */
        body .w {
            color: #bbbbbb
        }

        /* Text.Whitespace */
        body .mb {
            color: #666666
        }

        /* Literal.Number.Bin */
        body .mf {
            color: #666666
        }

        /* Literal.Number.Float */
        body .mh {
            color: #666666
        }

        /* Literal.Number.Hex */
        body .mi {
            color: #666666
        }

        /* Literal.Number.Integer */
        body .mo {
            color: #666666
        }

        /* Literal.Number.Oct */
        body .sa {
            color: #BA2121
        }

        /* Literal.String.Affix */
        body .sb {
            color: #BA2121
        }

        /* Literal.String.Backtick */
        body .sc {
            color: #BA2121
        }

        /* Literal.String.Char */
        body .dl {
            color: #BA2121
        }

        /* Literal.String.Delimiter */
        body .sd {
            color: #BA2121;
            font-style: italic
        }

        /* Literal.String.Doc */
        body .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */
        body .se {
            color: #AA5D1F;
            font-weight: bold
        }

        /* Literal.String.Escape */
        body .sh {
            color: #BA2121
        }

        /* Literal.String.Heredoc */
        body .si {
            color: #A45A77;
            font-weight: bold
        }

        /* Literal.String.Interpol */
        body .sx {
            color: #008000
        }

        /* Literal.String.Other */
        body .sr {
            color: #A45A77
        }

        /* Literal.String.Regex */
        body .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */
        body .ss {
            color: #19177C
        }

        /* Literal.String.Symbol */
        body .bp {
            color: #008000
        }

        /* Name.Builtin.Pseudo */
        body .fm {
            color: #0000FF
        }

        /* Name.Function.Magic */
        body .vc {
            color: #19177C
        }

        /* Name.Variable.Class */
        body .vg {
            color: #19177C
        }

        /* Name.Variable.Global */
        body .vi {
            color: #19177C
        }

        /* Name.Variable.Instance */
        body .vm {
            color: #19177C
        }

        /* Name.Variable.Magic */
        body .il {
            color: #666666
        }

        /* Literal.Number.Integer.Long */


        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 70%;
            margin-bottom: 1vmax;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 90%;
            max-height: 90%;
        }

        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }

        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 1.7em;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }


        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                font-size: 2.8vmin;
            }

            .content .flexcontent {
                font-size: 2.5vmin;
            }
        }
    </style>
    <script src="../../2021/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:2.5vw">Traitement de données massives</h1>
            <h3>Traitement automatique des langues naturelles (TAL)</h3>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Année</b>: 2025-2026<br />
                <b>Courriel</b>: john.samuel@cpe.fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles (TAL/NLP) </h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Intelligence artificielle</h3>
            <figure>
                <img src="../../../../../images/art/courses/deeplearningposition.svg" height="450px" />
                <figcaption>Intelligence artificielle</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <p>Le <b>traitement automatique des langues (TAL)</b> est un domaine interdisciplinaire de la linguistique
                informatique qui se concentre sur l'analyse et la compréhension du <b>langage naturel</b> (celui utilisé
                par les humains). Cette section aborde plusieurs aspects clés du TAL, notamment :</p>
            <ul>
                <li><b>Analyser et comprendre le langage naturel (humain)</b>: Le TAL se consacre à la compréhension du
                    langage naturel dans divers contextes, qu'il s'agisse de textes écrits ou de discours verbal.</li>
                <li>Interaction homme-machine</li>
                <li><b>Syntaxe d'une langue</b>
                    <ul>
                        <li>Parsing : Le parsing consiste à analyser la structure grammaticale des phrases.</li>
                        <li>L'étiquetage en parties du discours (PoS) : L'étiquetage PoS consiste à assigner des
                            catégories grammaticales (comme verbe, nom, adjectif, etc.) aux mots d'une phrase.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Sémantique d'une langue</b>
                    <ul>
                        <li>Traduction automatique</li>
                        <li>Reconnaissance d'entités nommées (NER): La NER consiste à identifier des entités spécifiques
                            (comme des noms de personnes, de lieux ou d'organisations) dans un texte.</li>
                        <li>Analyse des sentiments</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>5.1. Traitement automatique des langues naturelles</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Analyse de systèmes TAL</h3>
            <ul>
                <li><b>Racinisation</b> : La racinisation est le processus de réduction des mots à leur forme de base ou
                    de racine. </li>
                <li><b>Étiquetage morpho-syntaxique</b> : Cette étape consiste à attribuer des balises ou des étiquettes
                    aux mots dans un texte en fonction de leur rôle grammatical et de leur structure. </li>
                <li><b>Lemmatisation</b> : Contrairement à la racinisation, la lemmatisation consiste à ramener les mots
                    à leur forme canonique ou lemmes. </li>
                <li><b>Morphologie</b> : La morphologie concerne l'étude de la structure des mots, notamment comment ils
                    sont formés à partir de morphèmes (unités de sens). </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation [Frakes 2003]</h3>
            <ul>
                <li>La racination, souvent appelée <b>stemming</b> en anglais, est un processus de normalisation
                    linguistique visant à réduire les mots à leur forme racine, en ignorant les affixes. Elle est
                    utilisée pour simplifier les variations morphologiques des mots. </li>
                <li>Les <b>algorithmes de racination</b> appliquent généralement des règles heuristiques pour éliminer
                    les préfixes et suffixes courants.
                    <ul>
                        <li><b>Exemples</b> : Porter, Snowball</li>
                        <li><b>Limitations</b> : La racination peut conduire à des résultats non valides, car elle peut
                            produire des racines qui ne sont pas des mots réels.</li>
                    </ul>
                </li>
                <li>Exemples
                    <ul>
                        <li>engineer: engineer, engineered, engineering</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation</h3>
            <p>Quelques exemples d'issues potentiellement <b>non valides</b> :</p>
            <ul>
                <li>Racination excessive :
                    <ul>
                        <li>Mot d'origine : "happily"</li>
                        <li>Racination : "happi" (au lieu de la forme correcte "happy")</li>
                    </ul>
                </li>
                <li>Racination incorrecte :
                    <ul>
                        <li> Mot d'origine : "better"</li>
                        <li> Racination : "bet" (au lieu de la forme correcte "better")</li>
                    </ul>
                <li>Création de faux mots :
                    <ul>
                        <li> Mot d'origine : "unhappiness"</li>
                        <li> Racination : "unhappi" (crée un faux mot au lieu de "unhappy")</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation</h3>
            <ul>
                <li>Ambiguïté des règles :
                    <ul>
                        <li> Mot d'origine : "flies" (verbe)</li>
                        <li> Racination : "fli" (peut être confondu avec le nom "fly")</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: mesures d'évaluation [Frakes 2003]</h3>
            <ul>
                <li>La mesure dans laquelle un algorithm modifie des mots qu'elle réduit à ses racines est appelée la
                    <b>force</b> de l'algorithme
                </li>
                <li>Une métrique de <b>similarité</b> des algorithmes met en correspondance les n-tuples d'algorithmes
                    (n au moins 2), avec un nombre indiquant la similarité des algorithmes. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: distance de Hamming [Frakes 2003]</h3>
            <ol>
                <li>La distance de Hamming entre deux chaînes de longueur égale est définie comme le nombre de
                    caractères des deux chaînes qui sont différents à la même position.</li>
                <li>Pour les chaînes de longueur inégale, ajouter la différence de longueur à la distance de Hamming
                    pour obtenir une fonction de distance de Hamming modifiée \(d\)</li>
                <li>Exemples
                    <ul>
                        <li>tri: try, tried, trying</li>
                        <li>\(d\)(tri, try)= 1</li>
                        <li>\(d\)(tri, tried)= 2</li>
                        <li>\(d\)(tri, trying)= 4</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: force [Frakes 2003]</h3>
            <ol>
                <li>Le nombre moyen de mots par classe</li>
                <li>Facteur de compression de l'indice. Soit n est le nombre de mots dans le corpus et s est le nombre
                    de racines. \[\frac{n - s}{n}\]
                </li>
                <li>Le nombre de mots et de racines qui diffèrent</li>
                <li>Le nombre moyen de caractères supprimés lors de la formation des racines</li>
                <li>La médiane et la moyenne de la distance de Hamming modifiée entre les mots et leur racine</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: similarité [Frakes 2003]</h3>
            <ol>
                <li>Soit \(A1\) et \(A2\) sont deux algorithmes</li>
                <li>Soit \(W\) une liste de mots et \(n\) le nombre de mots dans \(W\) \[ M(A1,A2,W) = \frac{n}{\Sigma
                    d(x_i, y_i)}\]
                </li>
                <li>pour tous les mots \(w_i\) en W, \(x_i\) est le résultat de l'application de \(A1\) à \(w_i\) et
                    \(y_i\) est le résultat de l'application de \(A2\) à \(w_i\)</li>
                <li>des algorithmes plus similaires auront des valeurs plus élevées de M</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: nltk</h3>
            <p>L'objectif est de réduire les mots à leur forme de base ou racine, en éliminant les suffixes, ce qui
                permet de regrouper différentes formes d'un mot sous une forme commune. </p>
            <ul>
                <li><b>Porter [Porter 1980]</b> : Le Porter Stemming Algorithm, créé par Martin Porter en 1980, est basé
                    sur un ensemble de règles heuristiques. Il suit une approche itérative en appliquant une série de
                    transformations séquentielles aux mots.</li>
                <li><b>Snowball</b> Le Snowball (anciennement appelé Porter2) est une amélioration du Porter Stemmer. Il
                    suit également une approche basée sur des règles, mais il est plus systématique dans son traitement
                    des différents cas de racination. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <p>L'algorithme de Porter, également connu sous le nom de stemmer de Porter, est un algorithme de racination
                (stemming) développé par Martin Porter en 1980. Son objectif est de réduire les mots à leur forme racine
                ou base en éliminant les suffixes couramment utilisés en anglais.</p>
            <ul>
                <li><b>Prétraitement</b> : Convertir le mot en minuscules. Identifier le préfixe 'y' et le traiter comme
                    une voyelle s'il est en première position, sinon comme une consonne.</li>
                <li><b>Application des règles de racination</b> : L'algorithme de Porter utilise une série de règles
                    pour éliminer les suffixes. Ces règles sont appliquées séquentiellement jusqu'à ce qu'aucune d'entre
                    elles ne s'applique plus. Les règles comprennent des opérations comme la suppression de suffixes
                    spécifiques, la transformation de certains suffixes en d'autres, et la manipulation de la longueur
                    des mots. </li>
                <li><b>Post-traitement</b> : Certains ajustements post-traitement sont effectués pour améliorer la
                    précision de la racination.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <p>L'algorithme de Porter utilise une série de règles de racination pour réduire les mots à leur forme
                racine. Quelques-unes des règles de l'algorithme de Porter :</p>
            <ol>
                <li>Règles de suppression de suffixes :
                    <ul>
                        <li> "s" : Supprimer le suffixe "s" à la fin des mots.</li>
                        <li> "sses" : Remplacer par "ss" si la séquence se termine par "sses".</li>
                    </ul>
                </li>
                <li>Règles de traitement de suffixes spécifiques :
                    <ul>
                        <li> "eed" ou "eedly" : Remplacer par "ee" si la séquence se termine par "eed" ou "eedly".</li>
                        <li> "ed" : Supprimer "ed" à la fin du mot s'il y a une voyelle précédente.</li>
                        <li> "ing" : Supprimer "ing" à la fin du mot s'il y a une voyelle précédente.</li>
                    </ul>
                </li>

                </li>
                <ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Porter</h3>
            <ol start="3">
                <li>Règles de transformation de suffixes en d'autres suffixes :
                    <ul>
                        <li> "at" : Remplacer par "ate" si la séquence se termine par "at".</li>
                        <li> "bl" : Ajouter "e" à la fin si la séquence se termine par "bl".</li>
                    </ul>
                </li>

                <li>Règles de manipulation de la longueur des mots :
                    <ul>
                        <li> Si la séquence se termine par une consonne suivie de "y", remplacer par "i" à la fin.</li>
                        <li> Si la séquence se termine par deux consonnes, supprimer la dernière consonne si la
                            précédente est une voyelle.</li>
                    </ul>
                </li>

                <li>Règles de manipulation des doubles consonnes :
                    <ul>
                        <li> Supprimer une lettre double à la fin du mot.</li>
                    </ul>
                </li>
                <ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Porter</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Snowball</h3>
            <p>L'algorithme de Snowball, également connu sous le nom de Snowball stemmer, est un algorithme de
                racination (stemming) développé par Martin Porter comme une extension de son algorithme de Porter.
                Snowball a été conçu pour être plus modulaire et extensible, permettant aux utilisateurs de créer des
                stemmers pour différentes langues en utilisant un ensemble commun de conventions.</p>
            <p>Les caractéristiques principales de l'algorithme de Snowball :</p>
            <ul>
                <li><b>Modularité</b> : L'algorithme de Snowball est conçu de manière modulaire, permettant la
                    définition de règles spécifiques pour chaque langue. Chaque règle est encapsulée dans une unité
                    appelée "step."</li>
                <li><b>Structure du Langage</b> : L'algorithme de Snowball est souvent utilisé pour différentes langues,
                    et la structure du langage est définie par des fichiers de règles spécifiques à chaque langue. Ces
                    fichiers décrivent comment les suffixes et préfixes doivent être traités.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Snowball</h3>
            <ul>
                <li><b>Extensibilité</b> : Les utilisateurs peuvent étendre l'algorithme de Snowball pour traiter des
                    langues spécifiques en ajoutant des règles appropriées dans un fichier dédié à cette langue.</li>
                <li><b>Étape de Règle</b> : Chaque étape (step) de l'algorithme de Snowball est constituée de règles qui
                    décrivent comment transformer un mot. Chaque règle a une forme similaire à "condition -> action," où
                    la condition spécifie quand appliquer la règle, et l'action définit la transformation à effectuer.
                </li>
                <li><b>Itération</b> : L'algorithme de Snowball applique les étapes de règle itérativement jusqu'à ce
                    qu'aucune d'entre elles ne puisse être appliquée. Cette itération permet de réduire progressivement
                    les mots à leur forme racine.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>5.1.1. Racinisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Racinisation: Snowball</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">snowball</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">snowball</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <ul>
                <li>L'étiquetage morpho-syntaxique, également appelé <b style="color:#00549d">Part of Speech (PoS)
                        Tagging</b>, est un processus dans lequel chaque mot d'un texte se voit attribuer une balise
                    morpho-syntaxique appropriée en fonction de son rôle grammatical et de son contexte d'apparition.
                    Ces balises indiquent la catégorie grammaticale à laquelle chaque mot appartient. </li>
                <li>Il permet de capturer la <b style="color:#00549d">structure grammaticale</b> d'un texte, facilitant
                    ainsi la compréhension et l'analyse linguistique automatisées.</li>
                <li>Les algorithmes d'étiquetage morpho-syntaxique utilisent généralement des <b
                        style="color:#00549d">modèles statistiques</b> ou des <b style="color:#00549d">règles
                        linguistiques</b> pour assigner ces balises en fonction du contexte entourant chaque mot.</li>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <ul>
                <li>Exemples des balises
                    <ul>
                        <li><b style="color:#008000">Noms</b> : Indiquent des entités ou objets concrets. Exemple :
                            "chat," "maison," "fleur"</li>
                        <li><b style="color:#008000">Verbes</b> : Indiquent des actions ou des états. Exemple :
                            "marcher," "manger," "être"</li>
                        <li><b style="color:#008000">Adjectifs</b> : Décrivent ou qualifient des noms. Exemple : "beau,"
                            "rapide," "intelligent"</li>
                        <li><b style="color:#008000">Adverbes</b> : Modifient des verbes, des adjectifs ou d'autres
                            adverbes, fournissant des informations sur la manière, le lieu, le temps, etc. Exemple :
                            "rapidement," "bien," "ici"</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
            <ol>
                <li><b>Approche manuelle</b> :
                    <ul>
                        <li>Construction de règles linguistiques manuelles pour analyser la structure linguistique</li>
                        <li>Exemple : Définir des règles pour identifier les parties du discours en fonction de la
                            syntaxe.</li>
                    </ul>
                </li>
                <li><b>Approche statistique</b> :
                    <ul>
                        <li>Utilisation de statistiques et de probabilités pour modéliser les relations linguistiques.
                        </li>
                        <li>Collection de n-grammes (bi-grammes, tri-grammes, ...)</li>
                        <li>Ensemble de fréquences de cooccurrence</li>
                        <li>L'estimation de la probabilité d'une séquence de longueur n est calculée en tenant compte de
                            son occurrence dans le corpus d'entraînement</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
            <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
            <ol>
                <li><b>Apprentissage machine</b> :
                    <ul>
                        <li>Utilisation de techniques d'apprentissage machine pour apprendre automatiquement des modèles
                            linguistiques à partir de données d'entraînement.</li>
                        <li>Les algorithmes peuvent être entraînés à reconnaître des motifs et des structures
                            linguistiques complexes</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams</h3>
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{}</span><span class="s2">-grams&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: ngrams (affichage)</h3>
            <p class="codeexample">
                <code>
1-grams<br/>
('He',) ('went',) ('to',) ('school',) ('yesterday',) ('and',) ('attended',) ('the',) ('classes',) <br/>
2-grams<br/>
('He', 'went') ('went', 'to') ('to', 'school') ('school', 'yesterday') ('yesterday', 'and') ('and', 'attended') ('attended', 'the') ('the', 'classes') <br/>
3-grams<br/>
('He', 'went', 'to') ('went', 'to', 'school') ('to', 'school', 'yesterday') ('school', 'yesterday', 'and') ('yesterday', 'and', 'attended') ('and', 'attended', 'the') ('attended', 'the', 'classes') <br/>
4-grams<br/>
('He', 'went', 'to', 'school') ('went', 'to', 'school', 'yesterday') ('to', 'school', 'yesterday', 'and') ('school', 'yesterday', 'and', 'attended') ('yesterday', 'and', 'attended', 'the') ('and', 'attended', 'the', 'classes')<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>

            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He goes to school daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre>
            </div>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: pos_tag</h3>
            <p class="codeexample">
                <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
            </p>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>PRP</td>
                    <td>pronoun, personal</td>
                </tr>
                <tr>
                    <td>VBZ</td>
                    <td>verb, present tense, 3rd person singular</td>
                </tr>
                <tr>
                    <td>TO</td>
                    <td>"to" as preposition</td>
                </tr>
                <tr>
                    <td>NN</td>
                    <td>"noun, common, singular or mass</td>
                </tr>
                <tr>
                    <td>RB</td>
                    <td>adverb</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Installation</p>
            <p class="codeexample">
                <code>
				 $ pip3 install spacy<br/>
				 $ python3 -m spacy download en_core_web_sm<br/>
                         </code>
            </p>
            <p>Usage</p>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span></br>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  He PRON nsubj<br/>
                                  goes VERB ROOT<br/>
                                  to ADP prep<br/>
                                  school NOUN pobj<br/>
                                  daily ADV advmod<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>5.1.2. Étiquetage morpho-syntaxique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">tag_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">,</span>
            <span class="n">token</span><span class="o">.</span><span class="n">shape_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_alpha</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  He -PRON- PRON PRP nsubj Xx True True<br/>
                                  goes go VERB VBZ ROOT xxxx True False<br/>
                                  to to ADP IN prep xx True True<br/>
                                  school school NOUN NN pobj xxxx True False<br/>
                                  daily daily ADV RB advmod xxxx True False<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation, consiste à regrouper les différentes formes d'un mot qui appartiennent au même
                    paradigme morphologique flexionnel et à attribuer à chaque paradigme son lemme correspondant. </li>
                <li>Cette méthode vise à ramener les variations flexionnelles d'un mot à sa <b
                        style="color:#00549d">forme canonique</b> ou à sa racine. </li>
                <li>La lemmatisation permet de simplifier la représentation des mots en les ramenant à leur forme de
                    base, ce qui facilite la recherche, l'analyse et le traitement automatique du langage naturel. </li>
                <li>Exemples
                    <ul>
                        <li>go: go, goes, going, went, gone</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lemmatisation [Chrupała 2006, Gesmundo 2012]</h3>
            <ul>
                <li>La lemmatisation comme une tâche d'étiquetage</li>
                <li>Attribuer un label pour chaque transformation d'un label en lemme</li>
                <li>4 étapes [Gesmundo 2012]
                    <ol>
                        <li>supprimer un suffixe de longueur \(N_s\)</li>
                        <li>ajouter un nouveau suffixe de lemme \(L_s\)</li>
                        <li>supprimer un préfixe de longueur \(N_p\)</li>
                        <li>ajouter un nouveau préfixe lemme, \(L_p\)</li>
                    </ol>
                </li>
                <li>Transformation \(\tau = \langle N_s, L_s, N_p, L_p \rangle\)</li>
                <li>(going, go) = \(\langle 3, \emptyset, 0, \emptyset \rangle \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer</h3>
            <ul>
                <li><b style="color:#00549d">WordNet [Miller 1995]</b> : WordNet est une base de données lexicale de la
                    langue anglaise qui organise les mots en synsets (ensembles de synonymes) et les relie entre eux par
                    des relations lexicales telles que l'hypernymie (relation "est-un") et l'hyponymie (relation "a pour
                    instance").</li>
                <li><b style="color:#00549d">WordNetLemmatizer</b> : Le module WordNetLemmatizer dans NLTK utilise
                    WordNet pour la lemmatisation des mots. Il attribue à chaque mot sa forme canonique ou lemme, en
                    tenant compte des différentes formes flexionnelles. </li>
            </ul>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger&#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (sans les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     He went to school yesterday and attended the class
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">pos_tag</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>

<span class="c1"># Check the complete list of tags http://www.nltk.org/book/ch05.html</span>
<span class="k">def</span> <span class="nf">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;J&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADJ</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADV</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;N&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">NOUN</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">VERB</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;I went to school today and he goes daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">)),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			     I go to school today and he go daily
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>5.1.3. Lemmatisation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I went to school today and he goes daily&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <p class="codeexample">
                <code>
				  -PRON- go to school today and -PRON- go daily<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>5.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Morphologie</h3>
            <ul>
                <li>La morphologie lexicale est une branche de la linguistique qui se concentre sur l'étude des <b
                        style="color:#00549d">mots</b>, de leurs <b style="color:#00549d">formes</b>, de leurs <b
                        style="color:#00549d">paradigmes</b> et de l'organisation des <b
                        style="color:#00549d">catégories grammaticales</b>. </li>
                <li>Elle examine de près les parties du discours, l'intonation, l'accentuation, ainsi que la manière
                    dont le contexte peut influencer la prononciation et le sens d'un mot.</li>
                <li>Elle explore la structure interne des mots et comment ils interagissent avec la grammaire et le
                    contexte pour communiquer des significations spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>5.1.4. Morphologie</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">,</span> <span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
            </div>
            </p>
            <figure>
                <img src="../../2021/MachineLearning/spacy-dep-output.svg" width="400vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Les embeddings de mots sont une technique d'apprentissage de caractéristiques où des mots ou des phrases
                du vocabulaire sont associés à des vecteurs de nombres réels.</p>
            <ul>
                <li>L'idée principale est de représenter chaque mot par un vecteur dense dans un espace continu, de
                    telle sorte que des mots similaires aient des vecteurs similaires, capturant ainsi les relations
                    sémantiques entre les mots.</li>
                <li>Quantifier et catégoriser les similarités sémantiques entre les éléments linguistiques en fonction
                    de leurs propriétés de distribution dans de grands échantillons de données linguistiques.</li>
                <li>En d'autres termes, les mots qui ont des contextes similaires ou qui apparaissent dans des contextes
                    similaires auront des embeddings de mots similaires.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Avantages de Word Embeddings</p>
            <ul>
                <li><b>Représentation dense</b> : Les embeddings fournissent une représentation dense, contrairement à
                    une représentation creuse où chaque mot serait représenté par un vecteur binaire indiquant sa
                    présence ou son absence.</li>
                <li><b>Capture des relations sémantiques</b> : Les embeddings captent les relations sémantiques et les
                    similitudes entre les mots, ce qui les rend utiles dans de nombreuses tâches de traitement du
                    langage naturel.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
            <p>Applications de Word Embeddings</p>
            <ul>
                <li><b>Similarité sémantique</b> : Mesurer la similarité sémantique entre les mots.</li>
                <li><b>Traduction automatique</b> : Améliorer les performances des systèmes de traduction automatique.
                </li>
                <li><b>Analyse des sentiments</b> : Mieux comprendre le contexte et les relations sémantiques dans
                    l'analyse des sentiments, entre autres applications.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy est une bibliothèque open-source pour le traitement du langage naturel (NLP) en Python. Elle offre
                des outils performants et efficaces pour effectuer diverses tâches de traitement du langage naturel, de
                l'analyse syntaxique à la reconnaissance d'entités nommées. spaCy est conçu pour être rapide, précis et
                facile à utiliser.</p>
            <ul>
                <li><b>Collecte de données</b> : Les modèles spaCy sont souvent entraînés sur de vastes ensembles de
                    données annotées, qui peuvent inclure des corpus textuels avec des annotations pour l'analyse
                    syntaxique, la reconnaissance d'entités nommées, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>Annotation des données</b> : Les données collectées sont annotées manuellement avec des
                    informations linguistiques spécifiques telles que les parties du discours, les entités nommées, les
                    relations syntaxiques, etc.</li>
                <li><b>Entraînement initial</b> : Les modèles spaCy sont initialement entraînés sur ces ensembles de
                    données annotées pour apprendre les structures linguistiques. Ce processus peut inclure
                    l'utilisation d'algorithmes d'apprentissage automatique tels que les réseaux de neurones.</li>
                <li><b>Optimisation et réglage</b> : Les modèles sont ensuite optimisés et réglés pour améliorer leurs
                    performances sur des tâches spécifiques. Cela peut impliquer des itérations sur le processus
                    d'entraînement en ajustant les hyperparamètres du modèle.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <ul>
                <li><b>Évaluation</b> : Les modèles sont évalués sur des ensembles de données de test distincts pour
                    mesurer leur précision, leur rappel et d'autres métriques spécifiques à la tâche.</li>
                <li><b>Construction des modèles linguistiques pré-entraînés</b> : Une fois le modèle entraîné et évalué,
                    spaCy construit des modèles linguistiques pré-entraînés qui encapsulent les connaissances acquises
                    sur la structure linguistique.</li>
                <li><b>Téléchargement et utilisation</b> : Les utilisateurs peuvent télécharger ces modèles
                    pré-entraînés via spaCy et les utiliser dans leurs applications pour effectuer diverses tâches de
                    traitement du langage naturel sans avoir à entraîner un modèle de zéro.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>spaCy propose différents modèles linguistiques pré-entraînés pour différentes langues et tâches. Le
                modèle en_core_web_lg est un modèle vectoriel large d'anglais.</p>
            <h4>Installation du Modèle spaCy (en_core_web_lg) :</h4>
            <p class="codeexample">
                <code>
				 $ python3 -m spacy download en_core_web_lg<br/>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <h4>Chargement du Modèle spaCy :</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Avantages de spaCy :</p>
            <ul>
                <li><b>Performance élevée</b> : spaCy est reconnu pour sa rapidité d'exécution, ce qui le rend adapté au
                    traitement de grands volumes de texte en temps réel.</li>
                <li><b>Modèles pré-entraînés</b> : spaCy propose des modèles linguistiques pré-entraînés pour plusieurs
                    langues, ce qui facilite l'analyse de texte sans nécessiter d'entraînement à partir de zéro.</li>
                <li><b>Extraction d'informations linguistiques riches</b> : spaCy fournit des informations linguistiques
                    détaillées telles que les parties du discours, les entités nommées, les relations syntaxiques, et
                    plus encore.</li>
                <li><b>API conviviale</b> : L'API spaCy est conçue pour être intuitive et conviviale. Elle facilite la
                    réalisation de tâches complexes avec des lignes de code concises.</li>
                <li><b>Intégration avec d'autres bibliothèques</b> : spaCy s'intègre bien avec d'autres bibliothèques
                    Python populaires, facilitant son utilisation dans des projets plus larges.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy</h3>
            <p>Limites de spaCy :</p>
            <ul>
                <li><b>Dépendance des modèles linguistiques</b> : L'utilisation de modèles pré-entraînés signifie que la
                    qualité des résultats dépend de la qualité du modèle. Dans des domaines de spécialité ou pour des
                    langues moins courantes, les modèles peuvent ne pas être aussi performants.</li>
                <li><b>Gestion des entités nommées</b> : Bien que spaCy excelle dans la reconnaissance d'entités
                    nommées, il peut parfois avoir du mal avec des tâches plus complexes impliquant des variations
                    contextuelles.</li>
                <li><b>Taille des modèles</b> : Les modèles pré-entraînés peuvent être relativement volumineux, ce qui
                    peut être un inconvénient dans des environnements avec des restrictions de mémoire ou pour des
                    applications mobiles.</li>
                <li><b>Personnalisation limitée</b> : Bien que spaCy offre des fonctionnalités de personnalisation,
                    elles peuvent être limitées par rapport à d'autres bibliothèques NLP plus flexibles.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>

<span class="c1"># Définir les mots à comparer</span>
<span class="n">words_to_compare</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;apple&quot;</span><span class="p">]</span>

<span class="c1"># Calculer la similarité entre les paires de mots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
        <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="o">=</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">doc1</span><span class="p">,</span> <span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word1</span><span class="p">),</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
        <span class="n">similarity_score</span> <span class="o">=</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité (</span><span class="si">{}</span><span class="s2"> / </span><span class="si">{}</span><span class="s2">): </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span>
	    <span class="n">similarity_score</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: similarity</h3>
            <h4>Affichage</h4>
            <p class="codeexample">
                <code>
                <pre>
Similarité (dog / cat): ...
Similarité (dog / apple): ...
Similarité (cat / apple): ...
                </pre>

                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>5.2. Word Embeddings</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: vector</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>

<span class="c1"># Imprimer les vecteurs de chaque jeton sur une seule ligne</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteurs de &#39;</span><span class="si">{}</span><span class="s2">&#39; : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">,</span> <span class="n">vector_list</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>Word2Vec a marqué un tournant significatif dans la <b>représentation des mots</b> dans le domaine de
                l'apprentissage automatique.</p>
            <ul>
                <li>C'est une technique publiée en <b>2013</b> par une équipe de chercheurs dirigée par <b>Tomas
                        Mikolov</b> chez Google.</li>
                <li><b>Représentation vectorielle</b> : Word2Vec représente chaque mot distinct avec un vecteur dans un
                    espace continu. Ces vecteurs captent les relations sémantiques et syntaxiques entre les mots.</li>
                <li><b>Apprentissage basé sur un réseau neuronal</b> : Le modèle utilise un réseau neuronal pour
                    apprendre des associations de mots à partir d'un vaste corpus de texte. Cette approche permet de
                    capturer des nuances complexes dans la signification des mots.</li>
                <li><b>Entrée et sortie</b> : Word2Vec prend en entrée un large corpus de texte et produit un espace
                    vectoriel, généralement de plusieurs centaines de dimensions. Cette représentation vectorielle
                    permet de mesurer la similarité sémantique entre les mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <p>L'implémentation de Word2Vec se déroule en plusieurs étapes :</p>
            <ol>
                <li><b>Prétraitement des données</b> : Le texte est nettoyé et prétraité pour éliminer les éléments
                    indésirables tels que la ponctuation et les stopwords.</li>
                <li><b>Création d'un vocabulaire</b> : Les mots uniques du corpus sont utilisés pour construire un
                    vocabulaire. Chaque mot est ensuite associé à un index.</li>
                <li><b>Génération de paires mot-contexte</b> : Pour chaque mot du corpus, des paires mot-contexte sont
                    créées en utilisant une fenêtre contextuelle glissante. Ces paires servent d'exemples
                    d'entraînement.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
            <ol>
                <li><b>Construction du modèle Word2Vec</b> : Un modèle de réseau neuronal est créé, avec une couche
                    d'entrée représentant les mots, une couche cachée (skip-gram ou CBOW), et une couche de sortie pour
                    prédire le mot suivant dans le contexte.</li>
                <li><b>Entraînement du modèle</b> : Le modèle est entraîné sur les paires mot-contexte générées,
                    ajustant les poids du réseau pour minimiser la différence entre les prédictions et les vrais mots du
                    contexte.</li>
                <li><b>Obtention des embeddings</b> : Les vecteurs de mots appris pendant l'entraînement, appelés
                    embeddings, sont extraits. Chaque mot du vocabulaire est maintenant représenté par un vecteur dense
                    dans l'espace continu.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>5.3. Word2Vec</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Word2Vec</h3>
            <ul>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
                <li>une simple fonction mathématique (par exemple, la similarité cosinus entre les vecteurs) indique le
                    niveau de similarité sémantique entre les mots représentés par ces vecteurs \[\text{similarity} =
                    \cos(\theta) = {\mathbf{A} \cdot \mathbf{B}
                    \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i B_i} }{
                    \sqrt{\sum\limits_{i=1}^{n}{A_i^2}} \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} },\]
                </li>
                <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                    partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                    l'espace
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <p>CBOW est un modèle spécifique de Word2Vec. Dans ce modèle, la prédiction du mot courant se fait en
                utilisant une fenêtre de mots contextuels voisins. L'ordre des mots de contexte n'influence pas la
                prédiction, ce qui en fait une approche robuste.</p>
            <ul>
                <li><b>Modèle prédictif</b> : CBOW prédit le mot cible en se basant sur le contexte qui l'entoure, mais
                    contrairement à d'autres modèles, l'ordre spécifique des mots dans ce contexte n'est pas pris en
                    compte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
            <ul>
                <li><b>Entrée</b> : La donnée d'entrée du modèle CBOW est une fenêtre de mots contextuels entourant le
                    mot cible. Cette fenêtre est définie par un paramètre appelé la taille de la fenêtre.</li>
                <li><b>Architecture</b> : CBOW utilise une architecture de réseau neuronal à une seule couche cachée. La
                    couche d'entrée représente les mots du contexte, et la couche de sortie représente le mot cible à
                    prédire.</li>
                <li><b>Entraînement</b> : Le modèle est entraîné en ajustant les poids du réseau pour minimiser la
                    différence entre les prédictions du modèle et le mot cible réel. Cela se fait à travers des
                    techniques d'optimisation comme la rétropropagation du gradient.</li>
                <li><b>Sortie</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les relations sémantiques entre les mots, permettant
                    ainsi de représenter chaque mot par un vecteur dans un espace continu.</li>
                <li><b>Avantages</b> : CBOW est souvent plus rapide à entraîner que d'autres modèles comme le Skip-gram
                    (une autre variante de Word2Vec) et peut être plus efficace dans des contextes où l'ordre séquentiel
                    des mots n'est pas critique.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle CBOW avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
       <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>5.3.1. Context Bag of Words (CBOW)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: cbow</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>

<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,
                  </span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des deux mots les plus probables suivant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots suivant &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
	</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <p>Le modèle Skip-gram est une autre variante de Word2Vec qui se concentre sur la prédiction de la fenêtre
                voisine des mots de contexte à partir du mot courant. </p>
            <ul>
                <li><b>Objectif</b> : L'objectif principal du modèle Skip-gram est de prendre un mot source (le mot
                    courant) et de prédire les mots qui l'entourent dans une fenêtre de contexte donnée.</li>
                <li><b>Entrée</b> : Le mot source est utilisé comme donnée d'entrée du modèle, et la sortie souhaitée
                    est la distribution des probabilités des mots du contexte.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Skip grams</h3>
            <ul>
                <li><b>Architecture</b> : Skip-gram utilise une architecture de réseau neuronal à une seule couche
                    cachée. La couche d'entrée représente le mot source, et la couche de sortie représente les mots du
                    contexte.</li>
                <li><b>Entraînement</b> : Pendant l'entraînement, les poids du réseau sont ajustés pour minimiser la
                    différence entre les prédictions du modèle et la véritable distribution des mots du contexte. Cela
                    se fait généralement à l'aide de techniques d'optimisation comme la rétropropagation du gradient.
                </li>
                <li><b>Pondération du contexte</b> : Une caractéristique importante du modèle Skip-gram est que
                    l'architecture accorde plus de poids aux mots de contexte proches du mot source que ceux plus
                    éloignés. Cela permet de mieux capturer les relations sémantiques et syntaxiques locales.</li>
                <li><b>Embeddings</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                    embeddings de mots. Ces embeddings capturent les similitudes sémantiques entre les mots, permettant
                    de représenter chaque mot par un vecteur dans un espace continu.</li>
            </ul>
            <figure>
                <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle Skip-gram avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="c1"># sg: 1 pour skip-gram ; sinon CBOW.</span>
<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,
                </span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>5.3.2. Skip-grams</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">gensim: skip-gram</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des mots les plus probables dans le contexte entourant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots dans le contexte de &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <p>La Reconnaissance d'Entités Nommées (NER) consiste à identifier et classer des entités spécifiques dans
                un texte. Ces entités peuvent inclure des personnes, des lieux, des organisations, des dates, des
                montants monétaires, etc. Le but est d'extraire des informations structurées à partir de données
                textuelles non structurées.</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/datarepresentation.svg"
                    height="350vh" width="600vw" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <ul>
                <li><b>Identification d'entités</b> : La première étape de la NER consiste à identifier les mots ou
                    groupes de mots qui représentent des entités dans le texte. Ces entités peuvent être des noms de
                    personnes, des noms de lieux, des noms d'organisations, etc.</li>
                <li><b>Classification des entités</b> : Une fois les entités identifiées, elles sont classifiées dans
                    des catégories spécifiques. Par exemple, une entité peut être classée comme "PERSON" si elle
                    représente une personne, "LOCATION" si elle représente un lieu, "ORGANIZATION" si elle représente
                    une organisation, et ainsi de suite.</li>
                <li><b>Contextualisation</b> : La NER tient compte du contexte dans lequel une entité apparaît. Par
                    exemple, le mot "banc" peut être classé comme une entité financière dans le contexte d'une
                    discussion sur l'économie, mais comme une entité physique dans le contexte d'un parc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
            <ul>
                <li><b>Relations entre entités</b> : Dans certains cas, la NER peut également inclure la détection des
                    relations entre différentes entités dans le texte. Par exemple, la relation entre une personne et
                    l'organisation qu'elle travaille.</li>
                <li><b>Applications pratiques</b> : Les résultats de la NER peuvent être utilisés dans diverses
                    applications, telles que l'amélioration de la recherche d'informations, l'extraction de relations,
                    la catégorisation de documents, la création de résumés automatiques, etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <p>La Reconnaissance d'Entités Nommées (NER) est souvent réalisée à l'aide de modèles d'apprentissage
                automatique, et plusieurs algorithmes peuvent être utilisés dans ce contexte. Quelques-uns des
                algorithmes couramment employés :</p>
            <ul>
                <li><b>Modèles de markov cachés (HMM - Hidden Markov Models)</b> : Les HMM ont été utilisés pour la NER,
                    où l'idée est de modéliser la séquence des étiquettes d'entités en tant que séquence cachée derrière
                    la séquence observable de mots.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <ul>
                <li><b>Réseaux de neurones</b> : Les architectures de réseaux de neurones, y compris les réseaux de
                    neurones récurrents (RNN), les réseaux de neurones récurrents bidirectionnels (BiRNN), et les
                    réseaux de neurones récurrents à mémoire à court terme (LSTM), ont montré des performances
                    significatives dans la NER.</li>
                <li><b>Transformers</b> : Les modèles basés sur les transformers, tels que BERT (Bidirectional Encoder
                    Representations from Transformers) et ses variantes, ont considérablement amélioré les performances
                    en NER. Ces modèles sont pré-entraînés sur de grandes quantités de données textuelles et captent des
                    représentations contextuelles riches.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
            <ul>
                <li><b>Modèles statistiques traditionnels</b> : Des approches statistiques plus traditionnelles, comme
                    les modèles de séquence et les classificateurs basés sur des caractéristiques, ont également été
                    utilisées dans des scénarios où des quantités limitées de données annotées sont disponibles.</li>
                <li><b>Règles et expressions régulières</b> : Dans certains cas, des règles manuelles ou des expressions
                    régulières peuvent être utilisées pour extraire des entités spécifiques, surtout lorsque des motifs
                    clairs et récurrents peuvent être définis.</li>
                <li><b>Entraînement supervisé</b> : Les méthodes d'entraînement supervisé consistent à annoter
                    manuellement un ensemble de données avec des entités nommées, puis à entraîner un modèle sur ces
                    données annotées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France.&quot;</span> + <span class="s2">&quot;In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Analyser le texte</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre>
<span class="c1"># Afficher les informations sur les entités</span>
<span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">:</span>
    <span class="n">entity_text</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">text</span>
    <span class="n">start_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">start_char</span>
    <span class="n">end_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">end_char</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">label_</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entité: </span><span class="si">{}</span><span class="s2">, Début: </span><span class="si">{}</span><span class="s2">, Fin: </span><span class="si">{}</span><span class="s2">, Catégorie: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entity_text</span><span class="p">,
                 </span> <span class="n">start_char</span><span class="p">,</span> <span class="n">end_char</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
                <code>
                <pre>
Entité: Paris, Début: 0, Fin: 5, Catégorie: GPE
Entité: France, Début: 24, Fin: 30, Catégorie: GPE
Entité: 2015, Début: 35, Fin: 39, Catégorie: DATE
Entité: 2,206,488, Début: 72, Fin: 81, Catégorie: CARDINAL
                         </pre>
                </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">serve</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France.&quot;</span> + <span class="s2">&quot;In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">, </span><span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>5.4. Reconnaissance d'entités nommées (NER)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>GPE</td>
                    <td>Pays, villes, états.</td>
                </tr>
                <tr>
                    <td>DATE</td>
                    <td>Dates ou périodes absolues ou relatives</td>
                </tr>
                <tr>
                    <td>CARDINAL</td>
                    <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                </tr>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p> Le lexique <b>VADER</b> (Valence Aware Dictionary and sEntiment Reasoner) est spécifiquement conçu pour
                analyser les sentiments dans du texte en attribuant des scores de positivité, négativité et neutralité
                aux mots ainsi qu'aux expressions.</p>
            <h4>Installation</h4>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;vader_lexicon&#39;</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER</h3>
            <p>VADER est une bibliothèque d'analyse de sentiment conçue pour évaluer le sentiment d'un morceau de texte,
                généralement une phrase ou un paragraphe.</p>
            <ul>
                <li><b>Dictionnaire et Scores :</b> VADER utilise un dictionnaire pré-annoté avec des scores de
                    positivité, négativité et neutralité pour des milliers de mots et expressions. Chaque mot est
                    associé à un score qui indique dans quelle mesure il est perçu comme positif ou négatif.</li>
                <li><b>Polarité des Mots :</b> Pour chaque mot dans le texte, VADER examine son score dans le
                    dictionnaire. Certains mots ont des scores forts, indiquant une polarité positive ou négative,
                    tandis que d'autres ont des scores plus neutres.</li>
                <li><b>Modificateurs et Emphase :</b> VADER prend en compte les modificateurs, tels que les adverbes,
                    qui peuvent influencer la polarité d'un mot. Il reconnaît également l'emphase en attribuant des
                    poids différents aux mots en majuscules.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER</h3>
            <ul>
                <li><b>Calcul du Score Composé :</b> VADER agrège les scores des mots en utilisant une formule qui prend
                    en compte la distribution des polarités dans le texte. Le score composé résultant est une mesure
                    globale du sentiment de la phrase.</li>
                <li><b>Résultats :</b> Le résultat final de l'analyse est un ensemble de scores qui indiquent la
                    positivité, la négativité, la neutralité et un score composé global. Ces scores sont normalisés dans
                    une échelle de -1 à 1, où -1 représente un sentiment extrêmement négatif, 1 représente un sentiment
                    extrêmement positif, et 0 représente la neutralité.</li>
            </ul>
            <p>VADER est souvent utilisé pour l'analyse de sentiment rapide et basée sur des règles. Bien qu'il soit
                efficace dans de nombreux cas, il peut ne pas être aussi précis que des méthodes plus complexes basées
                sur l'apprentissage automatique, notamment dans des contextes où l'analyse nécessite une compréhension
                plus profonde du langage et de la syntaxe.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">VADER: Usage</h3>
            <p class="codeexample">
            <div class="highlight">
                <pre><span></span><span class="kn">from</span> <span class="nn">nltk.sentiment.vader</span> <span class="kn">import</span> <span class="n">SentimentIntensityAnalyzer</span>
<span class="n">sia</span> <span class="o">=</span> <span class="n">SentimentIntensityAnalyzer</span><span class="p">()</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is not very good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is bad&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>
</pre>
            </div>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Les scores renvoyés par VADER représentent différentes mesures du sentiment dans un texte. Une
                explication de chaque score :</p>
            <ul>
                <li><b>Positivité (Positive Score)</b> : Ce score mesure la positivité relative du texte. Il indique
                    dans quelle mesure le texte contient des éléments positifs. Plus le score est élevé, plus le texte
                    est perçu comme positif.</li>
                <li><b>Négativité (Negative Score)</b> : - Ce score mesure la négativité relative du texte. Il indique
                    dans quelle mesure le texte contient des éléments négatifs. Plus le score est élevé, plus le texte
                    est perçu comme négatif.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Neutralité (Neutral Score)</b> : Ce score mesure la neutralité relative du texte. Il indique dans
                    quelle mesure le texte est neutre, c'est-à-dire dépourvu d'éléments fortement positifs ou négatifs.
                    Plus le score est élevé, plus le texte est perçu comme neutre.</li>
                <li><b>Score Composé (Compound Score)</b> : Le score composé est une mesure agrégée du sentiment qui
                    prend en compte à la fois la positivité et la négativité du texte. Il combine les scores positif,
                    négatif et neutre en une seule valeur. Le score composé est souvent utilisé pour évaluer le
                    sentiment global du texte. Un score composé élevé indique un sentiment fort, qu'il soit positif ou
                    négatif, tandis qu'un score proche de zéro indique un texte neutre.</li>
            </ul>
            <p>Les scores sont normalisés dans une échelle de -1 à 1, où -1 représente un sentiment extrêmement négatif,
                1 représente un sentiment extrêmement positif, et 0 représente la neutralité. Les scores peuvent être
                interprétés individuellement ou conjointement pour obtenir une compréhension complète du sentiment dans
                le texte analysé.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>5.5. Analyse des sentiments (Sentiment Analysis)</h1>
        </div>
        <div class="content">
            <p>Affichage</p>
            <p class="codeexample">
                <code>
			{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}</br>
                        {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'compound': -0.3865}</br>
                        {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}</br>
                         </code>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Beel 2013a] Beel, Joeran, et al. “A Comparative Analysis of Offline and Online Evaluations and
                    Discussion of Research Paper Recommender System Evaluation.” Proceedings of the International
                    Workshop on Reproducibility and Replication in
                    Recommender Systems Evaluation, Association for Computing Machinery, 2013</li>
                <li>[Beel 2013b] Beel, Joeran, et al. “Sponsored vs. Organic (Research Paper) Recommendations and the
                    Impact of Labeling.” Research and Advanced Technology for Digital Libraries, edited by Trond Aalberg
                    et al., Springer, 2013, pp. 391–95.</li>
                <li>[Chrupała 2006] Chrupała, Grzegorz. Simple Data-Driven Context-Sensitive Lemmatization. 2006.
                    doras.dcu.ie, http://www.unizar.es/departamentos/filologia_inglesa/sepln2006/.</li>
                <li>[Frakes 2003] Frakes, William B., and Christopher J. Fox. “Strength and Similarity of Affix Removal
                    Stemming Algorithms.” ACM SIGIR Forum, vol. 37, no. 1, Apr. 2003, pp. 26–30. Spring 2003</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Gomez-Uribe 2016] Gomez-Uribe, Carlos A., and Neil Hunt. “The Netflix Recommender System:
                    Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems,
                    vol. 6, no. 4, Dec. 2016, p. 13:1–13:19. January
                    2016 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short
                    Papers -
                    Volume 2, Association for Computational Linguistics, 2012, pp. 368–372.</li>
                <li>[Gesmundo 2012] Gesmundo, Andrea, and Tanja Samardžić. “Lemmatisation as a Tagging Task.”
                <li>[Herlocker 2000] Herlocker, Jonathan L., et al. “Explaining Collaborative Filtering
                    Recommendations.” Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work,
                    Association for Computing Machinery, 2000, pp. 241–250.
                    ACM
                </li>
                <li>[Konstan 2012] Konstan, Joseph A., and John Riedl. “Recommender Systems: From Algorithms to User
                    Experience.” User Modeling and User-Adapted Interaction, vol. 22, no. 1–2, Apr. 2012, pp. 101–123.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Màrquez 2000] Màrquez, Lluís, et al. “A Machine Learning Approach to POS Tagging.” Machine
                    Learning, vol. 39, no. 1, Apr. 2000, pp. 59–91.</li>
                <li>[Mikolov 2013] Mikolov, Tomas, et al. “Efficient Estimation of Word Representations in Vector
                    Space.” ArXiv:1301.3781 [Cs], Sept. 2013.</li>
                <li>[Miller 1995] Miller, George A. “WordNet: A Lexical Database for English.” Communications of the
                    ACM, vol. 38, no. 11, Nov. 1995, pp. 39–41. Nov. 1995</li>
                <li>[Pazzani 2007] Pazzani, Michael J., and Daniel Billsus. “Content-Based Recommendation Systems.” The
                    Adaptive Web: Methods and Strategies of Web Personalization, edited by Peter Brusilovsky et al.,
                    Springer, 2007, pp. 325–41. </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Porter 1980] Porter, M. F. “An Algorithm for Suffix Stripping.” Program, vol. 14, no. 3, Jan. 1980,
                    pp. 130–37. Emerald Insight</li>
                <li>[Pu 2012] Pu, Pearl, et al. “Evaluating Recommender Systems from the User’s Perspective: Survey of
                    the State of the Art.” User Modeling and User-Adapted Interaction, vol. 22, no. 4, Oct. 2012, pp.
                    317–55.
                </li>
                <li>[Ricci 2011] Ricci, Francesco, et al. “Introduction to Recommender Systems Handbook.” Recommender
                    Systems Handbook, edited by Francesco Ricci et al., Springer US, 2011, pp. 1–35. </li>
                <li>[Ziegler 2005] Ziegler, Cai-Nicolas, et al. “Improving Recommendation Lists through Topic
                    Diversification.” Proceedings of the 14th International Conference on World Wide Web, Association
                    for Computing Machinery, 2005, pp. 22–32.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Web</h3>
            <ul>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation">https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation</a>
                </li>
                <li><a href="https://fr.wikipedia.org/wiki/Racinisation">https://fr.wikipedia.org/wiki/Racinisation</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Intelligence_artificielle">https://fr.wikipedia.org/wiki/Intelligence_artificielle</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Programmation_logique">https://fr.wikipedia.org/wiki/Programmation_logique</a>
                </li>
                <li><a
                        href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">https://en.wikipedia.org/wiki/Morphology_(linguistics)</a>
                </li>
                <li><a
                        href="https://en.wikipedia.org/wiki/Word_embedding">https://en.wikipedia.org/wiki/Word_embedding</a>
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
                <li><a href="https://www.nltk.org/howto/stem.html">https://www.nltk.org/howto/stem.html</a></li>
                <li><a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></li>
                <li><a href="https://spacy.io/usage/spacy-101">https://spacy.io/usage/spacy-101</a></li>
                <li><a href="https://spacy.io/usage/visualizers">https://spacy.io/usage/visualizers</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 185) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>
