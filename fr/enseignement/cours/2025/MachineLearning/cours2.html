<html>

    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Data Mining et Machine Learning (2025-2026): Traitement de données: John Samuel</title>
        <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
        <style type="text/css">
            /* ========================================
           CLEAN LAB + CYBER-MIDNIGHT THEME
           Minimalist Soft-Future (Light) / Cyberpunk High-Contrast (Dark)
           Digital Violet + Electric Cyan Accents
           ======================================== */

            :root {
                /* Light Theme - Clean Lab (Minimalist Soft-Future) */
                --bg-primary: #F0F4F8;
                --bg-secondary: #E3E8F0;
                --bg-tertiary: #FFFFFF;
                --text-primary: #102A43;
                --text-secondary: #243B53;
                --text-muted: #627D98;
                --accent-primary: #9F7AEA;
                --accent-secondary: #805AD5;
                --accent-tertiary: #66FCF1;
                --accent-gradient: linear-gradient(135deg, #805AD5 0%, #9F7AEA 50%, #B794F4 100%);
                --header-bg: linear-gradient(135deg, #102A43 0%, #243B53 50%, #334E68 100%);
                --footer-bg: linear-gradient(135deg, #102A43 0%, #243B53 100%);
                --card-bg: rgba(255, 255, 255, 0.95);
                --card-border: rgba(159, 122, 234, 0.2);
                --table-header-bg: linear-gradient(135deg, #243B53, #102A43);
                --table-cell-bg: rgba(227, 232, 240, 0.7);
                --code-bg: #102A43;
                --code-text: #F0F4F8;
                --shadow-color: rgba(16, 42, 67, 0.1);
                --glow-color: rgba(159, 122, 234, 0.15);
                --highlight-color: #9F7AEA;
                --highlight-bg: rgba(159, 122, 234, 0.1);
                --figcaption-bg: rgba(227, 232, 240, 0.95);
            }

            [data-theme="dark"] {
                /* Dark Theme - Cyber-Midnight (Cyberpunk High-Contrast) */
                --bg-primary: #0B0C10;
                --bg-secondary: #1F2833;
                --bg-tertiary: #2D3748;
                --text-primary: #FFFFFF;
                --text-secondary: #C5C6C7;
                --text-muted: #8B8D8F;
                --accent-primary: #66FCF1;
                --accent-secondary: #45A29E;
                --accent-tertiary: #9F7AEA;
                --accent-gradient: linear-gradient(135deg, #45A29E 0%, #66FCF1 50%, #7FFFD4 100%);
                --header-bg: linear-gradient(135deg, #0B0C10 0%, #1F2833 60%, #2D3748 100%);
                --footer-bg: linear-gradient(135deg, #0B0C10 0%, #1F2833 100%);
                --card-bg: rgba(31, 40, 51, 0.9);
                --card-border: rgba(102, 252, 241, 0.2);
                --table-header-bg: linear-gradient(135deg, #1F2833, #0B0C10);
                --table-cell-bg: rgba(45, 55, 72, 0.8);
                --code-bg: #1F2833;
                --code-text: #FFFFFF;
                --shadow-color: rgba(0, 0, 0, 0.5);
                --glow-color: rgba(102, 252, 241, 0.2);
                --highlight-color: #66FCF1;
                --highlight-bg: rgba(102, 252, 241, 0.12);
                --figcaption-bg: rgba(31, 40, 51, 0.95);
            }

            /* Base Styles */
            * {
                box-sizing: border-box;
            }

            html {
                height: 100%;
                scroll-behavior: smooth;
            }

            body {
                height: 100%;
                width: 100%;
                background: var(--bg-primary);
                margin: 0;
                overflow: hidden;
                font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
                color: var(--text-primary);
                transition: background-color 0.4s ease, color 0.4s ease;
            }

            /* Animated Background */
            body::before {
                content: '';
                position: fixed;
                top: 0;
                left: 0;
                width: 100%;
                height: 100%;
                background:
                    radial-gradient(ellipse at 15% 15%, var(--glow-color) 0%, transparent 45%),
                    radial-gradient(ellipse at 85% 85%, rgba(159, 122, 234, 0.1) 0%, transparent 50%),
                    radial-gradient(ellipse at 50% 50%, rgba(102, 252, 241, 0.05) 0%, transparent 60%);
                pointer-events: none;
                z-index: -1;
                animation: backgroundPulse 8s ease-in-out infinite;
            }

            @keyframes backgroundPulse {

                0%,
                100% {
                    opacity: 0.6;
                }

                50% {
                    opacity: 1;
                }
            }

            /* Theme Toggle Button */
            .theme-toggle {
                position: fixed;
                top: 1rem;
                right: 1rem;
                z-index: 1000;
                background: var(--card-bg);
                -webkit-backdrop-filter: blur(10px);
                backdrop-filter: blur(10px);
                border: 1px solid var(--card-border);
                border-radius: 50%;
                width: 48px;
                height: 48px;
                cursor: pointer;
                display: flex;
                align-items: center;
                justify-content: center;
                box-shadow: 0 4px 20px var(--shadow-color);
                transition: all 0.3s ease;
            }

            .theme-toggle:hover {
                transform: scale(1.1);
                box-shadow: 0 6px 30px var(--glow-color);
            }

            .theme-toggle svg {
                width: 24px;
                height: 24px;
                fill: var(--accent-primary);
                transition: fill 0.3s ease;
            }

            .theme-toggle .sun-icon {
                display: none;
            }

            .theme-toggle .moon-icon {
                display: block;
            }

            [data-theme="dark"] .theme-toggle .sun-icon {
                display: block;
            }

            [data-theme="dark"] .theme-toggle .moon-icon {
                display: none;
            }

            /* Slide Styles */
            .slide {
                height: 100vh;
                width: 100%;
                display: flex;
                flex-direction: column;
                animation: fadeIn 0.5s ease-out;
            }

            @keyframes fadeIn {
                from {
                    opacity: 0;
                    transform: translateY(10px);
                }

                to {
                    opacity: 1;
                    transform: translateY(0);
                }
            }

            /* Header Styles */
            .header {
                background: var(--header-bg);
                height: 5.5vmax;
                display: flex;
                align-items: center;
                justify-content: center;
                position: relative;
                overflow: hidden;
                box-shadow: 0 4px 20px var(--shadow-color);
            }

            .header::before {
                content: '';
                position: absolute;
                top: 0;
                left: -100%;
                width: 200%;
                height: 100%;
                background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.1), transparent);
                animation: headerShine 4s ease-in-out infinite;
            }

            @keyframes headerShine {
                0% {
                    transform: translateX(-50%);
                }

                100% {
                    transform: translateX(50%);
                }
            }

            .header h1 {
                color: #ffffff;
                text-align: center;
                font-size: 3vmax;
                line-height: 1.3;
                margin: 0;
                padding: 0 2rem;
                text-shadow: 0 2px 10px rgba(0, 0, 0, 0.2);
                font-weight: 600;
                letter-spacing: 0.5px;
                position: relative;
                z-index: 1;
            }

            /* Content Styles */
            .content {
                height: calc(100vh - 5.5vmax - 3.2vmax - 0.6vmax);
                width: 95vw;
                max-width: 95vw;
                display: flex;
                line-height: 1.8em;
                flex-direction: column;
                align-items: flex-start;
                margin: 0 auto;
                color: var(--text-primary);
                text-align: left;
                padding: 1.5vmax;
                overflow-x: auto;
                overflow-y: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
                background: var(--card-bg);
                -webkit-backdrop-filter: blur(10px);
                backdrop-filter: blur(10px);
            }

            .content h1,
            .content h2,
            .content h3,
            .content h4 {
                color: var(--accent-primary);
                font-weight: 600;
                margin-top: 0.5em;
                margin-bottom: 0.5em;
                text-shadow: 0 1px 2px var(--shadow-color);
            }

            .content h1 {
                font-size: 2.2em;
                background: var(--accent-gradient);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                background-clip: text;
            }

            .content h2 {
                font-size: 1.6em;
            }

            .content h3 {
                font-size: 1.3em;
            }

            .content h4 {
                font-size: 1.1em;
            }

            .content p {
                margin: 0.6em 0;
                line-height: 1.9;
                padding: 0.3em 0;
            }

            /* Bold/Strong terms - Key terminology highlighting */
            .content b,
            .content strong {
                color: var(--accent-primary);
                font-weight: 700;
                background: linear-gradient(120deg, var(--table-cell-bg) 0%, var(--table-cell-bg) 100%);
                background-size: 100% 0.3em;
                background-repeat: no-repeat;
                background-position: 0 88%;
                padding: 0 0.2em;
                border-radius: 0.2em;
                transition: all 0.3s ease;
            }

            .content li>b:first-child,
            .content li>strong:first-child {
                display: inline-block;
                background: var(--accent-gradient);
                color: white;
                padding: 0.1em 0.6em;
                border-radius: 0.4em;
                margin-right: 0.3em;
                font-size: 0.95em;
                box-shadow: 0 2px 6px var(--shadow-color);
                text-shadow: 0 1px 2px rgba(0, 0, 0, 0.2);
            }

            /* Superscript styling for references */
            .content sup {
                color: var(--accent-tertiary);
                font-weight: 600;
            }

            .content sup a {
                color: var(--accent-tertiary);
                font-weight: 600;
            }

            /* Divs with width styling (two-column layouts) */
            .content>div[style*="width"] {
                background: var(--card-bg);
                border-radius: 1rem;
                padding: 1rem;
                margin: 0.5rem;
                box-shadow: 0 2px 10px var(--shadow-color);
                border: 1px solid var(--card-border);
            }

            /* Topic Headings */
            .content .topichighlight {
                background: linear-gradient(135deg, #805AD5, #9F7AEA);
                color: #FFFFFF;
                padding: 0.5em 1em;
                border-radius: 0.5em;
                display: inline-block;
                box-shadow: 0 2px 8px rgba(159, 122, 234, 0.35);
            }

            .content .topicheading {
                background: var(--accent-gradient);
                color: #FFFFFF;
                vertical-align: middle;
                border-radius: 0 2vmax 2vmax 0;
                height: 4vmax;
                line-height: 4vmax;
                padding-left: 1.5vmax;
                padding-right: 1vmax;
                margin: 0.1vmax 0 1vmax 0;
                width: 55%;
                font-weight: 600;
                box-shadow: 0 4px 15px var(--shadow-color);
                position: relative;
                overflow: hidden;
            }

            .content .topicheading::after {
                content: '';
                position: absolute;
                top: 0;
                right: 0;
                width: 30%;
                height: 100%;
                background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.2));
            }

            .content .topicsubheading {
                background: var(--accent-gradient);
                color: #FFFFFF;
                vertical-align: middle;
                border-radius: 0 1.5vmax 1.5vmax 0;
                height: 3vmax;
                margin: 0.1vmax 0 1vmax 0;
                font-size: 95%;
                line-height: 3vmax;
                padding-left: 1.5vmax;
                padding-right: 1vmax;
                width: 55%;
                font-weight: 600;
                box-shadow: 0 3px 12px var(--shadow-color);
            }

            /* Flex and Grid Content */
            .content .flexcontent {
                display: flex;
                overflow-y: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
                gap: 1rem;
                align-items: center;
            }

            .content .gridcontent {
                display: grid;
                grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                grid-gap: 1rem;
            }

            /* Lists - Enhanced Visual Styling */
            .content ul,
            .content ol {
                padding-left: 0.5em;
                margin: 0.8em 0;
                list-style: none;
            }

            .content li {
                line-height: 1.8em;
                margin-bottom: 0.6em;
                position: relative;
                padding-left: 1.8em;
                padding-right: 0.5em;
                padding-top: 0.3em;
                padding-bottom: 0.3em;
                border-radius: 0.5em;
                transition: all 0.3s ease;
            }

            /* Unordered list custom bullet */
            .content ul>li::before {
                content: '';
                position: absolute;
                left: 0.4em;
                top: 0.85em;
                width: 0.5em;
                height: 0.5em;
                background: var(--accent-gradient);
                border-radius: 50%;
                box-shadow: 0 0 6px var(--glow-color);
            }

            /* Ordered list custom numbers */
            .content ol {
                counter-reset: item;
            }

            .content ol>li {
                counter-increment: item;
            }

            .content ol>li::before {
                content: counter(item);
                position: absolute;
                left: 0.2em;
                top: 0.3em;
                width: 1.4em;
                height: 1.4em;
                background: var(--accent-gradient);
                color: white;
                border-radius: 50%;
                font-size: 0.75em;
                font-weight: 700;
                display: flex;
                align-items: center;
                justify-content: center;
                box-shadow: 0 2px 8px var(--shadow-color);
            }

            /* Nested lists */
            .content ul ul,
            .content ol ul,
            .content ul ol,
            .content ol ol {
                margin: 0.4em 0 0.4em 0.5em;
                padding-left: 0.3em;
            }

            .content ul ul>li::before,
            .content ol ul>li::before {
                width: 0.4em;
                height: 0.4em;
                background: var(--accent-secondary);
                top: 0.9em;
            }

            .content ol ol>li::before,
            .content ul ol>li::before {
                width: 1.2em;
                height: 1.2em;
                font-size: 0.65em;
                background: var(--accent-secondary);
            }

            /* List item hover effect */
            .content li:hover {
                background: var(--table-cell-bg);
                border-radius: 0.5em;
            }

            /* Links */
            .content a:link,
            .content a:visited {
                color: var(--accent-tertiary);
                text-decoration: none;
                border-bottom: 1px dashed var(--accent-tertiary);
                transition: all 0.3s ease;
            }

            .content a:hover {
                color: var(--accent-secondary);
                border-bottom-style: solid;
            }

            /* Tables - Enhanced Visual Styling */
            .content table {
                color: var(--text-primary);
                font-size: 100%;
                width: 100%;
                border-collapse: separate;
                border-spacing: 0.4rem;
                margin: 1em 0;
                background: var(--card-bg);
                border-radius: 1rem;
                padding: 0.5rem;
                box-shadow: 0 4px 20px var(--shadow-color);
            }

            .content th {
                color: #FFFFFF;
                background: var(--table-header-bg);
                border-radius: 0.8rem;
                font-size: 105%;
                padding: 0.8em 1em;
                font-weight: 600;
                text-shadow: 0 1px 2px rgba(0, 0, 0, 0.2);
                position: relative;
                overflow: hidden;
            }

            .content th::after {
                content: '';
                position: absolute;
                top: 0;
                right: 0;
                width: 30%;
                height: 100%;
                background: linear-gradient(90deg, transparent, rgba(255, 255, 255, 0.15));
                pointer-events: none;
            }

            .content td {
                color: var(--text-primary);
                padding: 0.7em 1em;
                background: var(--table-cell-bg);
                border-radius: 0.6rem;
                -webkit-backdrop-filter: blur(5px);
                backdrop-filter: blur(5px);
                transition: all 0.3s ease;
                border-left: 3px solid transparent;
            }

            .content tr:hover td {
                background: var(--card-border);
                border-left-color: var(--accent-primary);
                transform: translateX(3px);
            }

            /* Figures and Images - Enhanced */
            .content figure {
                max-width: 90%;
                max-height: 90%;
                margin: 1em auto;
                text-align: center;
                padding: 0.8rem;
                background: var(--card-bg);
                border-radius: 1.2rem;
                box-shadow: 0 4px 20px var(--shadow-color);
                border: 1px solid var(--card-border);
            }

            .content .fullwidth img {
                max-width: 90%;
                max-height: 90%;
            }

            .content figure img {
                max-width: 50vmin;
                max-height: 50vmin;
                display: block;
                margin: 0 auto;
                border-radius: 0.8rem;
                box-shadow: 0 4px 15px var(--shadow-color);
                transition: transform 0.3s ease, box-shadow 0.3s ease;
            }

            .content figure img:hover {
                transform: scale(1.02);
                box-shadow: 0 8px 25px var(--glow-color);
            }

            .content figure figcaption {
                max-width: 95%;
                margin: 0.8rem auto 0;
                font-size: 90%;
                text-align: center;
                padding: 0.5rem 1rem;
                background: var(--accent-gradient);
                color: white;
                border-radius: 2rem;
                font-weight: 500;
                font-style: normal;
                box-shadow: 0 2px 10px var(--shadow-color);
            }

            /* Flexcontent images (multiple images) */
            .content .flexcontent {
                display: flex;
                overflow-y: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
                gap: 1.2rem;
                align-items: center;
                justify-content: center;
                padding: 0.5rem;
            }

            .content .flexcontent img {
                border-radius: 0.8rem;
                box-shadow: 0 4px 15px var(--shadow-color);
                transition: all 0.3s ease;
                border: 2px solid var(--card-border);
            }

            .content .flexcontent img:hover {
                transform: scale(1.03) rotate(1deg);
                box-shadow: 0 8px 25px var(--glow-color);
                border-color: var(--accent-primary);
            }

            /* Code and Pre - Enhanced */
            .content pre {
                background: var(--code-bg);
                color: var(--code-text);
                border: 1px solid var(--card-border);
                border-left: 4px solid var(--accent-primary);
                border-radius: 0 0.8rem 0.8rem 0;
                padding: 1.2rem 1.5rem;
                overflow-x: auto;
                font-family: 'Consolas', 'Monaco', 'Fira Code', monospace;
                font-size: 85%;
                line-height: 1.6;
                box-shadow: 0 4px 15px var(--shadow-color);
                position: relative;
            }

            .content pre::before {
                content: 'CODE';
                position: absolute;
                top: 0;
                right: 0;
                background: var(--accent-gradient);
                color: white;
                font-size: 0.6em;
                padding: 0.2em 0.8em;
                border-radius: 0 0.8rem 0 0.5rem;
                font-weight: 600;
                letter-spacing: 0.5px;
            }

            .content code {
                font-family: 'Consolas', 'Monaco', 'Fira Code', monospace;
                background: var(--table-cell-bg);
                padding: 0.15em 0.4em;
                border-radius: 0.3em;
                font-size: 90%;
                color: var(--accent-primary);
                border: 1px solid var(--card-border);
            }

            /* Inline code in pre should not have extra styling */
            .content pre code {
                background: transparent;
                padding: 0;
                border: none;
                color: inherit;
            }

            /* Footer Styles */
            .footer {
                height: 3.2vmax;
                line-height: 3.2vmax;
                background: var(--footer-bg);
                margin: 0;
                padding: 0 1vmax;
                display: flex;
                align-items: center;
                justify-content: space-between;
                box-shadow: 0 -4px 20px var(--shadow-color);
            }

            .footer .contact {
                color: #ffffff;
                text-align: left;
                font-size: 2.8vmin;
                font-weight: 500;
                text-shadow: 0 1px 3px rgba(0, 0, 0, 0.2);
            }

            .footer .navigation {
                display: flex;
                align-items: center;
                gap: 0.5rem;
                font-size: 2.8vmin;
                color: #ffffff;
                font-weight: 600;
            }

            .footer .navigation a {
                color: #ffffff;
                text-decoration: none;
                padding: 0.3rem 0.6rem;
                border-radius: 0.5rem;
                transition: all 0.3s ease;
                display: inline-flex;
                align-items: center;
            }

            .footer .navigation a:hover {
                background: rgba(255, 255, 255, 0.2);
            }

            .footer .navigation .next::after {
                content: " >";
                margin-left: 0.2rem;
            }

            .footer .navigation .prev::after {
                content: "< ";
                margin-right: 0.2rem;
            }

            /* Jupyter/Highlight Styles - Enhanced */
            .highlight {
                background: var(--code-bg);
                color: var(--code-text);
                border-radius: 0.8rem;
                padding: 1rem;
                border: 1px solid var(--card-border);
                border-left: 4px solid var(--accent-tertiary);
                box-shadow: 0 4px 15px var(--shadow-color);
                position: relative;
                margin: 0.5em 0;
            }

            .highlight::before {
                content: 'PYTHON';
                position: absolute;
                top: 0;
                right: 0;
                background: linear-gradient(135deg, #45A29E, #66FCF1);
                color: #0B0C10;
                font-size: 0.55em;
                padding: 0.2em 0.8em;
                border-radius: 0 0.8rem 0 0.5rem;
                font-weight: 600;
                letter-spacing: 0.5px;
            }

            .highlight pre {
                margin: 0;
                padding: 0;
                background: transparent;
                border: none;
                box-shadow: none;
                color: var(--code-text, var(--text-primary));
            }

            .highlight pre::before {
                display: none;
            }

            .highlight .c {
                color: #8B9DAF;
                font-style: italic
            }

            .highlight .err {
                border: 1px solid #ef4444;
                border-radius: 2px;
            }

            .highlight .k {
                color: #66FCF1;
                font-weight: bold
            }

            .highlight .o {
                color: #C5C6C7
            }

            .highlight .ch {
                color: #8B9DAF;
                font-style: italic
            }

            .highlight .c1 {
                color: #8B9DAF;
                font-style: italic
            }

            .highlight .cs {
                color: #8B9DAF;
                font-style: italic
            }

            .highlight .cm {
                color: #8B9DAF;
                font-style: italic
            }

            .highlight .nn {
                color: #B794F4;
                font-weight: bold
            }

            .highlight .s2 {
                color: #68D391
            }

            .highlight .s1 {
                color: #68D391
            }

            .highlight .kn {
                color: #66FCF1;
                font-weight: bold
            }

            .highlight .nb {
                color: #63B3ED
            }

            .highlight .n {
                color: #E6EAF0
            }

            .highlight .p {
                color: #C5C6C7
            }

            .highlight .mb {
                color: #F6AD55
            }

            .highlight .mf {
                color: #F6AD55
            }

            .highlight .mh {
                color: #F6AD55
            }

            .highlight .mi {
                color: #F6AD55
            }

            .highlight .mo {
                color: #F6AD55
            }

            /* Title slide (slide 1) special styling */
            #slide1 .content {
                display: flex;
                flex-direction: column;
                justify-content: center;
                align-items: center;
                text-align: center;
            }

            #slide1 .content h1 {
                font-size: 4vw;
                margin-bottom: 1rem;
                animation: titleGlow 3s ease-in-out infinite alternate;
            }

            @keyframes titleGlow {
                from {
                    text-shadow: 0 0 20px var(--glow-color);
                }

                to {
                    text-shadow: 0 0 40px var(--glow-color), 0 0 60px var(--accent-tertiary);
                }
            }

            #slide1 .content p {
                font-size: 1.2em;
                max-width: 600px;
                background: var(--card-bg);
                padding: 1.5rem 2rem;
                border-radius: 1rem;
                box-shadow: 0 8px 30px var(--shadow-color);
                border: 1px solid var(--card-border);
            }

            #slide1 .content img {
                margin-top: 1rem;
                border-radius: 0.5rem;
                box-shadow: 0 4px 15px var(--shadow-color);
            }

            /* Responsive Styles */
            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                .slide {
                    height: 100vh;
                    width: 100%;
                }

                .content {
                    height: calc(100vh - 5.5vmax - 3.2vmax - 0.6vmax);
                    width: 100%;
                    padding: 1vw;
                    line-height: 3.5vmax;
                    font-size: 1.8vmax;
                }

                .content .topicheading,
                .content .topicsubheading {
                    width: 95%;
                }

                .content h1,
                .content h2,
                .content h3,
                .content h4 {
                    width: 100%;
                }

                .content figure img {
                    max-width: 80vmin;
                    max-height: 50vmin;
                }

                .header h1 {
                    font-size: 2.5vmax;
                    padding: 0 1rem;
                }

                .theme-toggle {
                    width: 40px;
                    height: 40px;
                    top: 0.5rem;
                    right: 0.5rem;
                }
            }

            @media print {
                body {
                    max-width: 100%;
                    max-height: 100%;
                    background: white;
                }

                body::before {
                    display: none;
                }

                .theme-toggle {
                    display: none;
                }

                .content {
                    font-size: 2.8vmin;
                    background: white;
                }

                .content .flexcontent {
                    font-size: 2.5vmin;
                }

                .header,
                .footer {
                    background: #2563EB !important;
                    -webkit-print-color-adjust: exact;
                    print-color-adjust: exact;
                }
            }

            /* Smooth scrollbar */
            ::-webkit-scrollbar {
                width: 8px;
                height: 8px;
            }

            ::-webkit-scrollbar-track {
                background: var(--bg-secondary);
                border-radius: 4px;
            }

            ::-webkit-scrollbar-thumb {
                background: var(--accent-primary);
                border-radius: 4px;
            }

            ::-webkit-scrollbar-thumb:hover {
                background: var(--accent-secondary);
            }
        </style>
        <script src="../../../../../fr/enseignement/cours/2020/MachineLearning/tex-mml-chtml.js"
            id="MathJax-script"></script>
    </head>

    <body>
        <!-- Theme Toggle Button -->
        <button class="theme-toggle" onclick="toggleTheme()" aria-label="Toggle theme">
            <svg class="sun-icon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path
                    d="M12 3V1m0 22v-2M4.22 4.22l1.42 1.42m12.72 12.72l1.42 1.42M1 12h2m18 0h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42M12 6a6 6 0 100 12 6 6 0 000-12z" />
            </svg>
            <svg class="moon-icon" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
                <path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z" />
            </svg>
        </button>
        <section class="slide" id="slide1">
            <div class="header">
            </div>
            <div class="content">
                <h1 style="font-size:3.5vw">Data Mining et Machine Learning</h1>
                <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                    <b>Année</b>: 2025-2026<br />
                    <b>Courriel</b>: john.samuel@cpe.fr<br /><br />
                    <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                            alt="Creative Commons License" style="border-width:0"
                            src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">1

                    <a class="next" href="#slide2"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide2">
            <div class="header">
                <h1>Data Mining</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Objectifs</h3>
                <ol>
                    <li>Régularités</li>
                    <li>Exploration des données</li>
                    <li>Algorithmes</li>
                    <li>Sélection de caractéristiques</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">2
                    <a class="prev" href="#slide1"></a>
                    <a class="next" href="#slide3"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide3">
            <div class="header">
                <h1>2.1. Régularités</h1>
            </div>
            <div class="content">
                <figure class="flexcontent">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg"
                        height="300px" width="300px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg"
                        height="300px" width="300px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg"
                        height="300px" width="300px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG"
                        height="300px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">3
                    <a class="prev" href="#slide2"></a>
                    <a class="next" href="#slide4"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide4">
            <div class="header">
                <h1>2.1. Régularités</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Régularités naturelles</h1>
                <ul>
                    <li>Symétrie</li>
                    <li>Arbres, fractales</li>
                    <li>Spirales</li>
                    <li>Chaos</li>
                    <li>Ondes</li>
                    <li>Bulles, mousse</li>
                    <li>Pavages</li>
                    <li>Ruptures</li>
                    <li>Taches, bandes</li>
                </ul>
                <figure class="gridcontent">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Kittyply_edit1.jpg"
                        height="150px" width="150px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG"
                        height="150px" width="150px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg"
                        height="150px" width="150px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Rio_Negro_meanders.JPG"
                        height="150px" width="150px" />
                </figure>
                <Régularitésfigure class="gridcontent">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg"
                        height="150px" width="150px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Apis_florea_nest_closeup2.jpg"
                        height="150px" width="150px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg"
                        height="150px" width="150px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Equus_grevyi_(aka).jpg"
                        height="150px" width="150px" />
                    </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">4
                    <a class="prev" href="#slide3"></a>
                    <a class="next" href="#slide5"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide5">
            <div class="header">
                <h1>2.1. Régularités</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Créations humaines</h1>
                <ul>
                    <li><b>Bâtiments (Symétrie)</b> : Structures construites par l'homme avec des motifs de symétrie.
                        Exemple : Cathédrales gothiques, gratte-ciels modernes.</li>
                    <li><b>Villes</b> : Agglomérations planifiées ou organiques habitées par les humains.
                        Exemple : Paris, New York.</li>
                    <li><b>Environnement virtuel (e.g., jeux de vidéo)</b> : Espaces créés numériquement pour
                        l'interaction
                        humaine.
                        Exemple : Mondes ouverts dans les jeux vidéo, simulations virtuelles.</li>
                    <li><b>Les artefacts humains</b> : Objets fabriqués par les humains dans divers domaines.
                        Exemple : Outils préhistoriques, œuvres d'art contemporaines.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Roman_geometric_mosaic.jpg"
                        height="150px" width="600px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">5
                    <a class="prev" href="#slide4"></a>
                    <a class="next" href="#slide6"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide6">
            <div class="header">
                <h1>2.1. Régularités</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Création</h1>
                <ul>
                    <li>Répétition</li>
                    <li><b>Fractales</b> : Structures mathématiques auto-similaires à différentes échelles.
                        <ul>
                            <li>Ensemble de Julia: Un ensemble fractal défini par une fonction itérative <i>f(z) =
                                    z<sup>2</sup> + c</i></li>
                            <li><b>Caractéristiques</b> : produit des motifs répétitifs complexes lorsqu'il est
                                visualisé.
                            </li>
                        </ul>
                    </li>
                </ul>
                <figure class="gridcontent">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/400px-Tiling_Dual_Semiregular_V3-3-3-3-6_Floret_Pentagonal.svg.png"
                        height="200px" width="200px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Finite_subdivision_of_a_radial_link.png"
                        height="200px" width="200px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Julia_set_(indigo).png"
                        height="200px" width="200px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">6
                    <a class="prev" href="#slide5"></a>
                    <a class="next" href="#slide7"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide7">
            <div class="header">
                <h1>2.1. Régularités</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Synonymes</h1>
                <ul>
                    <li>Fouille de données</li>
                    <li>Forage de données</li>
                    <li>Extraction de connaissances à partir de données</li>
                    <li>Data mining</li>
                    <li>Machine learning</li>
                    <li>Apprentissage automatique</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">7
                    <a class="prev" href="#slide6"></a>
                    <a class="next" href="#slide8"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide8">
            <div class="header">
                <h1>2.1.2. Approches de l'apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Approches</h3>
                <ul>
                    <li><b>Apprentissage supervisé</b> : Le modèle est entraîné sur un ensemble de données étiquetées où
                        les
                        exemples d'entrée sont associés à des sorties désirées. Le modèle apprend à faire des
                        prédictions
                        sur de nouvelles données en se basant sur ces associations.</li>
                    <li><b>Apprentissage non supervisé</b> : Le modèle est exposé à des données non étiquetées et
                        cherche à
                        découvrir des modèles, des structures ou des relations intrinsèques dans les données.</li>
                    <li><b>Apprentissage semi-supervisé</b> : Une combinaison des deux précédents, utilisant à la fois
                        des
                        données étiquetées et non étiquetées pour l'entraînement.</li>
                    <li><b>Apprentissage par renforcement</b> : Le modèle apprend à prendre des décisions en
                        interagissant
                        avec son environnement. Il reçoit des récompenses ou des pénalités en fonction de ses actions,
                        ce
                        qui guide son apprentissage.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">8
                    <a class="prev" href="#slide7"></a>
                    <a class="next" href="#slide9"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide9">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation</h3>
                <ul>
                    <li><b>Vecteur euclidien</b>:
                        <ul>
                            <li>Un vecteur euclidien est un objet géométrique caractérisé par sa magnitude (longueur) et
                                sa
                                direction. </li>
                            <li>Les vecteurs euclidiens sont couramment utilisés pour représenter des données sous forme
                                de
                                points dans un espace multidimensionnel, où chaque dimension correspond à une
                                caractéristique ou une variable.</li>
                        </ul>
                    </li>
                    <li><b>Espace vectoriel</b>:
                        <ul>
                            <li>Un espace vectoriel est une collection de vecteurs qui peuvent être additionnés entre
                                eux et
                                multipliés par des nombres (scalaires).</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">9
                    <a class="prev" href="#slide8"></a>
                    <a class="next" href="#slide10"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide10">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation</h3>
                <ul>
                    <li><b>Vecteur de caractéristiques (features)</b>:
                        <ul>
                            <li>Un vecteur de caractéristiques est un vecteur n-dimensionnel qui représente les
                                caractéristiques ou les attributs d'une entité. </li>
                        </ul>
                    </li>
                    <li><b>Espace de caractéristiques</b>:
                        <ul>
                            <li>L'espace de caractéristiques est l'espace vectoriel associé aux vecteurs de
                                caractéristiques.</li>
                            <li>Chaque dimension de cet espace représente une caractéristique particulière, et les
                                vecteurs
                                sont utilisés pour positionner les données dans cet espace en fonction de leurs
                                caractéristiques.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">10
                    <a class="prev" href="#slide9"></a>
                    <a class="next" href="#slide11"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide11">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Exemples de caractéristiques</h3>
                <ul>
                    <li><b>Images</b>: Dans le contexte des images, les vecteurs de caractéristiques peuvent être
                        construits
                        à partir des valeurs des pixels. Chaque pixel peut être considéré comme une dimension, et un
                        vecteur
                        de caractéristiques contiendra les valeurs de tous les pixels, permettant ainsi de représenter
                        une
                        image sous forme de vecteur.</li>
                    <li><b>Textes</b>: Pour les textes, les vecteurs de caractéristiques sont souvent construits à
                        partir de
                        la fréquence d'apparition des mots, des phrases, ou des tokens dans un document. Cela permet de
                        représenter le contenu textuel en utilisant des valeurs numériques, ce qui est essentiel pour
                        l'analyse de texte et la recherche d'informations.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">11
                    <a class="prev" href="#slide10"></a>
                    <a class="next" href="#slide12"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide12">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation</h1>
                <ul>
                    <li><b>Construction de caractéristiques<sup>1</sup></b>:
                        <ul>
                            <li>La construction de caractéristiques consiste à créer de nouvelles variables ou attributs
                                à
                                partir de celles déjà présentes dans les données.</li>
                            <li>Cette étape peut être cruciale pour améliorer les performances des modèles
                                d'apprentissage
                                machine en introduisant des informations pertinentes et en éliminant du bruit.</li>
                        </ul>
                    </li>
                    <li><b>Opérateurs de construction pour les caractéristiques</b>
                        <ul>
                            <li>Les opérateurs de construction sont des fonctions ou des opérations mathématiques qui
                                permettent de créer de nouvelles caractéristiques à partir de celles existantes. </li>
                            <li>Parmi les opérateurs couramment utilisés, on trouve les opérateurs d'égalité
                                (comparaisons),
                                les opérateurs arithmétiques (addition, soustraction, multiplication, division), les
                                opérateurs de tableau (min, max, moyenne, médiane, etc.), les fonctions de
                                transformation,
                                etc.</li>
                        </ul>
                    </li>
                </ul>
                <ol style="font-size:2vh">
                    <li>https://en.wikipedia.org/wiki/Feature_vector</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">12
                    <a class="prev" href="#slide11"></a>
                    <a class="next" href="#slide13"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide13">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Exemple</h3>
                <ul>
                    <li>Soit <b>Année de naissance</b> et <b>Année de décès</b> deux caractéristiques existantes.</li>
                    <li>Une nouvelle caractéristique appelée <b>âge</b> est créée. <b>âge</b> = <b>Année de décès</b> -
                        <b>Année de naissance</b>
                    </li>
                </ul>
                <p>La construction de caractéristiques est une étape essentielle dans le pipeline de prétraitement des
                    données en apprentissage machine, car elle peut aider à rendre les données plus informatives pour
                    les
                    algorithmes d'apprentissage.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">13
                    <a class="prev" href="#slide12"></a>
                    <a class="next" href="#slide14"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide14">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                    <ul>
                        <li><b>Le nombre d'exemples d'entraînement (N)</b> : Cela représente la quantité d'exemples de
                            données que vous avez pour entraîner un modèle supervisé. Chaque exemple d'entraînement se
                            compose d'un vecteur de caractéristiques (x) et de son label (y).</li>
                        <li><b>L'espace de saisie des caractéristiques (X)</b> : C'est l'ensemble de toutes les
                            combinaisons
                            possibles de vecteurs de caractéristiques qui peuvent être utilisées comme entrée pour le
                            modèle. Cet espace est défini par les caractéristiques que vous avez extraites des données.
                        </li>
                        <li><b>L'espace des caractéristiques de sortie (Y)</b> : Il représente l'ensemble de toutes les
                            valeurs possibles que peuvent prendre les étiquettes ou les labels. </li>
                        <li><b>Exemples d'entraînement (D)</b> : C'est votre ensemble de données d'entraînement, composé
                            de
                            paires (x, y) où x est le vecteur de caractéristiques et y est le label correspondant.</li>
                    </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">14
                    <a class="prev" href="#slide13"></a>
                    <a class="next" href="#slide15"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide15">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                    <ul>
                        <li><b>Objectif de l'algorithme d'apprentissage supervisé</b> : Il s'agit de trouver une
                            fonction
                            (g) qui associe un vecteur de caractéristiques (x) à un label (y). L'ensemble des fonctions
                            possibles est appelé espace des hypothèses (G). L'objectif est de choisir la fonction (g)
                            qui
                            minimise l'erreur de prédiction sur les exemples d'entraînement et généralise bien sur de
                            nouvelles données.</li>
                        <li><b>Fonction d'évaluation (F)</b> : Elle indique l'espace des fonctions d'évaluation
                            utilisées
                            pour évaluer la performance des fonctions hypothétiques. L'objectif est de trouver la
                            fonction
                            (g) qui renvoie la fonction d'évaluation (f) la plus élevée, c'est-à-dire celle qui donne
                            les
                            prédictions les plus précises.</li>
                    </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">15
                    <a class="prev" href="#slide14"></a>
                    <a class="next" href="#slide16"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide16">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                    <p>Cette formalisation est au cœur de l'apprentissage supervisé, où l'objectif est d'apprendre à
                        partir
                        d'exemples étiquetés et de trouver une fonction qui puisse prédire de manière précise les
                        étiquettes
                        pour de nouvelles données non vues.</p>
                    <ul>
                        <li>Soit \(N\) le nombre d'exemples d'entraînement</li>
                        <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                        <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                        <li>Soit \({(x_1, y_1),...,(x_N, y_N)}\) les \(N\) exemples d'entraînement, où
                            <ul>
                                <li>\(x_i\) est le vecteur de caractéristiques de <i>i<sup>ème</sup></i> exemple
                                    d'entraînement.
                                </li>
                                <li>\(y_i\) est son label.</li>
                            </ul>
                        </li>
                    </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">16
                    <a class="prev" href="#slide15"></a>
                    <a class="next" href="#slide17"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide17">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                    <ul>
                        <li>L'objectif de l'algorithme d'apprentissage supervisé est de trouver \(g: X &#8594; Y\), où
                            <ul>
                                <li><i>g</i> est l'une des fonctions de l'ensemble des fonctions possibles <i>G</i>
                                    (espace
                                    des
                                    hypothèses)</li>
                            </ul>
                        </li>
                        <li><b>Fonction d'évaluation <i>F</i></b> indiquent l'espace des fonctions d'évaluation, où
                            <ul>
                                <li>\(f: X &#215; Y &#8594; R\) telle que <i>g</i> renvoie la fonction d'évaluation la
                                    plus
                                    élevée.</li>
                            </ul>
                        </li>
                    </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">17
                    <a class="prev" href="#slide16"></a>
                    <a class="next" href="#slide18"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide18">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation: Apprentissage non supervisé</h1>
                <ul>
                    <li><b>L'espace de saisie des caractéristiques (X)</b> : C'est l'ensemble de toutes les combinaisons
                        possibles de vecteurs de caractéristiques qui peuvent être utilisées comme entrée pour le modèle
                        en
                        apprentissage non supervisé. Cet espace est défini par les caractéristiques que vous avez
                        extraites
                        des données.</li>
                    <li><b>L'espace des caractéristiques de sortie (Y)</b> : Il représente l'ensemble des
                        caractéristiques
                        de sortie potentielles. Contrairement à l'apprentissage supervisé, en apprentissage non
                        supervisé, Y
                        ne consiste pas en des étiquettes ou des labels prédéfinis, mais plutôt en des transformations,
                        des
                        représentations, ou des caractéristiques extraites des données d'entrée.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">18
                    <a class="prev" href="#slide17"></a>
                    <a class="next" href="#slide19"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide19">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation: Apprentissage non supervisé</h1>
                <ul>
                    <li><b>Objectif de l'algorithme d'apprentissage non supervisé</b> : L'objectif est de trouver une
                        correspondance entre l'espace de saisie des caractéristiques (X) et l'espace des
                        caractéristiques de
                        sortie (Y). Cela peut impliquer diverses tâches, telles que la réduction de la dimensionnalité,
                        la
                        classification automatique de données non étiquetées, la détection d'anomalies, la segmentation,
                        ou
                        la représentation latente des données.</li>
                    <li><b>Mise en correspondance X → Y</b> : Cette mise en correspondance peut être réalisée de
                        différentes
                        manières, selon la tâche d'apprentissage non supervisé spécifique. Par exemple, dans la
                        réduction de
                        la dimensionnalité, X peut être une représentation à haute dimension des données, tandis que Y
                        représente la version réduite de ces données, souvent avec moins de dimensions.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">19
                    <a class="prev" href="#slide18"></a>
                    <a class="next" href="#slide20"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide20">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation: Apprentissage non supervisé</h1>
                <ul>
                    <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                    <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                    <li>L'objectif de l'algorithme d'apprentissage non supervisé est
                        <ul>
                            <li>trouver la mise en correspondance \(X &#8594; Y\)</li>
                        </ul>
                    </li>
                </ul>
                <p>L'apprentissage non supervisé est utilisé pour explorer et découvrir des modèles, des structures ou
                    des
                    caractéristiques inhérentes aux données, sans l'utilisation d'étiquettes ou de labels préalables. Il
                    est
                    couramment utilisé dans des domaines tels que la clustering, l'analyse de composantes principales
                    (PCA),
                    l'analyse en composantes indépendantes (ICA), et bien d'autres.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">20
                    <a class="prev" href="#slide19"></a>
                    <a class="next" href="#slide21"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide21">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation: Apprentissage semi-supervisé</h1>
                <ul>
                    <li><b>L'espace de saisie des caractéristiques (X)</b> : Il s'agit de l'ensemble de toutes les
                        combinaisons possibles de vecteurs de caractéristiques qui peuvent être utilisés comme entrée
                        pour
                        le modèle en apprentissage semi-supervisé.</li>
                    <li><b>L'espace des caractéristiques de sortie (Y)</b> : Il représente l'ensemble des
                        caractéristiques
                        de sortie potentielles, mais contrairement à l'apprentissage supervisé, il n'est pas
                        nécessairement
                        constitué d'étiquettes ou de labels prédéfinis.</li>
                    <li><b>Ensemble d'exemples d'exercices étiquetés (l)</b> : Cela correspond à un sous-ensemble
                        d'exemples
                        qui ont été annotés ou étiquetés avec des valeurs de sortie connues.</li>
                    <li><b>Ensembles des vecteurs de caractéristiques non étiquetées (u)</b> : Il s'agit des exemples
                        non
                        étiquetés, où les valeurs de sortie ne sont pas connues.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">21
                    <a class="prev" href="#slide20"></a>
                    <a class="next" href="#slide22"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide22">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation: Apprentissage semi-supervisé</h1>
                <ul>
                    <li><b>Objectif de l'algorithme d'apprentissage semi-supervisé</b> : L'objectif principal est de
                        trouver
                        des étiquettes correctes pour les exemples non étiquetés (apprentissage transductif), ainsi que
                        de
                        trouver la bonne mise en correspondance entre les caractéristiques d'entrée et les
                        caractéristiques
                        de sortie (apprentissage inductif).
                        <ul>
                            <li><b>Apprentissage transductif</b> : Il s'agit de trouver des étiquettes correctes pour
                                les
                                exemples non étiquetés. Cela revient à prédire les valeurs de sortie pour les exemples
                                non
                                étiquetés sans nécessairement chercher à généraliser à de nouvelles données.</li>
                            <li><b>Apprentissage inductif</b> : Cela concerne la recherche de la bonne mise en
                                correspondance entre les vecteurs de caractéristiques d'entrée et les caractéristiques
                                de
                                sortie. Cela peut inclure la généralisation à de nouvelles données en utilisant le
                                modèle
                                appris.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">22
                    <a class="prev" href="#slide21"></a>
                    <a class="next" href="#slide23"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide23">
            <div class="header">
                <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Formalisation: Apprentissage semi-supervisé</h1>
                <ul>
                    <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                    <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                    <li>Soit \({(x_1, y_1),...,(x_l, y_l)}\) l'ensemble d'exemples d'exercices étiquetés</li>
                    <li>Soit \({x_{l+1},...,x_{l+u}}\) sont les \(u\) ensembles des vecteurs de caractéristiques non
                        étiquetées de \(X\).</li>
                    <li>L'objectif de l'algorithme d'apprentissage semi-supervisé est de faire
                        <ul>
                            <li><b>l'apprentissage transductif</b>, c'est-à-dire trouver des étiquettes correctes pour
                                \({x_{l+1},...,x_{l+u}}\).</li>
                            <li><b>l'apprentissage inductif</b>, c'est-à-dire trouver la bonne mise en correspondance
                                \(X
                                &#8594; Y\)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">23
                    <a class="prev" href="#slide22"></a>
                    <a class="next" href="#slide24"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide24">
            <div class="header">
                <h1>2.2. Data Mining</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Activités</h1>
                <ol>
                    <li>Classification</li>
                    <li>Partitionnement de données (Clustering)</li>
                    <li>Régression</li>
                    <li>Étiquetage des séquences</li>
                    <li>Règles d'association</li>
                    <li>Détection d'anomalies</li>
                    <li>Récapitulation</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">24
                    <a class="prev" href="#slide23"></a>
                    <a class="next" href="#slide25"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide25">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">2.1.1 Introduction</h1>
                <ul>
                    <li><b>Catégorisation algorithmique d'objets</b> : Processus d'attribution de classes ou de
                        catégories à
                        des objets via des algorithmes. L'objectif est d'organiser les données en groupes distincts pour
                        faciliter l'analyse et la prise de décision.</li>
                    <li><b>Attribution de classes</b> :Attribuer une classe ou catégorie à chaque objet (ou individu).
                    </li>
                    <li><b>Types de classification :</b>
                        <ul>
                            <li><b>Classification binaire</b> : Assignation à deux classes.</li>
                            <li><b>Classification en classes multiples</b> : Assignation à plusieurs classes
                                simultanément.
                            </li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">25
                    <a class="prev" href="#slide24"></a>
                    <a class="next" href="#slide26"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide26">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Filtrage de contenu (e.g., spam/pourriel)</b> : Identifier et filtrer les emails non désirés
                        ou
                        indésirables.
                        Exemple : Filtrage des spams dans les boîtes de réception.</li>
                    <li><b>Classification de documents</b> : Organiser et catégoriser les documents en fonction de leur
                        contenu.
                        Exemple : Classification automatique des articles de presse par sujet.</li>
                    <li><b>Reconnaissance de l'écriture manuscrite</b> : Interprétation automatique des caractères
                        écrits à
                        la main.
                        Exemple : Reconnaissance des chiffres sur les chèques bancaires.</li>
                    <li><b>Reconnaissance automatique de la parole</b> : Convertir la parole en texte écrit de manière
                        automatique.
                        Exemple : Commandes vocales pour les assistants virtuels comme Siri ou Alexa.</li>
                    <li><b>Moteurs de recherche</b> : Classer et organiser les résultats de recherche en fonction de
                        leur
                        pertinence.
                        Exemple : Classement des pages web dans les résultats de recherche de moteurs de recherche.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">26
                    <a class="prev" href="#slide25"></a>
                    <a class="next" href="#slide27"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide27">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Classification: Définition formelle</h1>
                <ul>
                    <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                    <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                    <li>L'objectif de l'algorithme de classification (ou classificateur) est de trouver \({(x_1,
                        y_1),...,(x_l, y_k)}\), c'est-à-dire l'attribution d'une étiquette connue à chaque vecteur de
                        caractéristique d'entrée, où
                        <ul>
                            <li>\(x_i &#8712; X \)</li>
                            <li>\(y_i &#8712; Y \)</li>
                            <li>\(|X| = l \)</li>
                            <li>\(|Y| = k \)</li>
                            <li>\(l &gt;= k\)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">27
                    <a class="prev" href="#slide26"></a>
                    <a class="next" href="#slide28"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide28">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Classificateurs</h3>
                <ul>
                    <li>Algorithme de classification</li>
                    <li>Deux types de classificateurs:
                        <ul>
                            <li><b>Classificateurs binaires</b> attribue un objet à l'une des deux classes</li>
                            <li><b>Classificateurs multiclasses</b> attribue un objet à une ou plusieurs classes</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">28
                    <a class="prev" href="#slide27"></a>
                    <a class="next" href="#slide29"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide29">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Classification binaire</h2>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/binaryclassifier.svg"
                        height="400px" />
                    <figcaption>Classification binaire</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">29
                    <a class="prev" href="#slide28"></a>
                    <a class="next" href="#slide30"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide30">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Linear Classificateurs</h3>
                <ul>
                    <li>Fonction linéaire attribuant un score à chaque catégorie possible en combinant le vecteur de
                        caractéristiques d'une instance avec un vecteur de poids, en utilisant un produit de points.
                    </li>
                    <li>Formalisation :
                        <ul>
                            <li>Soit <i><b>X</b></i> être l'espace de saisie des caractéristiques et
                                <i><b>x</b><sub>i</sub>
                                    &#8712; <b>X</b></i>
                            </li>
                            <li>Soit <i><b>&#946;</b><sub>k</sub></i> un vecteur de poids pour la catégorie <i>k</i>
                            </li>
                            <li><i>score(<b>x</b><sub>i</sub>, k) = <b>x</b><sub>i</sub>.<b>&#946;</b><sub>k</sub></i>,
                                score pour l'attribution de la catégorie <i>k</i> à l'instance
                                <i><b>x</b><sub>i</sub></i>.
                                La catégorie qui donne le score le plus élevé est
                                attribuée à la catégorie de l'instance.
                            </li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">30
                    <a class="prev" href="#slide29"></a>
                    <a class="next" href="#slide31"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide31">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation</h3>
                <p>Dans le contexte de la classification en apprentissage machine, l'évaluation des performances d'un
                    modèle
                    implique la compréhension de différents types de prédictions qu'il peut faire par rapport à la
                    réalité.
                    Les vrais positifs (VP) et les vrais négatifs (VN) sont deux de ces éléments.</p>
                <ul>
                    <li><b>Vrais Positifs (VP/TP)</b> : Les vrais positifs représentent les cas où le modèle prédit
                        correctement la classe positive. En d'autres termes, il a correctement identifié les exemples
                        qui
                        appartiennent réellement à la classe que le modèle essaie de prédire.</li>
                    <li><b>Vrais Négatifs (VN/FN)</b> : Les vrais négatifs représentent les cas où le modèle prédit
                        correctement la classe négative. Cela signifie qu'il a correctement identifié les exemples qui
                        n'appartiennent pas à la classe que le modèle essaie de prédire.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">31
                    <a class="prev" href="#slide30"></a>
                    <a class="next" href="#slide32"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide32">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation</h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2018/DataMining/positivenegative.svg" width="400vw" />
                    <figcaption>Les vrais positifs et les vrais négatifs</figcaption>
                </figure>
                <figure>
                    <img src="../../../../../en/teaching/courses/2018/DataMining/Precisionrecall.svg" width="400vw" />
                    <figcaption>Précision et rappel</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">32
                    <a class="prev" href="#slide31"></a>
                    <a class="next" href="#slide33"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide33">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation</h3>
                <p>Soit</p>
                <ul>
                    <li><i>tp</i>: nombre de vrais postifs</li>
                    <li><i>fp</i>: nombre de faux positifs</li>
                    <li><i>fn</i>: nombre de faux négatifs</li>
                </ul>
                <figure class="gridcontent">
                    <img src="../../../../../en/teaching/courses/2018/DataMining/Precisionrecall.svg" height="400px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">33
                    <a class="prev" href="#slide32"></a>
                    <a class="next" href="#slide34"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide34">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation</h3>
                <p>La <b>précision</b> mesure la proportion de prédictions positives faites par le modèle qui étaient
                    <b>effectivement correctes</b>, tandis que le <b>rappel</b> mesure la proportion d'exemples positifs
                    réels qui ont été correctement identifiés par le modèle. Alors
                </p>
                <ul>
                    <li>Précision \[p = \frac{tp}{(tp + fp)}\]</li>
                    <li>Rappel (Recall) \[r = \frac{tp}{(tp + fn)}\]</i>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">34
                    <a class="prev" href="#slide33"></a>
                    <a class="next" href="#slide35"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide35">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation</h3>
                <p>Le F1-score est la moyenne harmonique de la précision et du rappel. Il fournit une mesure globale de
                    la
                    performance d'un modèle de classification, tenant compte à la fois de la précision et du rappel. Il
                    est
                    particulièrement utile lorsque les classes sont déséquilibrées.</p>
                <ul>
                    <li>F1-score \[f1 = 2 * \frac{(p * r)}{(p + r)}\]</li>
                    <li>F1-score: meilleure valeur à 1 (précision et rappel parfaits) et pire à 0.</li>
                </ul>
                <p>Le F1-score tient compte à la fois des <b>erreurs de type I (faux positifs)</b> et des <b>erreurs de
                        type
                        II (faux négatifs)</b>, fournissant ainsi une mesure équilibrée de la performance du modèle.</p>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">35
                    <a class="prev" href="#slide34"></a>
                    <a class="next" href="#slide36"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide36">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation</h3>
                <ul>
                    <li>\(F_\beta\)-score utilise un facteur réel positif β, où β est choisi de telle sorte que le
                        rappel
                        est considéré comme β fois plus important que la précision, est : </li>
                    <li>\(F_\beta\)-score \[F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{p} \cdot \mathrm{r}}{(\beta^2
                        \cdot
                        \mathrm{p}) + \mathrm{r}}\]</li>
                    <li>Exemple: <b>\(F_2\) score</b>: Cette métrique est souvent utilisée dans des situations où le
                        rappel
                        est jugé plus critique que la précision, par exemple, dans des tâches où la détection des
                        exemples
                        positifs est particulièrement importante, même si cela entraîne un nombre plus élevé de faux
                        positifs.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">36
                    <a class="prev" href="#slide35"></a>
                    <a class="next" href="#slide37"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide37">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <p>Le \(F_2\)-score est souvent utilisé dans des domaines où le rappel est considéré comme plus critique
                    que
                    la précision. </p>
                <ul>
                    <li><b>Détection de Maladies</b> : Dans le domaine médical, en particulier pour la détection de
                        maladies
                        graves, le F2-score peut être utilisé pour évaluer la performance des modèles. Il est crucial
                        d'identifier correctement autant de cas positifs que possible, même si cela conduit à quelques
                        faux
                        positifs.</li>
                    <li><b>Sécurité et Détection d'Intrusion</b> : Lors de la détection d'intrusions dans les systèmes
                        informatiques, il est souvent plus important de minimiser les faux négatifs (intrusions
                        manquées) au
                        profit de quelques faux positifs, d'où l'utilisation du F2-score.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">37
                    <a class="prev" href="#slide36"></a>
                    <a class="next" href="#slide38"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide38">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Recherche Biomédicale</b> : Dans des domaines de recherche biomédicale où la découverte de
                        certaines caractéristiques ou protéines spécifiques est critique, le F2-score peut être
                        privilégié
                        pour s'assurer que ces éléments sont correctement identifiés.</li>
                    <li><b>Prévision de Catastrophes Naturelles</b> : Lors de la prévision de catastrophes naturelles
                        comme
                        les tremblements de terre ou les tsunamis, il est essentiel de minimiser les faux négatifs pour
                        garantir que le maximum d'avertissements est donné, même au prix de quelques alertes erronées.
                    </li>
                    <li><b>Recherche en Astronomie</b> : Dans la recherche astronomique, la découverte de nouveaux
                        objets
                        célestes ou de phénomènes rares peut être cruciale. Le F2-score peut être utilisé pour évaluer
                        les
                        performances des algorithmes de détection.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">38
                    <a class="prev" href="#slide37"></a>
                    <a class="next" href="#slide39"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide39">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Évaluation: matrice de confusion</h2>
                <p>La matrice de confusion est un outil essentiel dans l'évaluation des performances d'un système de
                    classification. Elle fournit une vue détaillée des prédictions faites par le modèle par rapport aux
                    classes réelles.</p>
                <ul>
                    <li>Chaque ligne de la matrice représente les instances d'une classe prédite.</li>
                    <li>Chaque colonne représente les instances d'une classe réelle.</li>
                    <li>Toutes les prédictions correctes sont situées dans la diagonale du tableau.</li>
                    <li>Les erreurs de prédiction sont représentées par des valeurs situées en dehors de la diagonale
                        principale.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">39
                    <a class="prev" href="#slide38"></a>
                    <a class="next" href="#slide40"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide40">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation: matrice de confusion</h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/DataMining/confusionmatrix.png" height="400px" />
                    <figcaption>Matrice de confusion pour un classificateur SVM pour les chiffres manuscrits (MNIST)
                    </figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">40
                    <a class="prev" href="#slide39"></a>
                    <a class="next" href="#slide41"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide41">
            <div class="header">
                <h1>2.2. Méthodes de classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Évaluation: matrice de confusion</h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/DataMining/confusionmatrix1.png" height="400px" />
                    <figcaption>Matrice de confusion pour un perceptron pour les chiffres manuscrits (MNIST)
                    </figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Apprentissage machine | John Samuel</div>
                <div class="navigation">41
                    <a class="prev" href="#slide40"></a>
                    <a class="next" href="#slide42"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide42">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Classification binaire</h2>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/binaryclassifier.svg"
                        height="400px" />
                    <figcaption>Classification binaire</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">42
                    <a class="prev" href="#slide41"></a>
                    <a class="next" href="#slide43"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide43">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Classification multiclasse</h2>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/multiclassclassifier.svg"
                        height="400px" />
                    <figcaption>Classification multiclasse</figcaption>
                </figure>
            </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">43
                    <a class="prev" href="#slide42"></a>
                    <a class="next" href="#slide44"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide44">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Classification multiclasse [Aly 2005]</h3>
                <ul>
                    <li><b>Transformation en classification binaire</b> :
                        <ul>
                            <li><b>L'approche un contre le reste (Un contre tous)</b> : Chaque classe est traitée comme
                                une
                                classe positive et toutes les autres comme une classe négative.</li>
                            <li><b>L'approche un-contre-un</b> : Un classifieur binaire est construit pour chaque paire
                                de
                                classes.</li>
                        </ul>
                    </li>
                    <li><b>Extension de la classification binaire</b> :
                        <ul>
                            <li><b>Réseaux de neurones</b> : Adaptation des architectures pour prédire plusieurs classes
                                simultanément.</li>
                            <li><b>k-voisins les plus proches</b> : Extension de l'algorithme pour gérer plusieurs
                                classes.
                            </li>
                        </ul>
                    </li>
                    <li><b>Classification hiérarchique.</b> : Organisation des classes dans une structure arborescente
                        pour
                        une classification plus fine et précise.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">44
                    <a class="prev" href="#slide43"></a>
                    <a class="next" href="#slide45"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide45">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">One-vs.-rest (One-vs.-all) strategy</h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsall.svg" height="400px" />
                    <figcaption>La strategie un-contre le rest pour la classification multiclasse</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">45
                    <a class="prev" href="#slide44"></a>
                    <a class="next" href="#slide46"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide46">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">One-vs.-rest (One-vs.-all) strategy</h3>
                <ul>
                    <li>Entraîner un seul classificateur par classe, avec les échantillons de cette classe comme
                        échantillons positifs et tous les autres comme négatifs. </li>
                    <li>Chaque classificateur produit un score de confiance réel pour sa décision</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsall.svg" height="300px" />
                    <figcaption>La strategie un-contre le rest pour la classification multiclasse</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">46
                    <a class="prev" href="#slide45"></a>
                    <a class="next" href="#slide47"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide47">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">One-vs.-rest or One-vs.-all (OvR, OvA) strategy</h3>
                <ul>
                    <li>Entrées :
                        <ul>
                            <li>\(L\), un apprenant (algorithme d'entraînement pour les classificateurs binaires)</li>
                            <li>échantillons \(X\)</li>
                            <li>étiquettes \(y\), où \(y_i ∈ \{1,..,K \} \) est l'étiquette de l'échantillon \(X_i\)
                        </ul>
                    </li>
                    <li>Sortie :
                        <ul>
                            <li>une liste de classificateurs \(f_k\), où \(k ∈ \{1,..,K \} \)
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">47
                    <a class="prev" href="#slide46"></a>
                    <a class="next" href="#slide48"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide48">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">One-vs.-rest or One-vs.-all (OvR, OvA) strategy</h3>
                <p>Prendre des décisions signifie appliquer tous les classificateurs à un échantillon invisible x et
                    prédire
                    l'étiquette k pour laquelle le classificateur correspondant rapporte le score de confiance le plus
                    élevé
                    : \[\hat{y} = \underset{k \in
                    \{1 \ldots K\}}{\arg\!\max}\; f_k(x)\]</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">48
                    <a class="prev" href="#slide47"></a>
                    <a class="next" href="#slide49"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide49">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">One-vs.-one strategy</h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsone.svg" height="400px" />
                    <figcaption>La strategie un-contre-un pour la classification multiclasse</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">49
                    <a class="prev" href="#slide48"></a>
                    <a class="next" href="#slide50"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide50">
            <div class="header">
                <h1>2.2.1. Classification</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">One-vs.-one strategy</h3>
                <li>nécessite l'entraînement des \(\frac{K (K - 1)}{2}\) classificateurs binaires</li>
                <li>chaque classificateur reçoit les échantillons d'une paire de classes du jeu de formation original,
                    et
                    doit apprendre à distinguer ces deux classes.</li>
                <li>Au moment de la prédiction, un système de vote est appliqué : tous les \(\frac{K (K - 1)}{2}\)
                    classificateurs sont appliqués à un échantillon non vu et la classe qui a obtenu le plus grand
                    nombre de
                    prédictions est prédite par le classificateur
                    combiné.
                </li>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsone.svg" height="200px" />
                    <figcaption>La strategie un-contre-un pour la classification multiclasse</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">50
                    <a class="prev" href="#slide49"></a>
                    <a class="next" href="#slide51"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide51">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h1>2.2.2.1. Introduction</h1>
                <ul>
                    <li>Partitionnement de données est le processus de division d'un ensemble de données en différents
                        sous-ensembles homogènes ou groupes.</li>
                    <li><b>Objectif</b> : Regrouper les données partageant des caractéristiques similaires dans chaque
                        sous-ensemble.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cluster-2.svg.png"
                        height="300px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">51
                    <a class="prev" href="#slide50"></a>
                    <a class="next" href="#slide52"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide52">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Analyse des réseaux sociaux</b> : Identifier des communautés ou des groupes d'individus ayant
                        des
                        liens ou des intérêts similaires.
                        Exemple : Regrouper des utilisateurs de réseaux sociaux en fonction de leurs interactions ou de
                        leurs centres d'intérêt communs.</li>
                    <li><b>Segmentation d'image</b> : Diviser une image en régions homogènes selon des critères
                        prédéfinis.
                        Exemple : Identifier automatiquement les objets ou les régions d'intérêt dans une photographie.
                    </li>
                    <li><b>Systèmes de recommandation</b> : Regrouper les utilisateurs ou les produits en fonction de
                        leurs
                        caractéristiques ou de leurs préférences.
                        Exemple : Suggérer des produits ou des contenus similaires à des utilisateurs en se basant sur
                        leurs
                        historiques d'achats ou de navigation.</li>
                </ul>
                <figure class="gridconten">
                    <img src="../../2017/CN/Social_Network_Analysis_Visualization.png" height="100px" width="200px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">52
                    <a class="prev" href="#slide51"></a>
                    <a class="next" href="#slide53"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide53">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Définition formelle</h3>
                <ul>
                    <li>Soit \(X\) être l'espace de saisie des caractéristiques</li>
                    <li>L'objectif du regroupement est de trouver \(k\) des sous-ensembles de \(X\), de façon à ce que
                    </li>
                    <p>\[ C_1.. &#8746; ..C_k &#8746; C_{outliers} = X \] </i> et</p>
                    <p> \[ C_i &#8745; C_j = &#981;, i &#8800; j; 1 &lt;i,j &lt;k \]</i>
                    </p>
                    <p>\(C_{outliers}\) peut consister en des cas extrêmes (anomalie de données)</p>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">53
                    <a class="prev" href="#slide52"></a>
                    <a class="next" href="#slide54"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide54">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Modèles de regroupement</h3>
                <ul>
                    <li><b>Modèles de centroïdes</b> : Les groupes sont représentés par un seul vecteur moyen
                        (centroïde).
                        Exemple : K-Means, K-Médian.</li>
                    <li><b>Modèles de connectivité</b> : Les regroupements sont déterminés par la proximité de la
                        connectivité entre les points.
                        Exemple : Agglomératif Hiérarchique.</li>
                    <li><b>Modèles de distribution</b> : Les regroupements sont modélisés à l'aide de distributions
                        statistiques.
                        Exemple : Mélanges de Gaussiennes.</li>
                    <li><b>Modèles de densité</b> : Les regroupements sont définis par des régions denses connectées
                        dans
                        l'espace de données.
                        Exemple : DBSCAN, OPTICS.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">54
                    <a class="prev" href="#slide53"></a>
                    <a class="next" href="#slide55"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide55">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Modèles de regroupement</h3>
                <ul>
                    <li><b>Modèles de sous-espace</b> : Identifient des regroupements dans des sous-espaces spécifiques
                        des
                        données.
                        Exemple : CLIQUE, Subspace Clustering.</li>
                    <li><b>Modèles de groupes</b> : Organisent les données en groupes selon des critères spécifiques.
                        Exemple : K-Modes pour les données catégoriques.</li>
                    <li><b>Modèles graphiques</b> : Utilisent des structures de graphes pour représenter les relations
                        entre
                        les données.
                        Exemple : Algorithme de Marche Aléatoire pour la découverte de communautés.</li>
                    <li><b>Modèles neuronaux</b> : Utilisent des réseaux de neurones pour apprendre et découvrir des
                        structures dans les données.
                        Exemple : Autoencodeurs pour la réduction de dimensionnalité non linéaire.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">55
                    <a class="prev" href="#slide54"></a>
                    <a class="next" href="#slide56"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide56">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Modèles de regroupement</h3>
                <figure class="fullwidth">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                        height="400px" width="400px" />
                    <figcaption>k-means regroupement (voir section 3.3)</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">56
                    <a class="prev" href="#slide55"></a>
                    <a class="next" href="#slide57"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide57">
            <div class="header">
                <h1>2.2.2. Partitionnement de données</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Modèles de regroupement</h3>
                <figure class="fullwidth">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Iris_dendrogram.png"
                        height="300px" width="400px" />
                    <figcaption>Dendrogramme de regroupement hiérarchique de l'ensemble de données Iris</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">57
                    <a class="prev" href="#slide56"></a>
                    <a class="next" href="#slide58"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide58">
            <div class="header">
                <h1>2.2.3 Régression</h1>
            </div>
            <div class="content">
                <h1>2.2.3 Régression</h1>
                <ul>
                    <li>Processus visant à trouver une fonction mathématique qui modélise les relations entre les
                        variables.
                        L'objectif est d'estimer les relations et prédire les valeurs d'une variable en fonction
                        d'autres
                        variables.</li>
                    <li><b>Fonction de modélisation</b> : Trouver une fonction qui représente au mieux les données
                        observées
                        avec l'objectif de prédire ou estimer les valeurs d'une variable cible en fonction des variables
                        explicatives.</li>
                    <li><b>Analyse des relations</b> : Examiner la relation entre une variable cible et une ou plusieurs
                        variables explicatives. Méthodes : Identifier les tendances, les corrélations et les dépendances
                        entre les variables.</li>
                    <li><b>Attribution de valeurs</b> : Assigner des valeurs réelles à chaque entrée pour modéliser les
                        phénomènes du monde réel.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">58
                    <a class="prev" href="#slide57"></a>
                    <a class="next" href="#slide59"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide59">
            <div class="header">
                <h1>2.2.3 Régression</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Applications</h1>
                <ul>
                    <li><b>Prévisions météorologiques</b> : Prédire les conditions météorologiques futures en fonction
                        des
                        données historiques et des variables environnementales.
                        Exemple : Estimation de la température, des précipitations et des vents pour les prochains
                        jours.
                    </li>
                    <li><b>Prévisions de ventes</b> : Estimer les ventes futures en fonction des tendances passées, des
                        saisons et des stratégies marketing.
                        Exemple : Prédiction des ventes de produits pour une période donnée.</li>
                    <li><b>Apprentissage machine</b> : Utiliser la régression comme composante d'algorithmes
                        d'apprentissage
                        machine pour la prédiction et la classification.
                        Exemple : Modèles de régression linéaire dans les méthodes d'apprentissage supervisé.</li>
                    <li><b>Finance</b> : Évaluer les performances financières, prédire les prix des actions et des
                        actifs,
                        et estimer les risques.
                        Exemple : Modèles de régression pour prédire les rendements financiers ou évaluer les risques de
                        crédit.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Linear_regression.svg"
                        height="100px" width="200px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">59
                    <a class="prev" href="#slide58"></a>
                    <a class="next" href="#slide60"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide60">
            <div class="header">
                <h1>2.2.3 Régression</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Définition formelle</h3>
                <ul>
                    <li>La régression est représentée par une fonction qui associe un élément de données à une variable
                        de
                        prédiction.</li>
                    <li>Elle peut être exprimée en termes de variables indépendantes \(X\), de variables dépendantes
                        \(Y\)
                        et de paramètres inconnus \(β\).
                    <li>Le modèle de régression vise à approximer la relation entre \(X\) et \(Y\) avec une fonction
                        \(f(X,
                        β)\), où \(β\) représente les paramètres du modèle.</li>
                    <li>L'objectif est d'obtenir une approximation \(Y ≈ f(X, β)\) qui minimise l'écart entre les
                        valeurs
                        prédites et les valeurs observées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">60
                    <a class="prev" href="#slide59"></a>
                    <a class="next" href="#slide61"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide61">
            <div class="header">
                <h1>2.2.3 Régression</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Régression linéaire</h3>
                <p>La régression linéaire est un modèle mathématique qui représente une relation linéaire entre une
                    variable
                    indépendante \(x_i\) et une variable dépendante \(y_i\). Le modèle a une forme d'une ligne droite
                    (pour
                    la régression linéaire simple) ou d'une parabole (pour la régression linéaire multiple).</p>
                <ul>
                    <li>ligne droite: \(y_i = &#946;_0 + &#946;_1x_i + &#949;_i\) où \(β_0\) et \(β_1\) sont les
                        coefficients de régression, \(x_i\) est la variable indépendante, et \(ε_i\) est l'erreur
                        résiduelle.</li>
                    <li>parabole: \(y_i = &#946;_0 + &#946;_1x_i + &#946;_2x_i^2 +&#949;_i\) où \(β_0\), \(β_1\), et
                        \(β_2\)
                        sont les coefficients de régression pour chaque terme, \(x_i\) est la variable indépendante, et
                        \(ε_i\) est l'erreur résiduelle.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">61
                    <a class="prev" href="#slide60"></a>
                    <a class="next" href="#slide62"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide62">
            <div class="header">
                <h1>2.2.3 Régression</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Régression linéaire</h3>
                <p><b>Ligne droite</b>: \(y_i = &#946;_0 + &#946;_1x_i + &#949;_i\) où \(β_0\) et \(β_1\) sont les
                    coefficients de régression, \(x_i\) est la variable indépendante, et \(ε_i\) est l'erreur
                    résiduelle.
                </p>
                <p>Pour minimiser l'erreur :</<p>
                <ul>
                    <li>Calcul des prédictions : \( ŷ_i = &#946;_0 + &#946;_{1}x_i \)</li>
                    <li>Calcul des résidus: \(e_i = ŷ_i - y_i\)</li>
                    <li>Calcul de la somme des carrés des résidus (SSE) pour évaluer l'ajustement du modèle., \(SSE =
                        &#931;
                        e_i\), where \(1 &lt; i &lt; n\)</li>
                </ul>
                <p>L'objectif est de minimiser SSE pour obtenir la meilleure approximation de la relation linéaire entre
                    les
                    variables.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">62
                    <a class="prev" href="#slide61"></a>
                    <a class="next" href="#slide63"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide63">
            <div class="header">
                <h1>2.2.4. Étiquetage des séquences</h1>
            </div>
            <div class="content">
                <p>Processus consistant à attribuer une classe ou une étiquette à chaque élément d'une séquence de
                    valeurs
                    ou de tokens.
                    Exemple : Reconnaissance d'entités nommées (NER) avec spaCy, où des entités comme les noms de
                    personnes,
                    les lieux, ou les organisations sont identifiées et étiquetées dans un texte.</p>
                <h1 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h1>
                <figure style="margin-bottom: 6rem">
                    <div class="entities" style="line-height: 2.5; direction: ltr">
                        <mark class="entity"
                            style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            Paris
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                        </mark> is the capital of
                        <mark class="entity"
                            style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            France
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                        </mark> . In
                        <mark class="entity"
                            style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            2015
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                        </mark> , its population was recorded as
                        <mark class="entity"
                            style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            2,206,488
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                        </mark>
                    </div>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">63
                    <a class="prev" href="#slide62"></a>
                    <a class="next" href="#slide64"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide64">
            <div class="header">
                <h1>2.2.4. Étiquetage des séquences</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Reconnaissance d'entités nommées (spaCy)</h1>
                <figure style="margin-bottom: 6rem">
                    <div class="entities" style="line-height: 2.5; direction: ltr">
                        <mark class="entity"
                            style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            Paris
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                        </mark> is the capital of
                        <mark class="entity"
                            style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            France
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                        </mark> . In
                        <mark class="entity"
                            style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            2015
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                        </mark> , its population was recorded as
                        <mark class="entity"
                            style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            2,206,488
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                        </mark>
                    </div>
                </figure>
                <table>
                    <tr>
                        <th>Balise</th>
                        <th>Signification</th>
                    </tr>
                    <tr>
                        <td>GPE</td>
                        <td>Pays, villes, états.</td>
                    </tr>
                    <tr>
                        <td>DATE</td>
                        <td>Dates ou périodes absolues ou relatives</td>
                    </tr>
                    <tr>
                        <td>CARDINAL</td>
                        <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                    </tr>
                </table>
            </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">64
                    <a class="prev" href="#slide63"></a>
                    <a class="next" href="#slide65"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide65">
            <div class="header">
                <h1>2.2.4. Étiquetage des séquences</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Etiquetage de la partie du discours</b> : Assigner des étiquettes grammaticales à chaque mot
                        d'une phrase pour analyser sa structure syntaxique.</li>
                    <li><b>Traduction linguistique</b> : Identifier et étiqueter les mots ou phrases dans une langue
                        source
                        avant leur traduction dans une langue cible.</li>
                    <li><b>Analyse vidéo</b> : Marquer et catégoriser les actions ou objets identifiés dans une séquence
                        vidéo.</li>
                    <li><b>Reconnaissance de l'écriture manuscrite</b> : Attribuer des étiquettes à chaque caractère ou
                        symbole écrit à la main pour la reconnaissance de texte.</li>
                    <li><b>Extraction d'informations</b> : Identifier et étiqueter des éléments d'intérêt dans un
                        document
                        ou un ensemble de données pour extraire des informations pertinentes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">65
                    <a class="prev" href="#slide64"></a>
                    <a class="next" href="#slide66"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide66">
            <div class="header">
                <h1>2.2.4. Étiquetage des séquences</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Définition formelle</h3>
                <ul>
                    <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                    <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                    <li>Soit \(&#12296;x_1,...,x_T&#12297;\) une séquence de longueur \(T\).</li>
                    <li>L'objectif de l'étiquetage des séquences est de générer une séquence correspondante
                        <ul>
                            <li>\(&#12296;y_1,...,y_T&#12297;\) des étiquettes</li>
                            <li>\(x_i &#8712; X\) </li>
                            <li>\(y_j &#8712; Y\)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">66
                    <a class="prev" href="#slide65"></a>
                    <a class="next" href="#slide67"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide67">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Association Rules</h3>
                <p>Les règles d'association, également connues sous le nom de "Association Rules", sont un ensemble de
                    techniques d'analyse de données visant à découvrir les <b>relations et les associations entre les
                        variables</b> dans un ensemble de données. Cette méthode recherche des corrélations et des
                    co-occurrences entre les éléments, permettant ainsi de dégager des motifs ou des modèles
                    significatifs.
                </p>
                <p>Un exemple courant d'application des règles d'association est l'analyse de paniers d'achats dans le
                    domaine du commerce de détail, où ces règles sont utilisées pour identifier des schémas d'achat,
                    tels
                    que les <b>combinaisons de produits souvent achetés ensemble</b>. </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">67
                    <a class="prev" href="#slide66"></a>
                    <a class="next" href="#slide68"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide68">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Association Rules</h3>
                <p>Prenons un exemple concret avec un tableau de données représentant les transactions d'une épicerie :
                </p>
                <table>
                    <thead>
                        <tr>
                            <th>Transaction</th>
                            <th>Produits achetés</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1</td>
                            <td>Pain, Lait, Œufs</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>Pain, Beurre</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>Lait, Œufs, Fromage</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>Pain, Lait, Œufs, Bière</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td>Lait, Bière, Chips</td>
                        </tr>
                    </tbody>
                </table>

            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">68
                    <a class="prev" href="#slide67"></a>
                    <a class="next" href="#slide69"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide69">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Association Rules</h3>
                <p>Dans ce tableau, chaque colonne représente un produit et chaque ligne représente une transaction. Un
                    "1"
                    indique que le produit a été acheté lors de cette transaction, tandis qu'un "0" indique que le
                    produit
                    n'a pas été acheté.</p>
                <table>
                    <thead>
                        <tr>
                            <th>Transaction</th>
                            <th>Pain</th>
                            <th>Lait</th>
                            <th>Œufs</th>
                            <th>Beurre</th>
                            <th>Fromage</th>
                            <th>Bière</th>
                            <th>Chips</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>1</td>
                            <td>1</td>
                            <td>1</td>
                            <td>1</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>1</td>
                            <td>0</td>
                            <td>0</td>
                            <td>1</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>0</td>
                            <td>1</td>
                            <td>1</td>
                            <td>0</td>
                            <td>1</td>
                            <td>0</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>1</td>
                            <td>1</td>
                            <td>1</td>
                            <td>0</td>
                            <td>0</td>
                            <td>1</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td>0</td>
                            <td>1</td>
                            <td>0</td>
                            <td>0</td>
                            <td>0</td>
                            <td>1</td>
                            <td>1</td>
                        </tr>
                    </tbody>
                </table>

            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">69
                    <a class="prev" href="#slide68"></a>
                    <a class="next" href="#slide70"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide70">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Exploitation de l'utilisation du web</b> : Utilisées pour analyser les comportements des
                        utilisateurs en ligne, personnaliser les recommandations de produits et cibler les publicités.
                    </li>
                    <li><b>Détection d'intrusion</b> : Utilisées en sécurité informatique pour repérer les comportements
                        malveillants et détecter les tentatives d'intrusion.</li>
                    <li><b>Analyse d'affinité</b> : Utilisées dans le marketing pour identifier les relations entre les
                        produits souvent achetés ensemble, permettant ainsi de recommander des produits complémentaires
                        et
                        de créer des offres groupées attractives.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">70
                    <a class="prev" href="#slide69"></a>
                    <a class="next" href="#slide71"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide71">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Définition formelle</h3>
                <ul>
                    <li>Soit \(I\) un ensemble de \(n\) attributs binaires appelés items</li>
                    <li>Soit \(T\) un ensemble de \(m\) transactions appelé base de données</li>
                    <li>Soit \(I\) = \(\{(i_1,...,i_n)\}\) et \(T\) = \({(t_1,...,t_m)}\)</li>
                    <li>L'objectif de l'apprentissage des règles d'association est de trouver
                        <ul>
                            <li>\(X &#8658; Y\), where \(X &#8658; Y &#8838; I\)</li>
                            <li>\(X\) est l'antécédent</li>
                            <li>\(Y\) est la conséquence</li>
                        </ul>
                    </li>
                </ul>
                <p>Une règle d'association \(X &#8658; Y\) est valide si le support et la confiance de la règle
                    dépassent
                    les seuils spécifiés. Cela signifie que \(X\) et \(Y\) apparaissent fréquemment ensemble dans les
                    transactions, et que lorsque \(X\) est présent, \(Y\) est également souvent présent.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">71
                    <a class="prev" href="#slide70"></a>
                    <a class="next" href="#slide72"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide72">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Support</h3>
                <p>Le support d'un ensemble d'articles dans le contexte des règles d'association est défini comme <b>la
                        fréquence à laquelle cet ensemble d'articles apparaît dans la base de données</b>. En d'autres
                    termes, c'est le nombre de transactions dans lesquelles cet ensemble d'articles est présent, divisé
                    par
                    le nombre total de transactions dans la base de données. Le support mesure donc la popularité ou la
                    prévalence d'un ensemble d'articles. Il est utilisé pour évaluer à quel point une association entre
                    deux
                    ensembles d'articles est forte. Une valeur élevée de support indique que l'association est fréquente
                    dans la base de données, ce qui la rend potentiellement plus significative.</p>
                <p>\[supp(X) =
                    \frac{|t &#8712;T; X &#8838; t|}{ |T|}\]
                </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">72
                    <a class="prev" href="#slide71"></a>
                    <a class="next" href="#slide73"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide73">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Confidence</h3>
                <p>La confidence dans le contexte des règles d'association représente <b>la fréquence à laquelle la
                        règle a
                        été trouvée vraie dans la base de données</b>. Plus précisément, elle mesure la probabilité
                    conditionnelle que la conséquence Y se produise dans une transaction, étant donné que l'antécédent X
                    est
                    également présent dans cette transaction. </p>
                <p>La confidence d'une règle est calculée en divisant le nombre de transactions dans lesquelles à la
                    fois X
                    et Y sont présents par le nombre de transactions dans lesquelles X est présent. Ainsi, une
                    confidence
                    élevée indique que la conséquence Y est souvent vraie lorsque l'antécédent X est présent, ce qui
                    renforce la fiabilité de la règle d'association.</p>
                <p>\[conf(X &#8658; Y) = \frac{supp(X &#8746; Y)}{supp(X)}\]
                </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">73
                    <a class="prev" href="#slide72"></a>
                    <a class="next" href="#slide74"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide74">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <p>En utilisant les règles d'association, nous pouvons extraire des relations significatives entre les
                    produits. Par exemple, en appliquant un support minimum de 40% et un seuil de confiance de 60%, nous
                    pouvons identifier les règles d'association suivantes :</p>
                <ol>
                    <li>{Pain, Lait} \( &#8658;\) {Œufs} (Support : 40%, Confiance : 100%)</li>
                    <li>{Lait} \( &#8658;\) {Œufs} (Support : 60%, Confiance : 75%)</li>
                    <li>{Œufs} \( &#8658;\) {Lait} (Support : 60%, Confiance : 75%)</li>
                </ol>

                <p>Cela signifie que dans 40% des transactions, les clients ont acheté du pain et du lait ensemble, et
                    dans
                    100% de ces transactions, ils ont également acheté des œufs. De même, dans 60% des transactions, les
                    clients ont acheté du lait, et dans 75% de ces transactions, ils ont également acheté des œufs.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">74
                    <a class="prev" href="#slide73"></a>
                    <a class="next" href="#slide75"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide75">
            <div class="header">
                <h1>2.2.5. Règles d'association</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Lift</h3>
                <p>Le lift dans le contexte des règles d'association est <b>le rapport entre le "support" observé de la
                        règle et celui attendu si les ensembles d'items X et Y étaient indépendants</b>.Formellement, le
                    lift est calculé en divisant le support de la règle par le produit des supports individuels de X et
                    Y.
                    En d'autres termes, c'est la mesure de combien plus fréquemment la règle est observée que ce à quoi
                    on
                    s'attendrait si les événements X et Y étaient indépendants.</p>
                <p>Un lift supérieur à 1 indique que la règle a une association positive entre X et Y (c'est-à-dire que
                    les
                    items X et Y apparaissent ensemble plus fréquemment que prévu au hasard), tandis qu'un lift
                    inférieur à
                    1 indique une association négative ou non significative. Un lift de 1 indique une indépendance entre
                    les
                    items X et Y.</p>
                <ul>
                    <li>\[lift(X &#8658; Y) = \frac{supp(X &#8746; Y)}{(supp(X) &#10761; supp(Y))}\]
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">75
                    <a class="prev" href="#slide74"></a>
                    <a class="next" href="#slide76"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide76">
            <div class="header">
                <h1>2.2.6. Détection d'anomalies</h1>
            </div>
            <div class="content">
                <p>La détection d'anomalies, également connue sous le nom de détection des valeurs aberrantes, implique
                    l'identification de données inhabituelles ou divergentes dans un ensemble de données. Voici quelques
                    approches courantes pour détecter les anomalies :</p>
                <ul>
                    <li><b>Détection supervisé</b> : le modèle est entraîné sur un ensemble de données étiqueté avec des
                        exemples d'anomalies et de données normales. Le modèle est ensuite utilisé pour prédire si de
                        nouvelles données sont anormales ou normales en fonction de ces étiquettes.</li>
                    <li><b>Détection non-supervisé</b> : Contrairement à la détection supervisée, cette approche
                        n'utilise
                        pas d'étiquettes dans l'ensemble de données d'entraînement. Au lieu de cela, elle identifie les
                        anomalies en examinant les caractéristiques statistiques des données et en recherchant des
                        points de
                        données qui diffèrent significativement du reste de l'ensemble de données.</li>
                    <li><b>Détection semi-supervisé</b> : Cette approche combine des éléments des deux méthodes
                        précédentes.
                        Elle utilise à la fois des données étiquetées et non étiquetées pour entraîner le modèle. Cela
                        peut
                        être utile lorsque seules quelques anomalies sont disponibles pour l'entraînement, mais que
                        l'ensemble de données est principalement non étiqueté.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">76
                    <a class="prev" href="#slide75"></a>
                    <a class="next" href="#slide77"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide77">
            <div class="header">
                <h1>2.2.6. Détection d'anomalies</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Applications</h1>
                <ul>
                    <li><b>Détection d'intrusion</b> : Identifier les activités malveillantes ou non autorisées dans les
                        réseaux informatiques pour protéger les systèmes contre les cyberattaques.</li>
                    <li><b>Détection de fraude</b> : Repérer les transactions financières suspectes ou les activités
                        frauduleuses dans les transactions en ligne, les cartes de crédit, ou les assurances.</li>
                    <li><b>System health monitoring</b> : Surveiller en continu la santé des systèmes informatiques, des
                        machines industrielles ou des équipements médicaux pour détecter les pannes ou les défaillances
                        potentielles.</li>
                    <li><b>Détection d'événements dans les réseaux de capteurs</b> : Identifier les événements
                        inhabituels
                        ou les comportements anormaux dans les réseaux de capteurs environnementaux, tels que la
                        surveillance de la qualité de l'air ou la détection des intrusions dans les systèmes de
                        sécurité.
                    </li>
                    <li><b>Détection d'abus dans un système d'information</b> : Identifier les utilisateurs ou les
                        activités
                        qui abusent ou enfreignent les politiques de sécurité dans les systèmes d'information, les
                        applications en ligne ou les plateformes de réseaux sociaux.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">77
                    <a class="prev" href="#slide76"></a>
                    <a class="next" href="#slide78"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide78">
            <div class="header">
                <h1>2.2.6. Détection d'anomalies</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Caractéristiques</h3>
                <p><b>Des sursauts inattendus</b> : Les anomalies peuvent se manifester sous forme de sursauts ou de
                    pointes
                    inattendues dans les données. Par exemple, une augmentation soudaine du trafic sur un site Web peut
                    indiquer une attaque de déni de service (DDoS) dans le cas de la surveillance du trafic réseau, ou
                    une
                    augmentation anormale des transactions financières peut signaler une fraude.</p>
                <p>Les caractéristiques des données varient selon le domaine d'application et les types spécifiques
                    d'anomalies recherchées. Identifiez les schémas inhabituels ou les comportements aberrants dans les
                    données peut aider à détecter les anomalies et à prendre des mesures appropriées pour les gérer.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">78
                    <a class="prev" href="#slide77"></a>
                    <a class="next" href="#slide79"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide79">
            <div class="header">
                <h1>2.2.6. Détection d'anomalies</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation</h3>
                <ul>
                    <li>Soit \(Y\) un ensemble de mesures. Cela représente les données ou les variables observées qui
                        sont
                        surveillées pour détecter les anomalies.</li>
                    <li>Soit \(P_Y(y)\) un modèle statistique pour la distribution des \(Y\) dans des conditions
                        "normales". Les données normales sont généralement modélisées par une distribution statistique
                        telle
                        que la distribution normale (gaussienne). Ce modèle est utilisé pour estimer la probabilité que
                        les
                        données observées soient normales.</li>
                    <li>Soit \(T\) un seuil défini par l'utilisateur. C'est une valeur seuil fixée par l'utilisateur qui
                        détermine à partir de quelle probabilité une mesure est considérée comme anormale. Les mesures
                        dont
                        la probabilité estimée est inférieure à ce seuil sont considérées comme des anomalies.</li>
                    <li>Une mesure \(x\) est une valeur isolée si \(P_Y(x) &lt; T\). Cette condition spécifie que si la
                        probabilité d'une mesure est inférieure au seuil défini, cette mesure est considérée comme
                        isolée ou
                        anormale par rapport aux autres observations.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">79
                    <a class="prev" href="#slide78"></a>
                    <a class="next" href="#slide80"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide80">
            <div class="header">
                <h1>2.2.7. Récapitulation</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading"></h1>
                <ul>
                    <li><b>Synthèse courte d'un ensemble de données</b> : Elle consiste à résumer de manière concise les
                        principales caractéristiques et tendances des données. Cela peut inclure des statistiques
                        descriptives telles que la moyenne, la médiane, l'écart-type, ainsi que des visualisations
                        récapitulatives comme des histogrammes, des graphiques linéaires ou des diagrammes à barres.
                        L'objectif est de fournir une vue d'ensemble rapide et informative des données.</li>
                    <li><b>Génération de rapports</b> : la génération de rapports peut être utilisée pour communiquer
                        efficacement les informations clés aux parties prenantes. Ces rapports peuvent prendre
                        différentes
                        formes, telles que des documents écrits, des présentations visuelles ou des tableaux de bord
                        interactifs. L'accent est mis sur la clarté, la concision et la pertinence des informations
                        présentées pour aider les décideurs à prendre des décisions éclairées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">80
                    <a class="prev" href="#slide79"></a>
                    <a class="next" href="#slide81"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide81">
            <div class="header">
                <h1>2.2.7. Récapitulation</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Applications</h1>
                <ul>
                    <li><b>Extraction des mots-clès</b> : Identifier les mots ou expressions clés dans un texte ou un
                        ensemble de documents pour résumer leur contenu de manière succincte.</li>
                    <li><b>Récapitulation de documents</b> : Résumer le contenu et les principales idées d'un document
                        ou
                        d'un ensemble de documents pour en faciliter la compréhension et l'assimilation.</li>
                    <li><b>Moteurs de recherche</b> : Fournir des résumés pertinents des pages Web ou des résultats de
                        recherche afin d'aider les utilisateurs à trouver rapidement les informations qu'ils
                        recherchent.
                    </li>
                    <li><b>Récapitulation d'images</b> : Extraire les caractéristiques importantes d'une image pour
                        résumer
                        son contenu ou en faciliter la recherche et la classification.</li>
                    <li><b>Récapitulation de vidéos</b> : Identifier et résumer les événements principaux ou les moments
                        saillants dans une vidéo pour permettre aux utilisateurs de naviguer rapidement dans le contenu
                        et
                        de trouver des informations spécifiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">81
                    <a class="prev" href="#slide80"></a>
                    <a class="next" href="#slide82"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide82">
            <div class="header">
                <h1>2.2.7. Récapitulation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation: Synthèse multi-documents</h3>
                <ul>
                    <li>Soit \(\{D = D_1, ..., D_k\}\) une collection de \(k\) documents </li>
                    <li>Un document \(\{D = t_1, ..., t_m\}\) se compose de m unités textuelles (mots, phrases,
                        paragraphes,
                        etc.) </li>
                    <li>Soit \(\{D = t_1, ..., t_n\}\) être l'ensemble complet de toutes les unités textuelles de tous
                        les
                        documents, où
                        <ul>
                            <li>\(t_i &#8712; D\), si et seulement si \(&#8707; D_j\) de sorte que \(t_i &#8712; D_j\)
                            </li>
                        </ul>
                    </li>
                    <li>\(S &#8838; D\) constitue une synthèse</li>
                    <li> Deux fonctions de scoring
                        <ul>
                            <li>\(Rel(i)\): pertinence de l'unité textuelle \(i\) dans le résumé</li>
                            <li>\(Red(i,j)\): Redondance entre deux unités textuelles \(t_i, t_j\)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">82
                    <a class="prev" href="#slide81"></a>
                    <a class="next" href="#slide83"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide83">
            <div class="header">
                <h1>2.2.7. Récapitulation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Formalisation: Multidocument summarization</h3>
                <ul>
                    <li>La note pour un résumé \(S\)
                        <ul>
                            <li>\(s(S)\) note pour un résumé S</li>
                            <li>\(l(i)\) est la longueur de l'unité textuelle \(i\)</li>
                            <li>\(K\) est la longueur maximale fixée du résumé</li>
                        </ul>
                    </li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2018/DataMining/scoringfunction.png" height="200px"
                        width="500px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">83
                    <a class="prev" href="#slide82"></a>
                    <a class="next" href="#slide84"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide84">
            <div class="header">
                <h1>2.2.7. Récapitulation</h1>
            </div>
            <div class="content">
                <p>Trouver un sous-ensemble à partir de l'ensemble du sous-ensemble.</p>
                <ol>
                    <li><b>Extraction</b>: Cette approche implique la sélection d'un sous-ensemble de mots, de
                        phrases ou d'expressions existants dans le texte original sans aucune modification.
                        L'objectif est de repérer les parties les plus importantes du texte et de les présenter de
                        manière concise dans le résumé. Les techniques utilisées dans cette approche incluent
                        l'identification de phrases clés, la classification des phrases par importance, et
                        l'extraction de phrases représentatives.</li>
                    <li><b>Abstraction</b>: Contrairement à l'extraction, l'approche d'abstraction implique la
                        construction d'une représentation sémantique interne du texte, suivie de l'utilisation de
                        techniques de génération du langage naturel pour produire un résumé. Cela nécessite une
                        compréhension plus profonde du contenu du texte et la capacité de reformuler les idées de
                        manière concise tout en préservant leur signification. Les techniques d'abstraction peuvent
                        inclure la réécriture de phrases, la fusion d'informations similaires et la génération de
                        paraphrases.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">84
                    <a class="prev" href="#slide83"></a>
                    <a class="next" href="#slide85"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide85">
            <div class="header">
                <h1>2.2.7. Récapitulation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Résumé extractif</h3>
                <ol>
                    <li><b>Résumé générique</b>: Cette approche vise à obtenir un résumé général et représentatif du
                        contenu original en extrayant les informations les plus importantes et les plus pertinentes.
                        Elle cherche à capturer l'essence du texte original en identifiant les phrases ou les
                        sections clés qui révèlent les principaux points et concepts abordés. Ce type de résumé est
                        souvent utilisé dans des contextes où une vue d'ensemble est nécessaire sans se concentrer
                        sur des aspects spécifiques ou des détails.</li>
                    <li><b>Résumé pertinent pour la recherche</b> : Cette approche vise à produire un résumé qui
                        répond spécifiquement aux besoins ou aux intérêts d'un utilisateur ou d'une tâche de
                        recherche particulière. Elle utilise des techniques de sélection de phrases basées sur la
                        pertinence pour extraire les parties du texte qui correspondent aux critères de recherche
                        spécifiques de l'utilisateur. Cela permet de fournir des résumés plus ciblés et adaptés aux
                        besoins individuels, ce qui peut être particulièrement utile dans les domaines où la
                        précision et la pertinence sont essentielles, comme la recherche d'informations spécialisées
                        ou la prise de décision.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">85
                    <a class="prev" href="#slide84"></a>
                    <a class="next" href="#slide86"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide86">
            <div class="header">
                <h1>2.3. Algorithmes</h1>
            </div>
            <div class="content">
                <h1></h1>
                <ol>
                    <li>Support Vector Machines (SVM)</li>
                    <li>Descente du gradient stochastique</li>
                    <li>Voisins proches</li>
                    <li>Bayes naïfs</li>
                    <li>Arbres de décision</li>
                    <li>Ensemble Methods (Forêt d'arbres décisionnels)</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">86
                    <a class="prev" href="#slide85"></a>
                    <a class="next" href="#slide87"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide87">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Introduction</h3>
                <p>La machine à vecteurs de support (SVM) est une méthode d'apprentissage supervisé. SVM cherche à
                    trouver
                    la meilleure frontière de décision qui optimise la séparation des classes, ce qui permet une
                    classification précise même dans des espaces de données complexes.</p>
                <ul>
                    <li>Elle est principalement utilisée pour la classification binaire, bien qu'elle puisse être
                        étendue à
                        des problèmes de classification multiclasse.</li>
                    <li>L'objectif principal de SVM est de construire un hyperplan qui maximise la marge de séparation
                        entre
                        les deux classes. L'hyperplan est la frontière de décision qui sépare les données en deux
                        classes
                        distinctes.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/SVM Separating Hyperplanes.svg"
                        height="100px" width="350px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">87
                    <a class="prev" href="#slide86"></a>
                    <a class="next" href="#slide88"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide88">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Hyperplane</h3>
                <p>L'hyperplan dans l'espace n-dimensionnel est un sous-espace de dimension n-1 qui permet de séparer
                    les
                    données en deux classes.</p>
                <ul>
                    <li>Dans un espace à deux dimensions, l'hyperplan est une ligne à une dimension qui sépare les
                        données
                        en deux régions.</li>
                    <li>Dans un espace tridimensionnel, l'hyperplan est un plan bidimensionnel qui divise l'espace en
                        deux
                        parties distinctes. </li>
                    <li>L'hyperplan d'un espace tridimensionnel est un plan bidimensionnel</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/SVM Separating Hyperplanes.svg"
                        height="200px" width="350px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">88
                    <a class="prev" href="#slide87"></a>
                    <a class="next" href="#slide89"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide89">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Définition formelle</h3>
                <ul style="width:50%">
                    <li>Le but d'un SVM est d'estimer une fonction \(f: R^N &#10761; {+1,-1}\), c'est à dire,
                        <ul>
                            <li>Si \(x_1,...,x_l\) &#8712; \(R^N\) sont les \(N\) points de données d'entrée,</li>
                            <li>L'objectif est de trouver \((x_1,y_1),...,(x_l,y_l)\) &#8712; \(R^N &#10761; {+1,-1}\)
                            </li>
                        </ul>
                    </li>
                    <li>Tout hyperplan peut être écrit par l'équation en utilisant un ensemble de points d'entrée \(x\)
                        <ul>
                            <li>\(w.x - b = 0\), où</li>
                            <li>\(w &#8712; R^N\), un vecteur normal à la plane </li>
                            <li>\(b &#8712; R\)</li>
                        </ul>
                    </li>
                    <li> Une fonction de décision est donnée par \(f(x) = sign(w.x - b )\)
                    </li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Surface_normal_illustration.svg"
                        height="300px" />
                    <figcaption>Normal vector</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">89
                    <a class="prev" href="#slide88"></a>
                    <a class="next" href="#slide90"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide90">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Définition formelle</h1>
                <ul style="width:40%">
                    <li>Si les données de formation sont séparables linéairement, deux hyperplans peuvent être
                        sélectionnés
                    </li>
                    <li>Ils séparent les deux classes de données, <br> afin que la distance entre elles soit la plus
                        grande
                        possible.</li>
                    <li>Les hyperplans peuvent être donnés par les équations
                        <ul>
                            <li>\(w.x - b = 1\)</li>
                            <li>\(w.x - b = -1\)</li>
                        </ul>
                    </li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png"
                        height="500px" width="500px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">90
                    <a class="prev" href="#slide89"></a>
                    <a class="next" href="#slide91"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide91">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Définition formelle</h1>
                <ul style="width:40%">
                    <li>La distance entre les deux hyperplans peut être donnée par \( \frac{2}{||w||} \)</i>
                    </li>
                    <li>La région située entre ces deux hyperplans est appelée marge.</li>
                    <li>L'hyperplan à marge maximale est l'hyperplan <br> qui se trouve à mi-chemin entre eux.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png"
                        height="500px" width="500px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">91
                    <a class="prev" href="#slide90"></a>
                    <a class="next" href="#slide92"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide92">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Définition formelle</h1>
                <ul>
                    <li>Afin d'éviter que les points de données ne tombent dans la marge, les contraintes suivantes sont
                        ajoutées
                        <ul>
                            <li>\(w.x_i - b &gt;= 1\), si \(y_i = 1\)</li>
                            <li>\(w.x_i - b &lt;= -1\), si \(y_i = -1\)</li>
                        </ul>
                    <li>\(y_i(w.x_i - b) &gt;= 1\), \(1&lt;= i &lt;= n\)</li>
                    </li>
                    <li>L'objectif est de minimiser ||w|| sous réserve de \(y_i(w.x_i - b) &gt;= 1\), \(1&lt;= i &lt;=
                        n\)
                    </li>
                    <li>Une solution pour les deux \(w\) et \(b\) donne le classificateur \(f(x) = sign(w.x - b)\)</li>
                    <li>L'hyperplan à marge maximale est entièrement déterminé par les points qui en sont les plus
                        proches,
                        appelés vecteurs de soutien</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">92
                    <a class="prev" href="#slide91"></a>
                    <a class="next" href="#slide93"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide93">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Data mining</h3>
                <ul>
                    <li><b>Classification</b> : SVM peut être utilisée pour la classification binaire ainsi que pour la
                        classification multi-classes, où elle cherche à séparer les données en plusieurs catégories
                        distinctes en construisant des hyperplans dans un espace multidimensionnel.</li>
                    <li><b>Régression</b> : SVM peut également être appliquée à des problèmes de régression, où elle
                        cherche
                        à prédire une valeur continue plutôt que de classer des données en catégories discrètes.</li>
                    <li><b>Détection des anomalies</b> : SVM peut être utilisée pour détecter les anomalies dans les
                        données
                        en identifiant les points de données qui sont significativement différents du reste de
                        l'ensemble de
                        données, ce qui en fait un outil précieux pour la détection des fraudes ou des erreurs dans les
                        données.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">93
                    <a class="prev" href="#slide92"></a>
                    <a class="next" href="#slide94"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide94">
            <div class="header">
                <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Catégorisation des textes et des hypertextes</b> : Les SVM sont largement utilisées pour
                        classer
                        automatiquement les documents texte dans différentes catégories, comme la classification des
                        courriels en spam ou en non-spam, la catégorisation des articles de presse, etc.</li>
                    <li><b>Classification des images</b> : SVM est efficace pour classer des images dans des catégories
                        prédéfinies, comme la classification des images médicales en différentes maladies, la
                        reconnaissance
                        des visages, la détection d'objets dans des images, etc.</li>
                    <li><b>Reconnaissance de l'écriture manuscrite</b> : SVM est également utilisée dans les systèmes de
                        reconnaissance de l'écriture manuscrite pour identifier les caractères ou les mots écrits à la
                        main
                        et les transcrire en texte numérique.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">94
                    <a class="prev" href="#slide93"></a>
                    <a class="next" href="#slide95"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide95">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <p>Le gradient stochastique de descente est une technique d'optimisation utilisée pour minimiser une
                    fonction objective qui peut être exprimée comme une somme de fonctions différenciables. </p>
                <ul>
                    <li>Il s'agit d'une approximation stochastique de l'optimisation de la descente du gradient, où le
                        calcul du gradient est effectué de manière aléatoire sur un sous-ensemble des données à chaque
                        itération.</li>
                    <li>Cette méthode est itérative, ce qui signifie qu'elle effectue des mises à jour progressives des
                        paramètres du modèle pour se rapprocher du minimum ou du maximum de la fonction objectif.</li>
                    <li>Le gradient stochastique de descente est particulièrement efficace pour traiter de grands
                        ensembles
                        de données, car il permet de calculer les mises à jour des paramètres de manière incrémentielle,
                        ce
                        qui réduit la charge de calcul par rapport à l'optimisation classique du gradient.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">95
                    <a class="prev" href="#slide94"></a>
                    <a class="next" href="#slide96"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide96">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Gradient</h1>
                <p>Le gradient est une généralisation multi-variable de la notion de dérivée. </p>
                <ul style="width:40%">
                    <li>Le gradient donne la pente de la tangente du graphe d'une fonction à un point donné dans
                        l'espace
                        multi-dimensionnel. Il indique dans quelle direction et dans quelle mesure la fonction augmente
                        ou
                        diminue le plus rapidement à partir de ce point.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="200px"
                        width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">96
                    <a class="prev" href="#slide95"></a>
                    <a class="next" href="#slide97"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide97">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Gradient</h1>
                <ul style="width:40%">
                    <li>Dans le cas des fonctions à plusieurs variables, le gradient est un vecteur qui contient les
                        dérivées partielles de la fonction par rapport à chacune de ses variables. Chaque composante du
                        gradient correspond à la pente de la fonction dans la direction respective de la variable
                        correspondante.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="200px"
                        width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">97
                    <a class="prev" href="#slide96"></a>
                    <a class="next" href="#slide98"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide98">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Gradient</h1>
                <ul style="width:40%">
                    <li>Géométriquement, le gradient pointe dans la direction du plus grand taux d'augmentation de la
                        fonction. En d'autres termes, il indique la direction dans laquelle la fonction croît le plus
                        rapidement à partir du point considéré.</li>
                    <li>L'amplitude du gradient représente la pente du graphique de la fonction dans la direction
                        indiquée
                        par le gradient. Plus cette amplitude est grande, plus la fonction augmente rapidement dans
                        cette
                        direction.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="300px"
                        width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">98
                    <a class="prev" href="#slide97"></a>
                    <a class="next" href="#slide99"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide99">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Gradient ou dérivé</h3>
                <table>
                    <thead>
                        <tr>
                            <th></th>
                            <th>Dérivé</th>
                            <th>Gradient</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Définition</td>
                            <td>Taux de variation instantanée d'une fonction</td>
                            <td>Vecteur des dérivées partielles d'une fonction de plusieurs variables</td>
                        </tr>
                        <tr>
                            <td>Nombre de variables</td>
                            <td>Une seule</td>
                            <td>Plusieurs</td>
                        </tr>
                        <tr>
                            <td>Nature</td>
                            <td>Fonction scalaire</td>
                            <td>Fonction vectorielle</td>
                        </tr>
                        <tr>
                            <td>Représentation</td>
                            <td>Un seul nombre réel</td>
                            <td>Un vecteur de nombres réels</td>
                        </tr>
                        <tr>
                            <td>Utilisation</td>
                            <td>Fonctions d'une seule variable</td>
                            <td>Fonctions de plusieurs variables, notamment en optimisation et en machine learning</td>
                        </tr>
                        <tr>
                            <td>Géométrie</td>
                            <td>Pente de la tangente à un point d'une courbe</td>
                            <td>Direction et taux de variation le plus rapide d'une fonction dans un espace
                                multi-dimensionnel</td>
                        </tr>
                    </tbody>
                </table>

            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">99
                    <a class="prev" href="#slide98"></a>
                    <a class="next" href="#slide100"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide100">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Algorithme du gradient</h3>
                <p>L'algorithme du gradient stochastique de descente est un algorithme d'optimisation itératif largement
                    utilisé pour trouver le minimum d'une fonction.</p>
                <ol>
                    <li><b>Initialisation</b> : Choisissez un point de départ aléatoire ou prédéfini dans l'espace des
                        paramètres.</li>
                    <li><b>Calcul du gradient</b> : Calculez le gradient de la fonction objective par rapport aux
                        paramètres
                        au point courant.</li>
                    </ul>
                    <figure>
                        <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg"
                            height="150px" />
                    </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">100
                    <a class="prev" href="#slide99"></a>
                    <a class="next" href="#slide101"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide101">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Algorithme du gradient</h3>
                <ol start="3">
                    <li><b>Mise à jour des paramètres</b> : Mettez à jour les paramètres dans la direction opposée au
                        gradient. Cela implique de soustraire une fraction du gradient de chaque paramètre.</li>
                    <li><b>Répétition</b> : Répétez les étapes 2 et 3 jusqu'à ce qu'un critère d'arrêt soit satisfait,
                        par
                        exemple, un nombre fixe d'itérations, une petite variation de la fonction objective ou une
                        tolérance
                        pour le gradient.</li>
                    </ul>
                    <figure>
                        <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg"
                            height="200px" width="300px" />
                    </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">101
                    <a class="prev" href="#slide100"></a>
                    <a class="next" href="#slide102"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide102">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Algorithme du gradient</h3>
                <p> L'algorithme stochastique du gradient de descente est une variante où le gradient est calculé de
                    manière
                    stochastique, c'est-à-dire qu'au lieu d'utiliser l'ensemble complet des données pour calculer le
                    gradient à chaque itération, un sous-ensemble aléatoire ou une seule observation est utilisé. Cela
                    permet de gagner en efficacité, en particulier pour les grands ensembles de données.</p>
                <p>L'objectif principal de cet algorithme est de minimiser une fonction objective, souvent une fonction
                    de
                    perte dans le cadre de l'apprentissage automatique, et il est largement utilisé dans des domaines
                    tels
                    que l'optimisation convexe, l'apprentissage automatique et le traitement du signal.</p>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg"
                        height="300px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">102
                    <a class="prev" href="#slide101"></a>
                    <a class="next" href="#slide103"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide103">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Méthode standard de descente de gradient</h3>
                <ul>
                    <li>Prenons le problème de la minimisation d'une fonction objective
                        <ul>
                            <li>\(Q(w) = \frac{1}{n} (&#931;Q_i(w)), 1&lt;=i&lt;n\)</li>
                            <li>\(Q_i(w)\) est la valeur de la fonction objectif pour le \(i\)-ème exemple.</li>
                            <li>\(Q(w)\) est le risque empirique.</li>
                        </ul>
                    </li>
                    <li>\(w = w - &#951;.&#8711; Q(w)\)</li>
                    <li>\(w = w - \frac{\eta}{n} \sum_{i=1}^n \nabla Q_i(w)\), \(\eta\) est le pas de l'itération </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">103
                    <a class="prev" href="#slide102"></a>
                    <a class="next" href="#slide104"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide104">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Méthode itérative</h3>
                <ul>
                    <li>Choisissez un vecteur initial de paramètres \(w\) et le taux d'apprentissage &#951;.</li>
                    <li>Répétez l'opération jusqu'à l'obtention d'un minimum approximatif :
                        <ul>
                            <li>Mélangez aléatoirement les exemples dans le jeu de formation.</li>
                            <li>\(w = w - &#951;.&#8711; Q_i(w)\), \(i=1...n\)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">104
                    <a class="prev" href="#slide103"></a>
                    <a class="next" href="#slide105"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide105">
            <div class="header">
                <h1>2.3.2. Gradient stochastique de descente</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Classification</b> : le SGD est souvent utilisé pour entraîner des modèles de classification
                        tels
                        que les machines à vecteurs de support (SVM), les réseaux de neurones et les modèles de
                        régression
                        logistique. Il permet de trouver les paramètres optimaux du modèle en minimisant une fonction de
                        perte, ce qui conduit à une meilleure capacité de classification.</li>
                    <li><b>Régression</b> : Le SGD est également utilisé pour l'entraînement de modèles de régression,
                        où
                        l'objectif est de prédire une valeur continue en fonction d'un ensemble de variables
                        explicatives.
                        Il est couramment utilisé dans des domaines tels que l'analyse financière, la prédiction des
                        prix et
                        la modélisation des séries chronologiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">105
                    <a class="prev" href="#slide104"></a>
                    <a class="next" href="#slide106"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide106">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <p> La méthode des k plus proches voisins (kNN) et le partitionnement en k-moyennes (k-means clustering)
                    sont deux techniques importantes en apprentissage automatique et en exploration de données :</p>
                <ol>
                    <li><b>Méthode des k plus proches voisins (kNN)</b> : C'est un algorithme d'apprentissage supervisé
                        utilisé pour la classification et la régression. L'idée principale derrière kNN est de trouver
                        les k
                        échantillons d'entraînement les plus proches du point de données de test et de prédire
                        l'étiquette
                        de classe en fonction de la classe majoritaire parmi ces voisins. Pour la régression, la
                        prédiction
                        est la moyenne des valeurs cibles des k voisins les plus proches.</li>
                    <li><b>Partitionnement en k-moyennes (k-means clustering)</b> : C'est une méthode non supervisée de
                        partitionnement de données en k groupes distincts. L'algorithme fonctionne en répétant deux
                        étapes :
                        d'abord, il attribue chaque point de données au groupe dont le centroïde est le plus proche,
                        puis il
                        met à jour les centroïdes en calculant la moyenne de tous les points attribués à chaque groupe.
                        Ces
                        étapes sont répétées jusqu'à ce qu'une convergence soit atteinte et que les centroïdes ne
                        changent
                        plus de manière significative.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">106
                    <a class="prev" href="#slide105"></a>
                    <a class="next" href="#slide107"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide107">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <figure class="flexcontent">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/KnnClassification.svg"
                        height="350px" width="350px" />
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                        height="350px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">107
                    <a class="prev" href="#slide106"></a>
                    <a class="next" href="#slide108"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide108">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <ul>
                    <li><b>Méthode de partitionnement de données :</b> Le partitionnement en k-moyennes vise à diviser
                        un
                        ensemble de données en k groupes (clusters) distincts. Chaque cluster est représenté par son
                        centroïde, qui est la moyenne de tous les points appartenant à ce cluster.</li>
                    <li><b>Entrée :</b> L'entrée de l'algorithme est un ensemble de points de données ainsi que le
                        nombre k
                        de clusters souhaité. Ces points de données peuvent avoir plusieurs dimensions.</li>
                    <li><b>Objectif :</b> L'objectif principal du k-means clustering est de minimiser la variance
                        intra-cluster, c'est-à-dire de minimiser la somme des distances au carré de chaque point par
                        rapport
                        à son centroïde assigné.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                        height="100px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">108
                    <a class="prev" href="#slide107"></a>
                    <a class="next" href="#slide109"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide109">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <ul>
                    <li><b>Fonctionnement :</b> L'algorithme k-means fonctionne en itérant entre deux étapes principales
                        :
                        <ul>
                            <li><b>Attribution des points aux clusters</b> : Chaque point de données est assigné au
                                cluster
                                dont le centroïde est le plus proche en termes de distance euclidienne.</li>
                            <li><b>Mise à jour des centroïdes</b> : Une fois que tous les points ont été attribués à des
                                clusters, les centroïdes de chaque cluster sont mis à jour en calculant la moyenne des
                                points appartenant à ce cluster.</li>
                        </ul>
                    </li>
                    <li><b>Convergence :</b> Les deux étapes ci-dessus sont répétées de manière itérative jusqu'à ce
                        qu'une
                        convergence soit atteinte, c'est-à-dire que les centroïdes ne changent plus significativement
                        entre
                        deux itérations successives.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                        height="200px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">109
                    <a class="prev" href="#slide108"></a>
                    <a class="next" href="#slide110"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide110">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 1 (Initialisation)</h2>
                <p> Les "moyens", également appelés centroïdes, sont les points initiaux autour desquels les clusters
                    seront
                    formés. Dans cette étape, k points sont sélectionnés de manière aléatoire à partir de l'ensemble de
                    données pour servir de moyens initiaux.</p>
                <figure>
                    <img src="../../2021/DataMining/K_Means_Example_Step_1.svg" height="200px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">110
                    <a class="prev" href="#slide109"></a>
                    <a class="next" href="#slide111"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide111">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 2 (Étape d'affectation)</h2>
                <p>Dans la deuxième étape de l'algorithme de partitionnement en k-moyennes (k-means clustering),
                    également
                    connue sous le nom d'étape d'affectation, les k clusters sont créés en associant chaque observation
                    à la
                    moyenne la plus proche.Les partitions
                    représentent ici le diagramme de Voronoï généré par les moyennes.</p>
                <figure>
                    <img src="../../2021/DataMining/K_Means_Example_Step_2.svg" height="200px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">111
                    <a class="prev" href="#slide110"></a>
                    <a class="next" href="#slide112"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide112">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 2 (Étape d'affectation)</h2>
                <li><b>Calcul des distances :</b> Pour chaque observation de l'ensemble de données, la distance jusqu'à
                    chaque moyen est calculée. La distance la plus couramment utilisée est la distance euclidienne, mais
                    d'autres mesures de distance peuvent également être utilisées en fonction des besoins spécifiques de
                    l'application.</li>
                <li><b>Association des observations aux clusters :</b> Une fois les distances calculées, chaque
                    observation
                    est associée au cluster dont le moyen est le plus proche. Cela crée k partitions dans l'ensemble de
                    données, où chaque partition contient les observations associées à un cluster spécifique.</li>
                <li><b>Diagramme de Voronoï :</b> Les partitions formées dans cette étape peuvent être visualisées comme
                    un
                    diagramme de Voronoï dans l'espace des données. Chaque cluster est représenté par une région de
                    l'espace
                    des données où les points sont plus proches de son moyen que de tout autre moyen.</li>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">112
                    <a class="prev" href="#slide111"></a>
                    <a class="next" href="#slide113"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide113">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 3 (Étape de mise à jour et calcul du centroïde)</h2>
                <p>Les centroids de chacun des k agrégats sont recalculés pour devenir les nouvelles moyennes.</p>
                </ul>

                <figure>
                    <img src="../../2021/DataMining/K_Means_Example_Step_3.svg" height="200px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">113
                    <a class="prev" href="#slide112"></a>
                    <a class="next" href="#slide114"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide114">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 3 (Étape de mise à jour et calcul du centroïde)</h2>
                <ul>
                    <li><b>Calcul du centroïde</b> : Pour chaque cluster formé à l'étape précédente, le centroïde est
                        calculé. Le centroïde est simplement la moyenne de toutes les observations qui appartiennent à
                        ce
                        cluster. Il est calculé en prenant la moyenne des coordonnées de toutes les observations dans le
                        cluster, ce qui donne une position centrale représentative.</li>
                    <li><b>Mise à jour des moyennes</b> : Une fois que les centroïdes de tous les clusters ont été
                        calculés,
                        ils deviennent les nouvelles moyennes pour la prochaine itération de l'algorithme. Les anciennes
                        moyennes sont alors remplacées par les nouveaux centroïdes.</li>
                </ul>

                <figure>
                    <img src="../../2021/DataMining/K_Means_Example_Step_3.svg" height="100px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">114
                    <a class="prev" href="#slide113"></a>
                    <a class="next" href="#slide115"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide115">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 4 (Répéter jusqu'à la convergence)</h2>
                <p>La quatrième étape de l'algorithme de partitionnement en k-moyennes (k-means clustering) consiste à
                    répéter les étapes 2 et 3 jusqu'à ce que la convergence soit atteinte. </p>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                        height="200px" width="300px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">115
                    <a class="prev" href="#slide114"></a>
                    <a class="next" href="#slide116"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide116">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
                <h2 class="topicsubheading">Étape 4 (Répéter jusqu'à la convergence)</h2>
                <ul>
                    <li><b>Répétition des étapes précédentes</b> : Les étapes d'affectation (étape 2) et de mise à jour
                        des
                        moyennes (étape 3) sont répétées itérativement jusqu'à ce que la convergence soit atteinte. Cela
                        signifie que les observations sont successivement associées aux clusters en fonction de leur
                        proximité aux moyennes actuelles, puis les moyennes sont mises à jour en fonction des
                        observations
                        assignées à chaque cluster.</li>
                    <li><b>Critère de convergence</b> : L'algorithme a convergé lorsque les affectations ne changent
                        plus
                        entre les itérations successives. Cela signifie que les clusters ne subissent plus de
                        changements
                        significatifs et que les moyennes ne se déplacent plus de manière significative entre les
                        itérations.</li>
                    <li><b>Arrêt de l'algorithme</b> : Une fois que la convergence est atteinte, l'algorithme s'arrête
                        et
                        les clusters finaux ainsi que leurs centroïdes associés sont considérés comme la solution de
                        l'algorithme.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">116
                    <a class="prev" href="#slide115"></a>
                    <a class="next" href="#slide117"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide117">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
                <p>La méthode des k plus proches voisins (k-NN) est un algorithme d'apprentissage supervisé utilisé à la
                    fois pour la classification et la régression.</p>
                <ul>
                    <li><b>Classification k-NN</b> : Dans ce cas, la sortie est une appartenance à une classe. Pour
                        classer
                        un nouvel objet, l'algorithme k-NN examine les k exemples les plus proches dans l'ensemble
                        d'apprentissage et détermine la classe majoritaire parmi ces voisins. Plus précisément, chaque
                        voisin contribue à un vote, et la classe la plus fréquente parmi les k voisins est attribuée à
                        l'objet à classer. C'est un exemple de vote majoritaire parmi les voisins les plus proches.</li>
                    <li><b>Régression k-NN</b> : À la différence de la classification, dans la régression k-NN, la
                        sortie
                        est une valeur de propriété de l'objet. Pour prédire la valeur d'une nouvelle observation,
                        l'algorithme k-NN calcule la valeur moyenne (ou médiane) des valeurs cibles des k plus proches
                        voisins. Par conséquent, au lieu de voter pour une classe majoritaire, les valeurs cibles des k
                        voisins sont utilisées pour prédire la valeur cible de l'objet à estimer.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">117
                    <a class="prev" href="#slide116"></a>
                    <a class="next" href="#slide118"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide118">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
                <p>Supposons que nous ayons un ensemble de données d'apprentissage composé de points dans un espace
                    bidimensionnel, où chaque point est associé à une classe. Supposons que nous voulions classer un
                    nouveau
                    point avec des coordonnées (x = 4, y = 3).</p>
                <table>
                    <thead>
                        <tr>
                            <th>Point</th>
                            <th>Coordonnée x</th>
                            <th>Coordonnée y</th>
                            <th>Classe</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>A</td>
                            <td>2</td>
                            <td>3</td>
                            <td>Rouge</td>
                        </tr>
                        <tr>
                            <td>B</td>
                            <td>4</td>
                            <td>4</td>
                            <td>Rouge</td>
                        </tr>
                        <tr>
                            <td>C</td>
                            <td>3</td>
                            <td>2</td>
                            <td>Bleu</td>
                        </tr>
                        <tr>
                            <td>D</td>
                            <td>6</td>
                            <td>5</td>
                            <td>Rouge</td>
                        </tr>
                        <tr>
                            <td>E</td>
                            <td>5</td>
                            <td>3</td>
                            <td>Bleu</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">118
                    <a class="prev" href="#slide117"></a>
                    <a class="next" href="#slide119"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide119">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
                <ol>
                    <li><b>Choix de k :</b> Disons que nous choisissons k = 3.</li>
                    <li><b>Calcul de la distance :</b> Nous calculons la distance euclidienne entre le nouveau point et
                        chaque point de notre ensemble d'apprentissage.
                        <ul>
                            <li> - Pour le point A : Distance = sqrt((4 - 2)^2 + (3 - 3)^2) = sqrt(4) = 2</li>
                            <li> - Pour le point B : Distance = sqrt((4 - 4)^2 + (3 - 4)^2) = sqrt(1) = 1</li>
                            <li> - Pour le point C : Distance = sqrt((4 - 3)^2 + (3 - 2)^2) = sqrt(2) ≈ 1.41</li>
                            <li> - Pour le point D : Distance = sqrt((4 - 6)^2 + (3 - 5)^2) = sqrt(8) ≈ 2.83</li>
                            <li> - Pour le point E : Distance = sqrt((4 - 5)^2 + (3 - 3)^2) = sqrt(1) = 1</li>
                        </ul>
                    </li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">119
                    <a class="prev" href="#slide118"></a>
                    <a class="next" href="#slide120"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide120">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
                <ol start="3">
                    <li><b>Sélection des k plus proches voisins :</b> Nous identifions les k points les plus proches du
                        nouveau point en termes de distance.
                        <ul>
                            <li> - Pour k = 3, les trois plus proches voisins sont B, C et E.</li>
                        </ul>
                    </li>
                    <li><b>Vote majoritaire :</b> Enfin, nous attribuons la classe majoritaire parmi les k voisins les
                        plus
                        proches au nouveau point. Dans ce cas, deux des voisins les plus proches (C et E) sont de la
                        classe
                        "Bleu" et un (B) est de la classe "Rouge". Par conséquent, le nouveau point est classé comme
                        "Bleu".</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">120
                    <a class="prev" href="#slide119"></a>
                    <a class="next" href="#slide121"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide121">
            <div class="header">
                <h1>2.3.3. Méthode des plus proches voisins</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Régression</b> : En utilisant la méthode des plus proches voisins pour la régression, on peut
                        estimer la valeur d'une variable cible pour une nouvelle observation en prenant la moyenne des
                        valeurs de la variable cible des k voisins les plus proches. Par exemple, dans la régression
                        k-NN,
                        on peut prédire le prix d'une maison en prenant la moyenne des prix des k maisons les plus
                        proches
                        en termes de caractéristiques similaires (surface, nombre de chambres, etc.).</li>
                    <li><b>Détection des anomalies</b> : La méthode des plus proches voisins peut également être
                        utilisée
                        pour détecter les anomalies dans les données. Les observations qui sont très différentes de
                        leurs
                        voisins les plus proches peuvent être considérées comme des anomalies. Par exemple, dans la
                        surveillance de la santé, des valeurs de signes vitaux inhabituelles par rapport aux voisins les
                        plus proches peuvent indiquer un problème de santé potentiel et donc être considérées comme des
                        anomalies.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">121
                    <a class="prev" href="#slide120"></a>
                    <a class="next" href="#slide122"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide122">
            <div class="header">
                <h1>2.3.4. Classification naïve bayésienne</h1>
            </div>
            <div class="content">
                <p>La classification naïve bayésienne est une méthode de classification probabiliste simple basée sur
                    l'application du théorème de Bayes avec une forte hypothèse d'indépendance entre les
                    caractéristiques.
                </p>
                <ul>
                    <li><b>Théorème de Bayes</b> : La classification naïve bayésienne repose sur le théorème de Bayes,
                        qui
                        est une formule pour calculer les probabilités conditionnelles. Il permet de calculer la
                        probabilité
                        qu'une observation appartienne à une classe donnée en utilisant les probabilités des
                        caractéristiques étant donné chaque classe.</li>
                </ul>

            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">122
                    <a class="prev" href="#slide121"></a>
                    <a class="next" href="#slide123"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide123">
            <div class="header">
                <h1>2.3.4. Classification naïve bayésienne</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Hypothèse d'indépendance naïve</b> : La caractéristique principale de la classification naïve
                        bayésienne est l'hypothèse d'indépendance naïve, qui suppose que les caractéristiques sont
                        indépendantes les unes des autres conditionnellement à la classe. Cela signifie que la présence
                        d'une caractéristique particulière dans une classe ne dépend pas de la présence d'autres
                        caractéristiques.</li>
                    <li><b>Modélisation des probabilités</b> : Pour chaque classe, la classification naïve bayésienne
                        modélise les distributions de probabilité des caractéristiques. Ensuite, lorsqu'une nouvelle
                        observation est introduite, elle utilise le théorème de Bayes pour calculer la probabilité
                        qu'elle
                        appartienne à chaque classe et choisit la classe avec la probabilité la plus élevée.</li>
                </ul>

            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">123
                    <a class="prev" href="#slide122"></a>
                    <a class="next" href="#slide124"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide124">
            <div class="header">
                <h1>2.3.4. Classification naïve bayésienne</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Classification des documents</b> : Elle est largement utilisée pour la classification des
                        documents, tels que la détection de spam (classification des e-mails en spam ou non-spam) et la
                        catégorisation de documents dans différentes catégories.</li>
                    <li><b>Analyse de sentiments :</b> Dans le domaine du traitement du langage naturel, la
                        classification
                        naïve bayésienne est utilisée pour l'analyse des sentiments, où elle peut être utilisée pour
                        classifier les textes en fonction de leur tonalité émotionnelle, comme positif, négatif ou
                        neutre.
                    </li>
                    <li><b>Catégorisation de documents :</b> Elle est également utilisée pour la catégorisation
                        automatique
                        de documents, où elle peut être utilisée pour classifier les documents dans des catégories
                        spécifiques en fonction de leur contenu ou de leur sujet.</li>
                    <li><b>Reconnaissance de caractères :</b> Dans le domaine de la vision par ordinateur, la
                        classification
                        naïve bayésienne est utilisée pour la reconnaissance de caractères, où elle peut être utilisée
                        pour
                        classifier les caractères écrits à la main dans différentes classes, telles que les lettres de
                        l'alphabet ou les chiffres.</li>

                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">124
                    <a class="prev" href="#slide123"></a>
                    <a class="next" href="#slide125"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide125">
            <div class="header">
                <h1>2.3.4. Classification naïve bayésienne</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Théorème de Bayes</h3>
                <ul>
                    <li>\(P(A), P(B)\) sont des probabilités d'observer A et B indépendamment l'un de l'autre.</li>
                    <li>\(P(A|B)\) est une probabilité conditionnelle, la probabilité que l'événement \(A\) se produise
                        étant donné que \(B\) est vrai</li>
                    <li>\(P(B|A)\) est une probabilité conditionnelle, la probabilité que l'événement \(B\) se produise
                        étant donné que \(A\) est vrai</li>
                    <li> \(P(B) &#8800; 0\)</li>
                </ul>
                <p>\[P(A|B) = \frac{(P(B|A).P(A))}{P(B)}\]</i>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">125
                    <a class="prev" href="#slide124"></a>
                    <a class="next" href="#slide126"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide126">
            <div class="header">
                <h1>2.3.4. Classification naïve bayésienne</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Théorème de Bayes: Classification d'un message</h3>
                <ul>
                    <li>\(P(S)\) est la probabilité globale qu'un message donné soit un spam.</li>
                    <li>\(P(H)\) est la probabilité globale qu'un message donné ne soit pas du spam.</li>
                    <li>\(P(S|W)\) est la probabilité qu'un message soit un spam, sachant que le mot s'y trouve ;</li>
                    <li>\(P(W|S)\) est la probabilité que le mot apparaisse dans les messages de spam ;</li>
                    <li>\(P(W|H)\) est la probabilité que le mot "réplique" apparaisse dans les messages ham.</li>
                </ul>
                <p>\[P(S|W) = \frac{P(W|S) \cdot P(S)}{P(W|S) \cdot P(S) + P(W|H) \cdot P(H)}\]</i>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">126
                    <a class="prev" href="#slide125"></a>
                    <a class="next" href="#slide127"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide127">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <p>Les arbres de décision sont un outil puissant d'aide à la décision qui utilise un modèle arborescent
                    pour
                    représenter les décisions et leurs conséquences possibles</p>
                <ul>
                    <li><b>Modèle arborescent :</b> Les arbres de décision représentent les décisions sous forme d'un
                        arbre,
                        où chaque nœud interne représente une caractéristique (ou attribut), chaque branche représente
                        une
                        décision basée sur cette caractéristique, et chaque feuille représente un résultat ou une
                        classe.
                    </li>
                    <li><b>Facile à interpréter :</b> Les arbres de décision sont faciles à comprendre et à interpréter,
                        ce
                        qui les rend populaires pour la prise de décision dans de nombreux domaines.</li>
                    <li><b>Adaptabilité :</b> Ils peuvent être utilisés pour modéliser des problèmes de classification
                        ainsi
                        que des problèmes de régression.</li>
                    <li><b>Utilisation de règles simples :</b> Les décisions sont prises en suivant des règles simples
                        basées sur les valeurs des caractéristiques, ce qui rend l'interprétation des résultats facile
                        même
                        pour les non-experts.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Decision_tree_model.png"
                        height="400px" width="400px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">127
                    <a class="prev" href="#slide126"></a>
                    <a class="next" href="#slide128"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide128">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <figure class="fullwidth">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Decision_tree_model.png"
                        height="400px" width="400px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">128
                    <a class="prev" href="#slide127"></a>
                    <a class="next" href="#slide129"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide129">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">

                <table>
                    <thead>
                        <tr>
                            <th>Animal</th>
                            <th>Pelage</th>
                            <th>Plumes</th>
                            <th>Peut voler</th>
                            <th>Classe</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Chien</td>
                            <td>Poilu</td>
                            <td>Non</td>
                            <td>Non</td>
                            <td>Mammifère</td>
                        </tr>
                        <tr>
                            <td>Chat</td>
                            <td>Poilu</td>
                            <td>Non</td>
                            <td>Non</td>
                            <td>Mammifère</td>
                        </tr>
                        <tr>
                            <td>Aigle</td>
                            <td>Plumeux</td>
                            <td>Oui</td>
                            <td>Oui</td>
                            <td>Oiseau</td>
                        </tr>
                        <tr>
                            <td>Pingouin</td>
                            <td>Plumeux</td>
                            <td>Oui</td>
                            <td>Non</td>
                            <td>Oiseau</td>
                        </tr>
                        <tr>
                            <td>Serpent</td>
                            <td>Écaille</td>
                            <td>Non</td>
                            <td>Non</td>
                            <td>Reptile</td>
                        </tr>
                    </tbody>
                </table>

                <p>Nous voulons classer ces animaux en trois classes : Mammifère, Oiseau ou Reptile. Utilisons un arbre
                    de
                    décision pour ce faire.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">129
                    <a class="prev" href="#slide128"></a>
                    <a class="next" href="#slide130"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide130">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <p>L'algorithme de l'arbre de décision est une méthode d'apprentissage supervisé utilisée pour la
                    classification et la régression.</p>
                <ul>

                    <li><b>Choix de la caractéristique de division</b> : L'algorithme commence par choisir la meilleure
                        caractéristique pour diviser les données en sous-ensembles homogènes. Dans l'exemple, la
                        première
                        caractéristique choisie est le pelage.</li>
                    <li><b>Division des données</b> : Les données sont divisées en sous-groupes en fonction de la
                        caractéristique choisie. Dans l'exemple, les données sont divisées en deux groupes : ceux avec
                        un
                        pelage poilu et ceux avec un pelage plumeux.</li>
                    <li><b>Récursion</b> : Le processus est répété de manière récursive pour chaque sous-groupe obtenu.
                        Pour
                        chaque sous-groupe, l'algorithme choisit à nouveau la meilleure caractéristique de division et
                        divise les données en sous-groupes plus petits. Dans notre exemple, pour les animaux avec un
                        pelage
                        plumeux, la capacité de voler est la caractéristique de division suivante.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">130
                    <a class="prev" href="#slide129"></a>
                    <a class="next" href="#slide131"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide131">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Critère d'arrêt</b> : L'algorithme s'arrête lorsque l'une des conditions suivantes est
                        remplie :
                        <ul>
                            <li>Tous les éléments d'un sous-groupe appartiennent à la même classe. </li>
                            <li>Toutes les caractéristiques ont été utilisées pour la division.</li>
                            <li>Une profondeur maximale de l'arbre est atteinte.</li>
                            <li>Un nombre minimal d'échantillons dans un nœud est atteint.</li>
                        </ul>
                    </li>
                    <li><b>Construction de l'arbre</b> : Une fois que les divisions sont terminées, un arbre de décision
                        est
                        construit où chaque nœud représente une caractéristique de division et chaque feuille représente
                        une
                        classe de sortie.</li>
                    <li><b>Classification</b> : Lorsqu'un nouvel exemple est introduit, il est classé en parcourant
                        l'arbre
                        de décision en fonction de ses caractéristiques jusqu'à atteindre une feuille, où il est
                        attribué à
                        une classe.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">131
                    <a class="prev" href="#slide130"></a>
                    <a class="next" href="#slide132"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide132">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <p>Dans le contexte des arbres de décision, les données sont généralement représentées sous forme de
                    vecteurs où chaque élément du vecteur correspond à une caractéristique ou à une variable
                    indépendante,
                    et la variable dépendante est la cible que l'on cherche à prédire ou à classifier. </p>
                <ul>
                    <li><b>Données sous forme de vecteurs</b> : Chaque observation ou exemple dans l'ensemble de données
                        est
                        représenté sous forme d'un vecteur, où chaque composante du vecteur correspond à une
                        caractéristique
                        ou à une variable explicative. Par exemple, si nous examinons un ensemble de données sur les
                        prêts
                        bancaires, les caractéristiques pourraient inclure le revenu, le montant du prêt, le nombre
                        d'années
                        d'expérience professionnelle, etc.</li>
                    <li> Les données sont disponibles sous la forme \[(\textbf{x},Y) = (x_1, x_2, x_3, ..., x_k, Y)\]
                    </li>
                    <li>Le vecteur \(\textbf{x}\) est composé des caractéristiques suivantes \(x_1, x_2, x_3, ...\)</li>
                    <li>\(Y\) est la variable dépendante qui peut dépendre de \(\textbf{x}\)</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">132
                    <a class="prev" href="#slide131"></a>
                    <a class="next" href="#slide133"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide133">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Classification</b> : Les arbres de décision sont couramment utilisés pour la classification,
                        où
                        l'objectif est de catégoriser les observations dans des classes ou des catégories prédéfinies en
                        fonction de leurs caractéristiques. Par exemple, dans le domaine médical, les arbres de décision
                        peuvent être utilisés pour classifier les patients en fonction de leur diagnostic.</li>
                    <li><b>Régression</b> : Les arbres de décision peuvent également être utilisés pour la régression,
                        où
                        l'objectif est de prédire une valeur numérique continue en fonction des caractéristiques. Par
                        exemple, dans les finances, les arbres de décision peuvent être utilisés pour prédire le prix
                        d'une
                        maison en fonction de ses caractéristiques.</li>
                    <li><b>Analyse de la décision</b> : Les arbres de décision peuvent aider à identifier les stratégies
                        ou
                        les séquences d'actions les plus efficaces pour atteindre un objectif spécifique. Par exemple,
                        dans
                        la planification d'entreprise, les arbres de décision peuvent être utilisés pour déterminer les
                        meilleures décisions à prendre dans un processus de prise de décision complexe.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">133
                    <a class="prev" href="#slide132"></a>
                    <a class="next" href="#slide134"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide134">
            <div class="header">
                <h1>2.3.5. Arbres de décision</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Recherche opérationnelle</b> : Les arbres de décision sont également utilisés dans le domaine
                        de
                        la recherche opérationnelle pour résoudre des problèmes d'optimisation et de planification. Par
                        exemple, dans la logistique, les arbres de décision peuvent être utilisés pour déterminer le
                        meilleur itinéraire de livraison en fonction de divers facteurs tels que la distance, le coût et
                        les
                        contraintes de temps.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">134
                    <a class="prev" href="#slide133"></a>
                    <a class="next" href="#slide135"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide135">
            <div class="header">
                <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
            </div>
            <div class="content">
                <p>L'apprentissage ensembliste, en particulier les forêts d'arbres décisionnels, est une technique qui
                    combine plusieurs modèles d'apprentissage pour améliorer les performances prédictives par rapport à
                    un
                    seul modèle. Les forêts d'arbres décisionnels sont obtenues en construisant de multiples arbres de
                    décision lors de la phase d'entraînement.</p>
                <ul>
                    <li><b>Construction des arbres de décision</b> : Pendant la phase d'entraînement, plusieurs arbres
                        de
                        décision sont construits en utilisant différents sous-ensembles de données et/ou
                        caractéristiques.
                        Chaque arbre est formé de manière indépendante.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">135
                    <a class="prev" href="#slide134"></a>
                    <a class="next" href="#slide136"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide136">
            <div class="header">
                <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Vote majoritaire</b> : Pour la classification, chaque arbre de décision vote pour la classe
                        prédite d'un nouvel exemple. La classe finale attribuée à l'exemple est déterminée par un vote
                        majoritaire parmi tous les arbres de la forêt.
                        Pour la régression, les valeurs prédites par chaque arbre sont moyennées pour obtenir la valeur
                        finale.</li>
                    <li><b>Réduction de la variance</b> : L'apprentissage ensembliste vise à réduire la variance du
                        modèle
                        en agrégeant plusieurs modèles. Cela peut aider à éviter le surajustement (overfitting) en
                        compensant les défauts individuels de chaque arbre de décision.</li>
                    <li><b>Stabilité</b> : Les forêts d'arbres décisionnels sont généralement plus stables que les
                        arbres de
                        décision individuels, car elles sont moins sensibles aux variations des données d'entraînement.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">136
                    <a class="prev" href="#slide135"></a>
                    <a class="next" href="#slide137"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide137">
            <div class="header">
                <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Algorithme</h3>
                <ul>
                    <li>Soit \(X = x_1,x_2,..x_n\) un ensemble de données avec des réponses \(Y = y_1,y_2,..y_n\)</li>
                    <li> Soit \(b = 1, 2,..B\)
                        <ul>
                            <li>Échantillon, avec remplacement (un élément peut apparaître plusieurs fois dans un même
                                échantillon), \(n\) exemples de formation de \(X, Y\) ; appelez-les \(X_b, Y_b\).</li>
                            <li>Former un arbre de classification ou de régression \(f_b\) sur \(X_b, Y_b\).</li>
                        </ul>
                    </li>
                    <li>Après entraînement, les prédictions pour les échantillons non vus x' peuvent être faites en
                        faisant
                        la moyenne des prédictions de tous les arbres de régression individuels sur x' \[\hat{f} =
                        \frac{1}{B} \sum_{b=1}^Bf_b (x')\] ou par un
                        vote à la majorité dans le cas des arbres de classification.
                    </li>

                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">137
                    <a class="prev" href="#slide136"></a>
                    <a class="next" href="#slide138"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide138">
            <div class="header">
                <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Classification multiclasse</b> : les forêts d'arbres décisionnels sont utilisées pour classer
                        les
                        instances dans l'une des plusieurs classes mutuellement exclusives. Par exemple, la
                        classification
                        d'images en différentes catégories telles que les animaux, les véhicules, les objets ménagers,
                        etc.
                    </li>
                    <li><b>Classification multilabel</b> : Contrairement à la classification multiclasse, la
                        classification
                        multilabel permet qu'une instance soit assignée à plusieurs classes en même temps. Par exemple,
                        la
                        classification de documents dans lesquels un document peut être associé à plusieurs catégories
                        telles que "science", "technologie", "politique", etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">138
                    <a class="prev" href="#slide137"></a>
                    <a class="next" href="#slide139"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide139">
            <div class="header">
                <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Régression</b> : les forêts d'arbres décisionnels peuvent également être utilisées pour des
                        tâches de régression, où la sortie est une valeur continue plutôt qu'une classe discrète. Par
                        exemple, prédire le prix d'une maison en fonction de ses caractéristiques.</li>
                    <li><b>Détection des anomalies</b> : Elles peuvent également être employées pour détecter les
                        anomalies
                        ou les comportements inhabituels dans les données. Cela peut être utile dans divers domaines
                        tels
                        que la détection de fraudes financières, la détection de défaillances dans les systèmes
                        industriels,
                        etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">139
                    <a class="prev" href="#slide138"></a>
                    <a class="next" href="#slide140"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide140">
            <div class="header">
                <h1>2.4. Sélection de caractéristique</h1>
            </div>
            <div class="content">
                <p>La sélection de caractéristiques est un processus visant à choisir un sous-ensemble de
                    caractéristiques
                    pertinentes parmi un grand nombre de caractéristiques disponibles. </p>
                <ul>
                    <li>Cette technique est largement utilisée dans des domaines où le nombre de caractéristiques est
                        élevé
                        par rapport à la taille de l'échantillon de données, car cela peut entraîner des problèmes de
                        surajustement et de temps de calcul élevé.</li>
                    <li>La sélection de caractéristiques est également considérée comme une méthode de réduction de la
                        dimensionnalité, car elle vise à réduire le nombre de dimensions de l'espace des
                        caractéristiques
                        sans perdre d'informations discriminatives.</li>
                    <li>la sélection de caractéristiques vise à :
                        <ul>
                            <li>Identifier les caractéristiques les plus pertinentes qui contribuent le plus à la
                                variabilité des données ou à la capacité de prédiction du modèle.</li>
                            <li>Réduire la dimensionnalité de l'espace des caractéristiques pour améliorer les
                                performances
                                des modèles d'apprentissage automatique en termes de temps de calcul et de prévention du
                                surajustement.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">140
                    <a class="prev" href="#slide139"></a>
                    <a class="next" href="#slide141"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide141">
            <div class="header">
                <h1>2.4. Sélection de caractéristique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Applications</h3>
                <ul>
                    <li><b>Analyse des textes écrits</b> : Dans l'analyse de textes écrits, la sélection de
                        caractéristiques
                        est effectivement utilisée pour extraire les éléments les plus pertinents et informatifs des
                        données
                        textuelles. Cela peut inclure l'identification des mots clés, des entités nommées, des motifs
                        linguistiques spécifiques ou d'autres caractéristiques qui sont essentielles pour la tâche
                        d'analyse
                        de texte, comme la classification de documents, l'extraction d'informations ou la génération de
                        résumés. La sélection de caractéristiques dans ce contexte vise à réduire la dimensionnalité des
                        données textuelles tout en préservant leur pertinence et leur expressivité pour les tâches
                        d'analyse
                        ultérieures.</li>
                    <li><b>Analyse des données des puces à ADN</b> : En génomique et bioinformatique, les puces à ADN
                        génèrent des ensembles de données très dimensionnels qui nécessitent souvent une réduction de
                        dimension pour identifier les gènes ou les séquences d'ADN les plus significatifs associés à des
                        phénotypes particuliers, tels que des maladies ou des réponses biologiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">141
                    <a class="prev" href="#slide140"></a>
                    <a class="next" href="#slide142"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide142">
            <div class="header">
                <h1>2.4. Sélection de caractéristique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Définition formelle[8]</h3>
                <ul>
                    <li>Soit \(X\) l'ensemble original de \(n\) caractéristiques, c'est-à-dire, \(|X| = n\)</li>
                    <li>Soit \(w_i\) le poids attribué à l'élément \(x_i &#8712; X\)</li>
                    <li>La sélection binaire attribue des poids binaires tandis que la sélection continue attribue des
                        poids
                        en préservant l'ordre de sa pertinence.</li>
                    <li>Soit \(J(X')\) soit une mesure d'évaluation, définie comme \(J: X' &#8838; X &#8594; R\)</li>
                    <li> Le problème de la sélection des caractéristiques peut être défini de trois façons
                        <ol>
                            <li>\(|X'| = m &lt; n\). Trouver \(X' &#8834; X\) tel que \(J(X')\) est le maximum</li>
                            <li>Choisir \(J_0\), Trouver \(X' &#8838; X\), tel que \(J(X') &gt;= J_0\)</li>
                            <li>Trouver un compromis entre la minimisation de \(|X'|\) et la maximisation du \(J(X')\)
                            </li>
                        </ol>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">142
                    <a class="prev" href="#slide141"></a>
                    <a class="next" href="#slide143"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide143">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Articles de recherche</h3>
                <ol>
                    <li>From data mining to knowledge discovery in databases, Usama Fayyad, Gregory Piatetsky-Shapiro,
                        and
                        Padhraic Smyth, AI Magazine Volume 17 Number 3 (1996)</li>
                    <li>Survey of Clustering Data Mining Techniques, Pavel Berkhin</li>
                    <li>Mining association rules between sets of items in large databases, Agrawal, Rakesh, Tomasz
                        Imieliński, and Arun Swami. Proceedings of the 1993 ACM SIGMOD international conference on
                        Management of data - SIGMOD 1993. p. 207. </li>
                    <li>Comparisons of Sequence Labeling Algorithms and Extensions, Nguyen, Nam, and Yunsong Guo.
                        Proceedings of the 24th international conference on Machine learning. ACM, 2007. </li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">143
                    <a class="prev" href="#slide142"></a>
                    <a class="next" href="#slide144"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide144">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Articles de recherche</h3>
                <ol start="5">
                    <li>An Analysis of Active Learning Strategies for Sequence Labeling Tasks, Settles, Burr, and Mark
                        Craven. Proceedings of the conference on empirical methods in natural language processing.
                        Association for Computational Linguistics, 2008.</li>
                    <li>Anomaly detection in crowded scenes, Mahadevan; Vijay et al. Computer Vision and Pattern
                        Recognition
                        (CVPR), 2010 IEEE Conference on. IEEE, 2010</li>
                    <li>A Study of Global Inference Algorithms in Multi-Document Summarization. McDonald, Ryan. European
                        Conference on Information Retrieval. Springer, Berlin, Heidelberg, 2007.</li>
                    <li>Feature selection algorithms: A survey and experimental evaluation., Molina, Luis Carlos, Lluís
                        Belanche, and Àngela Nebot. Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International
                        Conference on. IEEE, 2002.</li>
                    <li>Support vector machines, Hearst, Marti A., et al. IEEE Intelligent Systems and their
                        applications
                        13.4 (1998): 18-28.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">144
                    <a class="prev" href="#slide143"></a>
                    <a class="next" href="#slide145"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide145">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Ressources en ligne</h3>
                <ul>
                    <li><a
                            href="https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124">Accuracy,
                            Recall, Precision, F-Score & Specificity, which to optimize on?</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Patterns_in_nature">Patterns in Nature</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Statistical_classification">Statistical
                            classification</a>
                    </li>
                    <li><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Association_rule_learning">Association rule learning</a>
                    </li>
                    <li><a href="https://en.wikipedia.org/wiki/Anomaly_detection">Anomaly detection</a></li>
                </ul>
                <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Sequence_labeling">Sequence labeling</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Automatic_summarization">Automatic summarization</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Pattern_recognition">Pattern recognition</a></li>
                    <li><a href="http://scikit-learn.org/stable/">Scikit-learn</a></li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">145
                    <a class="prev" href="#slide144"></a>
                    <a class="next" href="#slide146"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide146">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Ressources en ligne</h3>
                <ul>
                    <ul>
                        <li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a>
                        </li>
                        <li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a>
                        </li>
                        <li><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient
                                descent</a></li>
                    </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">146
                    <a class="prev" href="#slide145"></a>
                    <a class="next" href="#slide147"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide147">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Couleurs</h3>
                <ul>
                    <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
                </ul>
                <h3 class="topicsubheading">Images</h3>
                <ul>
                    <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">147
                    <a class="prev" href="#slide146"></a>
                    <a class="next" href="#slide148"></a>
                </div>
            </div>
        </section>
        <script>
            // Theme Toggle Functionality
            function toggleTheme() {
                const html = document.documentElement;
                const currentTheme = html.getAttribute('data-theme');
                const newTheme = currentTheme === 'dark' ? 'light' : 'dark';
                html.setAttribute('data-theme', newTheme);
                localStorage.setItem('theme', newTheme);
            }

            // Initialize theme from localStorage or system preference
            (function initTheme() {
                const savedTheme = localStorage.getItem('theme');
                if (savedTheme) {
                    document.documentElement.setAttribute('data-theme', savedTheme);
                } else if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
                    document.documentElement.setAttribute('data-theme', 'dark');
                }
            })();

            // Listen for system theme changes
            window.matchMedia('(prefers-color-scheme: dark)').addEventListener('change', e => {
                if (!localStorage.getItem('theme')) {
                    document.documentElement.setAttribute('data-theme', e.matches ? 'dark' : 'light');
                }
            });

            function changeCurrentURLSlideNumber(isIncrement) {
                url = window.location.href;
                position = url.indexOf("#slide");
                if (position != -1) { // Not on the first page
                    slideIdString = url.substr(position + 6);
                    if (!Number.isNaN(slideIdString)) {
                        slideId = parseInt(slideIdString);
                        if (isIncrement) {
                            if (slideId < 82) {
                                slideId = slideId + 1;
                            }
                        } else {
                            if (slideId > 1) {
                                slideId = slideId - 1;
                            }
                        }
                        /* regexp */
                        url = url.replace(/#slide\d+/g, "#slide" + slideId);
                        window.location.href = url;
                    }
                } else {
                    window.location.href = url + "#slide2";
                }
            }
            document.onkeydown = function (event) {

                event.preventDefault();
                /* This will ensure the default behavior of
                                                                page scroll behaviour (up, down, right, left)*/

                event = event || window.event;
                /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
                if (event.keyCode == '37') {
                    // left
                    changeCurrentURLSlideNumber(false);
                } else if (event.keyCode == '38') {
                    // up
                    changeCurrentURLSlideNumber(false);
                } else if (event.keyCode == '39') {
                    // right
                    changeCurrentURLSlideNumber(true);
                } else if (event.keyCode == '40') {
                    // down
                    changeCurrentURLSlideNumber(true);
                }
            }
            document.body.onmouseup = function (event) {
                event = event || window.event;
                event.preventDefault();
                changeCurrentURLSlideNumber(true);
            }
        </script>

    </body>

</html>