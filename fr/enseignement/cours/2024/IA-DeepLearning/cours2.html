<html>

<head>
    <meta charset="utf-8" />
    <title>Intelligence artificielle et Deep Learning (2024-2025): Cours: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            line-height: 1.7em;
            flex-direction: column;
            align-items: flex-start;
            margin: 0 auto;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .codeexample {
            background-color: #eeeeee;
        }

        /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
        pre {
            line-height: 125%;
        }

        td.linenos .normal {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos {
            color: inherit;
            background-color: transparent;
            padding-left: 5px;
            padding-right: 5px;
        }

        td.linenos .special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        span.linenos.special {
            color: #000000;
            background-color: #ffffc0;
            padding-left: 5px;
            padding-right: 5px;
        }

        body .hll {
            background-color: #ffffcc
        }

        body {
            background: #f8f8f8;
        }

        body .c {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment */
        body .err {
            border: 1px solid #FF0000
        }

        /* Error */
        body .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */
        body .o {
            color: #666666
        }

        /* Operator */
        body .ch {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Hashbang */
        body .cm {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Multiline */
        body .cp {
            color: #9C6500
        }

        /* Comment.Preproc */
        body .cpf {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.PreprocFile */
        body .c1 {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Single */
        body .cs {
            color: #3D7B7B;
            font-style: italic
        }

        /* Comment.Special */
        body .gd {
            color: #A00000
        }

        /* Generic.Deleted */
        body .ge {
            font-style: italic
        }

        /* Generic.Emph */
        body .gr {
            color: #E40000
        }

        /* Generic.Error */
        body .gh {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Heading */
        body .gi {
            color: #008400
        }

        /* Generic.Inserted */
        body .go {
            color: #717171
        }

        /* Generic.Output */
        body .gp {
            color: #000080;
            font-weight: bold
        }

        /* Generic.Prompt */
        body .gs {
            font-weight: bold
        }

        /* Generic.Strong */
        body .gu {
            color: #800080;
            font-weight: bold
        }

        /* Generic.Subheading */
        body .gt {
            color: #0044DD
        }

        /* Generic.Traceback */
        body .kc {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Constant */
        body .kd {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Declaration */
        body .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */
        body .kp {
            color: #008000
        }

        /* Keyword.Pseudo */
        body .kr {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Reserved */
        body .kt {
            color: #B00040
        }

        /* Keyword.Type */
        body .m {
            color: #666666
        }

        /* Literal.Number */
        body .s {
            color: #BA2121
        }

        /* Literal.String */
        body .na {
            color: #687822
        }

        /* Name.Attribute */
        body .nb {
            color: #008000
        }

        /* Name.Builtin */
        body .nc {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Class */
        body .no {
            color: #880000
        }

        /* Name.Constant */
        body .nd {
            color: #AA22FF
        }

        /* Name.Decorator */
        body .ni {
            color: #717171;
            font-weight: bold
        }

        /* Name.Entity */
        body .ne {
            color: #CB3F38;
            font-weight: bold
        }

        /* Name.Exception */
        body .nf {
            color: #0000FF
        }

        /* Name.Function */
        body .nl {
            color: #767600
        }

        /* Name.Label */
        body .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */
        body .nt {
            color: #008000;
            font-weight: bold
        }

        /* Name.Tag */
        body .nv {
            color: #19177C
        }

        /* Name.Variable */
        body .ow {
            color: #AA22FF;
            font-weight: bold
        }

        /* Operator.Word */
        body .w {
            color: #bbbbbb
        }

        /* Text.Whitespace */
        body .mb {
            color: #666666
        }

        /* Literal.Number.Bin */
        body .mf {
            color: #666666
        }

        /* Literal.Number.Float */
        body .mh {
            color: #666666
        }

        /* Literal.Number.Hex */
        body .mi {
            color: #666666
        }

        /* Literal.Number.Integer */
        body .mo {
            color: #666666
        }

        /* Literal.Number.Oct */
        body .sa {
            color: #BA2121
        }

        /* Literal.String.Affix */
        body .sb {
            color: #BA2121
        }

        /* Literal.String.Backtick */
        body .sc {
            color: #BA2121
        }

        /* Literal.String.Char */
        body .dl {
            color: #BA2121
        }

        /* Literal.String.Delimiter */
        body .sd {
            color: #BA2121;
            font-style: italic
        }

        /* Literal.String.Doc */
        body .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */
        body .se {
            color: #AA5D1F;
            font-weight: bold
        }

        /* Literal.String.Escape */
        body .sh {
            color: #BA2121
        }

        /* Literal.String.Heredoc */
        body .si {
            color: #A45A77;
            font-weight: bold
        }

        /* Literal.String.Interpol */
        body .sx {
            color: #008000
        }

        /* Literal.String.Other */
        body .sr {
            color: #A45A77
        }

        /* Literal.String.Regex */
        body .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */
        body .ss {
            color: #19177C
        }

        /* Literal.String.Symbol */
        body .bp {
            color: #008000
        }

        /* Name.Builtin.Pseudo */
        body .fm {
            color: #0000FF
        }

        /* Name.Function.Magic */
        body .vc {
            color: #19177C
        }

        /* Name.Variable.Class */
        body .vg {
            color: #19177C
        }

        /* Name.Variable.Global */
        body .vi {
            color: #19177C
        }

        /* Name.Variable.Instance */
        body .vm {
            color: #19177C
        }

        /* Name.Variable.Magic */
        body .il {
            color: #666666
        }

        /* Literal.Number.Integer.Long */


        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 40%;
            margin-bottom: 1vmax;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 90%;
            max-height: 90%;
        }

        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }

        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 1.7em;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }


        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                font-size: 2.8vmin;
            }

            .content .flexcontent {
                font-size: 2.5vmin;
            }
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:2.5vw">Intelligence artificielle et Deep Learning</h1>
            <h3>Apprentissage machine</h3>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Année</b>: 2024-2025<br />
                <b>Courriel</b>: john.samuel@cpe.fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>2.1. Neurones biologiques</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Neurones biologiques</h2>
            <figure>
                <img src="../../2021/MachineLearning/Neuron3.png" height="350px" />
                <figcaption>Neurone biologique<sup>1</sup></figcaption>
            </figure>
            <ol style="font-size:2vh">
                <li>https://en.wikipedia.org/wiki/File:Neuron3.png</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>2.1. Neurones biologiques</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Introduction</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Colored_neural_network.svg" />
                <figcaption>Réseaux de neurones artificiels</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>2.1. Neurones biologiques</h1>
        </div>
        <div class="content">
            <table>
                <colgroup>
                    <col style="width: 16%" />
                    <col style="width: 39%" />
                    <col style="width: 43%" />
                </colgroup>
                <thead>
                    <tr class="header">
                        <th><strong>Aspect</strong></th>
                        <th><strong>Neurone Biologique</strong></th>
                        <th><strong>Neurone Artificiel</strong></th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="odd">
                        <td><strong>Structure</strong></td>
                        <td>Composé de dendrites, d’un soma (corps cellulaire), et d’un axone</td>
                        <td>Composé de poids (équivalents aux connexions), biais, et activation</td>
                    </tr>
                    <tr class="even">
                        <td><strong>Fonction</strong></td>
                        <td>Transmet des impulsions électriques entre les neurones</td>
                        <td>Calcule une valeur de sortie en fonction de la somme pondérée des entrées</td>
                    </tr>
                    <tr class="odd">
                        <td><strong>Entrée</strong></td>
                        <td>Reçoit des signaux par les dendrites</td>
                        <td>Reçoit des valeurs pondérées (par les poids)</td>
                    </tr>
                    <tr class="even">
                        <td><strong>Poids des connexions</strong></td>
                        <td>La force des synapses influence l’intensité du signal transmis</td>
                        <td>Les poids déterminent l’importance de chaque entrée</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>2.1. Neurones biologiques</h1>
        </div>
        <div class="content">
            <table>
                <colgroup>
                    <col style="width: 16%" />
                    <col style="width: 39%" />
                    <col style="width: 43%" />
                </colgroup>
                <thead>
                    <tr class="header">
                        <th><strong>Aspect</strong></th>
                        <th><strong>Neurone Biologique</strong></th>
                        <th><strong>Neurone Artificiel</strong></th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="odd">
                        <td><strong>Activation</strong></td>
                        <td>Un potentiel d’action est déclenché si le signal dépasse un seuil</td>
                        <td>Une fonction d’activation est appliquée pour déterminer la sortie</td>
                    </tr>
                    <tr class="even">
                        <td><strong>Sortie</strong></td>
                        <td>Envoie un signal via l’axone vers d’autres neurones</td>
                        <td>Produit une sortie, souvent transmise aux neurones suivants dans le réseau</td>
                    </tr>
                    <tr class="odd">
                        <td><strong>Apprentissage</strong></td>
                        <td>Renforce les connexions synaptiques en fonction de l’expérience (plasticité synaptique)</td>
                        <td>Ajuste les poids via des algorithmes d’apprentissage (ex. rétropropagation)</td>
                    </tr>
                    <tr class="even">
                        <td><strong>Rôle</strong></td>
                        <td>Participe à des processus cognitifs complexes</td>
                        <td>Contribue aux calculs et à la reconnaissance de motifs</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>2.1. Neurones biologiques</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Réseau de neurones</h2>
            <p>Les réseaux de neurones sont couramment utilisés dans le domaine de l'apprentissage machine, en
                particulier dans des tâches telles que la classification, la régression, la reconnaissance d'images, le
                traitement du langage naturel, et bien d'autres. Un réseau de neurones artificiels est une collection
                d'unités interconnectées appelées neurones artificiels. Ces réseaux sont inspirés de la structure du
                cerveau biologique</p>
            <ul>
                <li><b>Connexions</b> : Chaque connexion entre les neurones, similaire aux synapses dans le cerveau
                    biologique, peut transmettre un signal aux autres neurones.</li>
                <li><b>Transmission de signal</b> : Un neurone artificiel reçoit un signal, le traite à l'aide d'une
                    fonction non linéaire, et peut ensuite transmettre un signal aux neurones qui lui sont connectés.
                </li>
                <li><b>Fonction d'activation</b> : La sortie de chaque neurone est calculée par une fonction non
                    linéaire appliquée à la somme pondérée de ses entrées. Cette fonction d'activation introduit une
                    non-linéarité dans le réseau, permettant de modéliser des relations complexes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>2.1. Neurones biologiques</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Réseau de neurones</h2>
            <ul>
                <li><b>Poids ajustables</b> : Les neurones et les connexions ont généralement des poids qui sont ajustés
                    au fur et à mesure de l'apprentissage. Ces poids déterminent l'importance relative des différentes
                    entrées pour chaque neurone.</li>
                <li><b>Ajustement des poids</b> : Les poids peuvent être ajustés pour augmenter ou diminuer la force du
                    signal au niveau d'une connexion, influençant ainsi la contribution de cette connexion aux calculs
                    du réseau.</li>
                <li><b>Seuil</b> : Les neurones peuvent avoir un seuil, de sorte qu'un signal n'est envoyé que si la
                    somme pondérée de ses entrées dépasse ce seuil. Cela permet au réseau de moduler sa sensibilité aux
                    entrées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Perceptron</h3>
            <p>Le perceptron est un <b>algorithme d'apprentissage supervisé</b> utilisé pour la <b>classification
                    binaire</b>. Il est conçu pour résoudre des problèmes où l'objectif est de déterminer si une entrée
                donnée appartient ou non à une classe particulière.</p>
            <ul>
                <li>Le perceptron a été inventé par <b>Frank Rosenblatt</b> en 1958. L'idée était de créer un modèle
                    simple de neurone artificiel inspiré du fonctionnement des neurones biologiques. Rosenblatt a
                    formulé un algorithme d'apprentissage qui permet au perceptron d'ajuster ses poids en fonction des
                    erreurs de classification, améliorant ainsi ses performances au fil du temps.</li>
                <li><b>Fonctionnement</b> : Le perceptron prend plusieurs entrées pondérées et les combine en une somme.
                    Ensuite, cette somme est soumise à une fonction d'activation, généralement une fonction échelon
                    (step function), qui produit la sortie binaire du perceptron.</li>
                <li><b>Limitations</b> : Le perceptron a des limitations, notamment sa capacité à résoudre des problèmes
                    non linéaires et son incapacité à apprendre des modèles complexes. Cependant, il a jeté les bases
                    pour le développement de réseaux de neurones plus avancés, en particulier les réseaux multicouches
                    qui peuvent apprendre des représentations hiérarchiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Perceptron</h1>
            <figure>
                <img src="../../2021/MachineLearning/Perceptron_example.svg" height="350px" />
                <figcaption>Perceptron en mettant à jour sa limite linéaire à mesure que d'autres exemples de formation
                    sont ajoutés.<sup>1</sup></figcaption>
            </figure>
            <ol style="font-size:2vh">
                <li>Source: https://en.wikipedia.org/wiki/File:Perceptron_example.svg</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Perceptron</h1>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Perceptron.svg" height="400px" />
                <figcaption>Perceptron</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Perceptron: Définition formelle</h1>
            <ul>
                <li>Soit \(y = f(z)\) la sortie du perceptron pour un vecteur d'entrée <i>z</i></li>
                <li>Soit \(N\) le nombre d'exemples d'entraînement</li>
                <li>Soit <i><b>X</b></i> l'espace de saisie des caractéristiques</li>
                <li>Soit \({(x_{1}, d_{1}),...,(x_{N}, d_{N})}\) be the <i><b>N</b></i> training examples, where
                    <ul>
                        <li>\(x_i\) est le vecteur caractéristique de <i>i<sup>ème</sup></i> exemple d'entraînement.
                        </li>
                        <li>\(d_i\) est la valeur de sortie souhaitée</li>
                        <li>\(x_{j,i}\) est la <i>i<sup>ème</sup></i> caractéristique de <i>j<sup>ème</sup></i> exemple
                            d'entraînement.</li>
                        <li>\(x_{j,0} = 1\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h1>Perceptron: Définition formelle</h1>
            <ul>
                <li>Les poids sont représentés de la manière suivante:
                    <ul>
                        <li>\(w_i\) est la <i>i<sup>ème</sup></i> valeur du vecteur de poids.</li>
                        <li>\(w_i(t)\) est la <i>i<sup>ème</sup></i> valeur du vecteur de poids à un moment donné t.
                        </li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h1>Perceptron : Étapes</h1>
            <ol>
                <li>Initialiser les poids et les seuils</li>
                <li>Pour chaque exemple, \((x_j, d_j)\) dans l'ensemble d'entraînement<i></i>
                    <ul>
                        <li>Calculer la sortie actuelle : \[y_j(t)= f[w(t).x_j]\] \[= f[w_0(t)x_{j,0} + w_1(t)x_{j,1} +
                            w_2(t)x_{j,2} + \dotsb + w_n(t)x_{j,n}]\]</li>
                        <li>Calculer le poids: \[w_i(t + 1) = w_i(t) + r. (d_j-y_j(t))x_{j,i}\]</li>
                    </ul> \(r\) est le taux d'apprentissage.
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h1>Perceptron : Étapes</h1>
            <ol start="3">
                <li>Répétez l'étape 2 jusqu'à l'erreur d'itération \[\frac{1}{s} (&#931; |d_j - y_j(t)|)\] est inférieur
                    au seuil spécifié par l'utilisateur \(\gamma\), ou un nombre prédéterminé d'itérations ont été
                    effectuées, où \(s\) est à nouveau la taille
                    de l'ensemble de l'échantillon.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'Échelon (Step Function)</h2>
            <p>Le perceptron utilise généralement une fonction d'activation simple, et la fonction d'échelon (step
                function) est fréquemment choisie pour cette tâche. </p>
            <h4>Définition</h4>
            <p>La fonction d'échelon attribue une sortie de 1 si la somme pondérée des entrées dépasse un certain seuil,
                et 0 sinon.</p>
            <p>\( f(x) = \begin{cases} 1 & \text{si } x \geq \text{seuil} \\ 0 & \text{sinon} \end{cases} \)</p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Perceptron</h2>
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">class</span> <span class="nc">Perceptron</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">taux_apprentissage</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">taux_apprentissage</span> <span class="o">=</span> <span class="n">taux_apprentissage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iterations</span> <span class="o">=</span> <span class="n">n_iterations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">poids</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biais</span> <span class="o">=</span> <span class="kc">None</span>

</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Perceptron</h2>
            <div class="highlight">
                <pre>
<span class="k">class</span> <span class="nc">Perceptron</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">ajuster</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">n_exemples</span><span class="p">,</span> <span class="n">n_caracteristiques</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">poids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_caracteristiques</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biais</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iterations</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_exemples</span><span class="p">):</span>
                <span class="n">ligne</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">y_calculé</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ligne</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">poids</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biais</span>
                <span class="n">prediction</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">y_calculé</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
                <span class="n">erreur</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">prediction</span>
                <span class="c1"># Mise à jour des poids et biais</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">poids</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">taux_apprentissage</span> <span class="o">*</span> <span class="n">erreur</span> <span class="o">*</span> <span class="n">ligne</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">biais</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">taux_apprentissage</span> <span class="o">*</span> <span class="n">erreur</span>

</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Perceptron</h2>
            <div class="highlight">
                <pre>

<span class="k">class</span> <span class="nc">Perceptron</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">predire</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_calculé</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">poids</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biais</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_calculé</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>2.2. Perceptron</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Perceptron</h2>
            <div class="highlight">
                <pre>
<span class="c1"># Données d&#39;exemple</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Création et entraînement du perceptron</span>
<span class="n">perceptron</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">taux_apprentissage</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">perceptron</span><span class="o">.</span><span class="n">ajuster</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Prédiction</span>
<span class="nb">print</span><span class="p">(</span><span class="n">perceptron</span><span class="o">.</span><span class="n">predire</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])))</span>  <span class="c1"># Sortie : [1 0]</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <p>Un MLP est composé de plusieurs couches de neurones. Chaque neurone dans une couche est connecté à tous
                les neurones de la couche suivante (d’où le nom de “réseau de neurones entièrement connecté”). Un MLP
                possède typiquement
            <p>
            <ul>
                <li>Une <strong>couche d’entrée</strong> : chaque neurone représente une caractéristique (feature) de
                    l’entrée.</li>
                <li>Une ou plusieurs <strong>couches cachées</strong> : ces couches permettent au modèle de capturer des
                    représentations plus abstraites.</li>
                <li>Une <strong>couche de sortie</strong> : elle donne la prédiction du réseau.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <p>Les poids \( W \) et les biais \( b \) sont initialisés aléatoirement pour chaque connexion entre les
                neurones. Par exemple, pour une couche \( l \) de \( n_l \) neurones connectée à la couche \( l+1 \) de
                \( n_{l+1} \) neurones :</p>
            <ul>
                <li>\( W^{(l)} \in \mathbb{R}^{n_l \times n_{l+1}} \) est la matrice des poids.</li>
                <li>\( b^{(l)} \in \mathbb{R}^{1 \times n_{l+1}} \) est le vecteur des biais.</li>
                <ul>


        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <div class="highlight">
                <pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Initialisation de la structure du réseau</span>
<span class="n">couches</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># exemple : 3 neurones d&#39;entrée, 2 couches cachées de 5 et 4 neurones, et 1 neurone de sortie</span>

<span class="c1"># Initialisation des poids et biais aléatoires</span>
<span class="n">poids</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">couches</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">couches</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">couches</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">biais</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">couches</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">couches</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Propagation Avant</h3>
            <p>La propagation avant consiste à faire passer les données d’entrée à travers chaque couche du réseau. À
                chaque neurone, on effectue une combinaison linéaire de ses entrées (poids * entrée + biais) suivie
                d’une <strong>fonction d’activation</strong>.</p>
            <p>Les fonctions d’activation courantes sont : - </p>
            <ul>
                <li><strong>Sigmoïde</strong> pour la sortie entre 0 et 1,</li>
                <li><strong>ReLU (Rectified Linear Unit)</strong> pour introduire de la non-linéarité.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Propagation Avant</h3>
            <p>La <strong>propagation avant</strong> est le processus par lequel les données d’entrée traversent les
                couches du réseau. À chaque neurone de la couche \( l \), on calcule une activation basée sur une
                combinaison linéaire des activations de la couche précédente, suivie d’une fonction d’activation.</p>
            <p>Soit \( a^{(l)} \) l’activation de la couche \( l \) : </p>
            <ol>
                <li><strong>Combinaison Linéaire</strong> : \[ z^{(l+1)} = a^{(l)} W^{(l)} + b^{(l)} \] où \( z^{(l+1)}
                    \) est l’entrée nette pour chaque neurone de la couche \( l+1 \).</li>
            </ol>

        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Propagation Avant</h3>
            <ol start="2">
                <li><strong>Application de la Fonction d’Activation</strong> :
                    <ul>
                        <li>Par exemple, pour la fonction d’activation ReLU utilisée dans les couches cachées : \[
                            a^{(l+1)} = \text{ReLU}(z^{(l+1)}) = \max(0, z^{(l+1)})
                            \]</li>
                        <li>Pour la couche de sortie, dans un problème de classification binaire, on utilise souvent la
                            fonction <strong>sigmoïde</strong> : \[
                            a^{(L)} = \sigma(z^{(L)}) = \frac{1}{1 + e^{-z^{(L)}}}
                            \] où \( L \) représente la dernière couche.</li>
                    </ul>
                </li>
            </ol>

        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <div class="highlight">
                <pre><span></span><span class="c1"># Fonction d&#39;activation Sigmoïde</span>
<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Fonction d&#39;activation ReLU</span>
<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <div class="highlight">
                <pre><span class="c1"># Propagation avant à travers le réseau</span>
<span class="k">def</span> <span class="nf">propagation_avant</span><span class="p">(</span><span class="n">entree</span><span class="p">,</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span><span class="p">):</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">entree</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">activation</span><span class="p">]</span>  <span class="c1"># stocke les activations de chaque couche pour le backprop</span>

    <span class="c1"># Propagation à travers chaque couche cachée</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poids</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">poids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">biais</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">activation</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>  <span class="c1"># on utilise ReLU pour les couches cachées</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>

    <span class="c1"># Couche de sortie (par ex., sigmoid pour une tâche de classification binaire)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">poids</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="n">biais</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">activations</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Calcul de l’erreur</h3>
            <p>Une fois les prédictions faites, il est essentiel de calculer l’erreur. Dans le cas de la classification
                binaire, la <strong>log-loss</strong> ou l’entropie croisée binaire est couramment utilisée. Cette étape
                permet de quantifier l’écart entre les prédictions et les vraies étiquettes.</p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Calcul de l’erreur</h3>
            <p>Pour évaluer la qualité des prédictions, on utilise une fonction de perte. Dans le cas d’une tâche de
                classification binaire, la <strong>log-loss</strong> (ou <strong>entropie croisée binaire</strong>) est
                souvent utilisée.</p>

            <p>Si \( y \) est la véritable étiquette et \( \hat{y} \) la prédiction, la perte pour un exemple est donnée
                par :
                \[
                \text{Perte} = - \left( y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right)
                \]</p>
            <p>Pour un ensemble de \( m \) exemples, la perte totale devient :
                \[
                \text{J}(W, b) = -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \cdot \log(\hat{y}^{(i)}) + (1 - y^{(i)})
                \cdot \log(1 - \hat{y}^{(i)}) \right)
                \]</p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <div class="highlight">
                <pre><span></span><span class="c1"># Fonction de perte (Binary Cross-Entropy)</span>
<span class="k">def</span> <span class="nf">calcul_perte</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_vrai</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">y_vrai</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">perte</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_vrai</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_vrai</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">))</span> <span class="o">/</span> <span class="n">m</span>
    <span class="k">return</span> <span class="n">perte</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation</h3>
            <p>La rétropropagation ajuste les poids et les biais en fonction de l’erreur obtenue. Elle utilise le
                <strong>gradient de la perte</strong> pour chaque paramètre du réseau, appliqué depuis la couche de
                sortie jusqu’à la couche d’entrée.
            </p>
            <ol type="1">
                <li><strong>Calcul du gradient pour la couche de sortie</strong>.</li>
                <li><strong>Propager les gradients en arrière</strong> à travers chaque couche cachée en utilisant la
                    dérivée des fonctions d’activation.</li>
            </ol>
            <p>La rétropropagation est utilisée pour calculer le gradient de la perte par rapport aux paramètres \( W \)
                et \( b \), et ajuster ces paramètres pour réduire l’erreur.</p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation: Calcul du Gradient pour la Couche de Sortie</h3>
            <ol>
                <li> Si \( L \) est la couche de sortie, l’erreur (ou delta) pour cette couche est :
                    \[
                    \delta^{(L)} = a^{(L)} - y
                    \]</li>
                <li>Le gradient de la perte par rapport aux poids de cette couche est alors :
                    \[
                    \frac{\partial J}{\partial W^{(L-1)}} = (a^{(L-1)})^T \delta^{(L)}
                    \]</li>
                <li>Et le gradient de la perte par rapport aux biais :
                    \[
                    \frac{\partial J}{\partial b^{(L-1)}} = \delta^{(L)}
                    \]</li>
            </ol>


        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation: Calcul des Gradients pour les Couches Cachées</h3>
            <ul>
                <li> Pour une couche \( l \), le delta se propage à partir de la couche suivante :
                    \[
                    \delta^{(l)} = (\delta^{(l+1)} W^{(l)}) \cdot f'(z^{(l)})
                    \]</li>
                où \( f'(z^{(l)}) \) est la dérivée de la fonction d’activation de la couche \( l \).
                <li>Le gradient par rapport aux poids de cette couche est :
                    \[
                    \frac{\partial J}{\partial W^{(l)}} = (a^{(l)})^T \delta^{(l+1)}
                    \]</li>
                <li>Le gradient par rapport aux biais est :
                    \[
                    \frac{\partial J}{\partial b^{(l)}} = \delta^{(l+1)}
                    \]</li>
            </ul>


        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation: Mise à Jour des Poids et des Biais</h3>
            <p> Les poids et les biais sont ajustés à chaque itération pour minimiser la perte en utilisant le
                **gradient descend** avec un taux d’apprentissage \( \alpha \) :</p>

            <p>\[
                W^{(l)} := W^{(l)} - \alpha \frac{\partial J}{\partial W^{(l)}}
                \]</p>
            <p>\[
                b^{(l)} := b^{(l)} - \alpha \frac{\partial J}{\partial b^{(l)}}
                \] </p>

        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation</h3>
            <div class="highlight">
                <pre><span></span><span class="c1"># Fonction de dérivée pour Sigmoïde et ReLU</span>
<span class="k">def</span> <span class="nf">derivee_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">derivee_relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation</h3>
            <div class="highlight">
                <pre><span></span><span class="c1"># Rétropropagation</span>
<span class="k">def</span> <span class="nf">retropropagation</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span><span class="p">,</span> <span class="n">y_vrai</span><span class="p">,</span> <span class="n">taux_apprentissage</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="c1"># Étape 1 : Calculer le gradient de la perte pour la couche de sortie</span>
    <span class="n">erreur</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_vrai</span>
    <span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">erreur</span> <span class="o">*</span> <span class="n">derivee_sigmoid</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>

    <span class="c1"># Étape 2 : Calculer les gradients pour chaque couche cachée</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poids</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">poids</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">derivee_relu</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">delta</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Rétropropagation</h3>
            <div class="highlight">
                <pre>
    <span class="c1"># Inverser les deltas pour qu&#39;ils correspondent à chaque couche du réseau</span>
    <span class="n">deltas</span> <span class="o">=</span> <span class="n">deltas</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Mise à jour des poids et biais</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">poids</span><span class="p">)):</span>
        <span class="n">poids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">taux_apprentissage</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">deltas</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">biais</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">taux_apprentissage</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">deltas</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Entraînement</h3>
            <p>L’entraînement du réseau consiste à itérer sur les étapes de propagation avant, de calcul de l’erreur, et
                de rétropropagation plusieurs fois (époques) pour ajuster les paramètres et minimiser l’erreur.</p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Entraînement</h3>
            <div class="highlight">
                <pre><span></span><span class="k">def</span> <span class="nf">entrainer_mlp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">couches</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">taux_apprentissage</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
    <span class="c1"># Initialisation des poids et biais</span>
    <span class="n">poids</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">couches</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">couches</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">couches</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
    <span class="n">biais</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">couches</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">couches</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="c1"># Boucle d&#39;entraînement</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="c1"># Propagation avant</span>
        <span class="n">activations</span> <span class="o">=</span> <span class="n">propagation_avant</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span><span class="p">)</span>
        <span class="c1"># Calcul de la perte</span>
        <span class="n">perte</span> <span class="o">=</span> <span class="n">calcul_perte</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># Rétropropagation</span>
        <span class="n">retropropagation</span><span class="p">(</span><span class="n">activations</span><span class="p">,</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">taux_apprentissage</span><span class="p">)</span>
        <span class="c1"># Afficher la perte à intervalles réguliers</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, Perte: </span><span class="si">{</span><span class="n">perte</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Prédiction</h3>
            <p>Après l’entraînement, le modèle peut être utilisé pour faire des prédictions sur de nouvelles données en
                effectuant simplement une propagation avant.</p>
            <p>Pour effectuer une prédiction après l'entraînement, on utilise simplement la propagation avant. La sortie
                de la couche finale \( a^{(L)} \) est interprétée en fonction de la tâche :</p>
            <p> Pour la classification binaire, on utilise une règle de décision, par exemple :
                \[
                \hat{y} = \begin{cases}
                1 & \text{si } a^{(L)} \geq 0.5 \\
                0 & \text{sinon}
                \end{cases}
                \]
            </p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Prédiction</h3>
            <div class="highlight">
                <pre><span></span><span class="c1"># Fonction de prédiction</span>
<span class="k">def</span> <span class="nf">predire</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span><span class="p">):</span>
    <span class="n">activations</span> <span class="o">=</span> <span class="n">propagation_avant</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">poids</span><span class="p">,</span> <span class="n">biais</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre>
            </div>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>2.3. Perceptron multicouches</h1>
        </div>
        <div class="content">
            <h3>Résumé des étapes</h3>
            <ol type="1">
                <li><strong>Initialisation</strong> : Créer et initialiser les poids et biais.</li>
                <li><strong>Propagation Avant</strong> : Calculer les activations de chaque couche.</li>
                <li><strong>Calcul de l’Erreur</strong> : Mesurer la différence entre la sortie prédite et la sortie
                    attendue.</li>
                <li><strong>Rétropropagation</strong> : Calculer les gradients et mettre à jour les poids et biais.</li>
                <li><strong>Entraînement</strong> : Répéter les étapes précédentes pour minimiser l’erreur.</li>
                <li><strong>Prédiction</strong> : Utiliser le réseau entraîné pour prédire les sorties de nouvelles
                    entrées.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Les couches</h2>
            <p>Les neurones sont organisés en couches. Il existe généralement trois types de couches dans un réseau de
                neurones :</p>
            <ul>
                <li><b>Couche d'Entrée (Input Layer)</b> : Cette couche reçoit les signaux initiaux ou les données en
                    entrée. Chaque neurone dans cette couche représente une caractéristique ou une variable d'entrée.
                </li>
                <li><b>Couches Cachées (Hidden Layers)</b> : Ces couches effectuent des transformations non linéaires
                    sur les entrées. Elles sont responsables de l'extraction et de la représentation des
                    caractéristiques importantes des données. Un réseau de neurones peut avoir une ou plusieurs couches
                    cachées.</li>
                <li><b>Couche de Sortie (Output Layer)</b> : Cette couche génère la sortie du réseau. Le nombre de
                    neurones dans cette couche dépend de la nature de la tâche, par exemple, une classification binaire
                    aurait un neurone de sortie, tandis qu'une classification multi-classes en aurait plusieurs.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Les couches</h2>
            <ul>
                <li><b>Transformations</b> : Chaque couche, y compris la couche d'entrée, effectue des transformations
                    sur les signaux qu'elle reçoit. Ces transformations sont déterminées par les poids des connexions
                    entre les neurones.</li>
                <li><b>Propagation des signaux</b> : Les signaux passent de la première couche (l'entrée) à la dernière
                    couche (la sortie) à travers les connexions pondérées entre les neurones. Ce processus est souvent
                    appelé la propagation avant (forward propagation). Pendant l'apprentissage, la rétropropagation
                    (backpropagation) est utilisée pour ajuster les poids afin de minimiser l'erreur de prédiction.</li>
                <li><b>Architecture</b> : La manière dont les couches sont organisées et connectées dans le réseau
                    constitue son architecture. Les réseaux de neurones peuvent avoir des architectures diverses, y
                    compris des réseaux profonds (avec de nombreuses couches cachées) ou des architectures plus simples.
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">L'entraînement</h2>
            <p>L'objectif global de l'entraînement est d'ajuster les poids du réseau de manière à ce qu'il puisse
                généraliser à de nouvelles données, produisant des résultats précis pour des exemples qu'il n'a pas vu
                pendant l'entraînement.</p>
            <ul>
                <li><b>Données d'entraînement</b> : Les réseaux neuronaux apprennent à partir d'exemples. Chaque exemple
                    se compose d'une "entrée" (les caractéristiques) et d'un "résultat" connu (l'étiquette ou la sortie
                    attendue).</li>
                <li><b>Calcul de l'erreur</b> : Lorsque le réseau produit une sortie pour une entrée donnée, l'erreur
                    est calculée en comparant cette sortie à la sortie cible (le résultat connu). Il existe différentes
                    mesures d'erreur, mais la somme des carrés des différences (Mean Squared Error, MSE) est couramment
                    utilisée.</li>
                <li><b>Rétropropagation (Backpropagation)</b> : Le réseau ajuste ses poids en utilisant la
                    rétropropagation. Cette technique minimise l'erreur en modifiant les poids à partir de la couche de
                    sortie jusqu'à la couche d'entrée. La règle de la chaîne du calcul différentiel est appliquée pour
                    propager l'erreur à travers le réseau.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">L'entraînement</h2>
            <ul>
                <li><b>Descente de gradient</b> : La règle d'apprentissage souvent utilisée pour ajuster les poids est
                    la descente de gradient. Elle utilise le gradient de l'erreur par rapport aux poids pour mettre à
                    jour les poids dans la direction qui minimise l'erreur.</li>
                <li><b>Itérations</b> : Le processus d'ajustement des poids en fonction de l'erreur est répété pour de
                    nombreux exemples du jeu de données d'entraînement. Chaque itération est appelée une "époque".
                    Plusieurs époques peuvent être nécessaires pour que le réseau converge vers un état où l'erreur est
                    suffisamment basse.</li>
                <li><b>Optimisation</b> : Différentes techniques d'optimisation peuvent être utilisées pour améliorer la
                    convergence du réseau, telles que l'ajustement adaptatif du taux d'apprentissage.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <ul>
                <li><b>Neurones</b> : Les neurones artificiels sont les unités de base d'un réseau de neurones. Chaque
                    neurone reçoit des signaux d'entrée, effectue un calcul sur ces signaux à l'aide d'une fonction
                    d'activation, et produit une sortie. Les neurones sont organisés en couches, à savoir la couche
                    d'entrée, les couches cachées, et la couche de sortie.</li>
                <li><b>Connexions et Poids</b> : Les connexions entre les neurones sont représentées par des poids.
                    Chaque connexion a un poids associé, qui détermine l'importance relative de cette connexion dans le
                    calcul du neurone de sortie. Pendant l'entraînement, ces poids sont ajustés pour minimiser l'erreur
                    de prédiction du réseau.</li>
                <li><b>Fonction de Propagation (Propagation avant)</b> : La fonction de propagation, également appelée
                    propagation avant, décrit le processus par lequel les signaux se propagent à travers le réseau
                    depuis la couche d'entrée jusqu'à la couche de sortie. Chaque neurone effectue une transformation
                    sur les signaux qu'il reçoit, et ces signaux modifiés sont transmis aux neurones de la couche
                    suivante.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Neurones</h2>
            <p>Chaque neurone artificiel a des entrées, qui peuvent être les valeurs caractéristiques d'un échantillon
                de données externe, et produit une seule sortie. Cette sortie peut être envoyée à plusieurs autres
                neurones, formant ainsi la structure interconnectée du réseau neuronal. La <b>fonction d'activation</b>
                joue un rôle crucial dans le calcul de la sortie d'un neurone. Le processus comprend les étapes
                suivantes :</p>
            <ul>
                <li><b>Somme pondérée</b> : Pour trouver la sortie du neurone, on prend la somme pondérée de tous les
                    intrants (entrées). Chaque entrée est multipliée par le poids correspondant à la connexion.</li>
                <li><b>Ajout d'un terme de biais</b> : Un terme de biais est ajouté à la somme pondérée. Le terme de
                    biais est un paramètre supplémentaire qui permet au modèle d'apprendre un décalage ou une
                    translation.</li>
                <li><b>Activation</b> : La somme pondérée, parfois appelée activation, est ensuite passée par une
                    fonction d'activation. Cette fonction est généralement non linéaire et introduit de la complexité
                    dans le modèle, permettant au réseau de capturer des relations non linéaires dans les données</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Connexions et poids</h2>
            <p>Le réseau de neurones est constitué de connexions, où chaque connexion transmet la sortie d'un neurone
                comme entrée à un autre neurone. Chaque connexion possède un poids qui représente son importance
                relative dans la transmission du signal.</p>
            <ul>
                <li>Un neurone donné peut avoir <b>plusieurs connexions d'entrée</b>, recevant des signaux de différents
                    neurones, et plusieurs connexions de sortie, transmettant des signaux à d'autres neurones. Les poids
                    associés à ces connexions permettent au réseau de moduler l'influence de chaque neurone sur les
                    autres, ajustant ainsi la force et la direction des signaux transmis à travers le réseau.</li>
                <li>Cette structure de connexion et de pondération est fondamentale dans le fonctionnement des réseaux
                    de neurones, car elle permet au réseau d'apprendre des représentations complexes des données et
                    d'ajuster ses paramètres pendant l'entraînement pour accomplir des tâches spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Fonction de propagation</h2>
            <p><b>Calcul de l'entrée d'un neurone</b> : La fonction de propagation calcule l'entrée d'un neurone en
                prenant la somme pondérée des sorties de ses prédécesseurs, où chaque sortie est multipliée par le poids
                de la connexion correspondante. Cela peut être représenté mathématiquement comme suit :</p>
            <p>\[ \text{Entrée du Neurone} = \sum_{i=1}^{n} (\text{Sortie du Prédécesseur}_i \times \text{Poids}_i) \]
                où \(n\) est le nombre de connexions d'entrée.</p>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Fonction de propagation</h2>
            <p><b>Ajout d'un terme de biais</b> : Un terme de biais peut être ajouté au résultat de la propagation. Le
                terme de biais est un paramètre supplémentaire, souvent représenté par \(b\) dans les équations, qui
                permet au modèle d'apprendre un décalage ou une translation. Cela donne la forme finale de l'entrée du
                neurone :</p>

            <p>\[ \text{Entrée du Neurone} = \sum_{i=1}^{n} (\text{Sortie du Prédécesseur}_i \times \text{Poids}_i) +
                \text{Biais} \]</p>

        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
            <h2 class="topicsubheading">Fonction de propagation</h2>
            <p><b>Fonction d'Activation</b> : Après avoir calculé l'entrée du neurone, celle-ci est passée à travers une
                fonction d'activation. Cette fonction introduit une non-linéarité dans le modèle, permettant au réseau
                de neurones de capturer des relations complexes et d'apprendre des modèles non linéaires. Certaines des
                fonctions d'activation couramment utilisées comprennent :</p>
            <ul>
                <li><b>Sigmoïde</b> : \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
                <li><b>Tangente hyperbolique (tanh)</b> : \( \text{tanh}(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \)
                </li>
                <li><b>ReLU (Rectified Linear Unit)</b> : \( \text{ReLU}(x) = \max(0, x) \)</li>
                <li><b>Softmax</b> (pour la couche de sortie dans la classification) : \( \text{Softmax}(x)_i =
                    \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: fonction d'identité</h2>
            <h4>Équation</h4>
            <p>\[f(x)=x\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=1\]</p>
            <h3></h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_identity.svg"
                    height="380px" />
                <figcaption>Fonction d'identité</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Fonction d'activation: pas binaire</h2>
                <h4>Équation</h4>
                <p>\[f(x) = \begin{cases} 0 & \text{for } x
                    < 0\\ 1 & \text{for } x \ge 0 \end{cases} \]</p>
                        <h4>Dérivée</h4>
                        <p>\[f'(x) = \begin{cases} 0 & \text{for } x \ne 0\\ ? & \text{for } x = 0\end{cases}\]</p>
                        <figure>
                            <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_binary_step.svg"
                                height="380px" />
                            <figcaption>Pas binaire</figcaption>
                        </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: fonction sigmoïde</h2>
            <h4>Équation</h4>
            <p>\[f(x)=\sigma(x)=\frac{1}{1+e^{-x}}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=f(x)(1-f(x))\]</p>
            <figure>
                <img src="../../2021/MachineLearning/Logistic-curve.svg" height="380px" />
                <figcaption>La fonction sigmoïde</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: TanH</h2>
            <h4>Équation</h4>
            <p>\[f(x)=\tanh(x)=\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=1-f(x)^2\]</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_tanh.svg" height="380px" />
                <figcaption>TanH</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: Rectified linear unit: ReLU</h2>
            <h4>Équation</h4>
            <p>\[f(x) = \begin{cases} 0 & \text{for } x \le 0\\ x & \text{for } x > 0\end{cases} = \max\{0,x\}= x
                \textbf{1}_{x>0}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x) = \begin{cases} 0 & \text{for } x \le 0\\ 1 & \text{for } x > 0\end{cases}\]</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_rectified_linear.svg"
                    height="380px" />
                <figcaption>Unité linéaire rectifiée (ReLU)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Fonction d'activation: Gaussien</h2>
            <h4>Équation</h4>
            <p>\[f(x)=e^{-x^2}\]</p>
            <h4>Dérivée</h4>
            <p>\[f'(x)=-2xe^{-x^2}\]</p>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_gaussian.svg"
                    height="380px" />
                <figcaption>Gaussien</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>2.4. Réseaux de neurones artificiels</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Perceptron multiclasse</h2>
            <ul>
                <li>Perceptron peut être généralisé à la classification multiclasse. </li>
                <li>Une fonction de représentation d'élément \(f( x , y )\) fait correspondre chaque paire
                    d'entrée/sortie possible à un vecteur d'élément à valeur réelle en dimension finie.</li>
                <li>le vecteur de caractéristique est multiplié par un vecteur de poids \(w\), mais le score obtenu est
                    maintenant utilisé pour choisir parmi de nombreux résultats possibles : \[\hat y =
                    \operatorname{argmax}_y f(x,y) \cdot w.\]</li>
                <li>La réapprentissage se fait par itération sur les exemples, en prédisant un résultat pour chacun, en
                    laissant les poids inchangés lorsque le résultat prédit correspond à l'objectif, et en les modifiant
                    lorsqu'il ne correspond pas. La mise
                    à jour devient : \[w_{t+1} = w_t + f(x, y) - f(x,\hat y)\].</li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h1>Articles de recherche</h1>
            <ul>
                <li>[Aly 2005] Aly, Mohamed. Survey on Multiclass Classification Methods. 2005.</li>
                <li>[Jaakkola 2019] Jaakkola, H., et al. “Artificial Intelligence Yesterday, Today and Tomorrow.” 2019
                    42nd International Convention on Information and Communication Technology, Electronics and
                    Microelectronics (MIPRO), 2019, pp. 860–67. IEEE
                    Xplore
                </li>
                <li>[Pan 2016] Pan, Yunhe, “Heading toward Artificial Intelligence 2.0.” Engineering, vol. 2, no. 4,
                    Dec. 2016, pp. 409–13. www.sciencedirect.com,</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Web</h1>
            <ol>
                <li>Google acquiert DNNresearch, spécialisé dans les réseaux de neurones profonds: <a
                        href="https://www.lemondeinformatique.fr/actualites/lire-google-acquiert-dnnresearch-specialise-dans-les-reseaux-de-neurones-profonds-52829.html">https://www.lemondeinformatique.fr/actualites/lire-google-acquiert-dnnresearch-specialise-dans-les-reseaux-de-neurones-profonds-52829.html</a>
                </li>
                <li>Pourquoi Microsoft rachète Linkedin: <a
                        href="https://www.lemondeinformatique.fr/actualites/lire-pourquoi-microsoft-rachete-linkedin-65136.html">https://www.lemondeinformatique.fr/actualites/lire-pourquoi-microsoft-rachete-linkedin-65136.html</a>
                </li>
                <li>Scikit-learn: <a href="http://scikit-learn.org/stable/">http://scikit-learn.org/stable/</a></li>
                <li>Perceptron: <a
                        href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Wikipédia</h1>
            <ul>
                <li>Perceptron: <a
                        href="https://en.wikipedia.org/wiki/Perceptron">https://en.wikipedia.org/wiki/Perceptron</a>
                </li>
                <li>Multiclass Classification: <a
                        href="https://en.wikipedia.org/wiki/Multiclass_classification">https://en.wikipedia.org/wiki/Multiclass_classification</a>
                </li>
                <li>Multilayer Perceptron: <a
                        href="https://en.wikipedia.org/wiki/Multilayer_perceptron">https://en.wikipedia.org/wiki/Multilayer_perceptron</a>
                </li>
                <li>Feedforward Neural Network: <a
                        href="https://en.wikipedia.org/wiki/Feedforward_neural_network">https://en.wikipedia.org/wiki/Feedforward_neural_network</a>
                </li>
                <li>Recurrent Neural Network: <a
                        href="https://en.wikipedia.org/wiki/Recurrent_neural_network">https://en.wikipedia.org/wiki/Recurrent_neural_network</a>
                </li>
                <li>Long Short-Term Memory: <a
                        href="https://en.wikipedia.org/wiki/Long_short-term_memory">https://en.wikipedia.org/wiki/Long_short-term_memory</a>
                </li>
                <li>Activation Function: <a
                        href="https://en.wikipedia.org/wiki/Activation_function">https://en.wikipedia.org/wiki/Activation_function</a>
                </li>
                <li>Logique et Raisonnement Mathématique: <a
                        href="https://fr.wikipedia.org/wiki/Logique_et_raisonnement_math%C3%A9matique">https://fr.wikipedia.org/wiki/Logique_et_raisonnement_math%C3%A9matique</a>
                </li>
                <li>Représentation des Connaissances: <a
                        href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                </li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>

    <section class="slide" id="slide63">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Wikipédia</h1>
            <ul>
                <li>Agent Intelligent: <a
                        href="https://fr.wikipedia.org/wiki/Agent_intelligent">https://fr.wikipedia.org/wiki/Agent_intelligent</a>
                </li>
                <li>Calcul des Propositions: <a
                        href="https://fr.wikipedia.org/wiki/Calcul_des_propositions">https://fr.wikipedia.org/wiki/Calcul_des_propositions</a>
                </li>
                <li>Calcul des Prédicats: <a
                        href="https://fr.wikipedia.org/wiki/Calcul_des_pr%C3%A9dicats">https://fr.wikipedia.org/wiki/Calcul_des_pr%C3%A9dicats</a>
                </li>
                <li>Logique Modale: <a
                        href="https://fr.wikipedia.org/wiki/Logique_modale">https://fr.wikipedia.org/wiki/Logique_modale</a>
                </li>
                <li>Raisonnement Automatisé: <a
                        href="https://fr.wikipedia.org/wiki/Raisonnement_automatis%C3%A9">https://fr.wikipedia.org/wiki/Raisonnement_automatis%C3%A9</a>
                </li>
                <li>Connaissance: <a
                        href="https://fr.wikipedia.org/wiki/Connaissance">https://fr.wikipedia.org/wiki/Connaissance</a>
                </li>
                <li>Gestion des connaissances: <a
                        href="https://fr.wikipedia.org/wiki/Gestion_des_connaissances">https://fr.wikipedia.org/wiki/Gestion_des_connaissances</a>
                </li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>Références:</h1>
        </div>
        <div class="content">
            <h1>Couleurs</h1>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h1>Images</h1>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Apprentissage machine | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 64) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
</body>

</html>