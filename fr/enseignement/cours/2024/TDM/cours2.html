<html>

<head>
    <meta charset="utf-8" />
    <title>Traitement de données massives (2024-2025): Traitement de données: John Samuel</title>
    <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
    <style type="text/css">
        body {
            height: 100%;
            width: 100%;
            background-color: white;
            margin: 0;
            overflow: hidden;
            font-family: Arial;
        }

        .slide {
            height: 100%;
            width: 100%;
        }

        .content {
            height: 79%;
            width: 95vw;
            display: flex;
            line-height: 1.7em;
            flex-direction: column;
            align-items: flex-start;
            margin: 0 auto;
            color: #000000;
            text-align: left;
            padding-left: 1.5vmax;
            padding-top: 1.5vmax;
            overflow-x: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content h1,
        h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content .topichighlight {
            background-color: #78002E;
            color: #FFFFFF;
        }

        .content .topicheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 2vmax 2vmax 0%;
            height: 4vmax;
            line-height: 4vmax;
            padding-left: 1vmax;
            margin: 0.1vmax;
            width: 50%;
            margin-bottom: 1vmax;
        }

        .content .flexcontent {
            display: flex;
            overflow-y: auto;
            font-size: 2.8vmin;
            flex-wrap: wrap;
        }

        .content .gridcontent {
            display: grid;
            grid-template-columns: auto auto auto auto;
            grid-column-gap: 0px;
            grid-row-gap: 0px;
            grid-gap: 0px;
        }

        .content .topicsubheading {
            background-color: #1B80CF;
            color: #FFFFFF;
            vertical-align: middle;
            border-radius: 0 1.5vmax 1.5vmax 0%;
            height: 3vmax;
            margin: 0.1vmax;
            font-size: 90%;
            line-height: 3vmax;
            padding-left: 1vmax;
            width: 40%;
            margin-bottom: 1vmax;
        }

        .content table {
            color: #000000;
            font-size: 100%;
            width: 100%;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
            text-decoration: none;
        }

        .content th {
            color: #FFFFFF;
            background-color: #1B80CF;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
            font-size: 120%;
            padding: 15px;
        }

        .content figure {
            max-width: 90%;
            max-height: 90%;
        }

        .content .fullwidth img {
            max-width: 90%;
            max-height: 90%;
        }

        .content figure img {
            max-width: 50vmin;
            max-height: 50vmin;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        .content figure figcaption {
            max-width: 90%;
            max-height: 90%;
            margin: 0.1vmax;
            font-size: 90%;
            text-align: center;
            padding: 0.5vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content td {
            color: #000000;
            width: 8%;
            padding-left: 3vmax;
            padding-top: 1vmax;
            padding-bottom: 1vmax;
            background-color: #E1F5FE;
            border-radius: 2vmax 2vmax 2vmax 2vmax;
        }

        .content li {
            line-height: 1.7em;
        }

        .header {
            color: #ffffff;
            background-color: #00549d;
            height: 5vmax;
        }

        .header h1 {
            text-align: center;
            vertical-align: middle;
            font-size: 3vmax;
            line-height: 4vmax;
            margin: 0;
        }

        .footer {
            height: 3vmax;
            line-height: 3vmax;
            vertical-align: middle;
            color: #ffffff;
            background-color: #00549d;
            margin: 0;
            padding: .3vmax;
            overflow: hidden;
        }

        .footer .contact {
            float: left;
            color: #ffffff;
            text-align: left;
            font-size: 3.2vmin;
        }

        .footer .navigation {
            float: right;
            text-align: right;
            width: 8vw;
            font-size: 3vmin;
        }

        .footer .navigation .next,
        .prev {
            font-size: 3vmin;
            color: #ffffff;
            text-decoration: none;
        }

        .footer .navigation .next::after {
            content: "| >";
        }

        .footer .navigation .prev::after {
            content: "< ";
        }

        /* Using same Jupyter CSS
     */

        .highlight {
            background: #f8f8f8;
        }

        .highlight .c {
            color: #408080;
            font-style: italic
        }

        /* Comment */

        .highlight .err {
            border: 1px solid #FF0000
        }

        /* Error */

        .highlight .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */

        .highlight .o {
            color: #666666
        }

        /* Operator */

        .highlight .ch {
            color: #408080;
            font-style: italic
        }

        /* Comment.Hashbang */

        .highlight .c1 {
            color: #408080;
            font-style: italic
        }

        /* Comment.Single */

        .highlight .cs {
            color: #408080;
            font-style: italic
        }

        /* Comment.Special */

        .highlight .cm {
            color: #408080;
            font-style: italic
        }

        /* Comment.Multiline */

        .highlight .nn {
            color: #0000FF;
            font-weight: bold
        }

        /* Name.Namespace */

        .highlight .k {
            color: #008000;
            font-weight: bold
        }

        /* Keyword */

        .highlight .s2 {
            color: #BA2121
        }

        /* Literal.String.Double */

        .highlight .s1 {
            color: #BA2121
        }

        /* Literal.String.Single */

        .highlight .kn {
            color: #008000;
            font-weight: bold
        }

        /* Keyword.Namespace */

        .highlight .nb {
            color: #008000
        }

        /* Name.Builtin */

        .highlight .mb {
            color: #666666
        }

        /* Literal.Number.Bin */

        .highlight .mf {
            color: #666666
        }

        /* Literal.Number.Float */

        .highlight .mh {
            color: #666666
        }

        /* Literal.Number.Hex */

        .highlight .mi {
            color: #666666
        }

        /* Literal.Number.Integer */

        .highlight .mo {
            color: #666666
        }

        /* Literal.Number.Oct */

        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                width: 100%;
                height: 92%;
                display: flex;
                flex-direction: row;
                text-align: left;
                padding: 1vw;
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                flex-wrap: wrap;
            }

            .content .topicheading {
                width: 90%;
            }

            .content h1,
            h2,
            h3,
            h4 {
                width: 100%;
            }

            .content figure img {
                max-width: 80vmin;
                max-height: 50vmin;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
            }
        }

        @media print {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            .content {
                font-size: 2.8vmin;
            }

            .content .flexcontent {
                font-size: 2.5vmin;
            }
        }
    </style>
    <script src="../../../../../fr/enseignement/cours/2020/MachineLearning/tex-mml-chtml.js"
        id="MathJax-script"></script>
</head>

<body>
    <section class="slide" id="slide1">
        <div class="header">
        </div>
        <div class="content">
            <h1 style="font-size:3.5vw">Traitement de données massives</h1>
            <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                <b>Année</b>: 2024-2025<br />
                <b>Courriel</b>: john.samuel@cpe.fr<br /><br />
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                        alt="Creative Commons License" style="border-width:0"
                        src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">1

                <a class="next" href="#slide2"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide2">
        <div class="header">
            <h1>Data Mining</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Objectifs</h3>
            <ol>
                <li>Régularités</li>
                <li>Exploration des données</li>
                <li>Algorithmes</li>
                <li>Sélection de caractéristiques</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">2
                <a class="prev" href="#slide1"></a>
                <a class="next" href="#slide3"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide3">
        <div class="header">
            <h1>2.1. Régularités</h1>
        </div>
        <div class="content">
            <figure class="flexcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg"
                    height="300px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg"
                    height="300px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg"
                    height="300px" width="300px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG"
                    height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">3
                <a class="prev" href="#slide2"></a>
                <a class="next" href="#slide4"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide4">
        <div class="header">
            <h1>2.1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Régularités naturelles</h1>
            <ul>
                <li>Symétrie</li>
                <li>Arbres, fractales</li>
                <li>Spirales</li>
                <li>Chaos</li>
                <li>Ondes</li>
                <li>Bulles, mousse</li>
                <li>Pavages</li>
                <li>Ruptures</li>
                <li>Taches, bandes</li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Kittyply_edit1.jpg"
                    height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG"
                    height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg"
                    height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Rio_Negro_meanders.JPG"
                    height="150px" width="150px" />
            </figure>
            <Régularitésfigure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg"
                    height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Apis_florea_nest_closeup2.jpg"
                    height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg"
                    height="150px" width="150px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Equus_grevyi_(aka).jpg"
                    height="150px" width="150px" />
                </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">4
                <a class="prev" href="#slide3"></a>
                <a class="next" href="#slide5"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide5">
        <div class="header">
            <h1>2.1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Créations humaines</h1>
            <ul>
                <li><b>Bâtiments (Symétrie)</b> : Structures construites par l'homme avec des motifs de symétrie.
                    Exemple : Cathédrales gothiques, gratte-ciels modernes.</li>
                <li><b>Villes</b> : Agglomérations planifiées ou organiques habitées par les humains.
                    Exemple : Paris, New York.</li>
                <li><b>Environnement virtuel (e.g., jeux de vidéo)</b> : Espaces créés numériquement pour l'interaction
                    humaine.
                    Exemple : Mondes ouverts dans les jeux vidéo, simulations virtuelles.</li>
                <li><b>Les artefacts humains</b> : Objets fabriqués par les humains dans divers domaines.
                    Exemple : Outils préhistoriques, œuvres d'art contemporaines.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Roman_geometric_mosaic.jpg"
                    height="150px" width="600px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">5
                <a class="prev" href="#slide4"></a>
                <a class="next" href="#slide6"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide6">
        <div class="header">
            <h1>2.1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Création</h1>
            <ul>
                <li>Répétition</li>
                <li><b>Fractales</b> : Structures mathématiques auto-similaires à différentes échelles.
                    <ul>
                        <li>Ensemble de Julia: Un ensemble fractal défini par une fonction itérative <i>f(z) =
                                z<sup>2</sup> + c</i></li>
                        <li><b>Caractéristiques</b> : produit des motifs répétitifs complexes lorsqu'il est visualisé.
                        </li>
                    </ul>
                </li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/400px-Tiling_Dual_Semiregular_V3-3-3-3-6_Floret_Pentagonal.svg.png"
                    height="200px" width="200px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Finite_subdivision_of_a_radial_link.png"
                    height="200px" width="200px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Julia_set_(indigo).png"
                    height="200px" width="200px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">6
                <a class="prev" href="#slide5"></a>
                <a class="next" href="#slide7"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide7">
        <div class="header">
            <h1>2.1. Régularités</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Synonymes</h1>
            <ul>
                <li>Fouille de données</li>
                <li>Forage de données</li>
                <li>Extraction de connaissances à partir de données</li>
                <li>Data mining</li>
                <li>Machine learning</li>
                <li>Apprentissage automatique</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">7
                <a class="prev" href="#slide6"></a>
                <a class="next" href="#slide8"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide8">
        <div class="header">
            <h1>2.1.2. Approches de l'apprentissage machine</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Approches</h3>
            <ul>
                <li><b>Apprentissage supervisé</b> : Le modèle est entraîné sur un ensemble de données étiquetées où les
                    exemples d'entrée sont associés à des sorties désirées. Le modèle apprend à faire des prédictions
                    sur de nouvelles données en se basant sur ces associations.</li>
                <li><b>Apprentissage non supervisé</b> : Le modèle est exposé à des données non étiquetées et cherche à
                    découvrir des modèles, des structures ou des relations intrinsèques dans les données.</li>
                <li><b>Apprentissage semi-supervisé</b> : Une combinaison des deux précédents, utilisant à la fois des
                    données étiquetées et non étiquetées pour l'entraînement.</li>
                <li><b>Apprentissage par renforcement</b> : Le modèle apprend à prendre des décisions en interagissant
                    avec son environnement. Il reçoit des récompenses ou des pénalités en fonction de ses actions, ce
                    qui guide son apprentissage.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">8
                <a class="prev" href="#slide7"></a>
                <a class="next" href="#slide9"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide9">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation</h3>
            <ul>
                <li><b>Vecteur euclidien</b>:
                    <ul>
                        <li>Un vecteur euclidien est un objet géométrique caractérisé par sa magnitude (longueur) et sa
                            direction. </li>
                        <li>Les vecteurs euclidiens sont couramment utilisés pour représenter des données sous forme de
                            points dans un espace multidimensionnel, où chaque dimension correspond à une
                            caractéristique ou une variable.</li>
                    </ul>
                </li>
                <li><b>Espace vectoriel</b>:
                    <ul>
                        <li>Un espace vectoriel est une collection de vecteurs qui peuvent être additionnés entre eux et
                            multipliés par des nombres (scalaires).</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">9
                <a class="prev" href="#slide8"></a>
                <a class="next" href="#slide10"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide10">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation</h3>
            <ul>
                <li><b>Vecteur de caractéristiques (features)</b>:
                    <ul>
                        <li>Un vecteur de caractéristiques est un vecteur n-dimensionnel qui représente les
                            caractéristiques ou les attributs d'une entité. </li>
                    </ul>
                </li>
                <li><b>Espace de caractéristiques</b>:
                    <ul>
                        <li>L'espace de caractéristiques est l'espace vectoriel associé aux vecteurs de
                            caractéristiques.</li>
                        <li>Chaque dimension de cet espace représente une caractéristique particulière, et les vecteurs
                            sont utilisés pour positionner les données dans cet espace en fonction de leurs
                            caractéristiques.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">10
                <a class="prev" href="#slide9"></a>
                <a class="next" href="#slide11"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide11">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Exemples de caractéristiques</h3>
            <ul>
                <li><b>Images</b>: Dans le contexte des images, les vecteurs de caractéristiques peuvent être construits
                    à partir des valeurs des pixels. Chaque pixel peut être considéré comme une dimension, et un vecteur
                    de caractéristiques contiendra les valeurs de tous les pixels, permettant ainsi de représenter une
                    image sous forme de vecteur.</li>
                <li><b>Textes</b>: Pour les textes, les vecteurs de caractéristiques sont souvent construits à partir de
                    la fréquence d'apparition des mots, des phrases, ou des tokens dans un document. Cela permet de
                    représenter le contenu textuel en utilisant des valeurs numériques, ce qui est essentiel pour
                    l'analyse de texte et la recherche d'informations.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">11
                <a class="prev" href="#slide10"></a>
                <a class="next" href="#slide12"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide12">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation</h1>
            <ul>
                <li><b>Construction de caractéristiques<sup>1</sup></b>:
                    <ul>
                        <li>La construction de caractéristiques consiste à créer de nouvelles variables ou attributs à
                            partir de celles déjà présentes dans les données.</li>
                        <li>Cette étape peut être cruciale pour améliorer les performances des modèles d'apprentissage
                            machine en introduisant des informations pertinentes et en éliminant du bruit.</li>
                    </ul>
                </li>
                <li><b>Opérateurs de construction pour les caractéristiques</b>
                    <ul>
                        <li>Les opérateurs de construction sont des fonctions ou des opérations mathématiques qui
                            permettent de créer de nouvelles caractéristiques à partir de celles existantes. </li>
                        <li>Parmi les opérateurs couramment utilisés, on trouve les opérateurs d'égalité (comparaisons),
                            les opérateurs arithmétiques (addition, soustraction, multiplication, division), les
                            opérateurs de tableau (min, max, moyenne, médiane, etc.), les fonctions de transformation,
                            etc.</li>
                    </ul>
                </li>
            </ul>
            <ol style="font-size:2vh">
                <li>https://en.wikipedia.org/wiki/Feature_vector</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">12
                <a class="prev" href="#slide11"></a>
                <a class="next" href="#slide13"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide13">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Exemple</h3>
            <ul>
                <li>Soit <b>Année de naissance</b> et <b>Année de décès</b> deux caractéristiques existantes.</li>
                <li>Une nouvelle caractéristique appelée <b>âge</b> est créée. <b>âge</b> = <b>Année de décès</b> -
                    <b>Année de naissance</b>
                </li>
            </ul>
            <p>La construction de caractéristiques est une étape essentielle dans le pipeline de prétraitement des
                données en apprentissage machine, car elle peut aider à rendre les données plus informatives pour les
                algorithmes d'apprentissage.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">13
                <a class="prev" href="#slide12"></a>
                <a class="next" href="#slide14"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide14">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                <ul>
                    <li><b>Le nombre d'exemples d'entraînement (N)</b> : Cela représente la quantité d'exemples de
                        données que vous avez pour entraîner un modèle supervisé. Chaque exemple d'entraînement se
                        compose d'un vecteur de caractéristiques (x) et de son label (y).</li>
                    <li><b>L'espace de saisie des caractéristiques (X)</b> : C'est l'ensemble de toutes les combinaisons
                        possibles de vecteurs de caractéristiques qui peuvent être utilisées comme entrée pour le
                        modèle. Cet espace est défini par les caractéristiques que vous avez extraites des données.</li>
                    <li><b>L'espace des caractéristiques de sortie (Y)</b> : Il représente l'ensemble de toutes les
                        valeurs possibles que peuvent prendre les étiquettes ou les labels. </li>
                    <li><b>Exemples d'entraînement (D)</b> : C'est votre ensemble de données d'entraînement, composé de
                        paires (x, y) où x est le vecteur de caractéristiques et y est le label correspondant.</li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">14
                <a class="prev" href="#slide13"></a>
                <a class="next" href="#slide15"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide15">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                <ul>
                    <li><b>Objectif de l'algorithme d'apprentissage supervisé</b> : Il s'agit de trouver une fonction
                        (g) qui associe un vecteur de caractéristiques (x) à un label (y). L'ensemble des fonctions
                        possibles est appelé espace des hypothèses (G). L'objectif est de choisir la fonction (g) qui
                        minimise l'erreur de prédiction sur les exemples d'entraînement et généralise bien sur de
                        nouvelles données.</li>
                    <li><b>Fonction d'évaluation (F)</b> : Elle indique l'espace des fonctions d'évaluation utilisées
                        pour évaluer la performance des fonctions hypothétiques. L'objectif est de trouver la fonction
                        (g) qui renvoie la fonction d'évaluation (f) la plus élevée, c'est-à-dire celle qui donne les
                        prédictions les plus précises.</li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">15
                <a class="prev" href="#slide14"></a>
                <a class="next" href="#slide16"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide16">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                <p>Cette formalisation est au cœur de l'apprentissage supervisé, où l'objectif est d'apprendre à partir
                    d'exemples étiquetés et de trouver une fonction qui puisse prédire de manière précise les étiquettes
                    pour de nouvelles données non vues.</p>
                <ul>
                    <li>Soit \(N\) le nombre d'exemples d'entraînement</li>
                    <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                    <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                    <li>Soit \({(x_1, y_1),...,(x_N, y_N)}\) les \(N\) exemples d'entraînement, où
                        <ul>
                            <li>\(x_i\) est le vecteur de caractéristiques de <i>i<sup>ème</sup></i> exemple
                                d'entraînement.
                            </li>
                            <li>\(y_i\) est son label.</li>
                        </ul>
                    </li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">16
                <a class="prev" href="#slide15"></a>
                <a class="next" href="#slide17"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide17">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation: Apprentissage supervisé</h1>
                <ul>
                    <li>L'objectif de l'algorithme d'apprentissage supervisé est de trouver \(g: X &#8594; Y\), où
                        <ul>
                            <li><i>g</i> est l'une des fonctions de l'ensemble des fonctions possibles <i>G</i> (espace
                                des
                                hypothèses)</li>
                        </ul>
                    </li>
                    <li><b>Fonction d'évaluation <i>F</i></b> indiquent l'espace des fonctions d'évaluation, où
                        <ul>
                            <li>\(f: X &#215; Y &#8594; R\) telle que <i>g</i> renvoie la fonction d'évaluation la plus
                                élevée.</li>
                        </ul>
                    </li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">17
                <a class="prev" href="#slide16"></a>
                <a class="next" href="#slide18"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide18">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation: Apprentissage non supervisé</h1>
            <ul>
                <li><b>L'espace de saisie des caractéristiques (X)</b> : C'est l'ensemble de toutes les combinaisons
                    possibles de vecteurs de caractéristiques qui peuvent être utilisées comme entrée pour le modèle en
                    apprentissage non supervisé. Cet espace est défini par les caractéristiques que vous avez extraites
                    des données.</li>
                <li><b>L'espace des caractéristiques de sortie (Y)</b> : Il représente l'ensemble des caractéristiques
                    de sortie potentielles. Contrairement à l'apprentissage supervisé, en apprentissage non supervisé, Y
                    ne consiste pas en des étiquettes ou des labels prédéfinis, mais plutôt en des transformations, des
                    représentations, ou des caractéristiques extraites des données d'entrée.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">18
                <a class="prev" href="#slide17"></a>
                <a class="next" href="#slide19"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide19">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation: Apprentissage non supervisé</h1>
            <ul>
                <li><b>Objectif de l'algorithme d'apprentissage non supervisé</b> : L'objectif est de trouver une
                    correspondance entre l'espace de saisie des caractéristiques (X) et l'espace des caractéristiques de
                    sortie (Y). Cela peut impliquer diverses tâches, telles que la réduction de la dimensionnalité, la
                    classification automatique de données non étiquetées, la détection d'anomalies, la segmentation, ou
                    la représentation latente des données.</li>
                <li><b>Mise en correspondance X → Y</b> : Cette mise en correspondance peut être réalisée de différentes
                    manières, selon la tâche d'apprentissage non supervisé spécifique. Par exemple, dans la réduction de
                    la dimensionnalité, X peut être une représentation à haute dimension des données, tandis que Y
                    représente la version réduite de ces données, souvent avec moins de dimensions.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">19
                <a class="prev" href="#slide18"></a>
                <a class="next" href="#slide20"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide20">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation: Apprentissage non supervisé</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>L'objectif de l'algorithme d'apprentissage non supervisé est
                    <ul>
                        <li>trouver la mise en correspondance \(X &#8594; Y\)</li>
                    </ul>
                </li>
            </ul>
            <p>L'apprentissage non supervisé est utilisé pour explorer et découvrir des modèles, des structures ou des
                caractéristiques inhérentes aux données, sans l'utilisation d'étiquettes ou de labels préalables. Il est
                couramment utilisé dans des domaines tels que la clustering, l'analyse de composantes principales (PCA),
                l'analyse en composantes indépendantes (ICA), et bien d'autres.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">20
                <a class="prev" href="#slide19"></a>
                <a class="next" href="#slide21"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide21">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation: Apprentissage semi-supervisé</h1>
            <ul>
                <li><b>L'espace de saisie des caractéristiques (X)</b> : Il s'agit de l'ensemble de toutes les
                    combinaisons possibles de vecteurs de caractéristiques qui peuvent être utilisés comme entrée pour
                    le modèle en apprentissage semi-supervisé.</li>
                <li><b>L'espace des caractéristiques de sortie (Y)</b> : Il représente l'ensemble des caractéristiques
                    de sortie potentielles, mais contrairement à l'apprentissage supervisé, il n'est pas nécessairement
                    constitué d'étiquettes ou de labels prédéfinis.</li>
                <li><b>Ensemble d'exemples d'exercices étiquetés (l)</b> : Cela correspond à un sous-ensemble d'exemples
                    qui ont été annotés ou étiquetés avec des valeurs de sortie connues.</li>
                <li><b>Ensembles des vecteurs de caractéristiques non étiquetées (u)</b> : Il s'agit des exemples non
                    étiquetés, où les valeurs de sortie ne sont pas connues.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">21
                <a class="prev" href="#slide20"></a>
                <a class="next" href="#slide22"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide22">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation: Apprentissage semi-supervisé</h1>
            <ul>
                <li><b>Objectif de l'algorithme d'apprentissage semi-supervisé</b> : L'objectif principal est de trouver
                    des étiquettes correctes pour les exemples non étiquetés (apprentissage transductif), ainsi que de
                    trouver la bonne mise en correspondance entre les caractéristiques d'entrée et les caractéristiques
                    de sortie (apprentissage inductif).
                    <ul>
                        <li><b>Apprentissage transductif</b> : Il s'agit de trouver des étiquettes correctes pour les
                            exemples non étiquetés. Cela revient à prédire les valeurs de sortie pour les exemples non
                            étiquetés sans nécessairement chercher à généraliser à de nouvelles données.</li>
                        <li><b>Apprentissage inductif</b> : Cela concerne la recherche de la bonne mise en
                            correspondance entre les vecteurs de caractéristiques d'entrée et les caractéristiques de
                            sortie. Cela peut inclure la généralisation à de nouvelles données en utilisant le modèle
                            appris.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">22
                <a class="prev" href="#slide21"></a>
                <a class="next" href="#slide23"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide23">
        <div class="header">
            <h1>2.1.3. Formalisation des problèmes d'apprentissage</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Formalisation: Apprentissage semi-supervisé</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>Soit \({(x_1, y_1),...,(x_l, y_l)}\) l'ensemble d'exemples d'exercices étiquetés</li>
                <li>Soit \({x_{l+1},...,x_{l+u}}\) sont les \(u\) ensembles des vecteurs de caractéristiques non
                    étiquetées de \(X\).</li>
                <li>L'objectif de l'algorithme d'apprentissage semi-supervisé est de faire
                    <ul>
                        <li><b>l'apprentissage transductif</b>, c'est-à-dire trouver des étiquettes correctes pour
                            \({x_{l+1},...,x_{l+u}}\).</li>
                        <li><b>l'apprentissage inductif</b>, c'est-à-dire trouver la bonne mise en correspondance \(X
                            &#8594; Y\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">23
                <a class="prev" href="#slide22"></a>
                <a class="next" href="#slide24"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide24">
        <div class="header">
            <h1>2.2. Data Mining</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Activités</h1>
            <ol>
                <li>Classification</li>
                <li>Partitionnement de données (Clustering)</li>
                <li>Régression</li>
                <li>Étiquetage des séquences</li>
                <li>Règles d'association</li>
                <li>Détection d'anomalies</li>
                <li>Récapitulation</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">24
                <a class="prev" href="#slide23"></a>
                <a class="next" href="#slide25"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide25">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">2.1.1 Introduction</h1>
            <ul>
                <li><b>Catégorisation algorithmique d'objets</b> : Processus d'attribution de classes ou de catégories à
                    des objets via des algorithmes. L'objectif est d'organiser les données en groupes distincts pour
                    faciliter l'analyse et la prise de décision.</li>
                <li><b>Attribution de classes</b> :Attribuer une classe ou catégorie à chaque objet (ou individu). </li>
                <li><b>Types de classification :</b>
                    <ul>
                        <li><b>Classification binaire</b> : Assignation à deux classes.</li>
                        <li><b>Classification en classes multiples</b> : Assignation à plusieurs classes simultanément.
                        </li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">25
                <a class="prev" href="#slide24"></a>
                <a class="next" href="#slide26"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide26">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Filtrage de contenu (e.g., spam/pourriel)</b> : Identifier et filtrer les emails non désirés ou
                    indésirables.
                    Exemple : Filtrage des spams dans les boîtes de réception.</li>
                <li><b>Classification de documents</b> : Organiser et catégoriser les documents en fonction de leur
                    contenu.
                    Exemple : Classification automatique des articles de presse par sujet.</li>
                <li><b>Reconnaissance de l'écriture manuscrite</b> : Interprétation automatique des caractères écrits à
                    la main.
                    Exemple : Reconnaissance des chiffres sur les chèques bancaires.</li>
                <li><b>Reconnaissance automatique de la parole</b> : Convertir la parole en texte écrit de manière
                    automatique.
                    Exemple : Commandes vocales pour les assistants virtuels comme Siri ou Alexa.</li>
                <li><b>Moteurs de recherche</b> : Classer et organiser les résultats de recherche en fonction de leur
                    pertinence.
                    Exemple : Classement des pages web dans les résultats de recherche de moteurs de recherche.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">26
                <a class="prev" href="#slide25"></a>
                <a class="next" href="#slide27"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide27">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Classification: Définition formelle</h1>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>L'objectif de l'algorithme de classification (ou classificateur) est de trouver \({(x_1,
                    y_1),...,(x_l, y_k)}\), c'est-à-dire l'attribution d'une étiquette connue à chaque vecteur de
                    caractéristique d'entrée, où
                    <ul>
                        <li>\(x_i &#8712; X \)</li>
                        <li>\(y_i &#8712; Y \)</li>
                        <li>\(|X| = l \)</li>
                        <li>\(|Y| = k \)</li>
                        <li>\(l &gt;= k\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">27
                <a class="prev" href="#slide26"></a>
                <a class="next" href="#slide28"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide28">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Classificateurs</h3>
            <ul>
                <li>Algorithme de classification</li>
                <li>Deux types de classificateurs:
                    <ul>
                        <li><b>Classificateurs binaires</b> attribue un objet à l'une des deux classes</li>
                        <li><b>Classificateurs multiclasses</b> attribue un objet à une ou plusieurs classes</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">28
                <a class="prev" href="#slide27"></a>
                <a class="next" href="#slide29"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide29">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Classification binaire</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/binaryclassifier.svg"
                    height="400px" />
                <figcaption>Classification binaire</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">29
                <a class="prev" href="#slide28"></a>
                <a class="next" href="#slide30"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide30">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Linear Classificateurs</h3>
            <ul>
                <li>Fonction linéaire attribuant un score à chaque catégorie possible en combinant le vecteur de
                    caractéristiques d'une instance avec un vecteur de poids, en utilisant un produit de points.</li>
                <li>Formalisation :
                    <ul>
                        <li>Soit <i><b>X</b></i> être l'espace de saisie des caractéristiques et <i><b>x</b><sub>i</sub>
                                &#8712; <b>X</b></i></li>
                        <li>Soit <i><b>&#946;</b><sub>k</sub></i> un vecteur de poids pour la catégorie <i>k</i></li>
                        <li><i>score(<b>x</b><sub>i</sub>, k) = <b>x</b><sub>i</sub>.<b>&#946;</b><sub>k</sub></i>,
                            score pour l'attribution de la catégorie <i>k</i> à l'instance <i><b>x</b><sub>i</sub></i>.
                            La catégorie qui donne le score le plus élevé est
                            attribuée à la catégorie de l'instance.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">30
                <a class="prev" href="#slide29"></a>
                <a class="next" href="#slide31"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide31">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation</h3>
            <p>Dans le contexte de la classification en apprentissage machine, l'évaluation des performances d'un modèle
                implique la compréhension de différents types de prédictions qu'il peut faire par rapport à la réalité.
                Les vrais positifs (VP) et les vrais négatifs (VN) sont deux de ces éléments.</p>
            <ul>
                <li><b>Vrais Positifs (VP/TP)</b> : Les vrais positifs représentent les cas où le modèle prédit
                    correctement la classe positive. En d'autres termes, il a correctement identifié les exemples qui
                    appartiennent réellement à la classe que le modèle essaie de prédire.</li>
                <li><b>Vrais Négatifs (VN/FN)</b> : Les vrais négatifs représentent les cas où le modèle prédit
                    correctement la classe négative. Cela signifie qu'il a correctement identifié les exemples qui
                    n'appartiennent pas à la classe que le modèle essaie de prédire.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">31
                <a class="prev" href="#slide30"></a>
                <a class="next" href="#slide32"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide32">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/positivenegative.svg" width="400vw" />
                <figcaption>Les vrais positifs et les vrais négatifs</figcaption>
            </figure>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/Precisionrecall.svg" width="400vw" />
                <figcaption>Précision et rappel</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">32
                <a class="prev" href="#slide31"></a>
                <a class="next" href="#slide33"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide33">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation</h3>
            <p>Soit</p>
            <ul>
                <li><i>tp</i>: nombre de vrais postifs</li>
                <li><i>fp</i>: nombre de faux positifs</li>
                <li><i>fn</i>: nombre de faux négatifs</li>
            </ul>
            <figure class="gridcontent">
                <img src="../../../../../en/teaching/courses/2018/DataMining/Precisionrecall.svg" height="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">33
                <a class="prev" href="#slide32"></a>
                <a class="next" href="#slide34"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide34">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation</h3>
            <p>La <b>précision</b> mesure la proportion de prédictions positives faites par le modèle qui étaient
                <b>effectivement correctes</b>, tandis que le <b>rappel</b> mesure la proportion d'exemples positifs
                réels qui ont été correctement identifiés par le modèle. Alors
            </p>
            <ul>
                <li>Précision \[p = \frac{tp}{(tp + fp)}\]</li>
                <li>Rappel (Recall) \[r = \frac{tp}{(tp + fn)}\]</i>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">34
                <a class="prev" href="#slide33"></a>
                <a class="next" href="#slide35"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide35">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation</h3>
            <p>Le F1-score est la moyenne harmonique de la précision et du rappel. Il fournit une mesure globale de la
                performance d'un modèle de classification, tenant compte à la fois de la précision et du rappel. Il est
                particulièrement utile lorsque les classes sont déséquilibrées.</p>
            <ul>
                <li>F1-score \[f1 = 2 * \frac{(p * r)}{(p + r)}\]</li>
                <li>F1-score: meilleure valeur à 1 (précision et rappel parfaits) et pire à 0.</li>
            </ul>
            <p>Le F1-score tient compte à la fois des <b>erreurs de type I (faux positifs)</b> et des <b>erreurs de type
                    II (faux négatifs)</b>, fournissant ainsi une mesure équilibrée de la performance du modèle.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">35
                <a class="prev" href="#slide34"></a>
                <a class="next" href="#slide36"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide36">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation</h3>
            <ul>
                <li>\(F_\beta\)-score utilise un facteur réel positif β, où β est choisi de telle sorte que le rappel
                    est considéré comme β fois plus important que la précision, est : </li>
                <li>\(F_\beta\)-score \[F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{p} \cdot \mathrm{r}}{(\beta^2 \cdot
                    \mathrm{p}) + \mathrm{r}}\]</li>
                <li>Exemple: <b>\(F_2\) score</b>: Cette métrique est souvent utilisée dans des situations où le rappel
                    est jugé plus critique que la précision, par exemple, dans des tâches où la détection des exemples
                    positifs est particulièrement importante, même si cela entraîne un nombre plus élevé de faux
                    positifs.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">36
                <a class="prev" href="#slide35"></a>
                <a class="next" href="#slide37"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide37">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <p>Le \(F_2\)-score est souvent utilisé dans des domaines où le rappel est considéré comme plus critique que
                la précision. </p>
            <ul>
                <li><b>Détection de Maladies</b> : Dans le domaine médical, en particulier pour la détection de maladies
                    graves, le F2-score peut être utilisé pour évaluer la performance des modèles. Il est crucial
                    d'identifier correctement autant de cas positifs que possible, même si cela conduit à quelques faux
                    positifs.</li>
                <li><b>Sécurité et Détection d'Intrusion</b> : Lors de la détection d'intrusions dans les systèmes
                    informatiques, il est souvent plus important de minimiser les faux négatifs (intrusions manquées) au
                    profit de quelques faux positifs, d'où l'utilisation du F2-score.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">37
                <a class="prev" href="#slide36"></a>
                <a class="next" href="#slide38"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide38">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Recherche Biomédicale</b> : Dans des domaines de recherche biomédicale où la découverte de
                    certaines caractéristiques ou protéines spécifiques est critique, le F2-score peut être privilégié
                    pour s'assurer que ces éléments sont correctement identifiés.</li>
                <li><b>Prévision de Catastrophes Naturelles</b> : Lors de la prévision de catastrophes naturelles comme
                    les tremblements de terre ou les tsunamis, il est essentiel de minimiser les faux négatifs pour
                    garantir que le maximum d'avertissements est donné, même au prix de quelques alertes erronées.</li>
                <li><b>Recherche en Astronomie</b> : Dans la recherche astronomique, la découverte de nouveaux objets
                    célestes ou de phénomènes rares peut être cruciale. Le F2-score peut être utilisé pour évaluer les
                    performances des algorithmes de détection.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">38
                <a class="prev" href="#slide37"></a>
                <a class="next" href="#slide39"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide39">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Évaluation: matrice de confusion</h2>
            <p>La matrice de confusion est un outil essentiel dans l'évaluation des performances d'un système de
                classification. Elle fournit une vue détaillée des prédictions faites par le modèle par rapport aux
                classes réelles.</p>
            <ul>
                <li>Chaque ligne de la matrice représente les instances d'une classe prédite.</li>
                <li>Chaque colonne représente les instances d'une classe réelle.</li>
                <li>Toutes les prédictions correctes sont situées dans la diagonale du tableau.</li>
                <li>Les erreurs de prédiction sont représentées par des valeurs situées en dehors de la diagonale
                    principale.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">39
                <a class="prev" href="#slide38"></a>
                <a class="next" href="#slide40"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide40">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation: matrice de confusion</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/DataMining/confusionmatrix.png" height="400px" />
                <figcaption>Matrice de confusion pour un classificateur SVM pour les chiffres manuscrits (MNIST)
                </figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">40
                <a class="prev" href="#slide39"></a>
                <a class="next" href="#slide41"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide41">
        <div class="header">
            <h1>2.2. Méthodes de classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Évaluation: matrice de confusion</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/DataMining/confusionmatrix1.png" height="400px" />
                <figcaption>Matrice de confusion pour un perceptron pour les chiffres manuscrits (MNIST)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">41
                <a class="prev" href="#slide40"></a>
                <a class="next" href="#slide42"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide42">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Classification binaire</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/binaryclassifier.svg"
                    height="400px" />
                <figcaption>Classification binaire</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">42
                <a class="prev" href="#slide41"></a>
                <a class="next" href="#slide43"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide43">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h2 class="topicsubheading">Classification multiclasse</h2>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/multiclassclassifier.svg"
                    height="400px" />
                <figcaption>Classification multiclasse</figcaption>
            </figure>
        </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">43
                <a class="prev" href="#slide42"></a>
                <a class="next" href="#slide44"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide44">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Classification multiclasse [Aly 2005]</h3>
            <ul>
                <li><b>Transformation en classification binaire</b> :
                    <ul>
                        <li><b>L'approche un contre le reste (Un contre tous)</b> : Chaque classe est traitée comme une
                            classe positive et toutes les autres comme une classe négative.</li>
                        <li><b>L'approche un-contre-un</b> : Un classifieur binaire est construit pour chaque paire de
                            classes.</li>
                    </ul>
                </li>
                <li><b>Extension de la classification binaire</b> :
                    <ul>
                        <li><b>Réseaux de neurones</b> : Adaptation des architectures pour prédire plusieurs classes
                            simultanément.</li>
                        <li><b>k-voisins les plus proches</b> : Extension de l'algorithme pour gérer plusieurs classes.
                        </li>
                    </ul>
                </li>
                <li><b>Classification hiérarchique.</b> : Organisation des classes dans une structure arborescente pour
                    une classification plus fine et précise.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">44
                <a class="prev" href="#slide43"></a>
                <a class="next" href="#slide45"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide45">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest (One-vs.-all) strategy</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsall.svg" height="400px" />
                <figcaption>La strategie un-contre le rest pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">45
                <a class="prev" href="#slide44"></a>
                <a class="next" href="#slide46"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide46">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest (One-vs.-all) strategy</h3>
            <ul>
                <li>Entraîner un seul classificateur par classe, avec les échantillons de cette classe comme
                    échantillons positifs et tous les autres comme négatifs. </li>
                <li>Chaque classificateur produit un score de confiance réel pour sa décision</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsall.svg" height="300px" />
                <figcaption>La strategie un-contre le rest pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">46
                <a class="prev" href="#slide45"></a>
                <a class="next" href="#slide47"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide47">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest or One-vs.-all (OvR, OvA) strategy</h3>
            <ul>
                <li>Entrées :
                    <ul>
                        <li>\(L\), un apprenant (algorithme d'entraînement pour les classificateurs binaires)</li>
                        <li>échantillons \(X\)</li>
                        <li>étiquettes \(y\), où \(y_i ∈ \{1,..,K \} \) est l'étiquette de l'échantillon \(X_i\)
                    </ul>
                </li>
                <li>Sortie :
                    <ul>
                        <li>une liste de classificateurs \(f_k\), où \(k ∈ \{1,..,K \} \)
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">47
                <a class="prev" href="#slide46"></a>
                <a class="next" href="#slide48"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide48">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-rest or One-vs.-all (OvR, OvA) strategy</h3>
            <p>Prendre des décisions signifie appliquer tous les classificateurs à un échantillon invisible x et prédire
                l'étiquette k pour laquelle le classificateur correspondant rapporte le score de confiance le plus élevé
                : \[\hat{y} = \underset{k \in
                \{1 \ldots K\}}{\arg\!\max}\; f_k(x)\]</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">48
                <a class="prev" href="#slide47"></a>
                <a class="next" href="#slide49"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide49">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-one strategy</h3>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsone.svg" height="400px" />
                <figcaption>La strategie un-contre-un pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">49
                <a class="prev" href="#slide48"></a>
                <a class="next" href="#slide50"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide50">
        <div class="header">
            <h1>2.2.1. Classification</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">One-vs.-one strategy</h3>
            <li>nécessite l'entraînement des \(\frac{K (K - 1)}{2}\) classificateurs binaires</li>
            <li>chaque classificateur reçoit les échantillons d'une paire de classes du jeu de formation original, et
                doit apprendre à distinguer ces deux classes.</li>
            <li>Au moment de la prédiction, un système de vote est appliqué : tous les \(\frac{K (K - 1)}{2}\)
                classificateurs sont appliqués à un échantillon non vu et la classe qui a obtenu le plus grand nombre de
                prédictions est prédite par le classificateur
                combiné.
            </li>
            <figure>
                <img src="../../../../../en/teaching/courses/2019/MachineLearning/onevsone.svg" height="200px" />
                <figcaption>La strategie un-contre-un pour la classification multiclasse</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">50
                <a class="prev" href="#slide49"></a>
                <a class="next" href="#slide51"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide51">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h1>2.2.2.1. Introduction</h1>
            <ul>
                <li>Partitionnement de données est le processus de division d'un ensemble de données en différents
                    sous-ensembles homogènes ou groupes.</li>
                <li><b>Objectif</b> : Regrouper les données partageant des caractéristiques similaires dans chaque
                    sous-ensemble.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/320px-Cluster-2.svg.png"
                    height="300px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">51
                <a class="prev" href="#slide50"></a>
                <a class="next" href="#slide52"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide52">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Analyse des réseaux sociaux</b> : Identifier des communautés ou des groupes d'individus ayant des
                    liens ou des intérêts similaires.
                    Exemple : Regrouper des utilisateurs de réseaux sociaux en fonction de leurs interactions ou de
                    leurs centres d'intérêt communs.</li>
                <li><b>Segmentation d'image</b> : Diviser une image en régions homogènes selon des critères prédéfinis.
                    Exemple : Identifier automatiquement les objets ou les régions d'intérêt dans une photographie.
                </li>
                <li><b>Systèmes de recommandation</b> : Regrouper les utilisateurs ou les produits en fonction de leurs
                    caractéristiques ou de leurs préférences.
                    Exemple : Suggérer des produits ou des contenus similaires à des utilisateurs en se basant sur leurs
                    historiques d'achats ou de navigation.</li>
            </ul>
            <figure class="gridconten">
                <img src="../../2017/CN/Social_Network_Analysis_Visualization.png" height="100px" width="200px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">52
                <a class="prev" href="#slide51"></a>
                <a class="next" href="#slide53"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide53">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Définition formelle</h3>
            <ul>
                <li>Soit \(X\) être l'espace de saisie des caractéristiques</li>
                <li>L'objectif du regroupement est de trouver \(k\) des sous-ensembles de \(X\), de façon à ce que</li>
                <p>\[ C_1.. &#8746; ..C_k &#8746; C_{outliers} = X \] </i> et</p>
                <p> \[ C_i &#8745; C_j = &#981;, i &#8800; j; 1 &lt;i,j &lt;k \]</i>
                </p>
                <p>\(C_{outliers}\) peut consister en des cas extrêmes (anomalie de données)</p>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">53
                <a class="prev" href="#slide52"></a>
                <a class="next" href="#slide54"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide54">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Modèles de regroupement</h3>
            <ul>
                <li><b>Modèles de centroïdes</b> : Les groupes sont représentés par un seul vecteur moyen (centroïde).
                    Exemple : K-Means, K-Médian.</li>
                <li><b>Modèles de connectivité</b> : Les regroupements sont déterminés par la proximité de la
                    connectivité entre les points.
                    Exemple : Agglomératif Hiérarchique.</li>
                <li><b>Modèles de distribution</b> : Les regroupements sont modélisés à l'aide de distributions
                    statistiques.
                    Exemple : Mélanges de Gaussiennes.</li>
                <li><b>Modèles de densité</b> : Les regroupements sont définis par des régions denses connectées dans
                    l'espace de données.
                    Exemple : DBSCAN, OPTICS.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">54
                <a class="prev" href="#slide53"></a>
                <a class="next" href="#slide55"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide55">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Modèles de regroupement</h3>
            <ul>
                <li><b>Modèles de sous-espace</b> : Identifient des regroupements dans des sous-espaces spécifiques des
                    données.
                    Exemple : CLIQUE, Subspace Clustering.</li>
                <li><b>Modèles de groupes</b> : Organisent les données en groupes selon des critères spécifiques.
                    Exemple : K-Modes pour les données catégoriques.</li>
                <li><b>Modèles graphiques</b> : Utilisent des structures de graphes pour représenter les relations entre
                    les données.
                    Exemple : Algorithme de Marche Aléatoire pour la découverte de communautés.</li>
                <li><b>Modèles neuronaux</b> : Utilisent des réseaux de neurones pour apprendre et découvrir des
                    structures dans les données.
                    Exemple : Autoencodeurs pour la réduction de dimensionnalité non linéaire.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">55
                <a class="prev" href="#slide54"></a>
                <a class="next" href="#slide56"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide56">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Modèles de regroupement</h3>
            <figure class="fullwidth">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                    height="400px" width="400px" />
                <figcaption>k-means regroupement (voir section 3.3)</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">56
                <a class="prev" href="#slide55"></a>
                <a class="next" href="#slide57"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide57">
        <div class="header">
            <h1>2.2.2. Partitionnement de données</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Modèles de regroupement</h3>
            <figure class="fullwidth">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Iris_dendrogram.png" height="300px"
                    width="400px" />
                <figcaption>Dendrogramme de regroupement hiérarchique de l'ensemble de données Iris</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">57
                <a class="prev" href="#slide56"></a>
                <a class="next" href="#slide58"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide58">
        <div class="header">
            <h1>2.2.3 Régression</h1>
        </div>
        <div class="content">
            <h1>2.2.3 Régression</h1>
            <ul>
                <li>Processus visant à trouver une fonction mathématique qui modélise les relations entre les variables.
                    L'objectif est d'estimer les relations et prédire les valeurs d'une variable en fonction d'autres
                    variables.</li>
                <li><b>Fonction de modélisation</b> : Trouver une fonction qui représente au mieux les données observées
                    avec l'objectif de prédire ou estimer les valeurs d'une variable cible en fonction des variables
                    explicatives.</li>
                <li><b>Analyse des relations</b> : Examiner la relation entre une variable cible et une ou plusieurs
                    variables explicatives. Méthodes : Identifier les tendances, les corrélations et les dépendances
                    entre les variables.</li>
                <li><b>Attribution de valeurs</b> : Assigner des valeurs réelles à chaque entrée pour modéliser les
                    phénomènes du monde réel.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">58
                <a class="prev" href="#slide57"></a>
                <a class="next" href="#slide59"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide59">
        <div class="header">
            <h1>2.2.3 Régression</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Applications</h1>
            <ul>
                <li><b>Prévisions météorologiques</b> : Prédire les conditions météorologiques futures en fonction des
                    données historiques et des variables environnementales.
                    Exemple : Estimation de la température, des précipitations et des vents pour les prochains jours.
                </li>
                <li><b>Prévisions de ventes</b> : Estimer les ventes futures en fonction des tendances passées, des
                    saisons et des stratégies marketing.
                    Exemple : Prédiction des ventes de produits pour une période donnée.</li>
                <li><b>Apprentissage machine</b> : Utiliser la régression comme composante d'algorithmes d'apprentissage
                    machine pour la prédiction et la classification.
                    Exemple : Modèles de régression linéaire dans les méthodes d'apprentissage supervisé.</li>
                <li><b>Finance</b> : Évaluer les performances financières, prédire les prix des actions et des actifs,
                    et estimer les risques.
                    Exemple : Modèles de régression pour prédire les rendements financiers ou évaluer les risques de
                    crédit.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Linear_regression.svg"
                    height="100px" width="200px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">59
                <a class="prev" href="#slide58"></a>
                <a class="next" href="#slide60"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide60">
        <div class="header">
            <h1>2.2.3 Régression</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Définition formelle</h3>
            <ul>
                <li>La régression est représentée par une fonction qui associe un élément de données à une variable de
                    prédiction.</li>
                <li>Elle peut être exprimée en termes de variables indépendantes \(X\), de variables dépendantes \(Y\)
                    et de paramètres inconnus \(β\).
                <li>Le modèle de régression vise à approximer la relation entre \(X\) et \(Y\) avec une fonction \(f(X,
                    β)\), où \(β\) représente les paramètres du modèle.</li>
                <li>L'objectif est d'obtenir une approximation \(Y ≈ f(X, β)\) qui minimise l'écart entre les valeurs
                    prédites et les valeurs observées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">60
                <a class="prev" href="#slide59"></a>
                <a class="next" href="#slide61"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide61">
        <div class="header">
            <h1>2.2.3 Régression</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Régression linéaire</h3>
            <p>La régression linéaire est un modèle mathématique qui représente une relation linéaire entre une variable
                indépendante \(x_i\) et une variable dépendante \(y_i\). Le modèle a une forme d'une ligne droite (pour
                la régression linéaire simple) ou d'une parabole (pour la régression linéaire multiple).</p>
            <ul>
                <li>ligne droite: \(y_i = &#946;_0 + &#946;_1x_i + &#949;_i\) où \(β_0\) et \(β_1\) sont les
                    coefficients de régression, \(x_i\) est la variable indépendante, et \(ε_i\) est l'erreur
                    résiduelle.</li>
                <li>parabole: \(y_i = &#946;_0 + &#946;_1x_i + &#946;_2x_i^2 +&#949;_i\) où \(β_0\), \(β_1\), et \(β_2\)
                    sont les coefficients de régression pour chaque terme, \(x_i\) est la variable indépendante, et
                    \(ε_i\) est l'erreur résiduelle.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">61
                <a class="prev" href="#slide60"></a>
                <a class="next" href="#slide62"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide62">
        <div class="header">
            <h1>2.2.3 Régression</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Régression linéaire</h3>
            <p><b>Ligne droite</b>: \(y_i = &#946;_0 + &#946;_1x_i + &#949;_i\) où \(β_0\) et \(β_1\) sont les
                coefficients de régression, \(x_i\) est la variable indépendante, et \(ε_i\) est l'erreur résiduelle.
            </p>
            <p>Pour minimiser l'erreur :</<p>
            <ul>
                <li>Calcul des prédictions : \( ŷ_i = &#946;_0 + &#946;_{1}x_i \)</li>
                <li>Calcul des résidus: \(e_i = ŷ_i - y_i\)</li>
                <li>Calcul de la somme des carrés des résidus (SSE) pour évaluer l'ajustement du modèle., \(SSE = &#931;
                    e_i\), where \(1 &lt; i &lt; n\)</li>
            </ul>
            <p>L'objectif est de minimiser SSE pour obtenir la meilleure approximation de la relation linéaire entre les
                variables.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">62
                <a class="prev" href="#slide61"></a>
                <a class="next" href="#slide63"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide63">
        <div class="header">
            <h1>2.2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <p>Processus consistant à attribuer une classe ou une étiquette à chaque élément d'une séquence de valeurs
                ou de tokens.
                Exemple : Reconnaissance d'entités nommées (NER) avec spaCy, où des entités comme les noms de personnes,
                les lieux, ou les organisations sont identifiées et étiquetées dans un texte.</p>
            <h1 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h1>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">63
                <a class="prev" href="#slide62"></a>
                <a class="next" href="#slide64"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide64">
        <div class="header">
            <h1>2.2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Reconnaissance d'entités nommées (spaCy)</h1>
            <figure style="margin-bottom: 6rem">
                <div class="entities" style="line-height: 2.5; direction: ltr">
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        Paris
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> is the capital of
                    <mark class="entity"
                        style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        France
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                    </mark> . In
                    <mark class="entity"
                        style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2015
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                    </mark> , its population was recorded as
                    <mark class="entity"
                        style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                        2,206,488
                        <span
                            style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                    </mark>
                </div>
            </figure>
            <table>
                <tr>
                    <th>Balise</th>
                    <th>Signification</th>
                </tr>
                <tr>
                    <td>GPE</td>
                    <td>Pays, villes, états.</td>
                </tr>
                <tr>
                    <td>DATE</td>
                    <td>Dates ou périodes absolues ou relatives</td>
                </tr>
                <tr>
                    <td>CARDINAL</td>
                    <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                </tr>
            </table>
        </div>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">64
                <a class="prev" href="#slide63"></a>
                <a class="next" href="#slide65"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide65">
        <div class="header">
            <h1>2.2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Etiquetage de la partie du discours</b> : Assigner des étiquettes grammaticales à chaque mot
                    d'une phrase pour analyser sa structure syntaxique.</li>
                <li><b>Traduction linguistique</b> : Identifier et étiqueter les mots ou phrases dans une langue source
                    avant leur traduction dans une langue cible.</li>
                <li><b>Analyse vidéo</b> : Marquer et catégoriser les actions ou objets identifiés dans une séquence
                    vidéo.</li>
                <li><b>Reconnaissance de l'écriture manuscrite</b> : Attribuer des étiquettes à chaque caractère ou
                    symbole écrit à la main pour la reconnaissance de texte.</li>
                <li><b>Extraction d'informations</b> : Identifier et étiqueter des éléments d'intérêt dans un document
                    ou un ensemble de données pour extraire des informations pertinentes.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">65
                <a class="prev" href="#slide64"></a>
                <a class="next" href="#slide66"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide66">
        <div class="header">
            <h1>2.2.4. Étiquetage des séquences</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Définition formelle</h3>
            <ul>
                <li>Soit \(X\) l'espace de saisie des caractéristiques</li>
                <li>Soit \(Y\) l'espace des caractéristiques de sortie (des étiquettes)</li>
                <li>Soit \(&#12296;x_1,...,x_T&#12297;\) une séquence de longueur \(T\).</li>
                <li>L'objectif de l'étiquetage des séquences est de générer une séquence correspondante
                    <ul>
                        <li>\(&#12296;y_1,...,y_T&#12297;\) des étiquettes</li>
                        <li>\(x_i &#8712; X\) </li>
                        <li>\(y_j &#8712; Y\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">66
                <a class="prev" href="#slide65"></a>
                <a class="next" href="#slide67"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide67">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Association Rules</h3>
            <p>Les règles d'association, également connues sous le nom de "Association Rules", sont un ensemble de
                techniques d'analyse de données visant à découvrir les <b>relations et les associations entre les
                    variables</b> dans un ensemble de données. Cette méthode recherche des corrélations et des
                co-occurrences entre les éléments, permettant ainsi de dégager des motifs ou des modèles significatifs.
            </p>
            <p>Un exemple courant d'application des règles d'association est l'analyse de paniers d'achats dans le
                domaine du commerce de détail, où ces règles sont utilisées pour identifier des schémas d'achat, tels
                que les <b>combinaisons de produits souvent achetés ensemble</b>. </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">67
                <a class="prev" href="#slide66"></a>
                <a class="next" href="#slide68"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide68">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Association Rules</h3>
            <p>Prenons un exemple concret avec un tableau de données représentant les transactions d'une épicerie :</p>
            <table>
                <thead>
                    <tr>
                        <th>Transaction</th>
                        <th>Produits achetés</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>Pain, Lait, Œufs</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>Pain, Beurre</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>Lait, Œufs, Fromage</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>Pain, Lait, Œufs, Bière</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>Lait, Bière, Chips</td>
                    </tr>
                </tbody>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">68
                <a class="prev" href="#slide67"></a>
                <a class="next" href="#slide69"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide69">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Association Rules</h3>
            <p>Dans ce tableau, chaque colonne représente un produit et chaque ligne représente une transaction. Un "1"
                indique que le produit a été acheté lors de cette transaction, tandis qu'un "0" indique que le produit
                n'a pas été acheté.</p>
            <table>
                <thead>
                    <tr>
                        <th>Transaction</th>
                        <th>Pain</th>
                        <th>Lait</th>
                        <th>Œufs</th>
                        <th>Beurre</th>
                        <th>Fromage</th>
                        <th>Bière</th>
                        <th>Chips</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>1</td>
                        <td>1</td>
                        <td>1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>0</td>
                        <td>1</td>
                        <td>1</td>
                        <td>0</td>
                        <td>1</td>
                        <td>0</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>1</td>
                        <td>1</td>
                        <td>1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>1</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>5</td>
                        <td>0</td>
                        <td>1</td>
                        <td>0</td>
                        <td>0</td>
                        <td>0</td>
                        <td>1</td>
                        <td>1</td>
                    </tr>
                </tbody>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">69
                <a class="prev" href="#slide68"></a>
                <a class="next" href="#slide70"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide70">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Exploitation de l'utilisation du web</b> : Utilisées pour analyser les comportements des
                    utilisateurs en ligne, personnaliser les recommandations de produits et cibler les publicités.</li>
                <li><b>Détection d'intrusion</b> : Utilisées en sécurité informatique pour repérer les comportements
                    malveillants et détecter les tentatives d'intrusion.</li>
                <li><b>Analyse d'affinité</b> : Utilisées dans le marketing pour identifier les relations entre les
                    produits souvent achetés ensemble, permettant ainsi de recommander des produits complémentaires et
                    de créer des offres groupées attractives.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">70
                <a class="prev" href="#slide69"></a>
                <a class="next" href="#slide71"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide71">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Définition formelle</h3>
            <ul>
                <li>Soit \(I\) un ensemble de \(n\) attributs binaires appelés items</li>
                <li>Soit \(T\) un ensemble de \(m\) transactions appelé base de données</li>
                <li>Soit \(I\) = \(\{(i_1,...,i_n)\}\) et \(T\) = \({(t_1,...,t_m)}\)</li>
                <li>L'objectif de l'apprentissage des règles d'association est de trouver
                    <ul>
                        <li>\(X &#8658; Y\), where \(X &#8658; Y &#8838; I\)</li>
                        <li>\(X\) est l'antécédent</li>
                        <li>\(Y\) est la conséquence</li>
                    </ul>
                </li>
            </ul>
            <p>Une règle d'association \(X &#8658; Y\) est valide si le support et la confiance de la règle dépassent
                les seuils spécifiés. Cela signifie que \(X\) et \(Y\) apparaissent fréquemment ensemble dans les
                transactions, et que lorsque \(X\) est présent, \(Y\) est également souvent présent.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">71
                <a class="prev" href="#slide70"></a>
                <a class="next" href="#slide72"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide72">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Support</h3>
            <p>Le support d'un ensemble d'articles dans le contexte des règles d'association est défini comme <b>la
                    fréquence à laquelle cet ensemble d'articles apparaît dans la base de données</b>. En d'autres
                termes, c'est le nombre de transactions dans lesquelles cet ensemble d'articles est présent, divisé par
                le nombre total de transactions dans la base de données. Le support mesure donc la popularité ou la
                prévalence d'un ensemble d'articles. Il est utilisé pour évaluer à quel point une association entre deux
                ensembles d'articles est forte. Une valeur élevée de support indique que l'association est fréquente
                dans la base de données, ce qui la rend potentiellement plus significative.</p>
            <p>\[supp(X) =
                \frac{|t &#8712;T; X &#8838; t|}{ |T|}\]
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">72
                <a class="prev" href="#slide71"></a>
                <a class="next" href="#slide73"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide73">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Confidence</h3>
            <p>La confidence dans le contexte des règles d'association représente <b>la fréquence à laquelle la règle a
                    été trouvée vraie dans la base de données</b>. Plus précisément, elle mesure la probabilité
                conditionnelle que la conséquence Y se produise dans une transaction, étant donné que l'antécédent X est
                également présent dans cette transaction. </p>
            <p>La confidence d'une règle est calculée en divisant le nombre de transactions dans lesquelles à la fois X
                et Y sont présents par le nombre de transactions dans lesquelles X est présent. Ainsi, une confidence
                élevée indique que la conséquence Y est souvent vraie lorsque l'antécédent X est présent, ce qui
                renforce la fiabilité de la règle d'association.</p>
            <p>\[conf(X &#8658; Y) = \frac{supp(X &#8746; Y)}{supp(X)}\]
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">73
                <a class="prev" href="#slide72"></a>
                <a class="next" href="#slide74"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide74">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <p>En utilisant les règles d'association, nous pouvons extraire des relations significatives entre les
                produits. Par exemple, en appliquant un support minimum de 40% et un seuil de confiance de 60%, nous
                pouvons identifier les règles d'association suivantes :</p>
            <ol>
                <li>{Pain, Lait} \( &#8658;\) {Œufs} (Support : 40%, Confiance : 100%)</li>
                <li>{Lait} \( &#8658;\) {Œufs} (Support : 60%, Confiance : 75%)</li>
                <li>{Œufs} \( &#8658;\) {Lait} (Support : 60%, Confiance : 75%)</li>
            </ol>

            <p>Cela signifie que dans 40% des transactions, les clients ont acheté du pain et du lait ensemble, et dans
                100% de ces transactions, ils ont également acheté des œufs. De même, dans 60% des transactions, les
                clients ont acheté du lait, et dans 75% de ces transactions, ils ont également acheté des œufs.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">74
                <a class="prev" href="#slide73"></a>
                <a class="next" href="#slide75"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide75">
        <div class="header">
            <h1>2.2.5. Règles d'association</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Lift</h3>
            <p>Le lift dans le contexte des règles d'association est <b>le rapport entre le "support" observé de la
                    règle et celui attendu si les ensembles d'items X et Y étaient indépendants</b>.Formellement, le
                lift est calculé en divisant le support de la règle par le produit des supports individuels de X et Y.
                En d'autres termes, c'est la mesure de combien plus fréquemment la règle est observée que ce à quoi on
                s'attendrait si les événements X et Y étaient indépendants.</p>
            <p>Un lift supérieur à 1 indique que la règle a une association positive entre X et Y (c'est-à-dire que les
                items X et Y apparaissent ensemble plus fréquemment que prévu au hasard), tandis qu'un lift inférieur à
                1 indique une association négative ou non significative. Un lift de 1 indique une indépendance entre les
                items X et Y.</p>
            <ul>
                <li>\[lift(X &#8658; Y) = \frac{supp(X &#8746; Y)}{(supp(X) &#10761; supp(Y))}\]
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">75
                <a class="prev" href="#slide74"></a>
                <a class="next" href="#slide76"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide76">
        <div class="header">
            <h1>2.2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <p>La détection d'anomalies, également connue sous le nom de détection des valeurs aberrantes, implique
                l'identification de données inhabituelles ou divergentes dans un ensemble de données. Voici quelques
                approches courantes pour détecter les anomalies :</p>
            <ul>
                <li><b>Détection supervisé</b> : le modèle est entraîné sur un ensemble de données étiqueté avec des
                    exemples d'anomalies et de données normales. Le modèle est ensuite utilisé pour prédire si de
                    nouvelles données sont anormales ou normales en fonction de ces étiquettes.</li>
                <li><b>Détection non-supervisé</b> : Contrairement à la détection supervisée, cette approche n'utilise
                    pas d'étiquettes dans l'ensemble de données d'entraînement. Au lieu de cela, elle identifie les
                    anomalies en examinant les caractéristiques statistiques des données et en recherchant des points de
                    données qui diffèrent significativement du reste de l'ensemble de données.</li>
                <li><b>Détection semi-supervisé</b> : Cette approche combine des éléments des deux méthodes précédentes.
                    Elle utilise à la fois des données étiquetées et non étiquetées pour entraîner le modèle. Cela peut
                    être utile lorsque seules quelques anomalies sont disponibles pour l'entraînement, mais que
                    l'ensemble de données est principalement non étiqueté.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">76
                <a class="prev" href="#slide75"></a>
                <a class="next" href="#slide77"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide77">
        <div class="header">
            <h1>2.2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Applications</h1>
            <ul>
                <li><b>Détection d'intrusion</b> : Identifier les activités malveillantes ou non autorisées dans les
                    réseaux informatiques pour protéger les systèmes contre les cyberattaques.</li>
                <li><b>Détection de fraude</b> : Repérer les transactions financières suspectes ou les activités
                    frauduleuses dans les transactions en ligne, les cartes de crédit, ou les assurances.</li>
                <li><b>System health monitoring</b> : Surveiller en continu la santé des systèmes informatiques, des
                    machines industrielles ou des équipements médicaux pour détecter les pannes ou les défaillances
                    potentielles.</li>
                <li><b>Détection d'événements dans les réseaux de capteurs</b> : Identifier les événements inhabituels
                    ou les comportements anormaux dans les réseaux de capteurs environnementaux, tels que la
                    surveillance de la qualité de l'air ou la détection des intrusions dans les systèmes de sécurité.
                </li>
                <li><b>Détection d'abus dans un système d'information</b> : Identifier les utilisateurs ou les activités
                    qui abusent ou enfreignent les politiques de sécurité dans les systèmes d'information, les
                    applications en ligne ou les plateformes de réseaux sociaux.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">77
                <a class="prev" href="#slide76"></a>
                <a class="next" href="#slide78"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide78">
        <div class="header">
            <h1>2.2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Caractéristiques</h3>
            <p><b>Des sursauts inattendus</b> : Les anomalies peuvent se manifester sous forme de sursauts ou de pointes
                inattendues dans les données. Par exemple, une augmentation soudaine du trafic sur un site Web peut
                indiquer une attaque de déni de service (DDoS) dans le cas de la surveillance du trafic réseau, ou une
                augmentation anormale des transactions financières peut signaler une fraude.</p>
            <p>Les caractéristiques des données varient selon le domaine d'application et les types spécifiques
                d'anomalies recherchées. Identifiez les schémas inhabituels ou les comportements aberrants dans les
                données peut aider à détecter les anomalies et à prendre des mesures appropriées pour les gérer.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">78
                <a class="prev" href="#slide77"></a>
                <a class="next" href="#slide79"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide79">
        <div class="header">
            <h1>2.2.6. Détection d'anomalies</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation</h3>
            <ul>
                <li>Soit \(Y\) un ensemble de mesures. Cela représente les données ou les variables observées qui sont
                    surveillées pour détecter les anomalies.</li>
                <li>Soit \(P_Y(y)\) un modèle statistique pour la distribution des \(Y\) dans des conditions
                    "normales". Les données normales sont généralement modélisées par une distribution statistique telle
                    que la distribution normale (gaussienne). Ce modèle est utilisé pour estimer la probabilité que les
                    données observées soient normales.</li>
                <li>Soit \(T\) un seuil défini par l'utilisateur. C'est une valeur seuil fixée par l'utilisateur qui
                    détermine à partir de quelle probabilité une mesure est considérée comme anormale. Les mesures dont
                    la probabilité estimée est inférieure à ce seuil sont considérées comme des anomalies.</li>
                <li>Une mesure \(x\) est une valeur isolée si \(P_Y(x) &lt; T\). Cette condition spécifie que si la
                    probabilité d'une mesure est inférieure au seuil défini, cette mesure est considérée comme isolée ou
                    anormale par rapport aux autres observations.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">79
                <a class="prev" href="#slide78"></a>
                <a class="next" href="#slide80"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide80">
        <div class="header">
            <h1>2.2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading"></h1>
            <ul>
                <li><b>Synthèse courte d'un ensemble de données</b> : Elle consiste à résumer de manière concise les
                    principales caractéristiques et tendances des données. Cela peut inclure des statistiques
                    descriptives telles que la moyenne, la médiane, l'écart-type, ainsi que des visualisations
                    récapitulatives comme des histogrammes, des graphiques linéaires ou des diagrammes à barres.
                    L'objectif est de fournir une vue d'ensemble rapide et informative des données.</li>
                <li><b>Génération de rapports</b> : la génération de rapports peut être utilisée pour communiquer
                    efficacement les informations clés aux parties prenantes. Ces rapports peuvent prendre différentes
                    formes, telles que des documents écrits, des présentations visuelles ou des tableaux de bord
                    interactifs. L'accent est mis sur la clarté, la concision et la pertinence des informations
                    présentées pour aider les décideurs à prendre des décisions éclairées.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">80
                <a class="prev" href="#slide79"></a>
                <a class="next" href="#slide81"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide81">
        <div class="header">
            <h1>2.2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Applications</h1>
            <ul>
                <li><b>Extraction des mots-clès</b> : Identifier les mots ou expressions clés dans un texte ou un
                    ensemble de documents pour résumer leur contenu de manière succincte.</li>
                <li><b>Récapitulation de documents</b> : Résumer le contenu et les principales idées d'un document ou
                    d'un ensemble de documents pour en faciliter la compréhension et l'assimilation.</li>
                <li><b>Moteurs de recherche</b> : Fournir des résumés pertinents des pages Web ou des résultats de
                    recherche afin d'aider les utilisateurs à trouver rapidement les informations qu'ils recherchent.
                </li>
                <li><b>Récapitulation d'images</b> : Extraire les caractéristiques importantes d'une image pour résumer
                    son contenu ou en faciliter la recherche et la classification.</li>
                <li><b>Récapitulation de vidéos</b> : Identifier et résumer les événements principaux ou les moments
                    saillants dans une vidéo pour permettre aux utilisateurs de naviguer rapidement dans le contenu et
                    de trouver des informations spécifiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">81
                <a class="prev" href="#slide80"></a>
                <a class="next" href="#slide82"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide82">
        <div class="header">
            <h1>2.2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation: Synthèse multi-documents</h3>
            <ul>
                <li>Soit \(\{D = D_1, ..., D_k\}\) une collection de \(k\) documents </li>
                <li>Un document \(\{D = t_1, ..., t_m\}\) se compose de m unités textuelles (mots, phrases, paragraphes,
                    etc.) </li>
                <li>Soit \(\{D = t_1, ..., t_n\}\) être l'ensemble complet de toutes les unités textuelles de tous les
                    documents, où
                    <ul>
                        <li>\(t_i &#8712; D\), si et seulement si \(&#8707; D_j\) de sorte que \(t_i &#8712; D_j\)</li>
                    </ul>
                </li>
                <li>\(S &#8838; D\) constitue une synthèse</li>
                <li> Deux fonctions de scoring
                    <ul>
                        <li>\(Rel(i)\): pertinence de l'unité textuelle \(i\) dans le résumé</li>
                        <li>\(Red(i,j)\): Redondance entre deux unités textuelles \(t_i, t_j\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">82
                <a class="prev" href="#slide81"></a>
                <a class="next" href="#slide83"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide83">
        <div class="header">
            <h1>2.2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Formalisation: Multidocument summarization</h3>
            <ul>
                <li>La note pour un résumé \(S\)
                    <ul>
                        <li>\(s(S)\) note pour un résumé S</li>
                        <li>\(l(i)\) est la longueur de l'unité textuelle \(i\)</li>
                        <li>\(K\) est la longueur maximale fixée du résumé</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2018/DataMining/scoringfunction.png" height="200px"
                    width="500px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">83
                <a class="prev" href="#slide82"></a>
                <a class="next" href="#slide84"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide84">
        <div class="header">
            <h1>2.2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <p>Trouver un sous-ensemble à partir de l'ensemble du sous-ensemble.</p>
            <ol>
                <li><b>Extraction</b>: Cette approche implique la sélection d'un sous-ensemble de mots, de
                    phrases ou d'expressions existants dans le texte original sans aucune modification.
                    L'objectif est de repérer les parties les plus importantes du texte et de les présenter de
                    manière concise dans le résumé. Les techniques utilisées dans cette approche incluent
                    l'identification de phrases clés, la classification des phrases par importance, et
                    l'extraction de phrases représentatives.</li>
                <li><b>Abstraction</b>: Contrairement à l'extraction, l'approche d'abstraction implique la
                    construction d'une représentation sémantique interne du texte, suivie de l'utilisation de
                    techniques de génération du langage naturel pour produire un résumé. Cela nécessite une
                    compréhension plus profonde du contenu du texte et la capacité de reformuler les idées de
                    manière concise tout en préservant leur signification. Les techniques d'abstraction peuvent
                    inclure la réécriture de phrases, la fusion d'informations similaires et la génération de
                    paraphrases.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">84
                <a class="prev" href="#slide83"></a>
                <a class="next" href="#slide85"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide85">
        <div class="header">
            <h1>2.2.7. Récapitulation</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Résumé extractif</h3>
            <ol>
                <li><b>Résumé générique</b>: Cette approche vise à obtenir un résumé général et représentatif du
                    contenu original en extrayant les informations les plus importantes et les plus pertinentes.
                    Elle cherche à capturer l'essence du texte original en identifiant les phrases ou les
                    sections clés qui révèlent les principaux points et concepts abordés. Ce type de résumé est
                    souvent utilisé dans des contextes où une vue d'ensemble est nécessaire sans se concentrer
                    sur des aspects spécifiques ou des détails.</li>
                <li><b>Résumé pertinent pour la recherche</b> : Cette approche vise à produire un résumé qui
                    répond spécifiquement aux besoins ou aux intérêts d'un utilisateur ou d'une tâche de
                    recherche particulière. Elle utilise des techniques de sélection de phrases basées sur la
                    pertinence pour extraire les parties du texte qui correspondent aux critères de recherche
                    spécifiques de l'utilisateur. Cela permet de fournir des résumés plus ciblés et adaptés aux
                    besoins individuels, ce qui peut être particulièrement utile dans les domaines où la
                    précision et la pertinence sont essentielles, comme la recherche d'informations spécialisées
                    ou la prise de décision.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">85
                <a class="prev" href="#slide84"></a>
                <a class="next" href="#slide86"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide86">
        <div class="header">
            <h1>2.3. Algorithmes</h1>
        </div>
        <div class="content">
            <h1></h1>
            <ol>
                <li>Support Vector Machines (SVM)</li>
                <li>Descente du gradient stochastique</li>
                <li>Voisins proches</li>
                <li>Bayes naïfs</li>
                <li>Arbres de décision</li>
                <li>Ensemble Methods (Forêt d'arbres décisionnels)</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">86
                <a class="prev" href="#slide85"></a>
                <a class="next" href="#slide87"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide87">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Introduction</h3>
            <p>La machine à vecteurs de support (SVM) est une méthode d'apprentissage supervisé. SVM cherche à trouver
                la meilleure frontière de décision qui optimise la séparation des classes, ce qui permet une
                classification précise même dans des espaces de données complexes.</p>
            <ul>
                <li>Elle est principalement utilisée pour la classification binaire, bien qu'elle puisse être étendue à
                    des problèmes de classification multiclasse.</li>
                <li>L'objectif principal de SVM est de construire un hyperplan qui maximise la marge de séparation entre
                    les deux classes. L'hyperplan est la frontière de décision qui sépare les données en deux classes
                    distinctes.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/SVM Separating Hyperplanes.svg"
                    height="100px" width="350px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">87
                <a class="prev" href="#slide86"></a>
                <a class="next" href="#slide88"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide88">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Hyperplane</h3>
            <p>L'hyperplan dans l'espace n-dimensionnel est un sous-espace de dimension n-1 qui permet de séparer les
                données en deux classes.</p>
            <ul>
                <li>Dans un espace à deux dimensions, l'hyperplan est une ligne à une dimension qui sépare les données
                    en deux régions.</li>
                <li>Dans un espace tridimensionnel, l'hyperplan est un plan bidimensionnel qui divise l'espace en deux
                    parties distinctes. </li>
                <li>L'hyperplan d'un espace tridimensionnel est un plan bidimensionnel</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/SVM Separating Hyperplanes.svg"
                    height="200px" width="350px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">88
                <a class="prev" href="#slide87"></a>
                <a class="next" href="#slide89"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide89">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Définition formelle</h3>
            <ul style="width:50%">
                <li>Le but d'un SVM est d'estimer une fonction \(f: R^N &#10761; {+1,-1}\), c'est à dire,
                    <ul>
                        <li>Si \(x_1,...,x_l\) &#8712; \(R^N\) sont les \(N\) points de données d'entrée,</li>
                        <li>L'objectif est de trouver \((x_1,y_1),...,(x_l,y_l)\) &#8712; \(R^N &#10761; {+1,-1}\)</li>
                    </ul>
                </li>
                <li>Tout hyperplan peut être écrit par l'équation en utilisant un ensemble de points d'entrée \(x\)
                    <ul>
                        <li>\(w.x - b = 0\), où</li>
                        <li>\(w &#8712; R^N\), un vecteur normal à la plane </li>
                        <li>\(b &#8712; R\)</li>
                    </ul>
                </li>
                <li> Une fonction de décision est donnée par \(f(x) = sign(w.x - b )\)
                </li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Surface_normal_illustration.svg"
                    height="300px" />
                <figcaption>Normal vector</figcaption>
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">89
                <a class="prev" href="#slide88"></a>
                <a class="next" href="#slide90"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide90">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Définition formelle</h1>
            <ul style="width:40%">
                <li>Si les données de formation sont séparables linéairement, deux hyperplans peuvent être sélectionnés
                </li>
                <li>Ils séparent les deux classes de données, <br> afin que la distance entre elles soit la plus grande
                    possible.</li>
                <li>Les hyperplans peuvent être donnés par les équations
                    <ul>
                        <li>\(w.x - b = 1\)</li>
                        <li>\(w.x - b = -1\)</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png"
                    height="500px" width="500px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">90
                <a class="prev" href="#slide89"></a>
                <a class="next" href="#slide91"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide91">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Définition formelle</h1>
            <ul style="width:40%">
                <li>La distance entre les deux hyperplans peut être donnée par \( \frac{2}{||w||} \)</i>
                </li>
                <li>La région située entre ces deux hyperplans est appelée marge.</li>
                <li>L'hyperplan à marge maximale est l'hyperplan <br> qui se trouve à mi-chemin entre eux.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png"
                    height="500px" width="500px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">91
                <a class="prev" href="#slide90"></a>
                <a class="next" href="#slide92"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide92">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Définition formelle</h1>
            <ul>
                <li>Afin d'éviter que les points de données ne tombent dans la marge, les contraintes suivantes sont
                    ajoutées
                    <ul>
                        <li>\(w.x_i - b &gt;= 1\), si \(y_i = 1\)</li>
                        <li>\(w.x_i - b &lt;= -1\), si \(y_i = -1\)</li>
                    </ul>
                <li>\(y_i(w.x_i - b) &gt;= 1\), \(1&lt;= i &lt;= n\)</li>
                </li>
                <li>L'objectif est de minimiser ||w|| sous réserve de \(y_i(w.x_i - b) &gt;= 1\), \(1&lt;= i &lt;= n\)
                </li>
                <li>Une solution pour les deux \(w\) et \(b\) donne le classificateur \(f(x) = sign(w.x - b)\)</li>
                <li>L'hyperplan à marge maximale est entièrement déterminé par les points qui en sont les plus proches,
                    appelés vecteurs de soutien</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">92
                <a class="prev" href="#slide91"></a>
                <a class="next" href="#slide93"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide93">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Data mining</h3>
            <ul>
                <li><b>Classification</b> : SVM peut être utilisée pour la classification binaire ainsi que pour la
                    classification multi-classes, où elle cherche à séparer les données en plusieurs catégories
                    distinctes en construisant des hyperplans dans un espace multidimensionnel.</li>
                <li><b>Régression</b> : SVM peut également être appliquée à des problèmes de régression, où elle cherche
                    à prédire une valeur continue plutôt que de classer des données en catégories discrètes.</li>
                <li><b>Détection des anomalies</b> : SVM peut être utilisée pour détecter les anomalies dans les données
                    en identifiant les points de données qui sont significativement différents du reste de l'ensemble de
                    données, ce qui en fait un outil précieux pour la détection des fraudes ou des erreurs dans les
                    données.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">93
                <a class="prev" href="#slide92"></a>
                <a class="next" href="#slide94"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide94">
        <div class="header">
            <h1>2.3.1. Machine à vecteurs de support (SVM)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Catégorisation des textes et des hypertextes</b> : Les SVM sont largement utilisées pour classer
                    automatiquement les documents texte dans différentes catégories, comme la classification des
                    courriels en spam ou en non-spam, la catégorisation des articles de presse, etc.</li>
                <li><b>Classification des images</b> : SVM est efficace pour classer des images dans des catégories
                    prédéfinies, comme la classification des images médicales en différentes maladies, la reconnaissance
                    des visages, la détection d'objets dans des images, etc.</li>
                <li><b>Reconnaissance de l'écriture manuscrite</b> : SVM est également utilisée dans les systèmes de
                    reconnaissance de l'écriture manuscrite pour identifier les caractères ou les mots écrits à la main
                    et les transcrire en texte numérique.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">94
                <a class="prev" href="#slide93"></a>
                <a class="next" href="#slide95"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide95">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <p>Le gradient stochastique de descente est une technique d'optimisation utilisée pour minimiser une
                fonction objective qui peut être exprimée comme une somme de fonctions différenciables. </p>
            <ul>
                <li>Il s'agit d'une approximation stochastique de l'optimisation de la descente du gradient, où le
                    calcul du gradient est effectué de manière aléatoire sur un sous-ensemble des données à chaque
                    itération.</li>
                <li>Cette méthode est itérative, ce qui signifie qu'elle effectue des mises à jour progressives des
                    paramètres du modèle pour se rapprocher du minimum ou du maximum de la fonction objectif.</li>
                <li>Le gradient stochastique de descente est particulièrement efficace pour traiter de grands ensembles
                    de données, car il permet de calculer les mises à jour des paramètres de manière incrémentielle, ce
                    qui réduit la charge de calcul par rapport à l'optimisation classique du gradient.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">95
                <a class="prev" href="#slide94"></a>
                <a class="next" href="#slide96"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide96">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Gradient</h1>
            <p>Le gradient est une généralisation multi-variable de la notion de dérivée. </p>
            <ul style="width:40%">
                <li>Le gradient donne la pente de la tangente du graphe d'une fonction à un point donné dans l'espace
                    multi-dimensionnel. Il indique dans quelle direction et dans quelle mesure la fonction augmente ou
                    diminue le plus rapidement à partir de ce point.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="200px"
                    width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">96
                <a class="prev" href="#slide95"></a>
                <a class="next" href="#slide97"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide97">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Gradient</h1>
            <ul style="width:40%">
                <li>Dans le cas des fonctions à plusieurs variables, le gradient est un vecteur qui contient les
                    dérivées partielles de la fonction par rapport à chacune de ses variables. Chaque composante du
                    gradient correspond à la pente de la fonction dans la direction respective de la variable
                    correspondante.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="200px"
                    width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">97
                <a class="prev" href="#slide96"></a>
                <a class="next" href="#slide98"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide98">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h1 class="topicsubheading">Gradient</h1>
            <ul style="width:40%">
                <li>Géométriquement, le gradient pointe dans la direction du plus grand taux d'augmentation de la
                    fonction. En d'autres termes, il indique la direction dans laquelle la fonction croît le plus
                    rapidement à partir du point considéré.</li>
                <li>L'amplitude du gradient représente la pente du graphique de la fonction dans la direction indiquée
                    par le gradient. Plus cette amplitude est grande, plus la fonction augmente rapidement dans cette
                    direction.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient2.svg" height="300px"
                    width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">98
                <a class="prev" href="#slide97"></a>
                <a class="next" href="#slide99"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide99">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Gradient ou dérivé</h3>
            <table>
                <thead>
                    <tr>
                        <th></th>
                        <th>Dérivé</th>
                        <th>Gradient</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Définition</td>
                        <td>Taux de variation instantanée d'une fonction</td>
                        <td>Vecteur des dérivées partielles d'une fonction de plusieurs variables</td>
                    </tr>
                    <tr>
                        <td>Nombre de variables</td>
                        <td>Une seule</td>
                        <td>Plusieurs</td>
                    </tr>
                    <tr>
                        <td>Nature</td>
                        <td>Fonction scalaire</td>
                        <td>Fonction vectorielle</td>
                    </tr>
                    <tr>
                        <td>Représentation</td>
                        <td>Un seul nombre réel</td>
                        <td>Un vecteur de nombres réels</td>
                    </tr>
                    <tr>
                        <td>Utilisation</td>
                        <td>Fonctions d'une seule variable</td>
                        <td>Fonctions de plusieurs variables, notamment en optimisation et en machine learning</td>
                    </tr>
                    <tr>
                        <td>Géométrie</td>
                        <td>Pente de la tangente à un point d'une courbe</td>
                        <td>Direction et taux de variation le plus rapide d'une fonction dans un espace
                            multi-dimensionnel</td>
                    </tr>
                </tbody>
            </table>

        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">99
                <a class="prev" href="#slide98"></a>
                <a class="next" href="#slide100"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide100">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Algorithme du gradient</h3>
            <p>L'algorithme du gradient stochastique de descente est un algorithme d'optimisation itératif largement
                utilisé pour trouver le minimum d'une fonction.</p>
            <ol>
                <li><b>Initialisation</b> : Choisissez un point de départ aléatoire ou prédéfini dans l'espace des
                    paramètres.</li>
                <li><b>Calcul du gradient</b> : Calculez le gradient de la fonction objective par rapport aux paramètres
                    au point courant.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg"
                        height="150px" />
                </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">100
                <a class="prev" href="#slide99"></a>
                <a class="next" href="#slide101"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide101">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Algorithme du gradient</h3>
            <ol start="3">
                <li><b>Mise à jour des paramètres</b> : Mettez à jour les paramètres dans la direction opposée au
                    gradient. Cela implique de soustraire une fraction du gradient de chaque paramètre.</li>
                <li><b>Répétition</b> : Répétez les étapes 2 et 3 jusqu'à ce qu'un critère d'arrêt soit satisfait, par
                    exemple, un nombre fixe d'itérations, une petite variation de la fonction objective ou une tolérance
                    pour le gradient.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg"
                        height="200px" width="300px" />
                </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">101
                <a class="prev" href="#slide100"></a>
                <a class="next" href="#slide102"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide102">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Algorithme du gradient</h3>
            <p> L'algorithme stochastique du gradient de descente est une variante où le gradient est calculé de manière
                stochastique, c'est-à-dire qu'au lieu d'utiliser l'ensemble complet des données pour calculer le
                gradient à chaque itération, un sous-ensemble aléatoire ou une seule observation est utilisé. Cela
                permet de gagner en efficacité, en particulier pour les grands ensembles de données.</p>
            <p>L'objectif principal de cet algorithme est de minimiser une fonction objective, souvent une fonction de
                perte dans le cadre de l'apprentissage automatique, et il est largement utilisé dans des domaines tels
                que l'optimisation convexe, l'apprentissage automatique et le traitement du signal.</p>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Gradient_descent.svg" height="300px"
                    width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">102
                <a class="prev" href="#slide101"></a>
                <a class="next" href="#slide103"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide103">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Méthode standard de descente de gradient</h3>
            <ul>
                <li>Prenons le problème de la minimisation d'une fonction objective
                    <ul>
                        <li>\(Q(w) = \frac{1}{n} (&#931;Q_i(w)), 1&lt;=i&lt;n\)</li>
                        <li>\(Q_i(w)\) est la valeur de la fonction objectif pour le \(i\)-ème exemple.</li>
                        <li>\(Q(w)\) est le risque empirique.</li>
                    </ul>
                </li>
                <li>\(w = w - &#951;.&#8711; Q(w)\)</li>
                <li>\(w = w - \frac{\eta}{n} \sum_{i=1}^n \nabla Q_i(w)\), \(\eta\) est le pas de l'itération </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">103
                <a class="prev" href="#slide102"></a>
                <a class="next" href="#slide104"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide104">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Méthode itérative</h3>
            <ul>
                <li>Choisissez un vecteur initial de paramètres \(w\) et le taux d'apprentissage &#951;.</li>
                <li>Répétez l'opération jusqu'à l'obtention d'un minimum approximatif :
                    <ul>
                        <li>Mélangez aléatoirement les exemples dans le jeu de formation.</li>
                        <li>\(w = w - &#951;.&#8711; Q_i(w)\), \(i=1...n\)</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">104
                <a class="prev" href="#slide103"></a>
                <a class="next" href="#slide105"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide105">
        <div class="header">
            <h1>2.3.2. Gradient stochastique de descente</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Classification</b> : le SGD est souvent utilisé pour entraîner des modèles de classification tels
                    que les machines à vecteurs de support (SVM), les réseaux de neurones et les modèles de régression
                    logistique. Il permet de trouver les paramètres optimaux du modèle en minimisant une fonction de
                    perte, ce qui conduit à une meilleure capacité de classification.</li>
                <li><b>Régression</b> : Le SGD est également utilisé pour l'entraînement de modèles de régression, où
                    l'objectif est de prédire une valeur continue en fonction d'un ensemble de variables explicatives.
                    Il est couramment utilisé dans des domaines tels que l'analyse financière, la prédiction des prix et
                    la modélisation des séries chronologiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">105
                <a class="prev" href="#slide104"></a>
                <a class="next" href="#slide106"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide106">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <p> La méthode des k plus proches voisins (kNN) et le partitionnement en k-moyennes (k-means clustering)
                sont deux techniques importantes en apprentissage automatique et en exploration de données :</p>
            <ol>
                <li><b>Méthode des k plus proches voisins (kNN)</b> : C'est un algorithme d'apprentissage supervisé
                    utilisé pour la classification et la régression. L'idée principale derrière kNN est de trouver les k
                    échantillons d'entraînement les plus proches du point de données de test et de prédire l'étiquette
                    de classe en fonction de la classe majoritaire parmi ces voisins. Pour la régression, la prédiction
                    est la moyenne des valeurs cibles des k voisins les plus proches.</li>
                <li><b>Partitionnement en k-moyennes (k-means clustering)</b> : C'est une méthode non supervisée de
                    partitionnement de données en k groupes distincts. L'algorithme fonctionne en répétant deux étapes :
                    d'abord, il attribue chaque point de données au groupe dont le centroïde est le plus proche, puis il
                    met à jour les centroïdes en calculant la moyenne de tous les points attribués à chaque groupe. Ces
                    étapes sont répétées jusqu'à ce qu'une convergence soit atteinte et que les centroïdes ne changent
                    plus de manière significative.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">106
                <a class="prev" href="#slide105"></a>
                <a class="next" href="#slide107"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide107">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <figure class="flexcontent">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/KnnClassification.svg"
                    height="350px" width="350px" />
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                    height="350px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">107
                <a class="prev" href="#slide106"></a>
                <a class="next" href="#slide108"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide108">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <ul>
                <li><b>Méthode de partitionnement de données :</b> Le partitionnement en k-moyennes vise à diviser un
                    ensemble de données en k groupes (clusters) distincts. Chaque cluster est représenté par son
                    centroïde, qui est la moyenne de tous les points appartenant à ce cluster.</li>
                <li><b>Entrée :</b> L'entrée de l'algorithme est un ensemble de points de données ainsi que le nombre k
                    de clusters souhaité. Ces points de données peuvent avoir plusieurs dimensions.</li>
                <li><b>Objectif :</b> L'objectif principal du k-means clustering est de minimiser la variance
                    intra-cluster, c'est-à-dire de minimiser la somme des distances au carré de chaque point par rapport
                    à son centroïde assigné.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                    height="100px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">108
                <a class="prev" href="#slide107"></a>
                <a class="next" href="#slide109"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide109">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <ul>
                <li><b>Fonctionnement :</b> L'algorithme k-means fonctionne en itérant entre deux étapes principales :
                    <ul>
                        <li><b>Attribution des points aux clusters</b> : Chaque point de données est assigné au cluster
                            dont le centroïde est le plus proche en termes de distance euclidienne.</li>
                        <li><b>Mise à jour des centroïdes</b> : Une fois que tous les points ont été attribués à des
                            clusters, les centroïdes de chaque cluster sont mis à jour en calculant la moyenne des
                            points appartenant à ce cluster.</li>
                    </ul>
                </li>
                <li><b>Convergence :</b> Les deux étapes ci-dessus sont répétées de manière itérative jusqu'à ce qu'une
                    convergence soit atteinte, c'est-à-dire que les centroïdes ne changent plus significativement entre
                    deux itérations successives.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                    height="200px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">109
                <a class="prev" href="#slide108"></a>
                <a class="next" href="#slide110"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide110">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 1 (Initialisation)</h2>
            <p> Les "moyens", également appelés centroïdes, sont les points initiaux autour desquels les clusters seront
                formés. Dans cette étape, k points sont sélectionnés de manière aléatoire à partir de l'ensemble de
                données pour servir de moyens initiaux.</p>
            <figure>
                <img src="../../2021/DataMining/K_Means_Example_Step_1.svg" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">110
                <a class="prev" href="#slide109"></a>
                <a class="next" href="#slide111"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide111">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 2 (Étape d'affectation)</h2>
            <p>Dans la deuxième étape de l'algorithme de partitionnement en k-moyennes (k-means clustering), également
                connue sous le nom d'étape d'affectation, les k clusters sont créés en associant chaque observation à la
                moyenne la plus proche.Les partitions
                représentent ici le diagramme de Voronoï généré par les moyennes.</p>
            <figure>
                <img src="../../2021/DataMining/K_Means_Example_Step_2.svg" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">111
                <a class="prev" href="#slide110"></a>
                <a class="next" href="#slide112"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide112">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 2 (Étape d'affectation)</h2>
            <li><b>Calcul des distances :</b> Pour chaque observation de l'ensemble de données, la distance jusqu'à
                chaque moyen est calculée. La distance la plus couramment utilisée est la distance euclidienne, mais
                d'autres mesures de distance peuvent également être utilisées en fonction des besoins spécifiques de
                l'application.</li>
            <li><b>Association des observations aux clusters :</b> Une fois les distances calculées, chaque observation
                est associée au cluster dont le moyen est le plus proche. Cela crée k partitions dans l'ensemble de
                données, où chaque partition contient les observations associées à un cluster spécifique.</li>
            <li><b>Diagramme de Voronoï :</b> Les partitions formées dans cette étape peuvent être visualisées comme un
                diagramme de Voronoï dans l'espace des données. Chaque cluster est représenté par une région de l'espace
                des données où les points sont plus proches de son moyen que de tout autre moyen.</li>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">112
                <a class="prev" href="#slide111"></a>
                <a class="next" href="#slide113"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide113">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 3 (Étape de mise à jour et calcul du centroïde)</h2>
            <p>Les centroids de chacun des k agrégats sont recalculés pour devenir les nouvelles moyennes.</p>
            </ul>

            <figure>
                <img src="../../2021/DataMining/K_Means_Example_Step_3.svg" height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">113
                <a class="prev" href="#slide112"></a>
                <a class="next" href="#slide114"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide114">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 3 (Étape de mise à jour et calcul du centroïde)</h2>
            <ul>
                <li><b>Calcul du centroïde</b> : Pour chaque cluster formé à l'étape précédente, le centroïde est
                    calculé. Le centroïde est simplement la moyenne de toutes les observations qui appartiennent à ce
                    cluster. Il est calculé en prenant la moyenne des coordonnées de toutes les observations dans le
                    cluster, ce qui donne une position centrale représentative.</li>
                <li><b>Mise à jour des moyennes</b> : Une fois que les centroïdes de tous les clusters ont été calculés,
                    ils deviennent les nouvelles moyennes pour la prochaine itération de l'algorithme. Les anciennes
                    moyennes sont alors remplacées par les nouveaux centroïdes.</li>
            </ul>

            <figure>
                <img src="../../2021/DataMining/K_Means_Example_Step_3.svg" height="100px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">114
                <a class="prev" href="#slide113"></a>
                <a class="next" href="#slide115"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide115">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 4 (Répéter jusqu'à la convergence)</h2>
            <p>La quatrième étape de l'algorithme de partitionnement en k-moyennes (k-means clustering) consiste à
                répéter les étapes 2 et 3 jusqu'à ce que la convergence soit atteinte. </p>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png"
                    height="200px" width="300px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">115
                <a class="prev" href="#slide114"></a>
                <a class="next" href="#slide116"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide116">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Partitionnement en k-moyennes</h3>
            <h2 class="topicsubheading">Étape 4 (Répéter jusqu'à la convergence)</h2>
            <ul>
                <li><b>Répétition des étapes précédentes</b> : Les étapes d'affectation (étape 2) et de mise à jour des
                    moyennes (étape 3) sont répétées itérativement jusqu'à ce que la convergence soit atteinte. Cela
                    signifie que les observations sont successivement associées aux clusters en fonction de leur
                    proximité aux moyennes actuelles, puis les moyennes sont mises à jour en fonction des observations
                    assignées à chaque cluster.</li>
                <li><b>Critère de convergence</b> : L'algorithme a convergé lorsque les affectations ne changent plus
                    entre les itérations successives. Cela signifie que les clusters ne subissent plus de changements
                    significatifs et que les moyennes ne se déplacent plus de manière significative entre les
                    itérations.</li>
                <li><b>Arrêt de l'algorithme</b> : Une fois que la convergence est atteinte, l'algorithme s'arrête et
                    les clusters finaux ainsi que leurs centroïdes associés sont considérés comme la solution de
                    l'algorithme.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">116
                <a class="prev" href="#slide115"></a>
                <a class="next" href="#slide117"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide117">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
            <p>La méthode des k plus proches voisins (k-NN) est un algorithme d'apprentissage supervisé utilisé à la
                fois pour la classification et la régression.</p>
            <ul>
                <li><b>Classification k-NN</b> : Dans ce cas, la sortie est une appartenance à une classe. Pour classer
                    un nouvel objet, l'algorithme k-NN examine les k exemples les plus proches dans l'ensemble
                    d'apprentissage et détermine la classe majoritaire parmi ces voisins. Plus précisément, chaque
                    voisin contribue à un vote, et la classe la plus fréquente parmi les k voisins est attribuée à
                    l'objet à classer. C'est un exemple de vote majoritaire parmi les voisins les plus proches.</li>
                <li><b>Régression k-NN</b> : À la différence de la classification, dans la régression k-NN, la sortie
                    est une valeur de propriété de l'objet. Pour prédire la valeur d'une nouvelle observation,
                    l'algorithme k-NN calcule la valeur moyenne (ou médiane) des valeurs cibles des k plus proches
                    voisins. Par conséquent, au lieu de voter pour une classe majoritaire, les valeurs cibles des k
                    voisins sont utilisées pour prédire la valeur cible de l'objet à estimer.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">117
                <a class="prev" href="#slide116"></a>
                <a class="next" href="#slide118"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide118">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
            <p>Supposons que nous ayons un ensemble de données d'apprentissage composé de points dans un espace
                bidimensionnel, où chaque point est associé à une classe. Supposons que nous voulions classer un nouveau
                point avec des coordonnées (x = 4, y = 3).</p>
            <table>
                <thead>
                    <tr>
                        <th>Point</th>
                        <th>Coordonnée x</th>
                        <th>Coordonnée y</th>
                        <th>Classe</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>A</td>
                        <td>2</td>
                        <td>3</td>
                        <td>Rouge</td>
                    </tr>
                    <tr>
                        <td>B</td>
                        <td>4</td>
                        <td>4</td>
                        <td>Rouge</td>
                    </tr>
                    <tr>
                        <td>C</td>
                        <td>3</td>
                        <td>2</td>
                        <td>Bleu</td>
                    </tr>
                    <tr>
                        <td>D</td>
                        <td>6</td>
                        <td>5</td>
                        <td>Rouge</td>
                    </tr>
                    <tr>
                        <td>E</td>
                        <td>5</td>
                        <td>3</td>
                        <td>Bleu</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">118
                <a class="prev" href="#slide117"></a>
                <a class="next" href="#slide119"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide119">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
            <ol>
                <li><b>Choix de k :</b> Disons que nous choisissons k = 3.</li>
                <li><b>Calcul de la distance :</b> Nous calculons la distance euclidienne entre le nouveau point et
                    chaque point de notre ensemble d'apprentissage.
                    <ul>
                        <li> - Pour le point A : Distance = sqrt((4 - 2)^2 + (3 - 3)^2) = sqrt(4) = 2</li>
                        <li> - Pour le point B : Distance = sqrt((4 - 4)^2 + (3 - 4)^2) = sqrt(1) = 1</li>
                        <li> - Pour le point C : Distance = sqrt((4 - 3)^2 + (3 - 2)^2) = sqrt(2) ≈ 1.41</li>
                        <li> - Pour le point D : Distance = sqrt((4 - 6)^2 + (3 - 5)^2) = sqrt(8) ≈ 2.83</li>
                        <li> - Pour le point E : Distance = sqrt((4 - 5)^2 + (3 - 3)^2) = sqrt(1) = 1</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">119
                <a class="prev" href="#slide118"></a>
                <a class="next" href="#slide120"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide120">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Méthode des k plus proches voisins</h3>
            <ol start="3">
                <li><b>Sélection des k plus proches voisins :</b> Nous identifions les k points les plus proches du
                    nouveau point en termes de distance.
                    <ul>
                        <li> - Pour k = 3, les trois plus proches voisins sont B, C et E.</li>
                    </ul>
                </li>
                <li><b>Vote majoritaire :</b> Enfin, nous attribuons la classe majoritaire parmi les k voisins les plus
                    proches au nouveau point. Dans ce cas, deux des voisins les plus proches (C et E) sont de la classe
                    "Bleu" et un (B) est de la classe "Rouge". Par conséquent, le nouveau point est classé comme
                    "Bleu".</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">120
                <a class="prev" href="#slide119"></a>
                <a class="next" href="#slide121"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide121">
        <div class="header">
            <h1>2.3.3. Méthode des plus proches voisins</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Régression</b> : En utilisant la méthode des plus proches voisins pour la régression, on peut
                    estimer la valeur d'une variable cible pour une nouvelle observation en prenant la moyenne des
                    valeurs de la variable cible des k voisins les plus proches. Par exemple, dans la régression k-NN,
                    on peut prédire le prix d'une maison en prenant la moyenne des prix des k maisons les plus proches
                    en termes de caractéristiques similaires (surface, nombre de chambres, etc.).</li>
                <li><b>Détection des anomalies</b> : La méthode des plus proches voisins peut également être utilisée
                    pour détecter les anomalies dans les données. Les observations qui sont très différentes de leurs
                    voisins les plus proches peuvent être considérées comme des anomalies. Par exemple, dans la
                    surveillance de la santé, des valeurs de signes vitaux inhabituelles par rapport aux voisins les
                    plus proches peuvent indiquer un problème de santé potentiel et donc être considérées comme des
                    anomalies.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">121
                <a class="prev" href="#slide120"></a>
                <a class="next" href="#slide122"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide122">
        <div class="header">
            <h1>2.3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <p>La classification naïve bayésienne est une méthode de classification probabiliste simple basée sur
                l'application du théorème de Bayes avec une forte hypothèse d'indépendance entre les caractéristiques.
            </p>
            <ul>
                <li><b>Théorème de Bayes</b> : La classification naïve bayésienne repose sur le théorème de Bayes, qui
                    est une formule pour calculer les probabilités conditionnelles. Il permet de calculer la probabilité
                    qu'une observation appartienne à une classe donnée en utilisant les probabilités des
                    caractéristiques étant donné chaque classe.</li>
            </ul>

        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">122
                <a class="prev" href="#slide121"></a>
                <a class="next" href="#slide123"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide123">
        <div class="header">
            <h1>2.3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Hypothèse d'indépendance naïve</b> : La caractéristique principale de la classification naïve
                    bayésienne est l'hypothèse d'indépendance naïve, qui suppose que les caractéristiques sont
                    indépendantes les unes des autres conditionnellement à la classe. Cela signifie que la présence
                    d'une caractéristique particulière dans une classe ne dépend pas de la présence d'autres
                    caractéristiques.</li>
                <li><b>Modélisation des probabilités</b> : Pour chaque classe, la classification naïve bayésienne
                    modélise les distributions de probabilité des caractéristiques. Ensuite, lorsqu'une nouvelle
                    observation est introduite, elle utilise le théorème de Bayes pour calculer la probabilité qu'elle
                    appartienne à chaque classe et choisit la classe avec la probabilité la plus élevée.</li>
            </ul>

        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">123
                <a class="prev" href="#slide122"></a>
                <a class="next" href="#slide124"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide124">
        <div class="header">
            <h1>2.3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Classification des documents</b> : Elle est largement utilisée pour la classification des
                    documents, tels que la détection de spam (classification des e-mails en spam ou non-spam) et la
                    catégorisation de documents dans différentes catégories.</li>
                <li><b>Analyse de sentiments :</b> Dans le domaine du traitement du langage naturel, la classification
                    naïve bayésienne est utilisée pour l'analyse des sentiments, où elle peut être utilisée pour
                    classifier les textes en fonction de leur tonalité émotionnelle, comme positif, négatif ou neutre.
                </li>
                <li><b>Catégorisation de documents :</b> Elle est également utilisée pour la catégorisation automatique
                    de documents, où elle peut être utilisée pour classifier les documents dans des catégories
                    spécifiques en fonction de leur contenu ou de leur sujet.</li>
                <li><b>Reconnaissance de caractères :</b> Dans le domaine de la vision par ordinateur, la classification
                    naïve bayésienne est utilisée pour la reconnaissance de caractères, où elle peut être utilisée pour
                    classifier les caractères écrits à la main dans différentes classes, telles que les lettres de
                    l'alphabet ou les chiffres.</li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">124
                <a class="prev" href="#slide123"></a>
                <a class="next" href="#slide125"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide125">
        <div class="header">
            <h1>2.3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Théorème de Bayes</h3>
            <ul>
                <li>\(P(A), P(B)\) sont des probabilités d'observer A et B indépendamment l'un de l'autre.</li>
                <li>\(P(A|B)\) est une probabilité conditionnelle, la probabilité que l'événement \(A\) se produise
                    étant donné que \(B\) est vrai</li>
                <li>\(P(B|A)\) est une probabilité conditionnelle, la probabilité que l'événement \(B\) se produise
                    étant donné que \(A\) est vrai</li>
                <li> \(P(B) &#8800; 0\)</li>
            </ul>
            <p>\[P(A|B) = \frac{(P(B|A).P(A))}{P(B)}\]</i>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">125
                <a class="prev" href="#slide124"></a>
                <a class="next" href="#slide126"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide126">
        <div class="header">
            <h1>2.3.4. Classification naïve bayésienne</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Théorème de Bayes: Classification d'un message</h3>
            <ul>
                <li>\(P(S)\) est la probabilité globale qu'un message donné soit un spam.</li>
                <li>\(P(H)\) est la probabilité globale qu'un message donné ne soit pas du spam.</li>
                <li>\(P(S|W)\) est la probabilité qu'un message soit un spam, sachant que le mot s'y trouve ;</li>
                <li>\(P(W|S)\) est la probabilité que le mot apparaisse dans les messages de spam ;</li>
                <li>\(P(W|H)\) est la probabilité que le mot "réplique" apparaisse dans les messages ham.</li>
            </ul>
            <p>\[P(S|W) = \frac{P(W|S) \cdot P(S)}{P(W|S) \cdot P(S) + P(W|H) \cdot P(H)}\]</i>
            </p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">126
                <a class="prev" href="#slide125"></a>
                <a class="next" href="#slide127"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide127">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <p>Les arbres de décision sont un outil puissant d'aide à la décision qui utilise un modèle arborescent pour
                représenter les décisions et leurs conséquences possibles</p>
            <ul>
                <li><b>Modèle arborescent :</b> Les arbres de décision représentent les décisions sous forme d'un arbre,
                    où chaque nœud interne représente une caractéristique (ou attribut), chaque branche représente une
                    décision basée sur cette caractéristique, et chaque feuille représente un résultat ou une classe.
                </li>
                <li><b>Facile à interpréter :</b> Les arbres de décision sont faciles à comprendre et à interpréter, ce
                    qui les rend populaires pour la prise de décision dans de nombreux domaines.</li>
                <li><b>Adaptabilité :</b> Ils peuvent être utilisés pour modéliser des problèmes de classification ainsi
                    que des problèmes de régression.</li>
                <li><b>Utilisation de règles simples :</b> Les décisions sont prises en suivant des règles simples
                    basées sur les valeurs des caractéristiques, ce qui rend l'interprétation des résultats facile même
                    pour les non-experts.</li>
            </ul>
            <figure>
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Decision_tree_model.png"
                    height="400px" width="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">127
                <a class="prev" href="#slide126"></a>
                <a class="next" href="#slide128"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide128">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <figure class="fullwidth">
                <img src="../../../../../en/teaching/courses/2017/DataMining/images/Decision_tree_model.png"
                    height="400px" width="400px" />
            </figure>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">128
                <a class="prev" href="#slide127"></a>
                <a class="next" href="#slide129"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide129">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">

            <table>
                <thead>
                    <tr>
                        <th>Animal</th>
                        <th>Pelage</th>
                        <th>Plumes</th>
                        <th>Peut voler</th>
                        <th>Classe</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Chien</td>
                        <td>Poilu</td>
                        <td>Non</td>
                        <td>Non</td>
                        <td>Mammifère</td>
                    </tr>
                    <tr>
                        <td>Chat</td>
                        <td>Poilu</td>
                        <td>Non</td>
                        <td>Non</td>
                        <td>Mammifère</td>
                    </tr>
                    <tr>
                        <td>Aigle</td>
                        <td>Plumeux</td>
                        <td>Oui</td>
                        <td>Oui</td>
                        <td>Oiseau</td>
                    </tr>
                    <tr>
                        <td>Pingouin</td>
                        <td>Plumeux</td>
                        <td>Oui</td>
                        <td>Non</td>
                        <td>Oiseau</td>
                    </tr>
                    <tr>
                        <td>Serpent</td>
                        <td>Écaille</td>
                        <td>Non</td>
                        <td>Non</td>
                        <td>Reptile</td>
                    </tr>
                </tbody>
            </table>

            <p>Nous voulons classer ces animaux en trois classes : Mammifère, Oiseau ou Reptile. Utilisons un arbre de
                décision pour ce faire.</p>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">129
                <a class="prev" href="#slide128"></a>
                <a class="next" href="#slide130"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide130">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <p>L'algorithme de l'arbre de décision est une méthode d'apprentissage supervisé utilisée pour la
                classification et la régression.</p>
            <ul>

                <li><b>Choix de la caractéristique de division</b> : L'algorithme commence par choisir la meilleure
                    caractéristique pour diviser les données en sous-ensembles homogènes. Dans l'exemple, la première
                    caractéristique choisie est le pelage.</li>
                <li><b>Division des données</b> : Les données sont divisées en sous-groupes en fonction de la
                    caractéristique choisie. Dans l'exemple, les données sont divisées en deux groupes : ceux avec un
                    pelage poilu et ceux avec un pelage plumeux.</li>
                <li><b>Récursion</b> : Le processus est répété de manière récursive pour chaque sous-groupe obtenu. Pour
                    chaque sous-groupe, l'algorithme choisit à nouveau la meilleure caractéristique de division et
                    divise les données en sous-groupes plus petits. Dans notre exemple, pour les animaux avec un pelage
                    plumeux, la capacité de voler est la caractéristique de division suivante.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">130
                <a class="prev" href="#slide129"></a>
                <a class="next" href="#slide131"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide131">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Critère d'arrêt</b> : L'algorithme s'arrête lorsque l'une des conditions suivantes est remplie :
                    <ul>
                        <li>Tous les éléments d'un sous-groupe appartiennent à la même classe. </li>
                        <li>Toutes les caractéristiques ont été utilisées pour la division.</li>
                        <li>Une profondeur maximale de l'arbre est atteinte.</li>
                        <li>Un nombre minimal d'échantillons dans un nœud est atteint.</li>
                    </ul>
                </li>
                <li><b>Construction de l'arbre</b> : Une fois que les divisions sont terminées, un arbre de décision est
                    construit où chaque nœud représente une caractéristique de division et chaque feuille représente une
                    classe de sortie.</li>
                <li><b>Classification</b> : Lorsqu'un nouvel exemple est introduit, il est classé en parcourant l'arbre
                    de décision en fonction de ses caractéristiques jusqu'à atteindre une feuille, où il est attribué à
                    une classe.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">131
                <a class="prev" href="#slide130"></a>
                <a class="next" href="#slide132"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide132">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <p>Dans le contexte des arbres de décision, les données sont généralement représentées sous forme de
                vecteurs où chaque élément du vecteur correspond à une caractéristique ou à une variable indépendante,
                et la variable dépendante est la cible que l'on cherche à prédire ou à classifier. </p>
            <ul>
                <li><b>Données sous forme de vecteurs</b> : Chaque observation ou exemple dans l'ensemble de données est
                    représenté sous forme d'un vecteur, où chaque composante du vecteur correspond à une caractéristique
                    ou à une variable explicative. Par exemple, si nous examinons un ensemble de données sur les prêts
                    bancaires, les caractéristiques pourraient inclure le revenu, le montant du prêt, le nombre d'années
                    d'expérience professionnelle, etc.</li>
                <li> Les données sont disponibles sous la forme \[(\textbf{x},Y) = (x_1, x_2, x_3, ..., x_k, Y)\]</li>
                <li>Le vecteur \(\textbf{x}\) est composé des caractéristiques suivantes \(x_1, x_2, x_3, ...\)</li>
                <li>\(Y\) est la variable dépendante qui peut dépendre de \(\textbf{x}\)</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">132
                <a class="prev" href="#slide131"></a>
                <a class="next" href="#slide133"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide133">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Classification</b> : Les arbres de décision sont couramment utilisés pour la classification, où
                    l'objectif est de catégoriser les observations dans des classes ou des catégories prédéfinies en
                    fonction de leurs caractéristiques. Par exemple, dans le domaine médical, les arbres de décision
                    peuvent être utilisés pour classifier les patients en fonction de leur diagnostic.</li>
                <li><b>Régression</b> : Les arbres de décision peuvent également être utilisés pour la régression, où
                    l'objectif est de prédire une valeur numérique continue en fonction des caractéristiques. Par
                    exemple, dans les finances, les arbres de décision peuvent être utilisés pour prédire le prix d'une
                    maison en fonction de ses caractéristiques.</li>
                <li><b>Analyse de la décision</b> : Les arbres de décision peuvent aider à identifier les stratégies ou
                    les séquences d'actions les plus efficaces pour atteindre un objectif spécifique. Par exemple, dans
                    la planification d'entreprise, les arbres de décision peuvent être utilisés pour déterminer les
                    meilleures décisions à prendre dans un processus de prise de décision complexe.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">133
                <a class="prev" href="#slide132"></a>
                <a class="next" href="#slide134"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide134">
        <div class="header">
            <h1>2.3.5. Arbres de décision</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Recherche opérationnelle</b> : Les arbres de décision sont également utilisés dans le domaine de
                    la recherche opérationnelle pour résoudre des problèmes d'optimisation et de planification. Par
                    exemple, dans la logistique, les arbres de décision peuvent être utilisés pour déterminer le
                    meilleur itinéraire de livraison en fonction de divers facteurs tels que la distance, le coût et les
                    contraintes de temps.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">134
                <a class="prev" href="#slide133"></a>
                <a class="next" href="#slide135"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide135">
        <div class="header">
            <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <p>L'apprentissage ensembliste, en particulier les forêts d'arbres décisionnels, est une technique qui
                combine plusieurs modèles d'apprentissage pour améliorer les performances prédictives par rapport à un
                seul modèle. Les forêts d'arbres décisionnels sont obtenues en construisant de multiples arbres de
                décision lors de la phase d'entraînement.</p>
            <ul>
                <li><b>Construction des arbres de décision</b> : Pendant la phase d'entraînement, plusieurs arbres de
                    décision sont construits en utilisant différents sous-ensembles de données et/ou caractéristiques.
                    Chaque arbre est formé de manière indépendante.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">135
                <a class="prev" href="#slide134"></a>
                <a class="next" href="#slide136"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide136">
        <div class="header">
            <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <ul>
                <li><b>Vote majoritaire</b> : Pour la classification, chaque arbre de décision vote pour la classe
                    prédite d'un nouvel exemple. La classe finale attribuée à l'exemple est déterminée par un vote
                    majoritaire parmi tous les arbres de la forêt.
                    Pour la régression, les valeurs prédites par chaque arbre sont moyennées pour obtenir la valeur
                    finale.</li>
                <li><b>Réduction de la variance</b> : L'apprentissage ensembliste vise à réduire la variance du modèle
                    en agrégeant plusieurs modèles. Cela peut aider à éviter le surajustement (overfitting) en
                    compensant les défauts individuels de chaque arbre de décision.</li>
                <li><b>Stabilité</b> : Les forêts d'arbres décisionnels sont généralement plus stables que les arbres de
                    décision individuels, car elles sont moins sensibles aux variations des données d'entraînement.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">136
                <a class="prev" href="#slide135"></a>
                <a class="next" href="#slide137"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide137">
        <div class="header">
            <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Algorithme</h3>
            <ul>
                <li>Soit \(X = x_1,x_2,..x_n\) un ensemble de données avec des réponses \(Y = y_1,y_2,..y_n\)</li>
                <li> Soit \(b = 1, 2,..B\)
                    <ul>
                        <li>Échantillon, avec remplacement (un élément peut apparaître plusieurs fois dans un même
                            échantillon), \(n\) exemples de formation de \(X, Y\) ; appelez-les \(X_b, Y_b\).</li>
                        <li>Former un arbre de classification ou de régression \(f_b\) sur \(X_b, Y_b\).</li>
                    </ul>
                </li>
                <li>Après entraînement, les prédictions pour les échantillons non vus x' peuvent être faites en faisant
                    la moyenne des prédictions de tous les arbres de régression individuels sur x' \[\hat{f} =
                    \frac{1}{B} \sum_{b=1}^Bf_b (x')\] ou par un
                    vote à la majorité dans le cas des arbres de classification.
                </li>

            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">137
                <a class="prev" href="#slide136"></a>
                <a class="next" href="#slide138"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide138">
        <div class="header">
            <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Classification multiclasse</b> : les forêts d'arbres décisionnels sont utilisées pour classer les
                    instances dans l'une des plusieurs classes mutuellement exclusives. Par exemple, la classification
                    d'images en différentes catégories telles que les animaux, les véhicules, les objets ménagers, etc.
                </li>
                <li><b>Classification multilabel</b> : Contrairement à la classification multiclasse, la classification
                    multilabel permet qu'une instance soit assignée à plusieurs classes en même temps. Par exemple, la
                    classification de documents dans lesquels un document peut être associé à plusieurs catégories
                    telles que "science", "technologie", "politique", etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">138
                <a class="prev" href="#slide137"></a>
                <a class="next" href="#slide139"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide139">
        <div class="header">
            <h1>2.3.6. Apprentissage ensembliste (Forêt d'arbres décisionnels)</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Régression</b> : les forêts d'arbres décisionnels peuvent également être utilisées pour des
                    tâches de régression, où la sortie est une valeur continue plutôt qu'une classe discrète. Par
                    exemple, prédire le prix d'une maison en fonction de ses caractéristiques.</li>
                <li><b>Détection des anomalies</b> : Elles peuvent également être employées pour détecter les anomalies
                    ou les comportements inhabituels dans les données. Cela peut être utile dans divers domaines tels
                    que la détection de fraudes financières, la détection de défaillances dans les systèmes industriels,
                    etc.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">139
                <a class="prev" href="#slide138"></a>
                <a class="next" href="#slide140"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide140">
        <div class="header">
            <h1>2.4. Sélection de caractéristique</h1>
        </div>
        <div class="content">
            <p>La sélection de caractéristiques est un processus visant à choisir un sous-ensemble de caractéristiques
                pertinentes parmi un grand nombre de caractéristiques disponibles. </p>
            <ul>
                <li>Cette technique est largement utilisée dans des domaines où le nombre de caractéristiques est élevé
                    par rapport à la taille de l'échantillon de données, car cela peut entraîner des problèmes de
                    surajustement et de temps de calcul élevé.</li>
                <li>La sélection de caractéristiques est également considérée comme une méthode de réduction de la
                    dimensionnalité, car elle vise à réduire le nombre de dimensions de l'espace des caractéristiques
                    sans perdre d'informations discriminatives.</li>
                <li>la sélection de caractéristiques vise à :
                    <ul>
                        <li>Identifier les caractéristiques les plus pertinentes qui contribuent le plus à la
                            variabilité des données ou à la capacité de prédiction du modèle.</li>
                        <li>Réduire la dimensionnalité de l'espace des caractéristiques pour améliorer les performances
                            des modèles d'apprentissage automatique en termes de temps de calcul et de prévention du
                            surajustement.</li>
                    </ul>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">140
                <a class="prev" href="#slide139"></a>
                <a class="next" href="#slide141"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide141">
        <div class="header">
            <h1>2.4. Sélection de caractéristique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Applications</h3>
            <ul>
                <li><b>Analyse des textes écrits</b> : Dans l'analyse de textes écrits, la sélection de caractéristiques
                    est effectivement utilisée pour extraire les éléments les plus pertinents et informatifs des données
                    textuelles. Cela peut inclure l'identification des mots clés, des entités nommées, des motifs
                    linguistiques spécifiques ou d'autres caractéristiques qui sont essentielles pour la tâche d'analyse
                    de texte, comme la classification de documents, l'extraction d'informations ou la génération de
                    résumés. La sélection de caractéristiques dans ce contexte vise à réduire la dimensionnalité des
                    données textuelles tout en préservant leur pertinence et leur expressivité pour les tâches d'analyse
                    ultérieures.</li>
                <li><b>Analyse des données des puces à ADN</b> : En génomique et bioinformatique, les puces à ADN
                    génèrent des ensembles de données très dimensionnels qui nécessitent souvent une réduction de
                    dimension pour identifier les gènes ou les séquences d'ADN les plus significatifs associés à des
                    phénotypes particuliers, tels que des maladies ou des réponses biologiques.</li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">141
                <a class="prev" href="#slide140"></a>
                <a class="next" href="#slide142"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide142">
        <div class="header">
            <h1>2.4. Sélection de caractéristique</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Définition formelle[8]</h3>
            <ul>
                <li>Soit \(X\) l'ensemble original de \(n\) caractéristiques, c'est-à-dire, \(|X| = n\)</li>
                <li>Soit \(w_i\) le poids attribué à l'élément \(x_i &#8712; X\)</li>
                <li>La sélection binaire attribue des poids binaires tandis que la sélection continue attribue des poids
                    en préservant l'ordre de sa pertinence.</li>
                <li>Soit \(J(X')\) soit une mesure d'évaluation, définie comme \(J: X' &#8838; X &#8594; R\)</li>
                <li> Le problème de la sélection des caractéristiques peut être défini de trois façons
                    <ol>
                        <li>\(|X'| = m &lt; n\). Trouver \(X' &#8834; X\) tel que \(J(X')\) est le maximum</li>
                        <li>Choisir \(J_0\), Trouver \(X' &#8838; X\), tel que \(J(X') &gt;= J_0\)</li>
                        <li>Trouver un compromis entre la minimisation de \(|X'|\) et la maximisation du \(J(X')\)</li>
                    </ol>
                </li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">142
                <a class="prev" href="#slide141"></a>
                <a class="next" href="#slide143"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide143">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Articles de recherche</h3>
            <ol>
                <li>From data mining to knowledge discovery in databases, Usama Fayyad, Gregory Piatetsky-Shapiro, and
                    Padhraic Smyth, AI Magazine Volume 17 Number 3 (1996)</li>
                <li>Survey of Clustering Data Mining Techniques, Pavel Berkhin</li>
                <li>Mining association rules between sets of items in large databases, Agrawal, Rakesh, Tomasz
                    Imieliński, and Arun Swami. Proceedings of the 1993 ACM SIGMOD international conference on
                    Management of data - SIGMOD 1993. p. 207. </li>
                <li>Comparisons of Sequence Labeling Algorithms and Extensions, Nguyen, Nam, and Yunsong Guo.
                    Proceedings of the 24th international conference on Machine learning. ACM, 2007. </li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">143
                <a class="prev" href="#slide142"></a>
                <a class="next" href="#slide144"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide144">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Articles de recherche</h3>
            <ol start="5">
                <li>An Analysis of Active Learning Strategies for Sequence Labeling Tasks, Settles, Burr, and Mark
                    Craven. Proceedings of the conference on empirical methods in natural language processing.
                    Association for Computational Linguistics, 2008.</li>
                <li>Anomaly detection in crowded scenes, Mahadevan; Vijay et al. Computer Vision and Pattern Recognition
                    (CVPR), 2010 IEEE Conference on. IEEE, 2010</li>
                <li>A Study of Global Inference Algorithms in Multi-Document Summarization. McDonald, Ryan. European
                    Conference on Information Retrieval. Springer, Berlin, Heidelberg, 2007.</li>
                <li>Feature selection algorithms: A survey and experimental evaluation., Molina, Luis Carlos, Lluís
                    Belanche, and Àngela Nebot. Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International
                    Conference on. IEEE, 2002.</li>
                <li>Support vector machines, Hearst, Marti A., et al. IEEE Intelligent Systems and their applications
                    13.4 (1998): 18-28.</li>
            </ol>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">144
                <a class="prev" href="#slide143"></a>
                <a class="next" href="#slide145"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide145">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Ressources en ligne</h3>
            <ul>
                <li><a
                        href="https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124">Accuracy,
                        Recall, Precision, F-Score & Specificity, which to optimize on?</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Patterns_in_nature">Patterns in Nature</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Statistical_classification">Statistical classification</a>
                </li>
                <li><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Association_rule_learning">Association rule learning</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Anomaly_detection">Anomaly detection</a></li>
            </ul>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/Sequence_labeling">Sequence labeling</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Automatic_summarization">Automatic summarization</a></li>
                <li><a href="https://en.wikipedia.org/wiki/Pattern_recognition">Pattern recognition</a></li>
                <li><a href="http://scikit-learn.org/stable/">Scikit-learn</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">145
                <a class="prev" href="#slide144"></a>
                <a class="next" href="#slide146"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide146">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Ressources en ligne</h3>
            <ul>
                <ul>
                    <li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a></li>
                    <li><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient
                            descent</a></li>
                </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">146
                <a class="prev" href="#slide145"></a>
                <a class="next" href="#slide147"></a>
            </div>
        </div>
    </section>
    <section class="slide" id="slide147">
        <div class="header">
            <h1>Références</h1>
        </div>
        <div class="content">
            <h3 class="topicsubheading">Couleurs</h3>
            <ul>
                <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
            </ul>
            <h3 class="topicsubheading">Images</h3>
            <ul>
                <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
            </ul>
        </div>
        <div class="footer">
            <div class="contact">Traitement de données massives | John Samuel</div>
            <div class="navigation">147
                <a class="prev" href="#slide146"></a>
                <a class="next" href="#slide148"></a>
            </div>
        </div>
    </section>

    <script>
        function changeCurrentURLSlideNumber(isIncrement) {
            url = window.location.href;
            position = url.indexOf("#slide");
            if (position != -1) { // Not on the first page
                slideIdString = url.substr(position + 6);
                if (!Number.isNaN(slideIdString)) {
                    slideId = parseInt(slideIdString);
                    if (isIncrement) {
                        if (slideId < 147) {
                            slideId = slideId + 1;
                        }
                    } else {
                        if (slideId > 1) {
                            slideId = slideId - 1;
                        }
                    }
                    /* regexp */
                    url = url.replace(/#slide\d+/g, "#slide" + slideId);
                    window.location.href = url;
                }
            } else {
                window.location.href = url + "#slide2";
            }
        }
        document.onkeydown = function (event) {

            event.preventDefault();
            /* This will ensure the default behavior of
                                                            page scroll behaviour (up, down, right, left)*/

            event = event || window.event;
            /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
            if (event.keyCode == '37') {
                // left
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '38') {
                // up
                changeCurrentURLSlideNumber(false);
            } else if (event.keyCode == '39') {
                // right
                changeCurrentURLSlideNumber(true);
            } else if (event.keyCode == '40') {
                // down
                changeCurrentURLSlideNumber(true);
            }
        }
        document.body.onmouseup = function (event) {
            event = event || window.event;
            event.preventDefault();
            changeCurrentURLSlideNumber(true);
        }
    </script>
    <script src="MathJax.js"></script>
</body>

</html>