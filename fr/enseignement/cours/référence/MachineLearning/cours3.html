<html>

    <head>
        <meta charset="utf-8" />
        <title>Data Mining et Machine Learning (référence): Construction des modèles de traitement: John Samuel</title>
        <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
        <style type="text/css">
            body {
                height: 100%;
                width: 100%;
                background-color: white;
                margin: 0;
                overflow: hidden;
                font-family: Arial;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                height: 79%;
                width: 95vw;
                display: flex;
                line-height: 1.7em;
                flex-direction: column;
                align-items: flex-start;
                margin: 0 auto;
                color: #000000;
                text-align: left;
                padding-left: 1.5vmax;
                padding-top: 1.5vmax;
                overflow-x: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
            }

            .content h1,
            h2,
            h3,
            h4 {
                color: #1B80CF;
            }

            .content .topichighlight {
                background-color: #78002E;
                color: #FFFFFF;
            }

            .content .topicheading {
                background-color: #1B80CF;
                color: #FFFFFF;
                vertical-align: middle;
                border-radius: 0 2vmax 2vmax 0%;
                height: 4vmax;
                line-height: 4vmax;
                padding-left: 1vmax;
                margin: 0.1vmax;
                width: 50%;
                margin-bottom: 1vmax;
            }

            .content .flexcontent {
                display: flex;
                overflow-y: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
            }

            .content .gridcontent {
                display: grid;
                grid-template-columns: auto auto auto auto;
                grid-column-gap: 0px;
                grid-row-gap: 0px;
                grid-gap: 0px;
            }

            .content .topicsubheading {
                background-color: #1B80CF;
                color: #FFFFFF;
                vertical-align: middle;
                border-radius: 0 1.5vmax 1.5vmax 0%;
                height: 3vmax;
                margin: 0.1vmax;
                font-size: 90%;
                line-height: 3vmax;
                padding-left: 1vmax;
                width: 50%;
                margin-bottom: 1vmax;
            }

            .content table {
                color: #000000;
                font-size: 100%;
                width: 100%;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
                text-decoration: none;
            }

            .content th {
                color: #FFFFFF;
                background-color: #1B80CF;
                border-radius: 2vmax 2vmax 2vmax 2vmax;
                font-size: 120%;
                padding: 15px;
            }

            .content figure {
                max-width: 90%;
                max-height: 90%;
            }

            .content .fullwidth img {
                max-width: 90%;
                max-height: 90%;
            }

            .content figure img {
                max-width: 50vmin;
                max-height: 50vmin;
                display: block;
                margin-left: auto;
                margin-right: auto;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
                margin: 0.1vmax;
                font-size: 90%;
                text-align: center;
                padding: 0.5vmax;
                background-color: #E1F5FE;
                border-radius: 2vmax 2vmax 2vmax 2vmax;
            }

            .content td {
                color: #000000;
                width: 8%;
                padding-left: 3vmax;
                padding-top: 1vmax;
                padding-bottom: 1vmax;
                background-color: #E1F5FE;
                border-radius: 2vmax 2vmax 2vmax 2vmax;
            }

            .content li {
                line-height: 1.7em;
            }

            .header {
                color: #ffffff;
                background-color: #00549d;
                height: 5vmax;
            }

            .header h1 {
                text-align: center;
                vertical-align: middle;
                font-size: 3vmax;
                line-height: 4vmax;
                margin: 0;
            }

            .footer {
                height: 3vmax;
                line-height: 3vmax;
                vertical-align: middle;
                color: #ffffff;
                background-color: #00549d;
                margin: 0;
                padding: .3vmax;
                overflow: hidden;
            }

            .footer .contact {
                float: left;
                color: #ffffff;
                text-align: left;
                font-size: 3.2vmin;
            }

            .footer .navigation {
                float: right;
                text-align: right;
                width: 8vw;
                font-size: 3vmin;
            }

            .footer .navigation .next,
            .prev {
                font-size: 3vmin;
                color: #ffffff;
                text-decoration: none;
            }

            .footer .navigation .next::after {
                content: "| >";
            }

            .footer .navigation .prev::after {
                content: "< ";
            }

            /* Using same Jupyter CSS
     */

            .highlight {
                background: #f8f8f8;
            }

            .highlight .c {
                color: #408080;
                font-style: italic
            }

            /* Comment */

            .highlight .err {
                border: 1px solid #FF0000
            }

            /* Error */

            .highlight .k {
                color: #008000;
                font-weight: bold
            }

            /* Keyword */

            .highlight .o {
                color: #666666
            }

            /* Operator */

            .highlight .ch {
                color: #408080;
                font-style: italic
            }

            /* Comment.Hashbang */

            .highlight .c1 {
                color: #408080;
                font-style: italic
            }

            /* Comment.Single */

            .highlight .cs {
                color: #408080;
                font-style: italic
            }

            /* Comment.Special */

            .highlight .cm {
                color: #408080;
                font-style: italic
            }

            /* Comment.Multiline */

            .highlight .nn {
                color: #0000FF;
                font-weight: bold
            }

            /* Name.Namespace */

            .highlight .k {
                color: #008000;
                font-weight: bold
            }

            /* Keyword */

            .highlight .s2 {
                color: #BA2121
            }

            /* Literal.String.Double */

            .highlight .s1 {
                color: #BA2121
            }

            /* Literal.String.Single */

            .highlight .kn {
                color: #008000;
                font-weight: bold
            }

            /* Keyword.Namespace */

            .highlight .nb {
                color: #008000
            }

            /* Name.Builtin */

            .highlight .mb {
                color: #666666
            }

            /* Literal.Number.Bin */

            .highlight .mf {
                color: #666666
            }

            /* Literal.Number.Float */

            .highlight .mh {
                color: #666666
            }

            /* Literal.Number.Hex */

            .highlight .mi {
                color: #666666
            }

            /* Literal.Number.Integer */

            .highlight .mo {
                color: #666666
            }

            /* Literal.Number.Oct */

            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                .slide {
                    height: 100%;
                    width: 100%;
                }

                .content {
                    width: 100%;
                    height: 92%;
                    display: flex;
                    flex-direction: row;
                    text-align: left;
                    padding: 1vw;
                    line-height: 3.8vmax;
                    font-size: 1.8vmax;
                    flex-wrap: wrap;
                }

                .content .topicheading {
                    width: 90%;
                }

                .content h1,
                h2,
                h3,
                h4 {
                    width: 100%;
                }

                .content figure img {
                    max-width: 80vmin;
                    max-height: 50vmin;
                }

                .content figure figcaption {
                    max-width: 90%;
                    max-height: 90%;
                }
            }

            @media print {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                .content {
                    font-size: 2.8vmin;
                }

                .content .flexcontent {
                    font-size: 2.5vmin;
                }
            }
        </style>
        <script src="../../../../../fr/enseignement/cours/2020/MachineLearning/tex-mml-chtml.js"
            id="MathJax-script"></script>
    </head>

    <body>
        <section class="slide" id="slide1">
            <div class="header">
            </div>
            <div class="content">
                <h1 style="font-size:3.5vw">Data Mining et Machine Learning</h1>
                <p><b>John Samuel</b></br>
                    CPE Lyon<br /><br />
                    <b>Référence</b><br />
                    <b>Courriel</b>: john.samuel@cpe.fr</br>
                    </br>
                    <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                            alt="Creative Commons License" style="border-width:0"
                            src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">1

                    <a class="next" href="#slide2"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide2">
            <div class="header">
                <h1>Data Mining</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Objectifs</h3>
                <ol>
                    <li>Apprentissage machine</li>
                    <li>Apprentissage profond</li>
                    <li>Apprentissage par renforcement</li>
                    <li>Licences de données, éthique et vie privée</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">2
                    <a class="prev" href="#slide1"></a>
                    <a class="next" href="#slide3"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide3">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Neurones biologiques</h2>
                <figure>
                    <img src="../../2021/MachineLearning/Neuron3.png" height="350px" />
                    <figcaption>Neurone biologique<sup>1</sup></figcaption>
                </figure>
                <ol style="font-size:2vh">
                    <li>https://en.wikipedia.org/wiki/File:Neuron3.png</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">3
                    <a class="prev" href="#slide2"></a>
                    <a class="next" href="#slide4"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide4">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Introduction</h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Colored_neural_network.svg" />
                    <figcaption>Réseaux de neurones artificiels</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">4
                    <a class="prev" href="#slide3"></a>
                    <a class="next" href="#slide5"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide5">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Réseau de neurones</h2>
                <p>Les réseaux de neurones sont couramment utilisés dans le domaine de l'apprentissage machine, en
                    particulier dans des tâches telles que la classification, la régression, la reconnaissance d'images,
                    le
                    traitement du langage naturel, et bien d'autres. Un réseau de neurones artificiels est une
                    collection
                    d'unités interconnectées appelées neurones artificiels. Ces réseaux sont inspirés de la structure du
                    cerveau biologique</p>
                <ul>
                    <li><b>Connexions</b> : Chaque connexion entre les neurones, similaire aux synapses dans le cerveau
                        biologique, peut transmettre un signal aux autres neurones.</li>
                    <li><b>Transmission de signal</b> : Un neurone artificiel reçoit un signal, le traite à l'aide d'une
                        fonction non linéaire, et peut ensuite transmettre un signal aux neurones qui lui sont
                        connectés.
                    </li>
                    <li><b>Fonction d'activation</b> : La sortie de chaque neurone est calculée par une fonction non
                        linéaire appliquée à la somme pondérée de ses entrées. Cette fonction d'activation introduit une
                        non-linéarité dans le réseau, permettant de modéliser des relations complexes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">5
                    <a class="prev" href="#slide4"></a>
                    <a class="next" href="#slide6"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide6">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Réseau de neurones</h2>
                <ul>
                    <li><b>Poids ajustables</b> : Les neurones et les connexions ont généralement des poids qui sont
                        ajustés
                        au fur et à mesure de l'apprentissage. Ces poids déterminent l'importance relative des
                        différentes
                        entrées pour chaque neurone.</li>
                    <li><b>Ajustement des poids</b> : Les poids peuvent être ajustés pour augmenter ou diminuer la force
                        du
                        signal au niveau d'une connexion, influençant ainsi la contribution de cette connexion aux
                        calculs
                        du réseau.</li>
                    <li><b>Seuil</b> : Les neurones peuvent avoir un seuil, de sorte qu'un signal n'est envoyé que si la
                        somme pondérée de ses entrées dépasse ce seuil. Cela permet au réseau de moduler sa sensibilité
                        aux
                        entrées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">6
                    <a class="prev" href="#slide5"></a>
                    <a class="next" href="#slide7"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide7">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Les couches</h2>
                <p>Les neurones sont organisés en couches. Il existe généralement trois types de couches dans un réseau
                    de
                    neurones :</p>
                <ul>
                    <li><b>Couche d'Entrée (Input Layer)</b> : Cette couche reçoit les signaux initiaux ou les données
                        en
                        entrée. Chaque neurone dans cette couche représente une caractéristique ou une variable
                        d'entrée.
                    </li>
                    <li><b>Couches Cachées (Hidden Layers)</b> : Ces couches effectuent des transformations non
                        linéaires
                        sur les entrées. Elles sont responsables de l'extraction et de la représentation des
                        caractéristiques importantes des données. Un réseau de neurones peut avoir une ou plusieurs
                        couches
                        cachées.</li>
                    <li><b>Couche de Sortie (Output Layer)</b> : Cette couche génère la sortie du réseau. Le nombre de
                        neurones dans cette couche dépend de la nature de la tâche, par exemple, une classification
                        binaire
                        aurait un neurone de sortie, tandis qu'une classification multi-classes en aurait plusieurs.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">7
                    <a class="prev" href="#slide6"></a>
                    <a class="next" href="#slide8"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide8">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Les couches</h2>
                <ul>
                    <li><b>Transformations</b> : Chaque couche, y compris la couche d'entrée, effectue des
                        transformations
                        sur les signaux qu'elle reçoit. Ces transformations sont déterminées par les poids des
                        connexions
                        entre les neurones.</li>
                    <li><b>Propagation des signaux</b> : Les signaux passent de la première couche (l'entrée) à la
                        dernière
                        couche (la sortie) à travers les connexions pondérées entre les neurones. Ce processus est
                        souvent
                        appelé la propagation avant (forward propagation). Pendant l'apprentissage, la rétropropagation
                        (backpropagation) est utilisée pour ajuster les poids afin de minimiser l'erreur de prédiction.
                    </li>
                    <li><b>Architecture</b> : La manière dont les couches sont organisées et connectées dans le réseau
                        constitue son architecture. Les réseaux de neurones peuvent avoir des architectures diverses, y
                        compris des réseaux profonds (avec de nombreuses couches cachées) ou des architectures plus
                        simples.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">8
                    <a class="prev" href="#slide7"></a>
                    <a class="next" href="#slide9"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide9">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">L'entraînement</h2>
                <p>L'objectif global de l'entraînement est d'ajuster les poids du réseau de manière à ce qu'il puisse
                    généraliser à de nouvelles données, produisant des résultats précis pour des exemples qu'il n'a pas
                    vu
                    pendant l'entraînement.</p>
                <ul>
                    <li><b>Données d'entraînement</b> : Les réseaux neuronaux apprennent à partir d'exemples. Chaque
                        exemple
                        se compose d'une "entrée" (les caractéristiques) et d'un "résultat" connu (l'étiquette ou la
                        sortie
                        attendue).</li>
                    <li><b>Calcul de l'erreur</b> : Lorsque le réseau produit une sortie pour une entrée donnée,
                        l'erreur
                        est calculée en comparant cette sortie à la sortie cible (le résultat connu). Il existe
                        différentes
                        mesures d'erreur, mais la somme des carrés des différences (Mean Squared Error, MSE) est
                        couramment
                        utilisée.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">9
                    <a class="prev" href="#slide8"></a>
                    <a class="next" href="#slide10"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide10">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">L'entraînement</h2>
                <ul>
                    <li><b>Rétropropagation (Backpropagation)</b> : Le réseau ajuste ses poids en utilisant la
                        rétropropagation. Cette technique minimise l'erreur en modifiant les poids à partir de la couche
                        de
                        sortie jusqu'à la couche d'entrée. La règle de la chaîne du calcul différentiel est appliquée
                        pour
                        propager l'erreur à travers le réseau.</li>
                    <li><b>Descente de gradient</b> : La règle d'apprentissage souvent utilisée pour ajuster les poids
                        est
                        la descente de gradient. Elle utilise le gradient de l'erreur par rapport aux poids pour mettre
                        à
                        jour les poids dans la direction qui minimise l'erreur.</li>
                    <li><b>Itérations</b> : Le processus d'ajustement des poids en fonction de l'erreur est répété pour
                        de
                        nombreux exemples du jeu de données d'entraînement. Chaque itération est appelée une "époque".
                        Plusieurs époques peuvent être nécessaires pour que le réseau converge vers un état où l'erreur
                        est
                        suffisamment basse.</li>
                    <li><b>Optimisation</b> : Différentes techniques d'optimisation peuvent être utilisées pour
                        améliorer la
                        convergence du réseau, telles que l'ajustement adaptatif du taux d'apprentissage.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">10
                    <a class="prev" href="#slide9"></a>
                    <a class="next" href="#slide11"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide11">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <ul>
                    <li><b>Neurones</b> : Les neurones artificiels sont les unités de base d'un réseau de neurones.
                        Chaque
                        neurone reçoit des signaux d'entrée, effectue un calcul sur ces signaux à l'aide d'une fonction
                        d'activation, et produit une sortie. Les neurones sont organisés en couches, à savoir la couche
                        d'entrée, les couches cachées, et la couche de sortie.</li>
                    <li><b>Connexions et Poids</b> : Les connexions entre les neurones sont représentées par des poids.
                        Chaque connexion a un poids associé, qui détermine l'importance relative de cette connexion dans
                        le
                        calcul du neurone de sortie. Pendant l'entraînement, ces poids sont ajustés pour minimiser
                        l'erreur
                        de prédiction du réseau.</li>
                    <li><b>Fonction de Propagation (Propagation avant)</b> : La fonction de propagation, également
                        appelée
                        propagation avant, décrit le processus par lequel les signaux se propagent à travers le réseau
                        depuis la couche d'entrée jusqu'à la couche de sortie. Chaque neurone effectue une
                        transformation
                        sur les signaux qu'il reçoit, et ces signaux modifiés sont transmis aux neurones de la couche
                        suivante.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">11
                    <a class="prev" href="#slide10"></a>
                    <a class="next" href="#slide12"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide12">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Neurones</h2>
                <p>Chaque neurone artificiel a des entrées, qui peuvent être les valeurs caractéristiques d'un
                    échantillon
                    de données externe, et produit une seule sortie. Cette sortie peut être envoyée à plusieurs autres
                    neurones, formant ainsi la structure interconnectée du réseau neuronal. La <b>fonction
                        d'activation</b>
                    joue un rôle crucial dans le calcul de la sortie d'un neurone. Le processus comprend les étapes
                    suivantes :</p>
                <ul>
                    <li><b>Somme pondérée</b> : Pour trouver la sortie du neurone, on prend la somme pondérée de tous
                        les
                        intrants (entrées). Chaque entrée est multipliée par le poids correspondant à la connexion.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">12
                    <a class="prev" href="#slide11"></a>
                    <a class="next" href="#slide13"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide13">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Neurones</h2>
                <ul>
                    <li><b>Ajout d'un terme de biais</b> : Un terme de biais est ajouté à la somme pondérée. Le terme de
                        biais est un paramètre supplémentaire qui permet au modèle d'apprendre un décalage ou une
                        translation.</li>
                    <li><b>Activation</b> : La somme pondérée, parfois appelée activation, est ensuite passée par une
                        fonction d'activation. Cette fonction est généralement non linéaire et introduit de la
                        complexité
                        dans le modèle, permettant au réseau de capturer des relations non linéaires dans les données
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">13
                    <a class="prev" href="#slide12"></a>
                    <a class="next" href="#slide14"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide14">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <p><b>Connexions et poids</b>: Le réseau de neurones est constitué de connexions, où chaque connexion
                    transmet la sortie d'un neurone
                    comme entrée à un autre neurone. Chaque connexion possède un poids qui représente son importance
                    relative dans la transmission du signal.</p>
                <ul>
                    <li>Un neurone donné peut avoir <b>plusieurs connexions d'entrée</b>, recevant des signaux de
                        différents
                        neurones, et plusieurs connexions de sortie, transmettant des signaux à d'autres neurones. Les
                        poids
                        associés à ces connexions permettent au réseau de moduler l'influence de chaque neurone sur les
                        autres, ajustant ainsi la force et la direction des signaux transmis à travers le réseau.</li>
                    <li>Cette structure de connexion et de pondération est fondamentale dans le fonctionnement des
                        réseaux
                        de neurones, car elle permet au réseau d'apprendre des représentations complexes des données et
                        d'ajuster ses paramètres pendant l'entraînement pour accomplir des tâches spécifiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">14
                    <a class="prev" href="#slide13"></a>
                    <a class="next" href="#slide15"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide15">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Fonction de propagation</h2>
                <p><b>Calcul de l'entrée d'un neurone</b> : La fonction de propagation calcule l'entrée d'un neurone en
                    prenant la somme pondérée des sorties de ses prédécesseurs, où chaque sortie est multipliée par le
                    poids
                    de la connexion correspondante. Cela peut être représenté mathématiquement comme suit :</p>
                <p>\[ \text{Entrée du Neurone} = \sum_{i=1}^{n} (\text{Sortie du Prédécesseur}_i \times \text{Poids}_i)
                    \]
                    où \(n\) est le nombre de connexions d'entrée.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">15
                    <a class="prev" href="#slide14"></a>
                    <a class="next" href="#slide16"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide16">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Fonction de propagation</h2>
                <p><b>Ajout d'un terme de biais</b> : Un terme de biais peut être ajouté au résultat de la propagation.
                    Le
                    terme de biais est un paramètre supplémentaire, souvent représenté par \(b\) dans les équations, qui
                    permet au modèle d'apprendre un décalage ou une translation. Cela donne la forme finale de l'entrée
                    du
                    neurone :</p>

                <p>\[ \text{Entrée du Neurone} = \sum_{i=1}^{n} (\text{Sortie du Prédécesseur}_i \times \text{Poids}_i)
                    +
                    \text{Biais} \]</p>

            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">16
                    <a class="prev" href="#slide15"></a>
                    <a class="next" href="#slide17"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide17">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Fonction de propagation</h2>
                <p><b>Fonction d'Activation</b> : Après avoir calculé l'entrée du neurone, celle-ci est passée à travers
                    une
                    fonction d'activation. Cette fonction introduit une non-linéarité dans le modèle, permettant au
                    réseau
                    de neurones de capturer des relations complexes et d'apprendre des modèles non linéaires. Certaines
                    des
                    fonctions d'activation couramment utilisées comprennent :</p>
                <ul>
                    <li><b>Sigmoïde</b> : \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
                    <li><b>Tangente hyperbolique (tanh)</b> : \( \text{tanh}(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
                        \)
                    </li>
                    <li><b>ReLU (Rectified Linear Unit)</b> : \( \text{ReLU}(x) = \max(0, x) \)</li>
                    <li><b>Softmax</b> (pour la couche de sortie dans la classification) : \( \text{Softmax}(x)_i =
                        \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">17
                    <a class="prev" href="#slide16"></a>
                    <a class="next" href="#slide18"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide18">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Perceptron</h3>
                <p>Le perceptron est un <b>algorithme d'apprentissage supervisé</b> utilisé pour la <b>classification
                        binaire</b>. Il est conçu pour résoudre des problèmes où l'objectif est de déterminer si une
                    entrée
                    donnée appartient ou non à une classe particulière.</p>
                <ul>
                    <li>Le perceptron a été inventé par <b>Frank Rosenblatt</b> en 1958. L'idée était de créer un modèle
                        simple de neurone artificiel inspiré du fonctionnement des neurones biologiques. Rosenblatt a
                        formulé un algorithme d'apprentissage qui permet au perceptron d'ajuster ses poids en fonction
                        des
                        erreurs de classification, améliorant ainsi ses performances au fil du temps.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">18
                    <a class="prev" href="#slide17"></a>
                    <a class="next" href="#slide19"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide19">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Perceptron</h3>
                <ul>
                    <li><b>Fonctionnement</b> : Le perceptron prend plusieurs entrées pondérées et les combine en une
                        somme.
                        Ensuite, cette somme est soumise à une fonction d'activation, généralement une fonction échelon
                        (step function), qui produit la sortie binaire du perceptron.</li>
                    <li><b>Limitations</b> : Le perceptron a des limitations, notamment sa capacité à résoudre des
                        problèmes
                        non linéaires et son incapacité à apprendre des modèles complexes. Cependant, il a jeté les
                        bases
                        pour le développement de réseaux de neurones plus avancés, en particulier les réseaux
                        multicouches
                        qui peuvent apprendre des représentations hiérarchiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">19
                    <a class="prev" href="#slide18"></a>
                    <a class="next" href="#slide20"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide20">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Perceptron</h1>
                <figure>
                    <img src="../../2021/MachineLearning/Perceptron_example.svg" height="350px" />
                    <figcaption>Perceptron en mettant à jour sa limite linéaire à mesure que d'autres exemples de
                        formation
                        sont ajoutés.<sup>1</sup></figcaption>
                </figure>
                <ol style="font-size:2vh">
                    <li>Source: https://en.wikipedia.org/wiki/File:Perceptron_example.svg</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">20
                    <a class="prev" href="#slide19"></a>
                    <a class="next" href="#slide21"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide21">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Perceptron</h1>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Perceptron.svg"
                        height="400px" />
                    <figcaption>Perceptron</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">21
                    <a class="prev" href="#slide20"></a>
                    <a class="next" href="#slide22"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide22">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Perceptron: Définition formelle</h1>
                <ul>
                    <li>Soit \(y = f(z)\) la sortie du perceptron pour un vecteur d'entrée <i>z</i></li>
                    <li>Soit \(N\) le nombre d'exemples d'entraînement</li>
                    <li>Soit <i><b>X</b></i> l'espace de saisie des caractéristiques</li>
                    <li>Soit \({(x_{1}, d_{1}),...,(x_{N}, d_{N})}\) be the <i><b>N</b></i> training examples, where
                        <ul>
                            <li>\(x_i\) est le vecteur caractéristique de <i>i<sup>ème</sup></i> exemple d'entraînement.
                            </li>
                            <li>\(d_i\) est la valeur de sortie souhaitée</li>
                            <li>\(x_{j,i}\) est la <i>i<sup>ème</sup></i> caractéristique de <i>j<sup>ème</sup></i>
                                exemple
                                d'entraînement.</li>
                            <li>\(x_{j,0} = 1\)</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">22
                    <a class="prev" href="#slide21"></a>
                    <a class="next" href="#slide23"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide23">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Perceptron: Définition formelle</h3>
                <ul>
                    <li>Les poids sont représentés de la manière suivante:
                        <ul>
                            <li>\(w_i\) est la <i>i<sup>ème</sup></i> valeur du vecteur de poids.</li>
                            <li>\(w_i(t)\) est la <i>i<sup>ème</sup></i> valeur du vecteur de poids à un moment donné t.
                            </li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">23
                    <a class="prev" href="#slide22"></a>
                    <a class="next" href="#slide24"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide24">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Perceptron : Étapes</h3>
                <ol>
                    <li>Initialiser les poids et les seuils</li>
                    <li>Pour chaque exemple, \((x_j, d_j)\) dans l'ensemble d'entraînement<i></i>
                        <ul>
                            <li>Calculer la sortie actuelle : \[y_j(t)= f[w(t).x_j]\] \[= f[w_0(t)x_{j,0} +
                                w_1(t)x_{j,1} +
                                w_2(t)x_{j,2} + \dotsb + w_n(t)x_{j,n}]\]</li>
                            <li>Calculer le poids: \[w_i(t + 1) = w_i(t) + r. (d_j-y_j(t))x_{j,i}\]</li>
                        </ul> \(r\) est le taux d'apprentissage.
                    </li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">24
                    <a class="prev" href="#slide23"></a>
                    <a class="next" href="#slide25"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide25">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Perceptron : Étapes</h3>
                <ol start="3">
                    <li>Répétez l'étape 2 jusqu'à l'erreur d'itération \[\frac{1}{s} (&#931; |d_j - y_j(t)|)\] est
                        inférieur
                        au seuil spécifié par l'utilisateur \(\gamma\), ou un nombre prédéterminé d'itérations ont été
                        effectuées, où \(s\) est à nouveau la taille
                        de l'ensemble de l'échantillon.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">25
                    <a class="prev" href="#slide24"></a>
                    <a class="next" href="#slide26"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide26">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Fonction d'Échelon (Step Function)</h2>
                <p>Le perceptron utilise généralement une fonction d'activation simple, et la fonction d'échelon (step
                    function) est fréquemment choisie pour cette tâche. </p>
                <h4>Définition</h4>
                <p>La fonction d'échelon attribue une sortie de 1 si la somme pondérée des entrées dépasse un certain
                    seuil,
                    et 0 sinon.</p>
                <p>\( f(x) = \begin{cases} 1 & \text{si } x \geq \text{seuil} \\ 0 & \text{sinon} \end{cases} \)</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">26
                    <a class="prev" href="#slide25"></a>
                    <a class="next" href="#slide27"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide27">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Fonction d'activation: fonction d'identité</h2>
                <h4>Équation</h4>
                <p>\[f(x)=x\]</p>
                <h4>Dérivée</h4>
                <p>\[f'(x)=1\]</p>
                <h3></h3>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_identity.svg"
                        height="380px" />
                    <figcaption>Fonction d'identité</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">27
                    <a class="prev" href="#slide26"></a>
                    <a class="next" href="#slide28"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide28">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Fonction d'activation: pas binaire</h2>
                    <h4>Équation</h4>
                    <p>\[f(x) = \begin{cases} 0 & \text{for } x
                        < 0\\ 1 & \text{for } x \ge 0 \end{cases} \]</p>
                            <h4>Dérivée</h4>
                            <p>\[f'(x) = \begin{cases} 0 & \text{for } x \ne 0\\ ? & \text{for } x = 0\end{cases}\]</p>
                            <figure>
                                <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_binary_step.svg"
                                    height="380px" />
                                <figcaption>Pas binaire</figcaption>
                            </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">28
                    <a class="prev" href="#slide27"></a>
                    <a class="next" href="#slide29"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide29">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Fonction d'activation: fonction sigmoïde</h2>
                <h4>Équation</h4>
                <p>\[f(x)=\sigma(x)=\frac{1}{1+e^{-x}}\]</p>
                <h4>Dérivée</h4>
                <p>\[f'(x)=f(x)(1-f(x))\]</p>
                <figure>
                    <img src="../../2021/MachineLearning/Logistic-curve.svg" height="380px" />
                    <figcaption>La fonction sigmoïde</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">29
                    <a class="prev" href="#slide28"></a>
                    <a class="next" href="#slide30"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide30">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Fonction d'activation: TanH</h2>
                <h4>Équation</h4>
                <p>\[f(x)=\tanh(x)=\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})}\]</p>
                <h4>Dérivée</h4>
                <p>\[f'(x)=1-f(x)^2\]</p>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_tanh.svg"
                        height="380px" />
                    <figcaption>TanH</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">30
                    <a class="prev" href="#slide29"></a>
                    <a class="next" href="#slide31"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide31">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Fonction d'activation: Rectified linear unit: ReLU</h2>
                <h4>Équation</h4>
                <p>\[f(x) = \begin{cases} 0 & \text{for } x \le 0\\ x & \text{for } x > 0\end{cases} = \max\{0,x\}= x
                    \textbf{1}_{x>0}\]</p>
                <h4>Dérivée</h4>
                <p>\[f'(x) = \begin{cases} 0 & \text{for } x \le 0\\ 1 & \text{for } x > 0\end{cases}\]</p>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_rectified_linear.svg"
                        height="380px" />
                    <figcaption>Unité linéaire rectifiée (ReLU)</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">31
                    <a class="prev" href="#slide30"></a>
                    <a class="next" href="#slide32"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide32">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Fonction d'activation: Gaussien</h2>
                <h4>Équation</h4>
                <p>\[f(x)=e^{-x^2}\]</p>
                <h4>Dérivée</h4>
                <p>\[f'(x)=-2xe^{-x^2}\]</p>
                <figure>
                    <img src="../../../../../en/teaching/courses/2019/MachineLearning/Activation_gaussian.svg"
                        height="380px" />
                    <figcaption>Gaussien</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">32
                    <a class="prev" href="#slide31"></a>
                    <a class="next" href="#slide33"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide33">
            <div class="header">
                <h1>3.1. Apprentissage machine</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Perceptron multiclasse</h2>
                <ul>
                    <li>Perceptron peut être généralisé à la classification multiclasse. </li>
                    <li>Une fonction de représentation d'élément \(f( x , y )\) fait correspondre chaque paire
                        d'entrée/sortie possible à un vecteur d'élément à valeur réelle en dimension finie.</li>
                    <li>le vecteur de caractéristique est multiplié par un vecteur de poids \(w\), mais le score obtenu
                        est
                        maintenant utilisé pour choisir parmi de nombreux résultats possibles : \[\hat y =
                        \operatorname{argmax}_y f(x,y) \cdot w.\]</li>
                    <li>La réapprentissage se fait par itération sur les exemples, en prédisant un résultat pour chacun,
                        en
                        laissant les poids inchangés lorsque le résultat prédit correspond à l'objectif, et en les
                        modifiant
                        lorsqu'il ne correspond pas. La mise
                        à jour devient : \[w_{t+1} = w_t + f(x, y) - f(x,\hat y)\].</li>

                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">33
                    <a class="prev" href="#slide32"></a>
                    <a class="next" href="#slide34"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide34">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <p>Un <b>réseau de neurones profond</b>, également connu sous le nom de réseau de neurones profondément
                    hiérarchisé ou réseau neuronal profond (DNN pour Deep Neural Network en anglais), est un type de
                    réseau
                    de neurones artificiels qui comprend plusieurs couches de traitement, généralement plus de deux. Ces
                    réseaux sont appelés "profonds" en raison de leur architecture empilée de couches, permettant la
                    création de représentations hiérarchiques complexes des données.</p>
                <p><b>Architecture en couches</b> : Les réseaux de neurones profonds sont composés de multiples couches,
                    généralement divisées en trois types principaux :</p>
                <ul>
                    <li><b>Couche d'Entrée</b> : Reçoit les données brutes ou caractéristiques en entrée.</li>
                    <li><b>Couches Cachées</b> : Effectuent des transformations non linéaires et apprennent des
                        représentations hiérarchiques des données.</li>
                    <li><b>Couche de Sortie</b> : Produit la sortie du réseau, adaptée à la tâche spécifique
                        (classification, régression, etc.).</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">34
                    <a class="prev" href="#slide33"></a>
                    <a class="next" href="#slide35"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide35">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Apprentissage Hiérarchique</b> : Les couches cachées d'un réseau de neurones profond
                        apprennent
                        des caractéristiques de plus en plus abstraites et complexes à mesure que l'on progresse en
                        profondeur. Chaque couche représente une abstraction des caractéristiques extraites par les
                        couches
                        précédentes.</li>
                    <li><b>Fonctions d'Activation</b> : Des fonctions d'activation non linéaires, telles que ReLU
                        (Rectified
                        Linear Unit) ou ses variantes, sont couramment utilisées dans les couches cachées pour permettre
                        au
                        réseau d'apprendre des relations non linéaires.</li>
                    <li><b>Apprentissage Profond</b> : L'apprentissage profond implique l'ajustement simultané des poids
                        de
                        toutes les couches du réseau pour minimiser l'erreur de prédiction. Cela est généralement
                        réalisé en
                        utilisant des techniques de rétropropagation et de descente de gradient.</li>
                    <li><b>Utilisations</b> : Les réseaux de neurones profonds sont utilisés dans une variété de tâches,
                        notamment la vision par ordinateur, la reconnaissance vocale, le traitement du langage naturel,
                        la
                        traduction automatique, la recommandation de contenu, et bien d'autres. Leur capacité à
                        apprendre
                        des représentations complexes a conduit à des avancées significatives dans de nombreux domaines
                        de
                        l'intelligence artificielle.</li>
                </ul>
                <p>L'entraînement de réseaux de neurones profonds peut nécessiter des volumes importants de données et
                    de
                    puissance de calcul. </p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">35
                    <a class="prev" href="#slide34"></a>
                    <a class="next" href="#slide36"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide36">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Apprentissage profond</h3>
                <ul>
                    <li>Le terme "profond" se réfère à un réseau qui a un grand nombre de couches, généralement plus de
                        trois.</li>
                    <li>Ces réseaux sont également appelés "réseaux de neurones profonds" ou "réseaux neuronaux
                        profonds".
                    </li>
                    <li>Les réseaux de neurones profonds ont été rendus populaires par leurs capacités à apprendre des
                        représentations hiérarchiques complexes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">36
                    <a class="prev" href="#slide35"></a>
                    <a class="next" href="#slide37"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide37">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Exemple: Tensorflow</h2>
                <div class="highlight">
                    <pre><span></span><span class="c1"># Importation des bibliothèques nécessaires de TensorFlow</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="c1"># Étape 1: Création d&#39;un modèle séquentiel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="c1"># Étape 2: Ajout d&#39;une couche dense avec une fonction d&#39;activation ReLU</span>
<span class="c1"># La couche a 4 neurones, une fonction d&#39;activation &#39;relu&#39;, et prend une entrée de forme (3,)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,)))</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">37
                    <a class="prev" href="#slide36"></a>
                    <a class="next" href="#slide38"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide38">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Exemple: Tensorflow</h2>
                <div class="highlight">
                    <pre><span class="c1"># Étape 3: Ajout d&#39;une couche dense de sortie avec une fonction d&#39;activation softmax</span>
<span class="c1"># La couche a 2 neurones pour une tâche de classification binaire, et softmax est utilisé</span>
<span class="c1"># pour obtenir des probabilités</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="c1"># Étape 4: Compilation du modèle</span>
<span class="c1"># Utilisation de la descente de gradient stochastique (SGD) comme optimiseur avec un taux d&#39;apprentissage de 0.01</span>
<span class="c1"># La fonction de perte est &#39;mean_squared_error&#39; pour un problème de régression</span>
<span class="c1"># Les performances du modèle seront mesurées en termes de &#39;accuracy&#39; (précision)</span>
<span class="n">sgd</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">sgd</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">38
                    <a class="prev" href="#slide37"></a>
                    <a class="next" href="#slide39"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide39">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Étape 1</b>: On crée un modèle séquentiel, qui est une pile linéaire de couches.</li>
                    <li><b>Étape 2</b>: On ajoute une couche dense avec 4 neurones utilisant la fonction d'activation
                        ReLU.
                        La couche prend une entrée de forme (3,) - cela signifie que chaque exemple d'entraînement a
                        trois
                        caractéristiques.</li>
                    <li><b>Étape 3</b>: On ajoute une couche dense de sortie avec 2 neurones utilisant la fonction
                        d'activation softmax. Cela est couramment utilisé pour les tâches de classification binaire,
                        fournissant des probabilités pour chaque classe.</li>
                    <li><b>Étape 4</b>: On compile le modèle en spécifiant l'optimiseur (SGD avec un taux
                        d'apprentissage de
                        0.01), la fonction de perte ('mean_squared_error' pour une tâche de régression), et les
                        métriques de
                        performance ('accuracy' pour mesurer la précision du modèle).</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">39
                    <a class="prev" href="#slide38"></a>
                    <a class="next" href="#slide40"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide40">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <figure>
                    <img src="../../2021/MachineLearning/Screenshot_2020-10-20 Tensorflow — Neural Network Playground.png"
                        height="450px" />
                    <figcaption>Source: https://playground.tensorflow.org/</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">40
                    <a class="prev" href="#slide39"></a>
                    <a class="next" href="#slide41"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide41">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <figure>
                    <img src="../../2021/MachineLearning/Screenshot_2020-10-20 Tensorflow 2 — Neural Network Playground.png"
                        height="450px" />
                    <figcaption>Source: https://playground.tensorflow.org/</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">41
                    <a class="prev" href="#slide40"></a>
                    <a class="next" href="#slide42"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide42">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Organisation</h2>
                <p>Un réseau de neurones profond est une architecture complexe où l'information circule de la couche
                    d'entrée à travers les couches cachées jusqu'à la couche de sortie. Chaque connexion entre les
                    neurones
                    est associée à un poids qui est ajusté pendant le processus d'apprentissage pour optimiser les
                    performances du modèle sur la tâche spécifique. L'utilisation de plusieurs couches cachées permet au
                    réseau d'apprendre des représentations de plus en plus abstraites et complexes des données.</p>
                <ul>
                    <li><b>Organisation en plusieurs couches</b> : Un réseau de neurones profond est structuré en
                        plusieurs
                        couches, généralement composées d'une couche d'entrée, de plusieurs couches cachées et d'une
                        couche
                        de sortie. Chaque couche est composée de neurones, également appelés nœuds ou unités.</li>
                    <li><b>Connexions entre les neurones</b> : Les neurones d'une couche sont connectés aux neurones de
                        la
                        couche immédiatement précédente et de la couche immédiatement suivante. Chaque connexion est
                        associée à un poids qui est ajusté pendant l'apprentissage.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">42
                    <a class="prev" href="#slide41"></a>
                    <a class="next" href="#slide43"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide43">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Organisation</h2>
                <ul>
                    <li><b>Couche d'entrée</b> : La couche d'entrée est la première couche du réseau. Elle reçoit les
                        données externes, souvent représentées par des caractéristiques d'un ensemble de données. Chaque
                        neurone dans la couche d'entrée correspond à une caractéristique spécifique.</li>
                    <li><b>Couche de sortie</b> : La couche de sortie est la dernière couche du réseau. Elle produit le
                        résultat final du modèle en fonction de la tâche spécifique, telle que la classification d'une
                        image, la prédiction d'une valeur, etc. Le nombre de neurones dans cette couche dépend du type
                        de
                        problème (par exemple, un neurone pour chaque classe dans une tâche de classification).</li>
                    <li><b>Couches cachées</b> : Entre la couche d'entrée et la couche de sortie, il peut y avoir zéro
                        ou
                        plusieurs couches cachées. Ces couches sont responsables de l'extraction de caractéristiques
                        complexes à partir des données d'entrée. Chaque neurone dans une couche cachée combine les
                        informations des neurones de la couche précédente pour apprendre des représentations
                        hiérarchiques.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">43
                    <a class="prev" href="#slide42"></a>
                    <a class="next" href="#slide44"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide44">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Composants des réseaux de neurones artificiels</h2>
                <h2 class="topicsubheading">Organisation et connectivité</h2>
                <p><b>Connectivité entièrement connectée</b>: Dans une connectivité entièrement connectée, chaque
                    neurone
                    d'une couche est connecté à chaque neurone de la couche suivante. Cela signifie que toutes les
                    informations de la couche précédente sont transmises à chaque neurone de la couche suivante. C'est
                    la
                    configuration la plus courante dans les couches totalement connectées, généralement présentes dans
                    les
                    parties du réseau proches de la sortie.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">44
                    <a class="prev" href="#slide43"></a>
                    <a class="next" href="#slide45"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide45">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
                <div class="fullwidth">
                    <figure>
                        <img src="../../2021/MachineLearning/Deep_Learning.jpg" height="400px" />
                        <figcaption>Source: https://en.wikipedia.org/wiki/File:Deep_Learning.jpg</figcaption>
                    </figure>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">45
                    <a class="prev" href="#slide44"></a>
                    <a class="next" href="#slide46"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide46">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
                <figure class="fullwidth">
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Typical_cnn.png"
                        height="400vh" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">46
                    <a class="prev" href="#slide45"></a>
                    <a class="next" href="#slide47"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide47">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
                <p>Les réseaux de neurones convolutionnels (CNN) sont une classe d'architectures de réseaux neuronaux
                    conçues principalement pour l'analyse des images. Ils ont été particulièrement efficaces dans des
                    tâches
                    telles que la classification d'images, la détection d'objets, et la segmentation d'images. </p>
                <ul>
                    <li><b>Analyse des Images</b> : Les CNN sont spécifiquement conçus pour travailler avec des données
                        structurées en grilles, comme les images. Ils sont capables de capturer des motifs et des
                        caractéristiques spatiales importantes dans les images.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">47
                    <a class="prev" href="#slide46"></a>
                    <a class="next" href="#slide48"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide48">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
                <ul>
                    <li><b>Utilise la convolution</b> La convolution est une opération mathématique linéaire utilisée
                        pour
                        extraire des caractéristiques locales à partir de l'image. Les filtres de convolution sont
                        appliqués
                        à l'image pour détecter des motifs tels que des bords, des textures, ou des formes.</li>
                    <li><b>Architecture en couches</b> : Les CNN suivent généralement une architecture en couches. Ils
                        ont
                        une couche d'entrée pour recevoir l'image, une ou plusieurs couches cachées composées
                        principalement
                        de couches convolutives, et une couche de sortie pour produire les résultats finaux.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">48
                    <a class="prev" href="#slide47"></a>
                    <a class="next" href="#slide49"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide49">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels</h3>
                <ul>
                    <li><b>Couches convolutives</b> : Les couches convolutives sont responsables de l'extraction des
                        caractéristiques de l'image. Chaque couche peut avoir plusieurs filtres de convolution qui
                        apprennent à détecter des motifs spécifiques. Ces couches sont souvent suivies de couches de
                        pooling
                        pour réduire la dimensionnalité de la représentation tout en préservant les caractéristiques
                        importantes.</li>
                    <li><b>Applications</b> : Les CNN sont largement utilisés dans des applications telles que la
                        classification d'images (par exemple, reconnaître des animaux dans des photos), la détection
                        d'objets (localiser et identifier des objets spécifiques), et la segmentation d'images (diviser
                        une
                        image en régions sémantiquement significatives)..</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">49
                    <a class="prev" href="#slide48"></a>
                    <a class="next" href="#slide50"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide50">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels: architecture</h3>
                <ul>
                    <li><b>Modèle hiérarchique des données</b> : Les réseaux neuronaux convolutifs (CNN) sont en effet
                        conçus pour capturer des caractéristiques hiérarchiques dans les données, en particulier dans le
                        contexte de l'analyse d'images. Cela signifie qu'ils peuvent apprendre des motifs simples dans
                        les
                        premières couches, puis combiner ces motifs pour former des caractéristiques plus complexes dans
                        les
                        couches suivantes.</li>
                    <li><b>Architecture d'un CNN</b> : Un réseau neuronal convolutif est généralement composé d'une
                        couche
                        d'entrée, de plusieurs couches cachées et d'une couche de sortie. Les couches cachées consistent
                        principalement en couches convolutionnelles, mais peuvent également inclure d'autres types de
                        couches telles que des couches de regroupement (pooling), des couches entièrement connectées, et
                        des
                        couches de normalisation.</li>
                </ul>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/Typical_cnn.png"
                        height="180px" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">50
                    <a class="prev" href="#slide49"></a>
                    <a class="next" href="#slide51"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide51">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réseaux de neurones convolutionnels: architecture</h3>
                <ul>
                    <li><b>Couches convolutionnelles et fonction d'activation</b> : Les couches convolutionnelles
                        appliquent
                        des filtres pour extraire des caractéristiques de l'image. La multiplication est effectuée par
                        la
                        convolution. La fonction d'activation la plus couramment utilisée est ReLU (Rectified Linear
                        Unit),
                        qui introduit une non-linéarité dans le modèle. Cette non-linéarité est importante pour
                        permettre au
                        réseau d'apprendre des relations complexes dans les données.</li>
                    <li><b>Couches supplémentaires</b> : Après les couches de convolution, on peut avoir des couches de
                        regroupement pour réduire la dimensionnalité, des couches entièrement connectées pour combiner
                        des
                        caractéristiques globales, et des couches de normalisation pour améliorer la stabilité de
                        l'apprentissage.</li>
                </ul>
                <p>En résumé, les CNN suivent une architecture hiérarchique, où les couches convolutives apprennent des
                    caractéristiques locales, et ces caractéristiques sont ensuite combinées dans les couches suivantes
                    pour
                    former des représentations plus complexes. La non-linéarité introduite par la fonction d'activation
                    ReLU
                    est cruciale pour permettre au modèle d'apprendre des relations non linéaires dans les données.</p>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">51
                    <a class="prev" href="#slide50"></a>
                    <a class="next" href="#slide52"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide52">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Noyau (traitement d'image)</h2>
                <p>Un noyau dans le contexte du traitement d'images, également appelé filtre ou masque, est une petite
                    matrice qui est appliquée sur une image à l'aide d'une opération de convolution. L'objectif de
                    l'application de ces noyaux est de réaliser diverses opérations de filtrage sur l'image, telles que
                    la
                    détection de contours, l'amélioration des détails, la mise en évidence de certaines
                    caractéristiques,
                    etc.</p>
                <ul>
                    <li><b>Convolution dans les CNN</b> : Dans les CNN, la convolution est une opération clé qui
                        consiste à
                        appliquer un ensemble de filtres (noyaux) à une image d'entrée. Chaque filtre est conçu pour
                        extraire des caractéristiques spécifiques de l'image, comme des bords, des textures, ou d'autres
                        motifs.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">52
                    <a class="prev" href="#slide51"></a>
                    <a class="next" href="#slide53"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide53">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Noyau (traitement d'image)</h2>
                <ul>
                    <li><b>Apprentissage des noyaux</b> : L'une des caractéristiques importantes des CNN est la capacité
                        d'apprendre les filtres (noyaux) de manière automatique pendant l'entraînement. Au lieu de
                        définir
                        manuellement les filtres comme dans le traitement d'images traditionnel, les CNN ajustent les
                        poids
                        des filtres pendant la phase d'apprentissage en fonction des caractéristiques qui sont
                        importantes
                        pour la tâche à accomplir.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">53
                    <a class="prev" href="#slide52"></a>
                    <a class="next" href="#slide54"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide54">
            <div class="header">
                <h1>3.2. Apprentissage profond</h1>
            </div>
            <div class="content">
                <h2 class="topicsubheading">Noyau (traitement d'image)</h2>
                <ul>
                    <li><b>Rôle dans la hiérarchie des caractéristiques</b> : Les premières couches d'un CNN apprennent
                        généralement des filtres simples qui détectent des contours ou des textures de base. À mesure
                        que
                        l'on progresse dans les couches du réseau, les filtres deviennent plus complexes, capturant des
                        caractéristiques de niveau supérieur, jusqu'à ce que la sortie finale représente des
                        caractéristiques abstraites de l'image d'entrée.</li>
                    <li><b>Réduction de dimension avec le pooling</b> : Après la convolution, les CNN utilisent souvent
                        des
                        couches de pooling pour réduire la dimension de la représentation, tout en préservant les
                        caractéristiques importantes extraites par les filtres. Cela permet d'économiser des ressources
                        computationnelles tout en maintenant les informations cruciales.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">54
                    <a class="prev" href="#slide53"></a>
                    <a class="next" href="#slide55"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide55">
            <div class="header">
                <h1>3.3. Apprentissage par renforcement</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Apprentissage par renforcement</h1>
                <div class="flexcontent">
                    <ul style="width:50%">
                        <li>L'apprentissage par renforcement (Reinforcement Learning - RL) est une branche de
                            l'apprentissage
                            automatique inspirée des théories de la psychologie animale.</li>
                        <li><b>Agent autonome</b> : RL implique un agent autonome interagissant avec un environnement.
                        </li>
                        <li><b>Prise de décision</b> : L'agent prend des décisions en fonction de son état actuel. </li>
                        <li><b>Récompenses et pénalités</b> : L'environnement fournit à l'agent des récompenses, qui
                            peuvent
                            être positives ou négatives. </li>
                        <li><b>Objectif</b> : L'objectif est de maximiser la somme des récompenses cumulatives au fil du
                            temps.
                        </li>
                    </ul>
                    <figure>
                        <img src="../../../../../en/teaching/courses/2017/DataMining/images/Reinforcement_learning_diagram.svg"
                            height="300vh" />
                    </figure>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">55
                    <a class="prev" href="#slide54"></a>
                    <a class="next" href="#slide56"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide56">
            <div class="header">
                <h1>3.4. Licences, Ethiques et la vie privé</h1>
            </div>
            <div class="content">
                <h1 class="topicsubheading">Licences, Éthique et la vie privé</h1>
                <ul>
                    <li>Droits d'utilisation des données</li>
                    <li>Confidentialité et vie privée</li>
                    <li>Éthique</li>
                </ul>
                <figure class="gridcontent">
                    <img src="../../../../../fr/enseignement/cours/2017/BigData/images/Open_Definition_logo.png"
                        height="100vh" />
                    <img src="../../../../../en/teaching/courses/2017/ArchitectureInformationSystems/images/Privacy_written_in_tiles.jpg"
                        height="300vh" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">56
                    <a class="prev" href="#slide55"></a>
                    <a class="next" href="#slide57"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide57">
            <div class="header">
                <h1>3.4. Licences, Ethiques et la vie privé</h1>
            </div>
            <div class="content">
                <div class="flexcontent">
                    <figure class="fullwidth">
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-NC-ND.svg"
                            height="100vh" />
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-NC-SA.svg"
                            height="100vh" />
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-NC.svg"
                            height="100vh" />
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-ND.svg"
                            height="100vh" />
                    </figure>
                    <figure class="fullwidth">
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY-SA.svg"
                            height="100vh" />
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/CC-BY.svg" height="100vh" />
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/Cc-zero.svg"
                            height="100vh" />
                        <img src="../../../../../fr/enseignement/cours/2018/BigData/images/Cc-public_domain_mark_white.svg"
                            height="100vh" />
                        <figcaption>Exemples: Creative Commons (CC)</figcaption>
                    </figure>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">57
                    <a class="prev" href="#slide56"></a>
                    <a class="next" href="#slide58"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide58">
            <div class="header">
                <h1>3.4. Licences, Ethiques et la vie privé</h1>
            </div>
            <div class="content">
                <figure>
                    <img src="../../../../../fr/enseignement/cours/2018/BigData/images/Creative_commons_license_spectrum.svg"
                        height="500vh" />
                    <figcaption>Exemples: Creative Commons (CC)</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">58
                    <a class="prev" href="#slide57"></a>
                    <a class="next" href="#slide59"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide59">
            <div class="header">
                <h1>3.4. Licences, Ethiques et la vie privé</h1>
            </div>
            <div class="content">
                <figure>

                    <img src="../../2018/BigData/images/Wikimedia_logo_family_complete-2013.svg" height="400vh" />
                    <figcaption>Données ouvertes</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">59
                    <a class="prev" href="#slide58"></a>
                    <a class="next" href="#slide60"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide60">
            <div class="header">
                <h1>3.4. Licences, Ethiques et la vie privé</h1>
            </div>
            <div class="content">
                <figure>
                    <img src="../../../../../fr/enseignement/cours/2017/BigData/images/LOD_Cloud_2014.svg.png"
                        height="400vh" />
                    <figcaption>Données ouvertes liées (Linked Open data: LOD) </figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">60
                    <a class="prev" href="#slide59"></a>
                    <a class="next" href="#slide61"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide61">
            <div class="header">
                <h1>3.4. Licences, Ethiques et la vie privé</h1>
            </div>
            <div class="content">
                <figure>
                    <img src="../../../../../fr/enseignement/cours/2017/BigData/images/Internet_Archive_logo_and_wordmark.svg.png"
                        height="400vh" />
                    <figcaption>Données archivées</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">61
                    <a class="prev" href="#slide60"></a>
                    <a class="next" href="#slide62"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide62">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Ressources en ligne</h3>
                <ul>
                    <ul>
                        <li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network">Artificial Neural
                                Network</a>
                        </li>
                        <li><a href="https://fr.wikipedia.org/wiki/Noyau_(traitement_d%27image)">Noyau (traitement
                                d'image)</a></li>
                        <li><a href="https://fr.wikipedia.org/wiki/R%C3%A9seau_neuronal_convolutif">Réseau neuronal
                                convolutif</a></li>
                        <li><a href="https://en.wikipedia.org/wiki/Perceptron">Perceptron</a></li>
                    </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">62
                    <a class="prev" href="#slide61"></a>
                    <a class="next" href="#slide63"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide63">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Couleurs</h3>
                <ul>
                    <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
                </ul>
                <h3 class="topicsubheading">Images</h3>
                <ul>
                    <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Data Mining et Machine Learning | John Samuel</div>
                <div class="navigation">63
                    <a class="prev" href="#slide62"></a>
                </div>
            </div>
        </section>


        <script>
            function changeCurrentURLSlideNumber(isIncrement) {
                url = window.location.href;
                position = url.indexOf("#slide");
                if (position != -1) { // Not on the first page
                    slideIdString = url.substr(position + 6);
                    if (!Number.isNaN(slideIdString)) {
                        slideId = parseInt(slideIdString);
                        if (isIncrement) {
                            if (slideId < 63) {
                                slideId = slideId + 1;
                            }
                        } else {
                            if (slideId > 1) {
                                slideId = slideId - 1;
                            }
                        }
                        /* regexp */
                        url = url.replace(/#slide\d+/g, "#slide" + slideId);
                        window.location.href = url;
                    }
                } else {
                    window.location.href = url + "#slide2";
                }
            }
            document.onkeydown = function (event) {

                event.preventDefault();
                /* This will ensure the default behavior of
                                                                page scroll behaviour (up, down, right, left)*/

                event = event || window.event;
                /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
                if (event.keyCode == '37') {
                    // left
                    changeCurrentURLSlideNumber(false);
                } else if (event.keyCode == '38') {
                    // up
                    changeCurrentURLSlideNumber(false);
                } else if (event.keyCode == '39') {
                    // right
                    changeCurrentURLSlideNumber(true);
                } else if (event.keyCode == '40') {
                    // down
                    changeCurrentURLSlideNumber(true);
                }
            }
            document.body.onmouseup = function (event) {
                event = event || window.event;
                event.preventDefault();
                changeCurrentURLSlideNumber(true);
            }
        </script>
    </body>

</html>