<html>

    <head>
        <meta charset="utf-8" />
        <title>Intelligence artificielle (référence): Cours: John Samuel</title>
        <link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
        <style type="text/css">
            body {
                height: 100%;
                width: 100%;
                background-color: white;
                margin: 0;
                overflow: hidden;
                font-family: Arial;
            }

            .slide {
                height: 100%;
                width: 100%;
            }

            .content {
                height: 79%;
                width: 95vw;
                display: flex;
                line-height: 1.7em;
                flex-direction: column;
                align-items: flex-start;
                margin: 0 auto;
                color: #000000;
                text-align: left;
                padding-left: 1.5vmax;
                padding-top: 1.5vmax;
                overflow-x: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
            }

            .codeexample {
                background-color: #eeeeee;
            }

            /*
generated by Pygments <https://pygments.org/>
Copyright 2006-2023 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
            pre {
                line-height: 125%;
            }

            td.linenos .normal {
                color: inherit;
                background-color: transparent;
                padding-left: 5px;
                padding-right: 5px;
            }

            span.linenos {
                color: inherit;
                background-color: transparent;
                padding-left: 5px;
                padding-right: 5px;
            }

            td.linenos .special {
                color: #000000;
                background-color: #ffffc0;
                padding-left: 5px;
                padding-right: 5px;
            }

            span.linenos.special {
                color: #000000;
                background-color: #ffffc0;
                padding-left: 5px;
                padding-right: 5px;
            }

            body .hll {
                background-color: #ffffcc
            }

            body {
                background: #f8f8f8;
            }

            body .c {
                color: #3D7B7B;
                font-style: italic
            }

            /* Comment */
            body .err {
                border: 1px solid #FF0000
            }

            /* Error */
            body .k {
                color: #008000;
                font-weight: bold
            }

            /* Keyword */
            body .o {
                color: #666666
            }

            /* Operator */
            body .ch {
                color: #3D7B7B;
                font-style: italic
            }

            /* Comment.Hashbang */
            body .cm {
                color: #3D7B7B;
                font-style: italic
            }

            /* Comment.Multiline */
            body .cp {
                color: #9C6500
            }

            /* Comment.Preproc */
            body .cpf {
                color: #3D7B7B;
                font-style: italic
            }

            /* Comment.PreprocFile */
            body .c1 {
                color: #3D7B7B;
                font-style: italic
            }

            /* Comment.Single */
            body .cs {
                color: #3D7B7B;
                font-style: italic
            }

            /* Comment.Special */
            body .gd {
                color: #A00000
            }

            /* Generic.Deleted */
            body .ge {
                font-style: italic
            }

            /* Generic.Emph */
            body .gr {
                color: #E40000
            }

            /* Generic.Error */
            body .gh {
                color: #000080;
                font-weight: bold
            }

            /* Generic.Heading */
            body .gi {
                color: #008400
            }

            /* Generic.Inserted */
            body .go {
                color: #717171
            }

            /* Generic.Output */
            body .gp {
                color: #000080;
                font-weight: bold
            }

            /* Generic.Prompt */
            body .gs {
                font-weight: bold
            }

            /* Generic.Strong */
            body .gu {
                color: #800080;
                font-weight: bold
            }

            /* Generic.Subheading */
            body .gt {
                color: #0044DD
            }

            /* Generic.Traceback */
            body .kc {
                color: #008000;
                font-weight: bold
            }

            /* Keyword.Constant */
            body .kd {
                color: #008000;
                font-weight: bold
            }

            /* Keyword.Declaration */
            body .kn {
                color: #008000;
                font-weight: bold
            }

            /* Keyword.Namespace */
            body .kp {
                color: #008000
            }

            /* Keyword.Pseudo */
            body .kr {
                color: #008000;
                font-weight: bold
            }

            /* Keyword.Reserved */
            body .kt {
                color: #B00040
            }

            /* Keyword.Type */
            body .m {
                color: #666666
            }

            /* Literal.Number */
            body .s {
                color: #BA2121
            }

            /* Literal.String */
            body .na {
                color: #687822
            }

            /* Name.Attribute */
            body .nb {
                color: #008000
            }

            /* Name.Builtin */
            body .nc {
                color: #0000FF;
                font-weight: bold
            }

            /* Name.Class */
            body .no {
                color: #880000
            }

            /* Name.Constant */
            body .nd {
                color: #AA22FF
            }

            /* Name.Decorator */
            body .ni {
                color: #717171;
                font-weight: bold
            }

            /* Name.Entity */
            body .ne {
                color: #CB3F38;
                font-weight: bold
            }

            /* Name.Exception */
            body .nf {
                color: #0000FF
            }

            /* Name.Function */
            body .nl {
                color: #767600
            }

            /* Name.Label */
            body .nn {
                color: #0000FF;
                font-weight: bold
            }

            /* Name.Namespace */
            body .nt {
                color: #008000;
                font-weight: bold
            }

            /* Name.Tag */
            body .nv {
                color: #19177C
            }

            /* Name.Variable */
            body .ow {
                color: #AA22FF;
                font-weight: bold
            }

            /* Operator.Word */
            body .w {
                color: #bbbbbb
            }

            /* Text.Whitespace */
            body .mb {
                color: #666666
            }

            /* Literal.Number.Bin */
            body .mf {
                color: #666666
            }

            /* Literal.Number.Float */
            body .mh {
                color: #666666
            }

            /* Literal.Number.Hex */
            body .mi {
                color: #666666
            }

            /* Literal.Number.Integer */
            body .mo {
                color: #666666
            }

            /* Literal.Number.Oct */
            body .sa {
                color: #BA2121
            }

            /* Literal.String.Affix */
            body .sb {
                color: #BA2121
            }

            /* Literal.String.Backtick */
            body .sc {
                color: #BA2121
            }

            /* Literal.String.Char */
            body .dl {
                color: #BA2121
            }

            /* Literal.String.Delimiter */
            body .sd {
                color: #BA2121;
                font-style: italic
            }

            /* Literal.String.Doc */
            body .s2 {
                color: #BA2121
            }

            /* Literal.String.Double */
            body .se {
                color: #AA5D1F;
                font-weight: bold
            }

            /* Literal.String.Escape */
            body .sh {
                color: #BA2121
            }

            /* Literal.String.Heredoc */
            body .si {
                color: #A45A77;
                font-weight: bold
            }

            /* Literal.String.Interpol */
            body .sx {
                color: #008000
            }

            /* Literal.String.Other */
            body .sr {
                color: #A45A77
            }

            /* Literal.String.Regex */
            body .s1 {
                color: #BA2121
            }

            /* Literal.String.Single */
            body .ss {
                color: #19177C
            }

            /* Literal.String.Symbol */
            body .bp {
                color: #008000
            }

            /* Name.Builtin.Pseudo */
            body .fm {
                color: #0000FF
            }

            /* Name.Function.Magic */
            body .vc {
                color: #19177C
            }

            /* Name.Variable.Class */
            body .vg {
                color: #19177C
            }

            /* Name.Variable.Global */
            body .vi {
                color: #19177C
            }

            /* Name.Variable.Instance */
            body .vm {
                color: #19177C
            }

            /* Name.Variable.Magic */
            body .il {
                color: #666666
            }

            /* Literal.Number.Integer.Long */


            .content h1,
            h2,
            h3,
            h4 {
                color: #1B80CF;
            }

            .content .topichighlight {
                background-color: #78002E;
                color: #FFFFFF;
            }

            .content .topicheading {
                background-color: #1B80CF;
                color: #FFFFFF;
                vertical-align: middle;
                border-radius: 0 2vmax 2vmax 0%;
                height: 4vmax;
                line-height: 4vmax;
                padding-left: 1vmax;
                margin: 0.1vmax;
                width: 50%;
                margin-bottom: 1vmax;
            }

            .content .flexcontent {
                display: flex;
                overflow-y: auto;
                font-size: 2.8vmin;
                flex-wrap: wrap;
            }

            .content .gridcontent {
                display: grid;
                grid-template-columns: auto auto auto auto;
                grid-column-gap: 0px;
                grid-row-gap: 0px;
                grid-gap: 0px;
            }

            .content .topicsubheading {
                background-color: #1B80CF;
                color: #FFFFFF;
                vertical-align: middle;
                border-radius: 0 1.5vmax 1.5vmax 0%;
                height: 3vmax;
                margin: 0.1vmax;
                font-size: 90%;
                line-height: 3vmax;
                padding-left: 1vmax;
                width: 70%;
                margin-bottom: 1vmax;
            }

            .content table {
                color: #000000;
                font-size: 100%;
                width: 100%;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
                text-decoration: none;
            }

            .content th {
                color: #FFFFFF;
                background-color: #1B80CF;
                border-radius: 2vmax 2vmax 2vmax 2vmax;
                font-size: 120%;
                padding: 15px;
            }

            .content figure {
                max-width: 90%;
                max-height: 90%;
            }

            .content .fullwidth img {
                max-width: 90%;
                max-height: 90%;
            }

            .content figure img {
                max-width: 50vmin;
                max-height: 50vmin;
                display: block;
                margin-left: auto;
                margin-right: auto;
            }

            .content figure figcaption {
                max-width: 90%;
                max-height: 90%;
                margin: 0.1vmax;
                font-size: 90%;
                text-align: center;
                padding: 0.5vmax;
                background-color: #E1F5FE;
                border-radius: 2vmax 2vmax 2vmax 2vmax;
            }

            .content td {
                color: #000000;
                width: 8%;
                padding-left: 3vmax;
                padding-top: 1vmax;
                padding-bottom: 1vmax;
                background-color: #E1F5FE;
                border-radius: 2vmax 2vmax 2vmax 2vmax;
            }

            .content li {
                line-height: 1.7em;
            }

            .header {
                color: #ffffff;
                background-color: #00549d;
                height: 5vmax;
            }

            .header h1 {
                text-align: center;
                vertical-align: middle;
                font-size: 3vmax;
                line-height: 4vmax;
                margin: 0;
            }

            .footer {
                height: 3vmax;
                line-height: 3vmax;
                vertical-align: middle;
                color: #ffffff;
                background-color: #00549d;
                margin: 0;
                padding: .3vmax;
                overflow: hidden;
            }

            .footer .contact {
                float: left;
                color: #ffffff;
                text-align: left;
                font-size: 3.2vmin;
            }

            .footer .navigation {
                float: right;
                text-align: right;
                width: 8vw;
                font-size: 3vmin;
            }

            .footer .navigation .next,
            .prev {
                font-size: 3vmin;
                color: #ffffff;
                text-decoration: none;
            }

            .footer .navigation .next::after {
                content: "| >";
            }

            .footer .navigation .prev::after {
                content: "< ";
            }


            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                .slide {
                    height: 100%;
                    width: 100%;
                }

                .content {
                    width: 100%;
                    height: 92%;
                    display: flex;
                    flex-direction: row;
                    text-align: left;
                    padding: 1vw;
                    line-height: 3.8vmax;
                    font-size: 1.8vmax;
                    flex-wrap: wrap;
                }

                .content .topicheading {
                    width: 90%;
                }

                .content h1,
                h2,
                h3,
                h4 {
                    width: 100%;
                }

                .content figure img {
                    max-width: 80vmin;
                    max-height: 50vmin;
                }

                .content figure figcaption {
                    max-width: 90%;
                    max-height: 90%;
                }
            }

            @media print {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                .content {
                    font-size: 2.8vmin;
                }

                .content .flexcontent {
                    font-size: 2.5vmin;
                }
            }
        </style>
        <script src="../../2021/MachineLearning/tex-mml-chtml.js" id="MathJax-script"></script>
    </head>

    <body>
        <section class="slide" id="slide1">
            <div class="header">
            </div>
            <div class="content">
                <h1 style="font-size:2.5vw">Intelligence artificielle et Deep Learning</h1>
                <h3>L'IA symbolique et certaines applications</h3>
                <p><b>John Samuel</b><br /> CPE Lyon<br /><br />
                    <b>Référence</b><br />
                    <b>Courriel</b>: john.samuel@cpe.fr<br /><br />
                    <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
                            alt="Creative Commons License" style="border-width:0"
                            src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">1

                    <a class="next" href="#slide2"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide2">
            <div class="header">
                <h1>4.1. Traitement automatique des langues naturelles (NLP) </h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Intelligence artificielle</h3>
                <figure>
                    <img src="../../../../../images/art/courses/deeplearningposition.svg" height="450px" />
                    <figcaption>Intelligence artificielle</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">2
                    <a class="prev" href="#slide1"></a>
                    <a class="next" href="#slide3"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide3">
            <div class="header">
                <h1>4.1. Traitement automatique des langues naturelles</h1>
            </div>
            <div class="content">
                <p>Le <b>traitement automatique des langues (TAL)</b> est un domaine interdisciplinaire de la
                    linguistique
                    informatique qui se concentre sur l'analyse et la compréhension du <b>langage naturel</b> (celui
                    utilisé
                    par les humains). Cette section aborde plusieurs aspects clés du TAL, notamment :</p>
                <ul>
                    <li><b>Analyser et comprendre le langage naturel (humain)</b>: Le TAL se consacre à la compréhension
                        du
                        langage naturel dans divers contextes, qu'il s'agisse de textes écrits ou de discours verbal.
                    </li>
                    <li>Interaction homme-machine</li>
                    <li><b>Syntaxe d'une langue</b>
                        <ul>
                            <li>Parsing : Le parsing consiste à analyser la structure grammaticale des phrases.</li>
                            <li>L'étiquetage en parties du discours (PoS) : L'étiquetage PoS consiste à assigner des
                                catégories grammaticales (comme verbe, nom, adjectif, etc.) aux mots d'une phrase.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">3
                    <a class="prev" href="#slide2"></a>
                    <a class="next" href="#slide4"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide4">
            <div class="header">
                <h1>4.1. Traitement automatique des langues naturelles</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Sémantique d'une langue</b>
                        <ul>
                            <li>Traduction automatique</li>
                            <li>Reconnaissance d'entités nommées (NER): La NER consiste à identifier des entités
                                spécifiques
                                (comme des noms de personnes, de lieux ou d'organisations) dans un texte.</li>
                            <li>Analyse des sentiments</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">4
                    <a class="prev" href="#slide3"></a>
                    <a class="next" href="#slide5"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide5">
            <div class="header">
                <h1>4.1. Traitement automatique des langues naturelles</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Analyse de systèmes TAL</h3>
                <ul>
                    <li><b>Racinisation</b> : La racinisation est le processus de réduction des mots à leur forme de
                        base ou
                        de racine. </li>
                    <li><b>Étiquetage morpho-syntaxique</b> : Cette étape consiste à attribuer des balises ou des
                        étiquettes
                        aux mots dans un texte en fonction de leur rôle grammatical et de leur structure. </li>
                    <li><b>Lemmatisation</b> : Contrairement à la racinisation, la lemmatisation consiste à ramener les
                        mots
                        à leur forme canonique ou lemmes. </li>
                    <li><b>Morphologie</b> : La morphologie concerne l'étude de la structure des mots, notamment comment
                        ils
                        sont formés à partir de morphèmes (unités de sens). </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">5
                    <a class="prev" href="#slide4"></a>
                    <a class="next" href="#slide6"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide6">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation [Frakes 2003]</h3>
                <ul>
                    <li>La racination, souvent appelée <b>stemming</b> en anglais, est un processus de normalisation
                        linguistique visant à réduire les mots à leur forme racine, en ignorant les affixes. Elle est
                        utilisée pour simplifier les variations morphologiques des mots. </li>
                    <li>Les <b>algorithmes de racination</b> appliquent généralement des règles heuristiques pour
                        éliminer
                        les préfixes et suffixes courants.
                        <ul>
                            <li><b>Exemples</b> : Porter, Snowball</li>
                            <li><b>Limitations</b> : La racination peut conduire à des résultats non valides, car elle
                                peut
                                produire des racines qui ne sont pas des mots réels.</li>
                        </ul>
                    </li>
                    <li>Exemples
                        <ul>
                            <li>engineer: engineer, engineered, engineering</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">6
                    <a class="prev" href="#slide5"></a>
                    <a class="next" href="#slide7"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide7">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation</h3>
                <p>Quelques exemples d'issues potentiellement <b>non valides</b> :</p>
                <ul>
                    <li>Racination excessive :
                        <ul>
                            <li>Mot d'origine : "happily"</li>
                            <li>Racination : "happi" (au lieu de la forme correcte "happy")</li>
                        </ul>
                    </li>
                    <li>Racination incorrecte :
                        <ul>
                            <li> Mot d'origine : "better"</li>
                            <li> Racination : "bet" (au lieu de la forme correcte "better")</li>
                        </ul>
                    <li>Création de faux mots :
                        <ul>
                            <li> Mot d'origine : "unhappiness"</li>
                            <li> Racination : "unhappi" (crée un faux mot au lieu de "unhappy")</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">7
                    <a class="prev" href="#slide6"></a>
                    <a class="next" href="#slide8"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide8">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation</h3>
                <ul>
                    <li>Ambiguïté des règles :
                        <ul>
                            <li> Mot d'origine : "flies" (verbe)</li>
                            <li> Racination : "fli" (peut être confondu avec le nom "fly")</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">8
                    <a class="prev" href="#slide7"></a>
                    <a class="next" href="#slide9"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide9">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: mesures d'évaluation [Frakes 2003]</h3>
                <ul>
                    <li>La mesure dans laquelle un algorithm modifie des mots qu'elle réduit à ses racines est appelée
                        la
                        <b>force</b> de l'algorithme
                    </li>
                    <li>Une métrique de <b>similarité</b> des algorithmes met en correspondance les n-tuples
                        d'algorithmes
                        (n au moins 2), avec un nombre indiquant la similarité des algorithmes. </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">9
                    <a class="prev" href="#slide8"></a>
                    <a class="next" href="#slide10"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide10">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: distance de Hamming [Frakes 2003]</h3>
                <ol>
                    <li>La distance de Hamming entre deux chaînes de longueur égale est définie comme le nombre de
                        caractères des deux chaînes qui sont différents à la même position.</li>
                    <li>Pour les chaînes de longueur inégale, ajouter la différence de longueur à la distance de Hamming
                        pour obtenir une fonction de distance de Hamming modifiée \(d\)</li>
                    <li>Exemples
                        <ul>
                            <li>tri: try, tried, trying</li>
                            <li>\(d\)(tri, try)= 1</li>
                            <li>\(d\)(tri, tried)= 2</li>
                            <li>\(d\)(tri, trying)= 4</li>
                        </ul>
                    </li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">10
                    <a class="prev" href="#slide9"></a>
                    <a class="next" href="#slide11"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide11">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: force [Frakes 2003]</h3>
                <ol>
                    <li>Le nombre moyen de mots par classe</li>
                    <li>Facteur de compression de l'indice. Soit n est le nombre de mots dans le corpus et s est le
                        nombre
                        de racines. \[\frac{n - s}{n}\]
                    </li>
                    <li>Le nombre de mots et de racines qui diffèrent</li>
                    <li>Le nombre moyen de caractères supprimés lors de la formation des racines</li>
                    <li>La médiane et la moyenne de la distance de Hamming modifiée entre les mots et leur racine</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">11
                    <a class="prev" href="#slide10"></a>
                    <a class="next" href="#slide12"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide12">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: similarité [Frakes 2003]</h3>
                <ol>
                    <li>Soit \(A1\) et \(A2\) sont deux algorithmes</li>
                    <li>Soit \(W\) une liste de mots et \(n\) le nombre de mots dans \(W\) \[ M(A1,A2,W) =
                        \frac{n}{\Sigma
                        d(x_i, y_i)}\]
                    </li>
                    <li>pour tous les mots \(w_i\) en W, \(x_i\) est le résultat de l'application de \(A1\) à \(w_i\) et
                        \(y_i\) est le résultat de l'application de \(A2\) à \(w_i\)</li>
                    <li>des algorithmes plus similaires auront des valeurs plus élevées de M</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">12
                    <a class="prev" href="#slide11"></a>
                    <a class="next" href="#slide13"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide13">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: nltk</h3>
                <p>L'objectif est de réduire les mots à leur forme de base ou racine, en éliminant les suffixes, ce qui
                    permet de regrouper différentes formes d'un mot sous une forme commune. </p>
                <ul>
                    <li><b>Porter [Porter 1980]</b> : Le Porter Stemming Algorithm, créé par Martin Porter en 1980, est
                        basé
                        sur un ensemble de règles heuristiques. Il suit une approche itérative en appliquant une série
                        de
                        transformations séquentielles aux mots.</li>
                    <li><b>Snowball</b> Le Snowball (anciennement appelé Porter2) est une amélioration du Porter
                        Stemmer. Il
                        suit également une approche basée sur des règles, mais il est plus systématique dans son
                        traitement
                        des différents cas de racination. </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">13
                    <a class="prev" href="#slide12"></a>
                    <a class="next" href="#slide14"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide14">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Porter</h3>
                <p>L'algorithme de Porter, également connu sous le nom de stemmer de Porter, est un algorithme de
                    racination
                    (stemming) développé par Martin Porter en 1980. Son objectif est de réduire les mots à leur forme
                    racine
                    ou base en éliminant les suffixes couramment utilisés en anglais.</p>
                <ul>
                    <li><b>Prétraitement</b> : Convertir le mot en minuscules. Identifier le préfixe 'y' et le traiter
                        comme
                        une voyelle s'il est en première position, sinon comme une consonne.</li>
                    <li><b>Application des règles de racination</b> : L'algorithme de Porter utilise une série de règles
                        pour éliminer les suffixes. Ces règles sont appliquées séquentiellement jusqu'à ce qu'aucune
                        d'entre
                        elles ne s'applique plus. Les règles comprennent des opérations comme la suppression de suffixes
                        spécifiques, la transformation de certains suffixes en d'autres, et la manipulation de la
                        longueur
                        des mots. </li>
                    <li><b>Post-traitement</b> : Certains ajustements post-traitement sont effectués pour améliorer la
                        précision de la racination.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">14
                    <a class="prev" href="#slide13"></a>
                    <a class="next" href="#slide15"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide15">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Porter</h3>
                <p>L'algorithme de Porter utilise une série de règles de racination pour réduire les mots à leur forme
                    racine. Quelques-unes des règles de l'algorithme de Porter :</p>
                <ol>
                    <li>Règles de suppression de suffixes :
                        <ul>
                            <li> "s" : Supprimer le suffixe "s" à la fin des mots.</li>
                            <li> "sses" : Remplacer par "ss" si la séquence se termine par "sses".</li>
                        </ul>
                    </li>
                    <li>Règles de traitement de suffixes spécifiques :
                        <ul>
                            <li> "eed" ou "eedly" : Remplacer par "ee" si la séquence se termine par "eed" ou "eedly".
                            </li>
                            <li> "ed" : Supprimer "ed" à la fin du mot s'il y a une voyelle précédente.</li>
                            <li> "ing" : Supprimer "ing" à la fin du mot s'il y a une voyelle précédente.</li>
                        </ul>
                    </li>

                    </li>
                    <ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">15
                    <a class="prev" href="#slide14"></a>
                    <a class="next" href="#slide16"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide16">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Porter</h3>
                <ol start="3">
                    <li>Règles de transformation de suffixes en d'autres suffixes :
                        <ul>
                            <li> "at" : Remplacer par "ate" si la séquence se termine par "at".</li>
                            <li> "bl" : Ajouter "e" à la fin si la séquence se termine par "bl".</li>
                        </ul>
                    </li>

                    <li>Règles de manipulation de la longueur des mots :
                        <ul>
                            <li> Si la séquence se termine par une consonne suivie de "y", remplacer par "i" à la fin.
                            </li>
                            <li> Si la séquence se termine par deux consonnes, supprimer la dernière consonne si la
                                précédente est une voyelle.</li>
                        </ul>
                    </li>

                    <li>Règles de manipulation des doubles consonnes :
                        <ul>
                            <li> Supprimer une lettre double à la fin du mot.</li>
                        </ul>
                    </li>
                    <ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">16
                    <a class="prev" href="#slide15"></a>
                    <a class="next" href="#slide17"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide17">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: Porter</h3>
                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.porter</span> <span class="kn">import</span> <span class="n">PorterStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">porter</span> <span class="o">=</span> <span class="n">PorterStemmer</span><span class="p">()</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">porter</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
                </div>
                <p>Affichage</p>
                <p class="codeexample">
                    <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">17
                    <a class="prev" href="#slide16"></a>
                    <a class="next" href="#slide18"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide18">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Snowball</h3>
                <p>L'algorithme de Snowball, également connu sous le nom de Snowball stemmer, est un algorithme de
                    racination (stemming) développé par Martin Porter comme une extension de son algorithme de Porter.
                    Snowball a été conçu pour être plus modulaire et extensible, permettant aux utilisateurs de créer
                    des
                    stemmers pour différentes langues en utilisant un ensemble commun de conventions.</p>
                <p>Les caractéristiques principales de l'algorithme de Snowball :</p>
                <ul>
                    <li><b>Modularité</b> : L'algorithme de Snowball est conçu de manière modulaire, permettant la
                        définition de règles spécifiques pour chaque langue. Chaque règle est encapsulée dans une unité
                        appelée "step."</li>
                    <li><b>Structure du Langage</b> : L'algorithme de Snowball est souvent utilisé pour différentes
                        langues,
                        et la structure du langage est définie par des fichiers de règles spécifiques à chaque langue.
                        Ces
                        fichiers décrivent comment les suffixes et préfixes doivent être traités.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">18
                    <a class="prev" href="#slide17"></a>
                    <a class="next" href="#slide19"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide19">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Snowball</h3>
                <ul>
                    <li><b>Extensibilité</b> : Les utilisateurs peuvent étendre l'algorithme de Snowball pour traiter
                        des
                        langues spécifiques en ajoutant des règles appropriées dans un fichier dédié à cette langue.
                    </li>
                    <li><b>Étape de Règle</b> : Chaque étape (step) de l'algorithme de Snowball est constituée de règles
                        qui
                        décrivent comment transformer un mot. Chaque règle a une forme similaire à "condition ->
                        action," où
                        la condition spécifie quand appliquer la règle, et l'action définit la transformation à
                        effectuer.
                    </li>
                    <li><b>Itération</b> : L'algorithme de Snowball applique les étapes de règle itérativement jusqu'à
                        ce
                        qu'aucune d'entre elles ne puisse être appliquée. Cette itération permet de réduire
                        progressivement
                        les mots à leur forme racine.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">19
                    <a class="prev" href="#slide18"></a>
                    <a class="next" href="#slide20"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide20">
            <div class="header">
                <h1>4.1.1. Racinisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Racinisation: Snowball</h3>
                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="s2">&quot;eating&quot;</span><span class="p">,</span> <span class="s2">&quot;went&quot;</span><span class="p">,</span> <span class="s2">&quot;engineer&quot;</span><span class="p">,</span> <span class="s2">&quot;tried&quot;</span><span class="p">]</span>
<span class="n">snowball</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">snowball</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">word</span><span class="p">))</span>
</pre>
                </div>
                <p>Affichage</p>
                <p class="codeexample">
                    <code>
				 word eat <span style="color:red">went</span> engin tri<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">20
                    <a class="prev" href="#slide19"></a>
                    <a class="next" href="#slide21"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide21">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
                <ul>
                    <li>L'étiquetage morpho-syntaxique, également appelé <b style="color:#00549d">Part of Speech (PoS)
                            Tagging</b>, est un processus dans lequel chaque mot d'un texte se voit attribuer une balise
                        morpho-syntaxique appropriée en fonction de son rôle grammatical et de son contexte
                        d'apparition.
                        Ces balises indiquent la catégorie grammaticale à laquelle chaque mot appartient. </li>
                    <li>Il permet de capturer la <b style="color:#00549d">structure grammaticale</b> d'un texte,
                        facilitant
                        ainsi la compréhension et l'analyse linguistique automatisées.</li>
                    <li>Les algorithmes d'étiquetage morpho-syntaxique utilisent généralement des <b
                            style="color:#00549d">modèles statistiques</b> ou des <b style="color:#00549d">règles
                            linguistiques</b> pour assigner ces balises en fonction du contexte entourant chaque mot.
                    </li>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">21
                    <a class="prev" href="#slide20"></a>
                    <a class="next" href="#slide22"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide22">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
                <ul>
                    <li>Exemples des balises
                        <ul>
                            <li><b style="color:#008000">Noms</b> : Indiquent des entités ou objets concrets. Exemple :
                                "chat," "maison," "fleur"</li>
                            <li><b style="color:#008000">Verbes</b> : Indiquent des actions ou des états. Exemple :
                                "marcher," "manger," "être"</li>
                            <li><b style="color:#008000">Adjectifs</b> : Décrivent ou qualifient des noms. Exemple :
                                "beau,"
                                "rapide," "intelligent"</li>
                            <li><b style="color:#008000">Adverbes</b> : Modifient des verbes, des adjectifs ou d'autres
                                adverbes, fournissant des informations sur la manière, le lieu, le temps, etc. Exemple :
                                "rapidement," "bien," "ici"</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">22
                    <a class="prev" href="#slide21"></a>
                    <a class="next" href="#slide23"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide23">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
                <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
                <ol>
                    <li><b>Approche manuelle</b> :
                        <ul>
                            <li>Construction de règles linguistiques manuelles pour analyser la structure linguistique
                            </li>
                            <li>Exemple : Définir des règles pour identifier les parties du discours en fonction de la
                                syntaxe.</li>
                        </ul>
                    </li>
                    <li><b>Approche statistique</b> :
                        <ul>
                            <li>Utilisation de statistiques et de probabilités pour modéliser les relations
                                linguistiques.
                            </li>
                            <li>Collection de n-grammes (bi-grammes, tri-grammes, ...)</li>
                            <li>Ensemble de fréquences de cooccurrence</li>
                            <li>L'estimation de la probabilité d'une séquence de longueur n est calculée en tenant
                                compte de
                                son occurrence dans le corpus d'entraînement</li>
                        </ul>
                    </li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">23
                    <a class="prev" href="#slide22"></a>
                    <a class="next" href="#slide24"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide24">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Étiquetage morpho-syntaxique [Màrquez 2000]</h3>
                <h3 class="topicsubheading">Construction de modèles linguistiques</h3>
                <ol>
                    <li><b>Apprentissage machine</b> :
                        <ul>
                            <li>Utilisation de techniques d'apprentissage machine pour apprendre automatiquement des
                                modèles
                                linguistiques à partir de données d'entraînement.</li>
                            <li>Les algorithmes peuvent être entraînés à reconnaître des motifs et des structures
                                linguistiques complexes</li>
                        </ul>
                    </li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">24
                    <a class="prev" href="#slide23"></a>
                    <a class="next" href="#slide25"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide25">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: ngrams</h3>
                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">ngrams</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{}</span><span class="s2">-grams&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">))</span>
    <span class="n">n_grams</span> <span class="o">=</span> <span class="n">ngrams</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">ngram</span> <span class="ow">in</span> <span class="n">n_grams</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ngram</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">25
                    <a class="prev" href="#slide24"></a>
                    <a class="next" href="#slide26"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide26">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: ngrams (affichage)</h3>
                <p class="codeexample">
                    <code>
1-grams<br/>
('He',) ('went',) ('to',) ('school',) ('yesterday',) ('and',) ('attended',) ('the',) ('classes',) <br/>
2-grams<br/>
('He', 'went') ('went', 'to') ('to', 'school') ('school', 'yesterday') ('yesterday', 'and') ('and', 'attended') ('attended', 'the') ('the', 'classes') <br/>
3-grams<br/>
('He', 'went', 'to') ('went', 'to', 'school') ('to', 'school', 'yesterday') ('school', 'yesterday', 'and') ('yesterday', 'and', 'attended') ('and', 'attended', 'the') ('attended', 'the', 'classes') <br/>
4-grams<br/>
('He', 'went', 'to', 'school') ('went', 'to', 'school', 'yesterday') ('to', 'school', 'yesterday', 'and') ('school', 'yesterday', 'and', 'attended') ('yesterday', 'and', 'attended', 'the') ('and', 'attended', 'the', 'classes')<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">26
                    <a class="prev" href="#slide25"></a>
                    <a class="next" href="#slide27"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide27">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: pos_tag</h3>

                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">pos_tag</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He goes to school daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
</pre>
                </div>
                <p>Affichage</p>
                <p class="codeexample">
                    <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">27
                    <a class="prev" href="#slide26"></a>
                    <a class="next" href="#slide28"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide28">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: pos_tag</h3>
                <p class="codeexample">
                    <code>
				[('He', 'PRP'), ('goes', 'VBZ'), ('to', 'TO'), ('school', 'NN'), ('daily', 'RB')]
                         </code>
                </p>
                <table>
                    <tr>
                        <th>Balise</th>
                        <th>Signification</th>
                    </tr>
                    <tr>
                        <td>PRP</td>
                        <td>pronoun, personal</td>
                    </tr>
                    <tr>
                        <td>VBZ</td>
                        <td>verb, present tense, 3rd person singular</td>
                    </tr>
                    <tr>
                        <td>TO</td>
                        <td>"to" as preposition</td>
                    </tr>
                    <tr>
                        <td>NN</td>
                        <td>"noun, common, singular or mass</td>
                    </tr>
                    <tr>
                        <td>RB</td>
                        <td>adverb</td>
                    </tr>
                </table>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">28
                    <a class="prev" href="#slide27"></a>
                    <a class="next" href="#slide29"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide29">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <p>Installation</p>
                <p class="codeexample">
                    <code>
				 $ pip3 install spacy<br/>
				 $ python3 -m spacy download en_core_web_sm<br/>
                         </code>
                </p>
                <p>Usage</p>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span></br>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">29
                    <a class="prev" href="#slide28"></a>
                    <a class="next" href="#slide30"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide30">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">)</span>
</pre>
                </div>
                </p>
                <p class="codeexample">
                    <code>
				  He PRON nsubj<br/>
                                  goes VERB ROOT<br/>
                                  to ADP prep<br/>
                                  school NOUN pobj<br/>
                                  daily ADV advmod<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">30
                    <a class="prev" href="#slide29"></a>
                    <a class="next" href="#slide31"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide31">
            <div class="header">
                <h1>4.1.2. Étiquetage morpho-syntaxique</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">tag_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">dep_</span><span class="p">,</span>
            <span class="n">token</span><span class="o">.</span><span class="n">shape_</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_alpha</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span><span class="p">)</span>
</pre>
                </div>
                </p>
                <p class="codeexample">
                    <code>
				  He -PRON- PRON PRP nsubj Xx True True<br/>
                                  goes go VERB VBZ ROOT xxxx True False<br/>
                                  to to ADP IN prep xx True True<br/>
                                  school school NOUN NN pobj xxxx True False<br/>
                                  daily daily ADV RB advmod xxxx True False<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">31
                    <a class="prev" href="#slide30"></a>
                    <a class="next" href="#slide32"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide32">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Lemmatisation [Gesmundo 2012]</h3>
                <ul>
                    <li>La lemmatisation, consiste à regrouper les différentes formes d'un mot qui appartiennent au même
                        paradigme morphologique flexionnel et à attribuer à chaque paradigme son lemme correspondant.
                    </li>
                    <li>Cette méthode vise à ramener les variations flexionnelles d'un mot à sa <b
                            style="color:#00549d">forme canonique</b> ou à sa racine. </li>
                    <li>La lemmatisation permet de simplifier la représentation des mots en les ramenant à leur forme de
                        base, ce qui facilite la recherche, l'analyse et le traitement automatique du langage naturel.
                    </li>
                    <li>Exemples
                        <ul>
                            <li>go: go, goes, going, went, gone</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">32
                    <a class="prev" href="#slide31"></a>
                    <a class="next" href="#slide33"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide33">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Lemmatisation [Chrupała 2006, Gesmundo 2012]</h3>
                <ul>
                    <li>La lemmatisation comme une tâche d'étiquetage</li>
                    <li>Attribuer un label pour chaque transformation d'un label en lemme</li>
                    <li>4 étapes [Gesmundo 2012]
                        <ol>
                            <li>supprimer un suffixe de longueur \(N_s\)</li>
                            <li>ajouter un nouveau suffixe de lemme \(L_s\)</li>
                            <li>supprimer un préfixe de longueur \(N_p\)</li>
                            <li>ajouter un nouveau préfixe lemme, \(L_p\)</li>
                        </ol>
                    </li>
                    <li>Transformation \(\tau = \langle N_s, L_s, N_p, L_p \rangle\)</li>
                    <li>(going, go) = \(\langle 3, \emptyset, 0, \emptyset \rangle \)</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">33
                    <a class="prev" href="#slide32"></a>
                    <a class="next" href="#slide34"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide34">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: WordNetLemmatizer</h3>
                <ul>
                    <li><b style="color:#00549d">WordNet [Miller 1995]</b> : WordNet est une base de données lexicale de
                        la
                        langue anglaise qui organise les mots en synsets (ensembles de synonymes) et les relie entre eux
                        par
                        des relations lexicales telles que l'hypernymie (relation "est-un") et l'hyponymie (relation "a
                        pour
                        instance").</li>
                    <li><b style="color:#00549d">WordNetLemmatizer</b> : Le module WordNetLemmatizer dans NLTK utilise
                        WordNet pour la lemmatisation des mots. Il attribue à chaque mot sa forme canonique ou lemme, en
                        tenant compte des différentes formes flexionnelles. </li>
                </ul>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;wordnet&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;averaged_perceptron_tagger&#39;</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">34
                    <a class="prev" href="#slide33"></a>
                    <a class="next" href="#slide35"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide35">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: WordNetLemmatizer (sans les balises PoS)</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;He went to school yesterday and attended the classes&quot;</span>
<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>

<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
                </div>
                </p>
                <p>Affichage</p>
                <p class="codeexample">
                    <code>
			     He went to school yesterday and attended the class
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">35
                    <a class="prev" href="#slide34"></a>
                    <a class="next" href="#slide36"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide36">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk.stem</span> <span class="kn">import</span> <span class="n">WordNetLemmatizer</span>
<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span><span class="p">,</span> <span class="n">pos_tag</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">wordnet</span> <span class="k">as</span> <span class="n">wn</span>

<span class="c1"># Check the complete list of tags http://www.nltk.org/book/ch05.html</span>
<span class="k">def</span> <span class="nf">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;J&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADJ</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;R&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">ADV</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;N&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">NOUN</span>
    <span class="k">elif</span> <span class="n">tag</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;V&quot;</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">wn</span><span class="o">.</span><span class="n">VERB</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">36
                    <a class="prev" href="#slide35"></a>
                    <a class="next" href="#slide37"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide37">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">nltk: WordNetLemmatizer (avec les balises PoS)</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;I went to school today and he goes daily&quot;</span>

<span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">pos_tag</span><span class="p">(</span><span class="n">tokens</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="n">wntag</span><span class="p">(</span><span class="n">tag</span><span class="p">)),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">token</span><span class="p">),</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
                </div>
                </p>
                <p>Affichage</p>
                <p class="codeexample">
                    <code>
			     I go to school today and he go daily
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">37
                    <a class="prev" href="#slide36"></a>
                    <a class="next" href="#slide38"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide38">
            <div class="header">
                <h1>4.1.3. Lemmatisation</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;I went to school today and he goes daily&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
</pre>
                </div>
                </p>
                <p class="codeexample">
                    <code>
				  -PRON- go to school today and -PRON- go daily<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">38
                    <a class="prev" href="#slide37"></a>
                    <a class="next" href="#slide39"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide39">
            <div class="header">
                <h1>4.1.4. Morphologie</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Morphologie</h3>
                <ul>
                    <li>La morphologie lexicale est une branche de la linguistique qui se concentre sur l'étude des <b
                            style="color:#00549d">mots</b>, de leurs <b style="color:#00549d">formes</b>, de leurs <b
                            style="color:#00549d">paradigmes</b> et de l'organisation des <b
                            style="color:#00549d">catégories grammaticales</b>. </li>
                    <li>Elle examine de près les parties du discours, l'intonation, l'accentuation, ainsi que la manière
                        dont le contexte peut influencer la prononciation et le sens d'un mot.</li>
                    <li>Elle explore la structure interne des mots et comment ils interagissent avec la grammaire et le
                        contexte pour communiquer des significations spécifiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">39
                    <a class="prev" href="#slide38"></a>
                    <a class="next" href="#slide40"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide40">
            <div class="header">
                <h1>4.1.4. Morphologie</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: mots vides, forme, PoS, lemme</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;He goes to school daily&quot;</span><span class="p">)</span>

<span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;dep&quot;</span><span class="p">,</span> <span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre>
                </div>
                </p>
                <figure>
                    <img src="../../2021/MachineLearning/spacy-dep-output.svg" width="400vw" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">40
                    <a class="prev" href="#slide39"></a>
                    <a class="next" href="#slide41"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide41">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
                <p>Les embeddings de mots sont une technique d'apprentissage de caractéristiques où des mots ou des
                    phrases
                    du vocabulaire sont associés à des vecteurs de nombres réels.</p>
                <ul>
                    <li>L'idée principale est de représenter chaque mot par un vecteur dense dans un espace continu, de
                        telle sorte que des mots similaires aient des vecteurs similaires, capturant ainsi les relations
                        sémantiques entre les mots.</li>
                    <li>Quantifier et catégoriser les similarités sémantiques entre les éléments linguistiques en
                        fonction
                        de leurs propriétés de distribution dans de grands échantillons de données linguistiques.</li>
                    <li>En d'autres termes, les mots qui ont des contextes similaires ou qui apparaissent dans des
                        contextes
                        similaires auront des embeddings de mots similaires.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">41
                    <a class="prev" href="#slide40"></a>
                    <a class="next" href="#slide42"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide42">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
                <p>Avantages de Word Embeddings</p>
                <ul>
                    <li><b>Représentation dense</b> : Les embeddings fournissent une représentation dense, contrairement
                        à
                        une représentation creuse où chaque mot serait représenté par un vecteur binaire indiquant sa
                        présence ou son absence.</li>
                    <li><b>Capture des relations sémantiques</b> : Les embeddings captent les relations sémantiques et
                        les
                        similitudes entre les mots, ce qui les rend utiles dans de nombreuses tâches de traitement du
                        langage naturel.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">42
                    <a class="prev" href="#slide41"></a>
                    <a class="next" href="#slide43"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide43">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word Embeddings (Incorporation de mots)</h3>
                <p>Applications de Word Embeddings</p>
                <ul>
                    <li><b>Similarité sémantique</b> : Mesurer la similarité sémantique entre les mots.</li>
                    <li><b>Traduction automatique</b> : Améliorer les performances des systèmes de traduction
                        automatique.
                    </li>
                    <li><b>Analyse des sentiments</b> : Mieux comprendre le contexte et les relations sémantiques dans
                        l'analyse des sentiments, entre autres applications.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">43
                    <a class="prev" href="#slide42"></a>
                    <a class="next" href="#slide44"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide44">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <p>spaCy est une bibliothèque open-source pour le traitement du langage naturel (NLP) en Python. Elle
                    offre
                    des outils performants et efficaces pour effectuer diverses tâches de traitement du langage naturel,
                    de
                    l'analyse syntaxique à la reconnaissance d'entités nommées. spaCy est conçu pour être rapide, précis
                    et
                    facile à utiliser.</p>
                <ul>
                    <li><b>Collecte de données</b> : Les modèles spaCy sont souvent entraînés sur de vastes ensembles de
                        données annotées, qui peuvent inclure des corpus textuels avec des annotations pour l'analyse
                        syntaxique, la reconnaissance d'entités nommées, etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">44
                    <a class="prev" href="#slide43"></a>
                    <a class="next" href="#slide45"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide45">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <ul>
                    <li><b>Annotation des données</b> : Les données collectées sont annotées manuellement avec des
                        informations linguistiques spécifiques telles que les parties du discours, les entités nommées,
                        les
                        relations syntaxiques, etc.</li>
                    <li><b>Entraînement initial</b> : Les modèles spaCy sont initialement entraînés sur ces ensembles de
                        données annotées pour apprendre les structures linguistiques. Ce processus peut inclure
                        l'utilisation d'algorithmes d'apprentissage automatique tels que les réseaux de neurones.</li>
                    <li><b>Optimisation et réglage</b> : Les modèles sont ensuite optimisés et réglés pour améliorer
                        leurs
                        performances sur des tâches spécifiques. Cela peut impliquer des itérations sur le processus
                        d'entraînement en ajustant les hyperparamètres du modèle.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">45
                    <a class="prev" href="#slide44"></a>
                    <a class="next" href="#slide46"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide46">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <ul>
                    <li><b>Évaluation</b> : Les modèles sont évalués sur des ensembles de données de test distincts pour
                        mesurer leur précision, leur rappel et d'autres métriques spécifiques à la tâche.</li>
                    <li><b>Construction des modèles linguistiques pré-entraînés</b> : Une fois le modèle entraîné et
                        évalué,
                        spaCy construit des modèles linguistiques pré-entraînés qui encapsulent les connaissances
                        acquises
                        sur la structure linguistique.</li>
                    <li><b>Téléchargement et utilisation</b> : Les utilisateurs peuvent télécharger ces modèles
                        pré-entraînés via spaCy et les utiliser dans leurs applications pour effectuer diverses tâches
                        de
                        traitement du langage naturel sans avoir à entraîner un modèle de zéro.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">46
                    <a class="prev" href="#slide45"></a>
                    <a class="next" href="#slide47"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide47">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <p>spaCy propose différents modèles linguistiques pré-entraînés pour différentes langues et tâches. Le
                    modèle en_core_web_lg est un modèle vectoriel large d'anglais.</p>
                <h4>Installation du Modèle spaCy (en_core_web_lg) :</h4>
                <p class="codeexample">
                    <code>
				 $ python3 -m spacy download en_core_web_lg<br/>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">47
                    <a class="prev" href="#slide46"></a>
                    <a class="next" href="#slide48"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide48">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <h4>Chargement du Modèle spaCy :</h4>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">48
                    <a class="prev" href="#slide47"></a>
                    <a class="next" href="#slide49"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide49">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <p>Avantages de spaCy :</p>
                <ul>
                    <li><b>Performance élevée</b> : spaCy est reconnu pour sa rapidité d'exécution, ce qui le rend
                        adapté au
                        traitement de grands volumes de texte en temps réel.</li>
                    <li><b>Modèles pré-entraînés</b> : spaCy propose des modèles linguistiques pré-entraînés pour
                        plusieurs
                        langues, ce qui facilite l'analyse de texte sans nécessiter d'entraînement à partir de zéro.
                    </li>
                    <li><b>Extraction d'informations linguistiques riches</b> : spaCy fournit des informations
                        linguistiques
                        détaillées telles que les parties du discours, les entités nommées, les relations syntaxiques,
                        et
                        plus encore.</li>
                    <li><b>API conviviale</b> : L'API spaCy est conçue pour être intuitive et conviviale. Elle facilite
                        la
                        réalisation de tâches complexes avec des lignes de code concises.</li>
                    <li><b>Intégration avec d'autres bibliothèques</b> : spaCy s'intègre bien avec d'autres
                        bibliothèques
                        Python populaires, facilitant son utilisation dans des projets plus larges.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">49
                    <a class="prev" href="#slide48"></a>
                    <a class="next" href="#slide50"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide50">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy</h3>
                <p>Limites de spaCy :</p>
                <ul>
                    <li><b>Dépendance des modèles linguistiques</b> : L'utilisation de modèles pré-entraînés signifie
                        que la
                        qualité des résultats dépend de la qualité du modèle. Dans des domaines de spécialité ou pour
                        des
                        langues moins courantes, les modèles peuvent ne pas être aussi performants.</li>
                    <li><b>Gestion des entités nommées</b> : Bien que spaCy excelle dans la reconnaissance d'entités
                        nommées, il peut parfois avoir du mal avec des tâches plus complexes impliquant des variations
                        contextuelles.</li>
                    <li><b>Taille des modèles</b> : Les modèles pré-entraînés peuvent être relativement volumineux, ce
                        qui
                        peut être un inconvénient dans des environnements avec des restrictions de mémoire ou pour des
                        applications mobiles.</li>
                    <li><b>Personnalisation limitée</b> : Bien que spaCy offre des fonctionnalités de personnalisation,
                        elles peuvent être limitées par rapport à d'autres bibliothèques NLP plus flexibles.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">50
                    <a class="prev" href="#slide49"></a>
                    <a class="next" href="#slide51"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide51">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: similarity</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_lg&quot;</span><span class="p">)</span>

<span class="c1"># Définir les mots à comparer</span>
<span class="n">words_to_compare</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;apple&quot;</span><span class="p">]</span>

<span class="c1"># Calculer la similarité entre les paires de mots</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words_to_compare</span><span class="p">)):</span>
        <span class="n">word1</span><span class="p">,</span> <span class="n">word2</span> <span class="o">=</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">words_to_compare</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">doc1</span><span class="p">,</span> <span class="n">doc2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word1</span><span class="p">),</span> <span class="n">nlp</span><span class="p">(</span><span class="n">word2</span><span class="p">)</span>
        <span class="n">similarity_score</span> <span class="o">=</span> <span class="n">doc1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">doc2</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité (</span><span class="si">{}</span><span class="s2"> / </span><span class="si">{}</span><span class="s2">): </span><span class="si">{:.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span>
	    <span class="n">similarity_score</span><span class="p">))</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">51
                    <a class="prev" href="#slide50"></a>
                    <a class="next" href="#slide52"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide52">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: similarity</h3>
                <h4>Affichage</h4>
                <p class="codeexample">
                    <code>
                <pre>
Similarité (dog / cat): ...
Similarité (dog / apple): ...
Similarité (cat / apple): ...
                </pre>

                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">52
                    <a class="prev" href="#slide51"></a>
                    <a class="next" href="#slide53"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide53">
            <div class="header">
                <h1>4.2. Word Embeddings</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: vector</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>

<span class="c1"># Imprimer les vecteurs de chaque jeton sur une seule ligne</span>
<span class="n">vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">token</span><span class="o">.</span><span class="n">vector</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteurs de &#39;</span><span class="si">{}</span><span class="s2">&#39; : </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">,</span> <span class="n">vector_list</span><span class="p">))</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">53
                    <a class="prev" href="#slide52"></a>
                    <a class="next" href="#slide54"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide54">
            <div class="header">
                <h1>4.3. Word2Vec</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
                <p>Word2Vec a marqué un tournant significatif dans la <b>représentation des mots</b> dans le domaine de
                    l'apprentissage automatique.</p>
                <ul>
                    <li>C'est une technique publiée en <b>2013</b> par une équipe de chercheurs dirigée par <b>Tomas
                            Mikolov</b> chez Google.</li>
                    <li><b>Représentation vectorielle</b> : Word2Vec représente chaque mot distinct avec un vecteur dans
                        un
                        espace continu. Ces vecteurs captent les relations sémantiques et syntaxiques entre les mots.
                    </li>
                    <li><b>Apprentissage basé sur un réseau neuronal</b> : Le modèle utilise un réseau neuronal pour
                        apprendre des associations de mots à partir d'un vaste corpus de texte. Cette approche permet de
                        capturer des nuances complexes dans la signification des mots.</li>
                    <li><b>Entrée et sortie</b> : Word2Vec prend en entrée un large corpus de texte et produit un espace
                        vectoriel, généralement de plusieurs centaines de dimensions. Cette représentation vectorielle
                        permet de mesurer la similarité sémantique entre les mots.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">54
                    <a class="prev" href="#slide53"></a>
                    <a class="next" href="#slide55"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide55">
            <div class="header">
                <h1>4.3. Word2Vec</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
                <p>L'implémentation de Word2Vec se déroule en plusieurs étapes :</p>
                <ol>
                    <li><b>Prétraitement des données</b> : Le texte est nettoyé et prétraité pour éliminer les éléments
                        indésirables tels que la ponctuation et les stopwords.</li>
                    <li><b>Création d'un vocabulaire</b> : Les mots uniques du corpus sont utilisés pour construire un
                        vocabulaire. Chaque mot est ensuite associé à un index.</li>
                    <li><b>Génération de paires mot-contexte</b> : Pour chaque mot du corpus, des paires mot-contexte
                        sont
                        créées en utilisant une fenêtre contextuelle glissante. Ces paires servent d'exemples
                        d'entraînement.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">55
                    <a class="prev" href="#slide54"></a>
                    <a class="next" href="#slide56"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide56">
            <div class="header">
                <h1>4.3. Word2Vec</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word2Vec [Mikolov 2013]</h3>
                <ol>
                    <li><b>Construction du modèle Word2Vec</b> : Un modèle de réseau neuronal est créé, avec une couche
                        d'entrée représentant les mots, une couche cachée (skip-gram ou CBOW), et une couche de sortie
                        pour
                        prédire le mot suivant dans le contexte.</li>
                    <li><b>Entraînement du modèle</b> : Le modèle est entraîné sur les paires mot-contexte générées,
                        ajustant les poids du réseau pour minimiser la différence entre les prédictions et les vrais
                        mots du
                        contexte.</li>
                    <li><b>Obtention des embeddings</b> : Les vecteurs de mots appris pendant l'entraînement, appelés
                        embeddings, sont extraits. Chaque mot du vocabulaire est maintenant représenté par un vecteur
                        dense
                        dans l'espace continu.</li>
                </ol>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">56
                    <a class="prev" href="#slide55"></a>
                    <a class="next" href="#slide57"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide57">
            <div class="header">
                <h1>4.3. Word2Vec</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Word2Vec</h3>
                <ul>
                    <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                        partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                        l'espace
                    </li>
                    <li>une simple fonction mathématique (par exemple, la similarité cosinus entre les vecteurs) indique
                        le
                        niveau de similarité sémantique entre les mots représentés par ces vecteurs \[\text{similarity}
                        =
                        \cos(\theta) = {\mathbf{A} \cdot \mathbf{B}
                        \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i B_i} }{
                        \sqrt{\sum\limits_{i=1}^{n}{A_i^2}} \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} },\]
                    </li>
                    <li>les vecteurs de mots sont positionnés dans l'espace vectoriel de telle sorte que les mots qui
                        partagent des contextes communs dans le corpus soient situés à proximité les uns des autres dans
                        l'espace
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">57
                    <a class="prev" href="#slide56"></a>
                    <a class="next" href="#slide58"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide58">
            <div class="header">
                <h1>4.3.1. Context Bag of Words (CBOW)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
                <p>CBOW est un modèle spécifique de Word2Vec. Dans ce modèle, la prédiction du mot courant se fait en
                    utilisant une fenêtre de mots contextuels voisins. L'ordre des mots de contexte n'influence pas la
                    prédiction, ce qui en fait une approche robuste.</p>
                <ul>
                    <li><b>Modèle prédictif</b> : CBOW prédit le mot cible en se basant sur le contexte qui l'entoure,
                        mais
                        contrairement à d'autres modèles, l'ordre spécifique des mots dans ce contexte n'est pas pris en
                        compte.</li>
                </ul>
                <figure>
                    <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">58
                    <a class="prev" href="#slide57"></a>
                    <a class="next" href="#slide59"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide59">
            <div class="header">
                <h1>4.3.1. Context Bag of Words (CBOW)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Context Bag of Words (CBOW)</h3>
                <ul>
                    <li><b>Entrée</b> : La donnée d'entrée du modèle CBOW est une fenêtre de mots contextuels entourant
                        le
                        mot cible. Cette fenêtre est définie par un paramètre appelé la taille de la fenêtre.</li>
                    <li><b>Architecture</b> : CBOW utilise une architecture de réseau neuronal à une seule couche
                        cachée. La
                        couche d'entrée représente les mots du contexte, et la couche de sortie représente le mot cible
                        à
                        prédire.</li>
                    <li><b>Entraînement</b> : Le modèle est entraîné en ajustant les poids du réseau pour minimiser la
                        différence entre les prédictions du modèle et le mot cible réel. Cela se fait à travers des
                        techniques d'optimisation comme la rétropropagation du gradient.</li>
                    <li><b>Sortie</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés comme
                        embeddings de mots. Ces embeddings capturent les relations sémantiques entre les mots,
                        permettant
                        ainsi de représenter chaque mot par un vecteur dans un espace continu.</li>
                    <li><b>Avantages</b> : CBOW est souvent plus rapide à entraîner que d'autres modèles comme le
                        Skip-gram
                        (une autre variante de Word2Vec) et peut être plus efficace dans des contextes où l'ordre
                        séquentiel
                        des mots n'est pas critique.</li>
                </ul>
                <figure>
                    <img src="../../2021/MachineLearning/cbow.svg" height="250vh" width="600vw" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">59
                    <a class="prev" href="#slide58"></a>
                    <a class="next" href="#slide60"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide60">
            <div class="header">
                <h1>4.3.1. Context Bag of Words (CBOW)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">gensim: cbow</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle CBOW avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="n">cbow_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
       <span class="n">window</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">60
                    <a class="prev" href="#slide59"></a>
                    <a class="next" href="#slide61"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide61">
            <div class="header">
                <h1>4.3.1. Context Bag of Words (CBOW)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">gensim: cbow</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre>

<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,
                  </span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des deux mots les plus probables suivant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">cbow_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots suivant &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
	</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">61
                    <a class="prev" href="#slide60"></a>
                    <a class="next" href="#slide62"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide62">
            <div class="header">
                <h1>4.3.2. Skip-grams</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Skip grams</h3>
                <p>Le modèle Skip-gram est une autre variante de Word2Vec qui se concentre sur la prédiction de la
                    fenêtre
                    voisine des mots de contexte à partir du mot courant. </p>
                <ul>
                    <li><b>Objectif</b> : L'objectif principal du modèle Skip-gram est de prendre un mot source (le mot
                        courant) et de prédire les mots qui l'entourent dans une fenêtre de contexte donnée.</li>
                    <li><b>Entrée</b> : Le mot source est utilisé comme donnée d'entrée du modèle, et la sortie
                        souhaitée
                        est la distribution des probabilités des mots du contexte.</li>
                </ul>
                <figure>
                    <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">62
                    <a class="prev" href="#slide61"></a>
                    <a class="next" href="#slide63"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide63">
            <div class="header">
                <h1>4.3.2. Skip-grams</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Skip grams</h3>
                <ul>
                    <li><b>Architecture</b> : Skip-gram utilise une architecture de réseau neuronal à une seule couche
                        cachée. La couche d'entrée représente le mot source, et la couche de sortie représente les mots
                        du
                        contexte.</li>
                    <li><b>Entraînement</b> : Pendant l'entraînement, les poids du réseau sont ajustés pour minimiser la
                        différence entre les prédictions du modèle et la véritable distribution des mots du contexte.
                        Cela
                        se fait généralement à l'aide de techniques d'optimisation comme la rétropropagation du
                        gradient.
                    </li>
                    <li><b>Pondération du contexte</b> : Une caractéristique importante du modèle Skip-gram est que
                        l'architecture accorde plus de poids aux mots de contexte proches du mot source que ceux plus
                        éloignés. Cela permet de mieux capturer les relations sémantiques et syntaxiques locales.</li>
                    <li><b>Embeddings</b> : Une fois le modèle entraîné, les poids de la couche d'entrée sont utilisés
                        comme
                        embeddings de mots. Ces embeddings capturent les similitudes sémantiques entre les mots,
                        permettant
                        de représenter chaque mot par un vecteur dans un espace continu.</li>
                </ul>
                <figure>
                    <img src="../../2021/MachineLearning/skipgram.svg" height="250vh" width="600vw" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">63
                    <a class="prev" href="#slide62"></a>
                    <a class="next" href="#slide64"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide64">
            <div class="header">
                <h1>4.3.2. Skip-grams</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">gensim: skip-gram</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">gensim</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>

<span class="c1"># Données d&#39;exemple</span>
<span class="n">data</span> <span class="o">=</span> <span class="s2">&quot;This is a class. This is a table&quot;</span>

<span class="c1"># Prétraitement des données en utilisant nltk pour obtenir des phrases et des mots</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>

<span class="c1"># Construction du modèle Skip-gram avec Gensim</span>
<span class="c1"># min_count: Ignorer tous les mots dont la fréquence totale est inférieure à cette valeur.</span>
<span class="c1"># vector_size: Dimension des embeddings de mots</span>
<span class="c1"># window: Distance maximale entre le mot courant et le mot prédit dans une phrase</span>
<span class="c1"># sg: 1 pour skip-gram ; sinon CBOW.</span>
<span class="n">skipgram_model</span> <span class="o">=</span> <span class="n">gensim</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Word2Vec</span><span class="p">(</span><span class="n">sentences</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vector_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,
                </span> <span class="n">window</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">64
                    <a class="prev" href="#slide63"></a>
                    <a class="next" href="#slide65"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide65">
            <div class="header">
                <h1>4.3.2. Skip-grams</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">gensim: skip-gram</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre>
<span class="c1"># Affichage du vecteur du mot &quot;this&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vecteur du mot &#39;this&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="s2">&quot;this&quot;</span><span class="p">])</span>

<span class="c1"># Similarité entre les mots &quot;this&quot; et &quot;class&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Similarité entre &#39;this&#39; et &#39;class&#39;:&quot;</span><span class="p">,</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;this&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">))</span>

<span class="c1"># Prédiction des mots les plus probables dans le contexte entourant le mot &quot;is&quot;</span>
<span class="n">predicted_words</span> <span class="o">=</span> <span class="n">skipgram_model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;is&quot;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prédiction des mots dans le contexte de &#39;is&#39;:&quot;</span><span class="p">,</span> <span class="n">predicted_words</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">65
                    <a class="prev" href="#slide64"></a>
                    <a class="next" href="#slide66"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide66">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
                <p>La Reconnaissance d'Entités Nommées (NER) consiste à identifier et classer des entités spécifiques
                    dans
                    un texte. Ces entités peuvent inclure des personnes, des lieux, des organisations, des dates, des
                    montants monétaires, etc. Le but est d'extraire des informations structurées à partir de données
                    textuelles non structurées.</p>
                <figure>
                    <img src="../../../../../en/teaching/courses/2017/DataMining/images/datarepresentation.svg"
                        height="350vh" width="600vw" />
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">66
                    <a class="prev" href="#slide65"></a>
                    <a class="next" href="#slide67"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide67">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
                <ul>
                    <li><b>Identification d'entités</b> : La première étape de la NER consiste à identifier les mots ou
                        groupes de mots qui représentent des entités dans le texte. Ces entités peuvent être des noms de
                        personnes, des noms de lieux, des noms d'organisations, etc.</li>
                    <li><b>Classification des entités</b> : Une fois les entités identifiées, elles sont classifiées
                        dans
                        des catégories spécifiques. Par exemple, une entité peut être classée comme "PERSON" si elle
                        représente une personne, "LOCATION" si elle représente un lieu, "ORGANIZATION" si elle
                        représente
                        une organisation, et ainsi de suite.</li>
                    <li><b>Contextualisation</b> : La NER tient compte du contexte dans lequel une entité apparaît. Par
                        exemple, le mot "banc" peut être classé comme une entité financière dans le contexte d'une
                        discussion sur l'économie, mais comme une entité physique dans le contexte d'un parc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">67
                    <a class="prev" href="#slide66"></a>
                    <a class="next" href="#slide68"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide68">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Reconnaissance d'entités nommées</h3>
                <ul>
                    <li><b>Relations entre entités</b> : Dans certains cas, la NER peut également inclure la détection
                        des
                        relations entre différentes entités dans le texte. Par exemple, la relation entre une personne
                        et
                        l'organisation qu'elle travaille.</li>
                    <li><b>Applications pratiques</b> : Les résultats de la NER peuvent être utilisés dans diverses
                        applications, telles que l'amélioration de la recherche d'informations, l'extraction de
                        relations,
                        la catégorisation de documents, la création de résumés automatiques, etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">68
                    <a class="prev" href="#slide67"></a>
                    <a class="next" href="#slide69"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide69">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
                <p>La Reconnaissance d'Entités Nommées (NER) est souvent réalisée à l'aide de modèles d'apprentissage
                    automatique, et plusieurs algorithmes peuvent être utilisés dans ce contexte. Quelques-uns des
                    algorithmes couramment employés :</p>
                <ul>
                    <li><b>Modèles de markov cachés (HMM - Hidden Markov Models)</b> : Les HMM ont été utilisés pour la
                        NER,
                        où l'idée est de modéliser la séquence des étiquettes d'entités en tant que séquence cachée
                        derrière
                        la séquence observable de mots.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">69
                    <a class="prev" href="#slide68"></a>
                    <a class="next" href="#slide70"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide70">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
                <ul>
                    <li><b>Réseaux de neurones</b> : Les architectures de réseaux de neurones, y compris les réseaux de
                        neurones récurrents (RNN), les réseaux de neurones récurrents bidirectionnels (BiRNN), et les
                        réseaux de neurones récurrents à mémoire à court terme (LSTM), ont montré des performances
                        significatives dans la NER.</li>
                    <li><b>Transformers</b> : Les modèles basés sur les transformers, tels que BERT (Bidirectional
                        Encoder
                        Representations from Transformers) et ses variantes, ont considérablement amélioré les
                        performances
                        en NER. Ces modèles sont pré-entraînés sur de grandes quantités de données textuelles et captent
                        des
                        représentations contextuelles riches.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">70
                    <a class="prev" href="#slide69"></a>
                    <a class="next" href="#slide71"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide71">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Reconnaissance d'entités nommées : Algorithmes</h3>
                <ul>
                    <li><b>Modèles statistiques traditionnels</b> : Des approches statistiques plus traditionnelles,
                        comme
                        les modèles de séquence et les classificateurs basés sur des caractéristiques, ont également été
                        utilisées dans des scénarios où des quantités limitées de données annotées sont disponibles.
                    </li>
                    <li><b>Règles et expressions régulières</b> : Dans certains cas, des règles manuelles ou des
                        expressions
                        régulières peuvent être utilisées pour extraire des entités spécifiques, surtout lorsque des
                        motifs
                        clairs et récurrents peuvent être définis.</li>
                    <li><b>Entraînement supervisé</b> : Les méthodes d'entraînement supervisé consistent à annoter
                        manuellement un ensemble de données avec des entités nommées, puis à entraîner un modèle sur ces
                        données annotées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">71
                    <a class="prev" href="#slide70"></a>
                    <a class="next" href="#slide72"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide72">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>

<span class="c1"># Charger le modèle spaCy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France.&quot;</span> + <span class="s2">&quot;In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Analyser le texte</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">72
                    <a class="prev" href="#slide71"></a>
                    <a class="next" href="#slide73"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide73">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre>
<span class="c1"># Afficher les informations sur les entités</span>
<span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">doc</span><span class="o">.</span><span class="n">ents</span><span class="p">:</span>
    <span class="n">entity_text</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">text</span>
    <span class="n">start_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">start_char</span>
    <span class="n">end_char</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">end_char</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">entity</span><span class="o">.</span><span class="n">label_</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Entité: </span><span class="si">{}</span><span class="s2">, Début: </span><span class="si">{}</span><span class="s2">, Fin: </span><span class="si">{}</span><span class="s2">, Catégorie: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">entity_text</span><span class="p">,
                 </span> <span class="n">start_char</span><span class="p">,</span> <span class="n">end_char</span><span class="p">,</span> <span class="n">label</span><span class="p">))</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">73
                    <a class="prev" href="#slide72"></a>
                    <a class="next" href="#slide74"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide74">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
                <p class="codeexample">
                    <code>
                <pre>
Entité: Paris, Début: 0, Fin: 5, Catégorie: GPE
Entité: France, Début: 24, Fin: 30, Catégorie: GPE
Entité: 2015, Début: 35, Fin: 39, Catégorie: DATE
Entité: 2,206,488, Début: 72, Fin: 81, Catégorie: CARDINAL
                         </pre>
                </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">74
                    <a class="prev" href="#slide73"></a>
                    <a class="next" href="#slide75"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide75">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">serve</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France.&quot;</span> + <span class="s2">&quot;In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">75
                    <a class="prev" href="#slide74"></a>
                    <a class="next" href="#slide76"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide76">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="kn">from</span> <span class="nn">spacy</span> <span class="kn">import</span> <span class="n">displacy</span>

<span class="k">def</span> <span class="nf">visualize_entities</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># Charger le modèle spaCy</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">)</span>
    <span class="c1"># Analyser le texte</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="c1"># Visualiser les entités nommées avec displaCy</span>
    <span class="n">displacy</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">doc</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;ent&quot;</span><span class="p">, </span><span class="n">jupyter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Texte à analyser et visualiser</span>
<span class="n">text_to_analyze</span> <span class="o">=</span> <span class="s2">&quot;Paris is the capital of France. In 2015, its population was recorded as 2,206,488&quot;</span>

<span class="c1"># Appeler la fonction pour analyser et visualiser les entités</span>
<span class="n">visualize_entities</span><span class="p">(</span><span class="n">text_to_analyze</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">76
                    <a class="prev" href="#slide75"></a>
                    <a class="next" href="#slide77"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide77">
            <div class="header">
                <h1>4.4. Reconnaissance d'entités nommées (NER)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">spaCy: Reconnaissance d'entités nommées</h3>
                <figure style="margin-bottom: 6rem">
                    <div class="entities" style="line-height: 2.5; direction: ltr">
                        <mark class="entity"
                            style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            Paris
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                        </mark> is the capital of
                        <mark class="entity"
                            style="background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            France
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">GPE</span>
                        </mark> . In
                        <mark class="entity"
                            style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            2015
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">DATE</span>
                        </mark> , its population was recorded as
                        <mark class="entity"
                            style="background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;">
                            2,206,488
                            <span
                                style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem">CARDINAL</span>
                        </mark>
                    </div>
                </figure>
                <table>
                    <tr>
                        <th>Balise</th>
                        <th>Signification</th>
                    </tr>
                    <tr>
                        <td>GPE</td>
                        <td>Pays, villes, états.</td>
                    </tr>
                    <tr>
                        <td>DATE</td>
                        <td>Dates ou périodes absolues ou relatives</td>
                    </tr>
                    <tr>
                        <td>CARDINAL</td>
                        <td>Les chiffres qui ne correspondent à aucun autre type.</td>
                    </tr>
                </table>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">77
                    <a class="prev" href="#slide76"></a>
                    <a class="next" href="#slide78"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide78">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <p> Le lexique <b>VADER</b> (Valence Aware Dictionary and sEntiment Reasoner) est spécifiquement conçu
                    pour
                    analyser les sentiments dans du texte en attribuant des scores de positivité, négativité et
                    neutralité
                    aux mots ainsi qu'aux expressions.</p>
                <h4>Installation</h4>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;vader_lexicon&#39;</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">78
                    <a class="prev" href="#slide77"></a>
                    <a class="next" href="#slide79"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide79">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">VADER</h3>
                <p>VADER est une bibliothèque d'analyse de sentiment conçue pour évaluer le sentiment d'un morceau de
                    texte,
                    généralement une phrase ou un paragraphe.</p>
                <ul>
                    <li><b>Dictionnaire et Scores :</b> VADER utilise un dictionnaire pré-annoté avec des scores de
                        positivité, négativité et neutralité pour des milliers de mots et expressions. Chaque mot est
                        associé à un score qui indique dans quelle mesure il est perçu comme positif ou négatif.</li>
                    <li><b>Polarité des Mots :</b> Pour chaque mot dans le texte, VADER examine son score dans le
                        dictionnaire. Certains mots ont des scores forts, indiquant une polarité positive ou négative,
                        tandis que d'autres ont des scores plus neutres.</li>
                    <li><b>Modificateurs et Emphase :</b> VADER prend en compte les modificateurs, tels que les
                        adverbes,
                        qui peuvent influencer la polarité d'un mot. Il reconnaît également l'emphase en attribuant des
                        poids différents aux mots en majuscules.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">79
                    <a class="prev" href="#slide78"></a>
                    <a class="next" href="#slide80"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide80">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">VADER</h3>
                <ul>
                    <li><b>Calcul du Score Composé :</b> VADER agrège les scores des mots en utilisant une formule qui
                        prend
                        en compte la distribution des polarités dans le texte. Le score composé résultant est une mesure
                        globale du sentiment de la phrase.</li>
                    <li><b>Résultats :</b> Le résultat final de l'analyse est un ensemble de scores qui indiquent la
                        positivité, la négativité, la neutralité et un score composé global. Ces scores sont normalisés
                        dans
                        une échelle de -1 à 1, où -1 représente un sentiment extrêmement négatif, 1 représente un
                        sentiment
                        extrêmement positif, et 0 représente la neutralité.</li>
                </ul>
                <p>VADER est souvent utilisé pour l'analyse de sentiment rapide et basée sur des règles. Bien qu'il soit
                    efficace dans de nombreux cas, il peut ne pas être aussi précis que des méthodes plus complexes
                    basées
                    sur l'apprentissage automatique, notamment dans des contextes où l'analyse nécessite une
                    compréhension
                    plus profonde du langage et de la syntaxe.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">80
                    <a class="prev" href="#slide79"></a>
                    <a class="next" href="#slide81"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide81">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">VADER: Usage</h3>
                <p class="codeexample">
                <div class="highlight">
                    <pre><span></span><span class="kn">from</span> <span class="nn">nltk.sentiment.vader</span> <span class="kn">import</span> <span class="n">SentimentIntensityAnalyzer</span>
<span class="n">sia</span> <span class="o">=</span> <span class="n">SentimentIntensityAnalyzer</span><span class="p">()</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is not very good&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>

<span class="n">sentiment</span> <span class="o">=</span> <span class="n">sia</span><span class="o">.</span><span class="n">polarity_scores</span><span class="p">(</span><span class="s2">&quot;this movie is bad&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sentiment</span><span class="p">)</span>
</pre>
                </div>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">81
                    <a class="prev" href="#slide80"></a>
                    <a class="next" href="#slide82"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide82">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <p>Les scores renvoyés par VADER représentent différentes mesures du sentiment dans un texte. Une
                    explication de chaque score :</p>
                <ul>
                    <li><b>Positivité (Positive Score)</b> : Ce score mesure la positivité relative du texte. Il indique
                        dans quelle mesure le texte contient des éléments positifs. Plus le score est élevé, plus le
                        texte
                        est perçu comme positif.</li>
                    <li><b>Négativité (Negative Score)</b> : - Ce score mesure la négativité relative du texte. Il
                        indique
                        dans quelle mesure le texte contient des éléments négatifs. Plus le score est élevé, plus le
                        texte
                        est perçu comme négatif.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">82
                    <a class="prev" href="#slide81"></a>
                    <a class="next" href="#slide83"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide83">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <ul>
                    <li><b>Neutralité (Neutral Score)</b> : Ce score mesure la neutralité relative du texte. Il indique
                        dans
                        quelle mesure le texte est neutre, c'est-à-dire dépourvu d'éléments fortement positifs ou
                        négatifs.
                        Plus le score est élevé, plus le texte est perçu comme neutre.</li>
                    <li><b>Score Composé (Compound Score)</b> : Le score composé est une mesure agrégée du sentiment qui
                        prend en compte à la fois la positivité et la négativité du texte. Il combine les scores
                        positif,
                        négatif et neutre en une seule valeur. Le score composé est souvent utilisé pour évaluer le
                        sentiment global du texte. Un score composé élevé indique un sentiment fort, qu'il soit positif
                        ou
                        négatif, tandis qu'un score proche de zéro indique un texte neutre.</li>
                </ul>
                <p>Les scores sont normalisés dans une échelle de -1 à 1, où -1 représente un sentiment extrêmement
                    négatif,
                    1 représente un sentiment extrêmement positif, et 0 représente la neutralité. Les scores peuvent
                    être
                    interprétés individuellement ou conjointement pour obtenir une compréhension complète du sentiment
                    dans
                    le texte analysé.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">83
                    <a class="prev" href="#slide82"></a>
                    <a class="next" href="#slide84"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide84">
            <div class="header">
                <h1>4.5. Analyse des sentiments (Sentiment Analysis)</h1>
            </div>
            <div class="content">
                <p>Affichage</p>
                <p class="codeexample">
                    <code>
			{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}</br>
                        {'neg': 0.344, 'neu': 0.656, 'pos': 0.0, 'compound': -0.3865}</br>
                        {'neg': 0.538, 'neu': 0.462, 'pos': 0.0, 'compound': -0.5423}</br>
                         </code>
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">84
                    <a class="prev" href="#slide83"></a>
                    <a class="next" href="#slide85"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide85">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique</h3>
                <p>La traduction automatique est le processus d'utilisation de logiciels pour traduire un texte ou un
                    discours d'une langue à une autre. Il existe plusieurs approches pour aborder ce problème complexe :
                </p>
                <ul>
                    <li><b>Approche manuelle (règles)</b> : Cette approche repose sur des règles linguistiques définies
                        manuellement par des linguistes ou des experts en langues. Les règles peuvent inclure des
                        correspondances mot à mot, des règles de grammaire, et d'autres connaissances linguistiques
                        spécifiques.</li>
                    <li><b>Approche statistique</b> : Dans cette approche, les modèles statistiques sont utilisés pour
                        apprendre les relations entre les phrases dans différentes langues à partir de grands ensembles
                        de
                        données parallèles. Ces modèles peuvent être basés sur des probabilités conditionnelles et des
                        méthodes statistiques avancées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">85
                    <a class="prev" href="#slide84"></a>
                    <a class="next" href="#slide86"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide86">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique</h3>
                <ul>
                    <li><b>Approche hybride (règles et statistique)</b> : L'approche hybride combine des éléments des
                        approches manuelles et statistiques. Elle peut utiliser des règles pour des aspects spécifiques
                        de
                        la traduction tout en tirant parti de l'apprentissage statistique pour d'autres parties du
                        processus.</li>
                    <li><b>Apprentissage machine</b> : L'apprentissage machine, en particulier les modèles neuronaux, a
                        considérablement amélioré les performances de la traduction automatique. Les réseaux de
                        neurones,
                        tels que les réseaux de neurones récurrents (RNN) et les transformers, ont montré des résultats
                        impressionnants en capturant des structures linguistiques complexes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">86
                    <a class="prev" href="#slide85"></a>
                    <a class="next" href="#slide87"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide87">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche manuelle (règles)</h3>
                <p>L'approche manuelle dans la traduction automatique repose sur l'utilisation de règles linguistiques
                    préalablement définies par des linguistes ou des experts en langues. Ces règles spécifient comment
                    traduire des éléments spécifiques d'une langue source vers une langue cible.</p>
                <h4>Exemple : Traduction de phrases simples (anglais vers français)</h4>
                <p>Supposons que nous avons une règle manuelle qui dit que le mot "hello" en anglais doit être traduit
                    par
                    "bonjour" en français. De plus, nous avons une règle indiquant que la phrase "I love" doit être
                    traduite
                    par "j'aime". Ces règles sont simples et sont utilisées mot à mot.</p>
                <ul>
                    <li><b>Application des règles manuelles</b> : "Hello" est traduit en "bonjour." "I love" est traduit
                        en
                        "j'aime." "programming" n'a pas de règle spécifique, donc nous le laissons tel quel.</li>
                    <li><b>Traduction résultante</b> : La phrase anglaise "Hello, I love programming" serait traduite en
                        français par "Bonjour, j'aime programming."</li>
                </ul>
                <p> Dans des situations réelles, les règles peuvent devenir extrêmement complexes, impliquant des
                    contextes
                    grammaticaux, des variations lexicales, des idiomes, etc.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">87
                    <a class="prev" href="#slide86"></a>
                    <a class="next" href="#slide88"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide88">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
                <p>L'approche statistique de la traduction automatique repose sur l'utilisation de modèles statistiques
                    pour
                    apprendre les relations entre les phrases dans différentes langues à partir de grands ensembles de
                    données parallèles.</p>
                <p>Contrairement à l'approche manuelle basée sur des règles linguistiques définies par
                    des experts, l'approche statistique utilise des statistiques et des probabilités pour estimer la
                    probabilité d'une traduction donnée.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">88
                    <a class="prev" href="#slide87"></a>
                    <a class="next" href="#slide89"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide89">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
                <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
                <ul>
                    <li><b>Ensembles de données parallèles</b> : Pour entraîner un modèle statistique, des ensembles de
                        données parallèles sont nécessaires. Ces ensembles contiennent des phrases dans la langue source
                        et
                        leurs traductions correspondantes dans la langue cible.</li>
                    <li><b>Alignement de phrases</b> : Les phrases équivalentes dans les deux langues sont alignées dans
                        l'ensemble de données parallèles. Cela crée des paires de phrases qui serviront de données
                        d'entraînement pour le modèle.</li>
                    <li><b>Extraction de caractéristiques</b> : Des caractéristiques pertinentes sont extraites à partir
                        des
                        paires de phrases alignées. Ces caractéristiques peuvent inclure des n-grammes (groupes de mots
                        consécutifs), des séquences de mots, des informations sur la syntaxe, etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">89
                    <a class="prev" href="#slide88"></a>
                    <a class="next" href="#slide90"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide90">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
                <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
                <ul>
                    <li><b>Entraînement du modèle</b> : Un modèle statistique, souvent basé sur des méthodes
                        probabilistes
                        comme les modèles de langue conditionnels, est entraîné sur ces caractéristiques extraites. Le
                        modèle apprend les probabilités conditionnelles des traductions données les phrases sources.
                    </li>
                    <li><b>Estimation des probabilités</b> : Lors de la traduction d'une nouvelle phrase, le modèle
                        estime
                        les probabilités des différentes traductions possibles en fonction des caractéristiques de cette
                        phrase.</li>
                    <li><b>Sélection de la meilleure traduction</b> : La traduction avec la probabilité la plus élevée
                        est
                        sélectionnée comme la traduction finale de la phrase source.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">90
                    <a class="prev" href="#slide89"></a>
                    <a class="next" href="#slide91"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide91">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche statistique</h3>
                <h4>Les étapes clés de l'approche statistique de la traduction automatique :</h4>
                <ul>
                    <li><b>Évaluation et ajustement</b> : Le modèle est évalué sur des ensembles de données de test pour
                        mesurer sa performance. Si nécessaire, le modèle peut être ajusté pour améliorer ses
                        performances.
                    </li>
                </ul>
                <p>Un exemple concret de cette approche pourrait être l'utilisation de modèles de langues conditionnels
                    (par
                    exemple, les modèles de Markov conditionnels) pour estimer la probabilité d'une traduction donnée
                    une
                    phrase source.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">91
                    <a class="prev" href="#slide90"></a>
                    <a class="next" href="#slide92"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide92">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche Hybride (Règles et Statistique)</h3>
                <p>L'approche hybride dans la traduction automatique combine des éléments des approches manuelles
                    (basées
                    sur des règles) et des approches statistiques. L'idée est d'utiliser des règles linguistiques pour
                    certaines parties de la traduction tout en tirant parti de modèles statistiques pour d'autres
                    parties du
                    processus. Cette combinaison vise à exploiter les avantages des deux approches pour obtenir des
                    traductions de meilleure qualité.</p>
                <h4>Les étapes clés :</h4>
                <li><b>Règles linguistiques</b> : Des règles linguistiques sont définies manuellement pour certaines
                    constructions grammaticales, expressions idiomatiques, ou autres aspects linguistiques spécifiques.
                    Ces
                    règles peuvent être élaborées par des experts linguistes.</li>
                <li><b>Ensembles de données parallèles</b> : Comme dans l'approche statistique, des ensembles de données
                    parallèles contenant des phrases dans la langue source et leurs traductions dans la langue cible
                    sont
                    nécessaires.</li>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">92
                    <a class="prev" href="#slide91"></a>
                    <a class="next" href="#slide93"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide93">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Approche Hybride (Règles et Statistique)</h3>
                <h4>Les étapes clés :</h4>
                <li><b>Apprentissage statistique</b> : Un modèle statistique est entraîné sur ces ensembles de données
                    parallèles, en utilisant des méthodes statistiques pour apprendre les relations entre les phrases
                    dans
                    les deux langues.</li>
                <li><b>Application des règles</b> : Lors du processus de traduction, les règles linguistiques sont
                    appliquées en priorité. Si une partie du texte correspond à une règle prédéfinie, la traduction
                    basée
                    sur la règle est utilisée.</li>
                <li><b>Utilisation du modèle statistique</b> : Pour les parties du texte qui ne correspondent pas à des
                    règles prédéfinies, le modèle statistique est utilisé pour générer la traduction en se basant sur
                    les
                    relations apprises à partir des ensembles de données parallèles.</li>
                <li><b>Intégration des traductions partielles</b> : Les traductions générées par les règles et celles
                    générées par le modèle statistique sont intégrées pour former la traduction finale du texte.</li>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">93
                    <a class="prev" href="#slide92"></a>
                    <a class="next" href="#slide94"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide94">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
                <p>L'approche machine dans la traduction automatique fait référence à l'utilisation de techniques
                    d'apprentissage machine, en particulier d'algorithmes d'apprentissage automatique, pour automatiser
                    le
                    processus de traduction entre langues. Cette approche a considérablement évolué avec le
                    développement de
                    modèles plus avancés basés sur l'apprentissage profond, notamment les modèles de séquence à
                    séquence.
                </p>
                <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
                <ul>
                    <li><b>Ensembles de données parallèles</b> : Des ensembles de données parallèles sont nécessaires,
                        contenant des phrases dans la langue source et leurs traductions correspondantes dans la langue
                        cible. Ces ensembles serviront de données d'entraînement pour le modèle.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">94
                    <a class="prev" href="#slide93"></a>
                    <a class="next" href="#slide95"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide95">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
                <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
                <ul>
                    <li><b>Modèle de séquence à séquence (Seq2Seq)</b> : Le modèle central dans cette approche est
                        généralement un modèle de séquence à séquence (Seq2Seq). Ce type de modèle utilise un réseau
                        neuronal récurrent (RNN) ou une architecture de transformer pour traiter des séquences de
                        données.
                    </li>
                    <li><b>Encodage de la phrase source</b> : La phrase source est encodée en une représentation
                        vectorielle
                        par le réseau neuronal. Cela capture les informations sémantiques et syntaxiques de la phrase
                        source.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">95
                    <a class="prev" href="#slide94"></a>
                    <a class="next" href="#slide96"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide96">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
                <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
                <ul>
                    <li><b>Décodage vers la langue cible</b> : Le modèle décode ensuite la représentation encodée pour
                        générer la séquence de mots dans la langue cible. C'est le processus de génération de la
                        traduction.
                    </li>
                    <li><b>Entraînement supervisé</b> : Le modèle est entraîné sur les ensembles de données parallèles
                        en
                        utilisant une approche supervisée. Il apprend à minimiser la différence entre la séquence
                        générée et
                        la traduction attendue dans la langue cible.</li>
                    <li><b>Optimisation</b> : Des techniques d'optimisation, telles que la descente de gradient
                        stochastique
                        (SGD) ou des optimiseurs plus avancés comme Adam, sont utilisées pour ajuster les poids du
                        modèle
                        pendant l'entraînement.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">96
                    <a class="prev" href="#slide95"></a>
                    <a class="next" href="#slide97"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide97">
            <div class="header">
                <h1>4.6. Traduction automatique (Machine Translation)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Traduction automatique : Apprentissage machine</h3>
                <h4>Les étapes clés de l'approche machine dans la traduction automatique :</h4>
                <ul>
                    <li><b>Évaluation et ajustement</b> : Le modèle est évalué sur des ensembles de données de test
                        indépendants pour mesurer sa performance. Des ajustements peuvent être effectués pour améliorer
                        les
                        résultats.</li>
                    <li><b>Incorporation de méthodes plus avancées</b> : Les modèles de traduction automatique basés sur
                        l'apprentissage machine peuvent être améliorés en incorporant des techniques plus avancées
                        telles
                        que l'attention, qui permettent au modèle de se concentrer sur des parties spécifiques de la
                        phrase
                        source lors de la génération de la traduction.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">97
                    <a class="prev" href="#slide96"></a>
                    <a class="next" href="#slide98"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide98">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Transformer</h3>
                <p>Le Transformer est une architecture de réseau de neurones qui a révolutionné le domaine du traitement
                    du
                    langage naturel depuis son introduction par Vaswani et al. en 2017. Deux des modèles les plus
                    célèbres
                    basés sur l'architecture Transformer sont : </p>
                <ul>
                    <li> BERT (Bidirectional Encoder Representations from Transformers)</li>
                    <li> GPT (Generative Pre-trained Transformer)</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">98
                    <a class="prev" href="#slide97"></a>
                    <a class="next" href="#slide99"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide99">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT (Bidirectional Encoder Representations from Transformers) :</h3>
                <p> BERT, développé par Google, est un modèle pré-entraîné qui a été formé sur de vastes corpus de
                    texte. Ce
                    qui distingue BERT, c'est son approche bidirectionnelle pour la représentation des mots.
                    Contrairement à
                    des modèles précédents qui utilisaient une compréhension unidirectionnelle du contexte, BERT prend
                    en
                    compte le contexte à la fois avant et après un mot dans une phrase, améliorant ainsi la
                    compréhension du
                    sens.</p>
                <ul>
                    <li><b>Utilisation</b> : BERT a été pré-entraîné sur des tâches telles que la prédiction de mots
                        masqués
                        dans une phrase (Masked Language Model) et la prédiction de la relation entre deux phrases (Next
                        Sentence Prediction). Ces pré-entraînements permettent à BERT d'acquérir une compréhension
                        profonde
                        du langage.</li>
                    <li><b>Applications</b> : BERT est souvent utilisé comme base pour des tâches spécifiques telles que
                        la
                        classification de texte, l'extraction d'entités nommées, la compréhension de texte, etc. Des
                        versions pré-entraînées de BERT sont disponibles, et les modèles peuvent être fine-tunés pour
                        des
                        tâches spécifiques.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">99
                    <a class="prev" href="#slide98"></a>
                    <a class="next" href="#slide100"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide100">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT : composants</h3>
                <p> Un modèle BERT (Bidirectional Encoder Representations from Transformers) est composé de plusieurs
                    éléments clés, reflétant l'architecture Transformer. Les composants principaux d'un modèle BERT :
                </p>
                <ul>
                    <li><b>Embedding token</b> : Cette couche transforme les tokens (mots ou sous-mots) d'une séquence
                        en
                        vecteurs d'embedding. Chaque token est représenté par un vecteur qui capture son sens
                        sémantique.
                        Ces embeddings peuvent également inclure des informations de position pour indiquer la position
                        de
                        chaque token dans la séquence.</li>
                    <li><b>Embedding de segment</b> : BERT prend en compte le contexte global d'une séquence, même
                        lorsque
                        celle-ci contient plusieurs phrases. Pour ce faire, une couche d'embedding de segment est
                        utilisée.
                        Elle attribue un segment spécifique à chaque token pour indiquer à quel segment il appartient.
                        Cela
                        permet au modèle de distinguer différentes parties de la séquence.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">100
                    <a class="prev" href="#slide99"></a>
                    <a class="next" href="#slide101"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide101">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT : composants</h3>
                <ul>
                    <li><b>Encoder BERT (Transformateur)</b> : L'élément central de BERT est l'encodeur, qui suit
                        l'architecture du Transformer. L'encodeur est composé de plusieurs couches d'attention
                        multi-têtes.
                        Chaque couche prend en compte les relations entre les tokens et utilise l'attention pour
                        attribuer
                        des poids aux différents tokens en fonction de leur importance contextuelle.</li>
                    <li><b>Attention multi-tête</b> : Chaque couche d'attention contient plusieurs "têtes" d'attention.
                        Chaque tête capture des aspects différents des relations entre les tokens. L'utilisation de
                        plusieurs têtes permet au modèle de capturer des relations complexes dans le contexte.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">101
                    <a class="prev" href="#slide100"></a>
                    <a class="next" href="#slide102"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide102">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT : composants</h3>
                <ul>
                    <li><b>Couche de pooling</b> : BERT utilise souvent une couche de pooling pour agréger les
                        représentations de tous les tokens en une seule représentation. Cette représentation agrégée
                        peut
                        être utilisée pour des tâches spécifiques, comme la classification de texte.</li>
                    <li><b>Couche de classification</b> : Pour les tâches de classification, une couche de
                        classification
                        est ajoutée au-dessus du modèle BERT. Cette couche peut consister en une ou plusieurs couches
                        denses
                        qui projettent la représentation agrégée sur l'espace de sortie de la tâche.</li>
                    <li><b>Fine-tuning et couches spécifiques à la tâche</b> : Lorsque BERT est fine-tuné pour une tâche
                        spécifique, des couches supplémentaires peuvent être ajoutées pour adapter le modèle à la tâche
                        en
                        question. Cela peut inclure des couches de classification, des couches de régression, ou
                        d'autres
                        couches spécifiques à la sortie souhaitée.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">102
                    <a class="prev" href="#slide101"></a>
                    <a class="next" href="#slide103"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide103">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT : fonctionnement</h3>
                <ul>
                    <li><b>Prétraitement des données</b> : Avant d'entraîner ou d'utiliser BERT, les données doivent
                        être
                        prétraitées. Cela inclut la tokenisation, où les phrases sont divisées en tokens (mots ou
                        sous-mots), et l'ajout de tokens spéciaux, tels que [CLS] (pour le début de la phrase) et [SEP]
                        (pour la séparation entre les phrases). De plus, des embeddings de segment peuvent être ajoutés
                        pour
                        indiquer à quel segment appartient chaque token, ce qui est utile pour les tâches de
                        compréhension
                        de texte.</li>
                    <li><b>Architecture du modèle BERT</b> : BERT utilise une architecture Transformer avec un encodeur
                        bidirectionnel. La partie "bidirectionnelle" signifie que le modèle prend en compte le contexte
                        à la
                        fois avant et après chaque mot dans une phrase, ce qui améliore la compréhension du sens.</li>
                    <li><b>Pré-entraînement</b> : BERT est pré-entraîné sur de grandes quantités de données textuelles
                        non
                        annotées. Il est formé à prédire les mots masqués dans une séquence (Masked Language Model, MLM)
                        et
                        à prédire la relation entre deux phrases (Next Sentence Prediction, NSP). Le modèle apprend
                        ainsi
                        une représentation profonde et contextuelle du langage.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">103
                    <a class="prev" href="#slide102"></a>
                    <a class="next" href="#slide104"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide104">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT : fonctionnement</h3>
                <ul>
                    <li><b>Fine-tuning</b> : Après le pré-entraînement, BERT peut être fine-tuné sur des tâches
                        spécifiques.
                        Par exemple, pour la classification de texte, une couche de classification peut être ajoutée
                        au-dessus de la représentation BERT, et le modèle peut être entraîné sur des données annotées
                        pour
                        la tâche spécifique.</li>
                    <li><b>Utilisation du modèle fine-tuné</b> : Une fois fine-tuné, le modèle BERT peut être utilisé
                        pour
                        effectuer des tâches spécifiques, telles que la classification de texte, la reconnaissance
                        d'entités
                        nommées, ou d'autres tâches de traitement du langage naturel.</li>
                    <li><b>Gestion de la longueur des séquences</b> : BERT a une limitation sur la longueur maximale des
                        séquences qu'il peut traiter. Pour les textes plus longs, des techniques comme le fractionnement
                        du
                        texte ou le choix d'une sous-séquence significative peuvent être utilisées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">104
                    <a class="prev" href="#slide103"></a>
                    <a class="next" href="#slide105"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide105">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer)</h3>
                <p>GPT, développé par OpenAI, est un modèle basé sur l'architecture Transformer, mais avec une approche
                    de
                    génération de texte. GPT utilise un modèle de langage pré-entraîné qui a appris à prédire le mot
                    suivant
                    dans une séquence de mots. Il est capable de générer du texte cohérent et contextuellement
                    approprié.
                </p>
                <ul>
                    <li><b>Utilisation</b> : GPT est pré-entraîné sur un large corpus de texte, et la pré-entraînement
                        vise
                        à enseigner au modèle la structure et les motifs du langage. Il peut ensuite être fine-tuné sur
                        des
                        tâches spécifiques selon les besoins.</li>
                    <li><b>Applications</b> : GPT est souvent utilisé pour des tâches de génération de texte, comme la
                        rédaction automatique de contenu, la complétion de texte, et d'autres applications où la
                        création de
                        texte naturel est requise.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">105
                    <a class="prev" href="#slide104"></a>
                    <a class="next" href="#slide106"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide106">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : composants</h3>
                <p> Le modèle GPT (Generative Pre-trained Transformer) est composé de plusieurs éléments clés qui
                    suivent
                    l'architecture Transformer. Les composants principaux de GPT : </p>
                <ul>
                    <li><b>Embedding token</b> : Tout comme dans BERT, GPT utilise une couche d'embedding pour convertir
                        les
                        tokens (mots ou sous-mots) en vecteurs d'embedding. Chaque token est représenté par un vecteur
                        qui
                        capture son sens sémantique.</li>
                    <li><b>Positional encoding</b> : GPT prend en compte la position des mots dans une séquence. Pour ce
                        faire, une couche de Positional Encoding est ajoutée aux embeddings de token pour introduire des
                        informations de position.</li>
                    <li><b>Encodeur-décodeur transformer</b> : Contrairement à BERT, GPT utilise une architecture
                        encodeur-décodeur basée sur le Transformer. L'encodeur capture les relations entre les tokens
                        dans
                        une séquence, tandis que le décodeur est utilisé pour générer la séquence de sortie.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">106
                    <a class="prev" href="#slide105"></a>
                    <a class="next" href="#slide107"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide107">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : composants</h3>
                <ul>
                    <li><b>Attention multi-tête</b> : Comme dans BERT, GPT utilise une attention multi-tête pour
                        capturer
                        des relations complexes entre les mots. Chaque tête d'attention se concentre sur différents
                        aspects
                        des relations entre les tokens.</li>
                    <li><b>Couche de pooling (ou moyenne)</b> : GPT utilise souvent une couche de pooling pour agréger
                        les
                        représentations de tous les tokens en une seule représentation. Cette représentation agrégée est
                        ensuite utilisée comme entrée pour le décodeur.</li>
                    <li><b>Décodeur GPT</b> : Le décodeur est la partie du modèle qui génère la séquence de sortie. Il
                        prend
                        la représentation agrégée en entrée et génère séquentiellement les tokens de sortie un par un.
                    </li>
                    <li><b>Fine-tuning</b> : Après le pré-entraînement, GPT peut être fine-tuné pour des tâches
                        spécifiques
                        en ajoutant des couches spécifiques à la tâche. Ces couches supplémentaires sont souvent des
                        couches
                        de classification ou de régression, selon la nature de la tâche.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">107
                    <a class="prev" href="#slide106"></a>
                    <a class="next" href="#slide108"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide108">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : fonctionnement</h3>
                <ul>
                    <li><b>Pré-entraînement</b> : GPT est pré-entraîné sur un vaste corpus de texte non annoté.
                        L'objectif
                        du pré-entraînement est d'apprendre une représentation riche et contextuelle du langage. Pendant
                        cette phase, le modèle apprend à prédire le mot suivant dans une séquence de mots. Il utilise
                        une
                        architecture encodeur-décodeur basée sur le Transformer.</li>
                    <li><b>Embedding et positional encoding</b> : Chaque mot dans une séquence est représenté par un
                        vecteur
                        d'embedding. GPT prend également en compte la position de chaque mot dans la séquence en
                        utilisant
                        une couche de Positional Encoding.</li>
                    <li><b>Encodeur-décodeur transformer</b> : GPT utilise une architecture de Transformer où l'encodeur
                        capture les relations entre les mots dans une séquence, et le décodeur est utilisé pour générer
                        la
                        séquence de sortie. Cependant, GPT est principalement utilisé de manière autoregressive, ce qui
                        signifie qu'il génère séquentiellement un mot à la fois en utilisant les mots précédemment
                        générés
                        comme contexte.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">108
                    <a class="prev" href="#slide107"></a>
                    <a class="next" href="#slide109"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide109">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">GPT (Generative Pre-trained Transformer) : fonctionnement</h3>
                <ul>
                    <li><b>Attention multi-tête</b> : GPT utilise l'attention multi-tête pour capturer des relations
                        complexes entre les mots. Chaque tête d'attention se concentre sur différents aspects du
                        contexte,
                        permettant au modèle de saisir des dépendances à long terme et des relations subtiles.</li>
                    <li><b>Génération de texte</b> : Lors de la génération de texte, le modèle prend une séquence
                        initiale
                        en entrée et génère séquentiellement les mots suivants. À chaque étape, le modèle utilise les
                        mots
                        générés précédemment comme contexte pour prédire le mot suivant. Ce processus se répète jusqu'à
                        ce
                        qu'une séquence complète soit générée.</li>
                    <li><b>Fine-tuning pour des tâches spécifiques</b> : Après le pré-entraînement, GPT peut être
                        fine-tuné
                        pour des tâches spécifiques en ajoutant des couches supplémentaires spécifiques à la tâche. Par
                        exemple, pour la classification de texte, des couches de classification peuvent être ajoutées
                        au-dessus du modèle pré-entraîné.</li>
                    <li><b>Gestion de la longueur des séquences</b> : GPT peut gérer des séquences de longueur variable,
                        mais il a une limite pratique sur la longueur maximale de la séquence qu'il peut générer.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">109
                    <a class="prev" href="#slide108"></a>
                    <a class="next" href="#slide110"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide110">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT vs. GPT</h3>
                <table>
                    <tr>
                        <th> Caractéristique </th>
                        <th> BERT </th>
                        <th> GPT </th>
                    </tr>
                    <tr>
                        <td> Objectif du Pré-entraînement </td>
                        <td> Prédiction bidirectionnelle des mots (MLM) et prédiction de relation entre deux phrases
                            (NSP)
                        </td>
                        <td> Génération de texte autoregressive </td>
                    </tr>
                    <tr>
                        <td> Architecture </td>
                        <td> Encodeur bidirectionnel </td>
                        <td> Encodeur-décodeur avec orientation autoregressive </td>
                    </tr>
                    <tr>
                        <td> Utilisation en Fine-Tuning </td>
                        <td> Classification de texte, extraction d'entités nommées, etc. </td>
                        <td> Génération de texte, complétion automatique, etc. </td>
                    </tr>
                </table>

            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">110
                    <a class="prev" href="#slide109"></a>
                    <a class="next" href="#slide111"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide111">
            <div class="header">
                <h1>4.7. Modèles de langage Transformer</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">BERT vs. GPT</h3>
                <table>
                    <tr>
                        <th> Caractéristique </th>
                        <th> BERT </th>
                        <th> GPT </th>
                    </tr>
                    <tr>
                        <td> Approche du Contexte </td>
                        <td> Bidirectionnelle, prend en compte le contexte avant et après chaque mot </td>
                        <td> Autoregressive, génère du texte séquentiellement en utilisant le contexte précédent </td>
                    </tr>
                    <tr>
                        <td> Applications Pratiques </td>
                        <td> Classification, extraction d'entités, détection de paraphrases </td>
                        <td> Génération de texte, complétion automatique, conversation naturelle </td>
                    </tr>
                    <tr>
                        <td> Taille des Modèles </td>
                        <td> Généralement plus petits </td>
                        <td> Souvent plus grands, surtout les versions plus récentes comme GPT-3 </td>
                    </tr>
                </table>

            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">111
                    <a class="prev" href="#slide110"></a>
                    <a class="next" href="#slide112"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide112">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Système de recommandation</h3>
                <p>Le système de recommandation est un domaine essentiel de l'informatique qui vise à réduire la
                    surcharge
                    d'informations en fournissant des suggestions filtrées et pertinentes aux utilisateurs.</p>
                <p><b>Réduction de la surcharge d'informations</b> : Les systèmes de recommandation visent à aider les
                    utilisateurs à naviguer à travers une grande quantité d'informations en fournissant des
                    recommandations
                    ciblées et adaptées à leurs préférences.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">112
                    <a class="prev" href="#slide111"></a>
                    <a class="next" href="#slide113"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide113">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Système de recommandation</h3>
                <h4>Fonctionnement</h4>
                <ul>
                    <li><b>Prédire la préférence de l'utilisateur</b> : Les systèmes de recommandation utilisent des
                        algorithmes pour analyser le comportement passé de l'utilisateur, ses préférences, et d'autres
                        données pertinentes afin de prédire les éléments qui pourraient l'intéresser à l'avenir.</li>
                    <li><b>Gestion de la surcharge d'informations</b> : En filtrant et en triant les informations, ces
                        systèmes aident à éviter la surcharge cognitive en présentant à l'utilisateur uniquement ce qui
                        est
                        le plus susceptible de l'intéresser.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">113
                    <a class="prev" href="#slide112"></a>
                    <a class="next" href="#slide114"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide114">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Système de recommandation</h3>
                <h4>Types de Recommandations</h4>
                <ul>
                    <li><b>Recommandations personnalisées</b> : Basées sur l'historique et les préférences individuelles
                        de
                        l'utilisateur.</li>
                    <li><b>Recommandations non personnalisées</b> : Générales et applicables à un large groupe
                        d'utilisateurs.</li>
                </ul>
                <h4>Algorithmes couramment utilisés</h4>
                <ul>
                    <li><b>Filtrage collaboratif</b> : Basé sur les comportements et les préférences d'utilisateurs
                        similaires.</li>
                    <li><b>Filtrage basé sur le contenu</b> : Utilise des caractéristiques du produit ou de l'élément
                        lui-même pour faire des recommandations.</li>
                    <li><b>Apprentissage profond</b> : Des techniques telles que les réseaux de neurones profonds
                        peuvent
                        être utilisées pour modéliser des modèles complexes de préférences.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">114
                    <a class="prev" href="#slide113"></a>
                    <a class="next" href="#slide115"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide115">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Système de recommandation</h3>
                <h4>Applications</h4>
                <ul>
                    <li><b>Générateurs de playlists</b> : Pour les services de vidéo et de musique, offrant des
                        recommandations de chansons basées sur le goût musical de l'utilisateur.</li>
                    <li><b>Recommandations de produits</b> : Dans les plateformes de commerce électronique, suggérant
                        des
                        articles basés sur les achats antérieurs ou les préférences.</li>
                    <li><b>Recommandations de livres</b> : Sur les plateformes de vente de livres en ligne, proposant
                        des
                        ouvrages similaires à ceux déjà appréciés.</li>
                    <li><b>Recommandations de contenu sur les réseaux sociaux</b> : Proposant des publications, des amis
                        ou
                        des groupes en fonction de l'activité passée et des préférences de l'utilisateur.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">115
                    <a class="prev" href="#slide114"></a>
                    <a class="next" href="#slide116"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide116">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
                <h4>Hypothèse</h4>
                <p>Les individus suivent souvent les recommandations des autres utilisateurs. Cela suppose que les
                    préférences et les actions des utilisateurs peuvent être des indicateurs fiables pour recommander
                    des
                    articles ou des objets similaires à d'autres utilisateurs.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">116
                    <a class="prev" href="#slide115"></a>
                    <a class="next" href="#slide117"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide117">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
                <h4>Sources des données : </h4>
                <ul>
                    <li><b>Utilisateurs</b> : Les informations relatives aux utilisateurs, y compris leurs préférences,
                        comportements, et actions.</li>
                    <li><b>Articles ou objets</b> : Les données concernant les articles ou objets à recommander. Cela
                        peut
                        inclure des détails sur les caractéristiques des articles, leurs catégories, etc.</li>
                    <li><b>Transactions</b> : Les interactions et transactions entre les utilisateurs et les articles.
                        Cela
                        peut inclure des achats, des clics, des évaluations, etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">117
                    <a class="prev" href="#slide116"></a>
                    <a class="next" href="#slide118"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide118">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réalisation [Pazzani 2007, Ricci 2011]</h3>
                <h4>Collecte des Préférences des Utilisateurs : </h4>
                <ul>
                    <li><b>Préférences explicitement exprimées</b> : Les évaluations et les actions des utilisateurs qui
                        sont directement exprimées. Cela peut inclure les avis positifs et négatifs, les évaluations
                        numériques, etc.</li>
                    <li><b>Interprétation des actions des utilisateurs</b> : Observation et interprétation des actions
                        des
                        utilisateurs, en particulier dans le contexte de la navigation web. Cela pourrait inclure des
                        comportements tels que les pages visitées, le temps passé sur une page, les articles ajoutés au
                        panier, etc.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">118
                    <a class="prev" href="#slide117"></a>
                    <a class="next" href="#slide119"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide119">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Réalisation</h3>
                <h4>Méthodes de Collecte des Données : </h4>
                <ul>
                    <li><b>Surveillance des activités utilisateurs</b> : Utilisation de technologies de suivi pour
                        observer
                        et enregistrer les actions des utilisateurs, généralement dans le contexte d'une plateforme en
                        ligne.</li>
                    <li><b>Systèmes de retour d'information</b> : Encouragement des utilisateurs à fournir des retours
                        d'information explicites sous forme d'évaluations, de commentaires, etc.</li>
                    <li><b>Analyse des transactions</b> : Extraction d'informations à partir des transactions entre
                        utilisateurs et articles pour déduire les préférences et les comportements.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">119
                    <a class="prev" href="#slide118"></a>
                    <a class="next" href="#slide120"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide120">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Fonctions [Ricci 2011]</h3>
                <ul>
                    <li><b>Augmenter le nombre d'articles vendus</b> : Les systèmes de recommandation visent à stimuler
                        les
                        ventes en suggérant des articles pertinents aux utilisateurs, augmentant ainsi les opportunités
                        d'achat.</li>
                    <li><b>Vendre des articles plus variés</b> : En diversifiant les recommandations, les systèmes
                        cherchent
                        à élargir le choix des utilisateurs et à promouvoir la vente d'une gamme plus large d'articles.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">120
                    <a class="prev" href="#slide119"></a>
                    <a class="next" href="#slide121"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide121">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Fonctions [Ricci 2011]</h3>
                <ul>
                    <li><b>Augmenter la satisfaction des utilisateurs</b> : En fournissant des recommandations
                        pertinentes
                        et adaptées aux préférences individuelles, les systèmes visent à accroître la satisfaction des
                        utilisateurs.</li>
                    <li><b>Augmenter la fidélité des utilisateurs</b> : En offrant des expériences personnalisées et en
                        répondant aux besoins des utilisateurs, les systèmes cherchent à fidéliser les clients, les
                        incitant
                        à revenir pour davantage d'achats.</li>
                    <li><b>Mieux comprendre ce que veut l'utilisateur</b> : Les systèmes de recommandation sont conçus
                        pour
                        comprendre les préférences des utilisateurs au fil du temps, améliorant ainsi la précision des
                        recommandations et la compréhension des besoins individuels.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">121
                    <a class="prev" href="#slide120"></a>
                    <a class="next" href="#slide122"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide122">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
                <ul>
                    <li><b>Trouver de bons objets</b> : Fournir des recommandations pour des articles ou des objets qui
                        correspondent aux préférences individuelles de l'utilisateur.</li>
                    <li><b>Trouver tous les bons articles</b> : Identifier de manière exhaustive tous les articles
                        pertinents en fonction des préférences spécifiques de l'utilisateur</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">122
                    <a class="prev" href="#slide121"></a>
                    <a class="next" href="#slide123"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide123">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
                <h4>Contexte et Variations de Recommandations</h4>
                <ul>
                    <li><b>Annotation dans le contexte</b> : Intégrer des informations contextuelles dans les
                        recommandations pour les rendre plus pertinentes et adaptées à la situation actuelle.</li>
                    <li><b>Recommander une séquence</b> : Proposer une séquence d'articles ou de contenus, tels que des
                        livres ou des vidéos, sur un sujet donné pour une expérience d'apprentissage ou de
                        divertissement
                        cohérente.</li>
                    <li><b>Recommander une combinaison</b> : Offrir des suggestions de combinaisons d'articles, comme un
                        itinéraire de voyage complet, pour répondre à des besoins complexes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">123
                    <a class="prev" href="#slide122"></a>
                    <a class="next" href="#slide124"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide124">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
                <h4>Navigation et consultation</h4>
                <ul>
                    <li><b>Navigation (consultation)</b> : Faciliter la navigation et la consultation d'articles en
                        recommandant des éléments pertinents à mesure que l'utilisateur explore la plateforme.</li>
                    <li><b>Trouver un système de recommandation crédible</b> : Identifier des systèmes de recommandation
                        réputés et fiables pour garantir des suggestions de qualité.</li>
                    <li><b>Améliorer le profil</b> : Continuellement ajuster et améliorer le profil de l'utilisateur en
                        fonction de ses préférences changeantes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">124
                    <a class="prev" href="#slide123"></a>
                    <a class="next" href="#slide125"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide125">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Objectifs [Herlocker 2000, Ricci 2011]</h3>
                <h4>Interaction Sociale</h4>
                <ul>
                    <li><b>S'exprimer</b> : Permettre aux utilisateurs de s'exprimer en fournissant des retours et en
                        influençant les recommandations.</li>
                    <li><b>Aider les autres</b> : Offrir des mécanismes pour que les utilisateurs puissent recommander
                        des
                        articles à d'autres utilisateurs.</li>
                    <li><b>Influencer les autres</b> : Permettre aux utilisateurs d'influencer les préférences d'autres
                        utilisateurs en partageant leurs recommandations.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">125
                    <a class="prev" href="#slide124"></a>
                    <a class="next" href="#slide126"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide126">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
                <ul>
                    <li><b>Filtrage collaboratif</b>
                        <ul>
                            <li>Principe : basé sur les évaluations de plusieurs utilisateurs</li>
                            <li>Fonctionnement : Identifie des utilisateurs similaires à celui pour lequel la
                                recommandation
                                est générée et propose des articles appréciés par ces utilisateurs similaires.</li>
                        </ul>
                    </li>
                    <li><b>Filtrage basé sur le contenu</b>
                        <ul>
                            <li>Principe : basé sur les profils des utilisateurs</li>
                            <li>Fonctionnement : Recommande des articles similaires à ceux que l'utilisateur a aimés par
                                le
                                passé, en se basant sur les caractéristiques ou le contenu des articles.</li>
                        </ul>
                    </li>
                    <li><b>Démographiques</b>
                        <ul>
                            <li>Principe : le profil démographique de l'utilisateur, par exemple le lieu et la langue
                            </li>
                            <li>Fonctionnement : Propose des recommandations en fonction des caractéristiques
                                démographiques
                                de l'utilisateur.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">126
                    <a class="prev" href="#slide125"></a>
                    <a class="next" href="#slide127"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide127">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Approches [Pazzani 2007, Ricci 2011]</h3>
                <ul>
                    <li><b>Basé sur la connaissance</b>
                        <ul>
                            <li>Principe :des recommandations basées sur la connaissance du domaine </li>
                            <li>Fonctionnement : Utilise une compréhension approfondie du contenu ou du domaine pour
                                recommander des articles pertinents.</li>
                        </ul>
                    </li>
                    <li><b>Basé sur la communauté</b>
                        <ul>
                            <li>Principe :des recommandations basées sur les préférences des amis des utilisateurs </li>
                            <li>Fonctionnement : Identifie les goûts et les préférences des amis de l'utilisateur pour
                                proposer des recommandations similaires.</li>
                        </ul>
                    </li>
                    <li><b>Systèmes hybrides de recommandation [Gomez-Uribe 2016]</b>
                        <ul>
                            <li>Principe : Intégration de plusieurs approches.</li>
                            <li>Fonctionnement : Combiner différentes méthodes de recommandation pour tirer parti de
                                leurs
                                avantages respectifs et fournir des suggestions plus précises et diversifiées.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">127
                    <a class="prev" href="#slide126"></a>
                    <a class="next" href="#slide128"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide128">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage collaboratif</h3>
                <p>Le filtrage collaboratif basé sur les transactions implique l'analyse des transactions entre
                    utilisateurs
                    et articles pour générer des recommandations. </p>
                <ul>
                    <li><b>Transactions</b> : Les transactions font référence aux interactions ou aux actions effectuées
                        par
                        les utilisateurs lorsqu'ils interagissent avec des articles, tels que des achats, des
                        évaluations,
                        des clics, etc. Les informations sur les transactions sont collectées et utilisées comme base
                        pour
                        comprendre les préférences des utilisateurs et générer des recommandations.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">128
                    <a class="prev" href="#slide127"></a>
                    <a class="next" href="#slide129"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide129">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage collaboratif</h3>
                <ul>
                    <li><b>Algorithmes: Règles de l'association</b>
                        <ul>
                            <li><b>Fonctionnement</b> : Les règles de l'association identifient des modèles de
                                co-occurrence
                                dans les transactions. Par exemple, si les utilisateurs qui ont acheté l'article A ont
                                également tendance à acheter l'article B, une règle de l'association peut être établie
                                entre
                                A et B.</li>
                            <li><b>Application</b> : Ces règles peuvent être utilisées pour recommander des articles à
                                un
                                utilisateur en se basant sur les préférences d'autres utilisateurs ayant des
                                transactions
                                similaires.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">129
                    <a class="prev" href="#slide128"></a>
                    <a class="next" href="#slide130"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide130">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage collaboratif</h3>
                <h4>Avantages</h4>
                <ul>
                    <li><b>Personnalisation</b> : Les recommandations sont personnalisées en fonction des comportements
                        d'achat et des transactions passées de l'utilisateur.</li>
                    <li><b>Découverte de modèles</b> : Permet de découvrir des modèles de comportement d'achat et
                        d'identifier des associations entre différents articles.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">130
                    <a class="prev" href="#slide129"></a>
                    <a class="next" href="#slide131"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide131">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage collaboratif</h3>
                <h4>Limitations</h4>
                <ul>
                    <li><b>Sparsité des données</b> : Si un utilisateur a effectué un nombre limité de transactions, les
                        recommandations peuvent être moins précises en raison de la sparsité des données.</li>
                    <li><b>Problème du démarrage à froid</b> : Il peut y avoir des difficultés lorsqu'un nouvel
                        utilisateur
                        effectue peu ou pas de transactions, entraînant des défis dans la génération de recommandations.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">131
                    <a class="prev" href="#slide130"></a>
                    <a class="next" href="#slide132"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide132">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
                <p>Le filtrage collaboratif basé sur le contenu repose sur la description des objets et le profil des
                    intérêts de l'utilisateur. </p>
                <ul>
                    <li><b>Description d'objet </b>
                        <ul>
                            <li>Les caractéristiques ou le contenu des objets (articles, produits, etc.) sont utilisés
                                pour
                                décrire chaque élément de manière détaillée.</li>
                        </ul>
                    <li><b>Profil d'intérêts de l'utilisateur</b>
                        <ul>
                            <li><b>Modèle des préférences</b> : Un modèle représentant les préférences de l'utilisateur
                                basé
                                sur la description des objets qu'il a aimés par le passé.</li>
                            <li><b>Historique d'interactions</b> : L'historique des interactions de l'utilisateur avec
                                le
                                système de recommandation est également utilisé pour ajuster le modèle au fil du temps.
                            </li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">132
                    <a class="prev" href="#slide131"></a>
                    <a class="next" href="#slide133"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide133">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
                <h4>Algorithmes</h4>
                <ul>
                    <li><b>Arbres de décision</b> : Les arbres de décision sont utilisés pour modéliser les préférences
                        de
                        l'utilisateur en fonction des caractéristiques des objets. L'arbre est construit pour prendre
                        des
                        décisions sur les recommandations en se basant sur ces caractéristiques.</li>
                    <li><b>Méthodes du plus proche voisin</b> : Les méthodes du plus proche voisin comparent le profil
                        d'intérêts de l'utilisateur avec ceux d'autres utilisateurs pour trouver les plus similaires.
                        Les
                        objets appréciés par ces utilisateurs similaires sont recommandés.</li>
                    <li><b>Classificateurs linéaires</b> : Les classificateurs linéaires modélisent la relation entre
                        les
                        caractéristiques des objets et les préférences de l'utilisateur de manière linéaire. Cela peut
                        inclure des modèles tels que la régression logistique.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">133
                    <a class="prev" href="#slide132"></a>
                    <a class="next" href="#slide134"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide134">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
                <h4>Avantages</h4>
                <ul>
                    <li><b>Personnalisation</b> : Les recommandations sont personnalisées en fonction du profil
                        d'intérêts
                        de l'utilisateur, ce qui les rend adaptées à ses goûts spécifiques.</li>
                    <li><b>Gestion du problème du démarrage à froid</b> : Peut mieux gérer le problème du démarrage à
                        froid
                        pour de nouveaux utilisateurs en se basant sur la description d'objets.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">134
                    <a class="prev" href="#slide133"></a>
                    <a class="next" href="#slide135"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide135">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Filtrage basé sur le contenu [Pazzani 2007]</h3>
                <h4>Limitations</h4>
                <ul>
                    <li><b>Dépendance à la description d'objet</b> : L'efficacité dépend de la qualité de la description
                        des
                        objets, et certaines approches peuvent être sensibles à des descriptions incomplètes ou
                        subjectives.
                    </li>
                    <li><b>Manque de diversité</b> : Peut avoir tendance à recommander des objets similaires à ceux que
                        l'utilisateur a aimés, limitant la diversité des suggestions.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">135
                    <a class="prev" href="#slide134"></a>
                    <a class="next" href="#slide136"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide136">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Systèmes hybrides [Gomez-Uribe 2016]</h3>
                <ul>
                    <li><b>Filtrage basé sur le contenu et filtrage collaboratif</b> : Combiner ces approches peut
                        compenser
                        les limitations individuelles, offrant des recommandations plus robustes qui tiennent compte à
                        la
                        fois du contenu des objets et des comportements d'autres utilisateurs.</li>
                    <li><b>Filtrage basé sur le contenu, filtrage collaboratif et démographique</b> : Cette approche
                        hybride
                        prend en compte non seulement les préférences individuelles de l'utilisateur mais aussi des
                        aspects
                        démographiques pour des recommandations plus contextualisées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">136
                    <a class="prev" href="#slide135"></a>
                    <a class="next" href="#slide137"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide137">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Systèmes hybrides</h3>
                <h4>Avantages</h4>
                <ul>
                    <li><b>Meilleure précision</b> : En combinant différentes approches, les systèmes hybrides peuvent
                        fournir des recommandations plus précises et diversifiées.</li>
                    <li><b>Gestion des limitations</b> : Compenser les limitations spécifiques de chaque approche
                        individuelle.</li>
                    <li><b>Adaptabilité</b> : S'adapter à différents types d'utilisateurs et de contextes.</li>
                    <li><b>Réduction du problème du démarrage à froid</b> : En intégrant des aspects démographiques, par
                        exemple, les systèmes hybrides peuvent mieux gérer le démarrage à froid pour de nouveaux
                        utilisateurs.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">137
                    <a class="prev" href="#slide136"></a>
                    <a class="next" href="#slide138"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide138">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Mesures de performance [Ziegler 2005, Ricci 2011]</h3>
                <ul>
                    <li><b>Précision et efficacité [Beel 2013a]</b> : La capacité du système à fournir des
                        recommandations
                        pertinentes et la rapidité avec laquelle le système génère des recommandations.</li>
                    <li><b>Diversité</b> : La variété des recommandations fournies pour éviter la redondance et
                        introduire
                        de nouveaux éléments.</li>
                    <li><b>Persistance de la recommandation</b> : La cohérence des recommandations au fil du temps,
                        offrant
                        une expérience utilisateur stable.</li>
                    <li><b>Vie privée [Pu 2012]</b> : La protection et le respect de la vie privée des utilisateurs lors
                        de
                        la collecte et de l'utilisation des données.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">138
                    <a class="prev" href="#slide137"></a>
                    <a class="next" href="#slide139"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide139">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Mesures de performance [Ziegler 2005, Ricci 2011]</h3>
                <ul>
                    <li><b>Démographie des utilisateurs</b> : L'intégration de facteurs démographiques pour des
                        recommandations plus contextuelles.</li>
                    <li><b>Robustesse (lutte contre la fraude) [Konstan 2012]</b> : La capacité à résister aux
                        tentatives de
                        manipulation ou de fraude dans le système.</li>
                    <li><b>Sérendipité</b> : La capacité à surprendre l'utilisateur en proposant des recommandations
                        inattendues mais appréciées.</li>
                    <li><b>Confiance</b> : La fiabilité perçue du système, renforçant la confiance de l'utilisateur dans
                        les
                        recommandations fournies.</li>
                    <li><b>Étiquetage (recommandations organiques ou sponsorisées) [Beel 2013b]</b> : La transparence
                        dans
                        la distinction entre les recommandations générées organiquement et celles sponsorisées.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">139
                    <a class="prev" href="#slide138"></a>
                    <a class="next" href="#slide140"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide140">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
                <p>Les recommandations à haut risque se réfèrent à des domaines où les conséquences d'une recommandation
                    incorrecte ou inappropriée peuvent être significatives.</p>
                <h4>Exemple</h4>
                <p><b>Assurance</b> : Les recommandations dans le domaine de l'assurance peuvent avoir des implications
                    financières importantes. Par exemple, une recommandation inappropriée en matière d'assurance
                    pourrait
                    entraîner des conséquences financières négatives pour l'utilisateur.</p>
                <h4>Intégration des Capacités d'Explication</h4>
                <p>Les systèmes de recommandation à haut risque devraient intégrer des capacités d'explication,
                    permettant
                    de fournir des justifications claires pour chaque recommandation.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">140
                    <a class="prev" href="#slide139"></a>
                    <a class="next" href="#slide141"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide141">
            <div class="header">
                <h1>4.8. Systèmes de recommandation (Recommender Systems)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Domaines à haut risque [Herlocker 2000]</h3>
                <h4>Avantages des explications</h4>
                <li><b>Justification</b> : Les explications fournissent une justification transparente du raisonnement
                    derrière chaque recommandation.</li>
                <li><b>Participation des utilisateurs</b> : Les explications encouragent la participation des
                    utilisateurs
                    en les aidant à comprendre pourquoi une recommandation particulière a été faite.</li>
                <li><b>Éducation</b> : Les explications contribuent à l'éducation des utilisateurs en les informant sur
                    les
                    critères pris en compte par le système pour générer des recommandations.</li>
                <li><b>Acceptation</b> : Les explications améliorent l'acceptation des recommandations en fournissant
                    une
                    visibilité sur le processus de prise de décision du système.</li>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">141
                    <a class="prev" href="#slide140"></a>
                    <a class="next" href="#slide142"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide142">
            <div class="header">
                <h1>4.9. Représentation des connaissances et raisonnement</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
                <p>La Représentation des connaissances et raisonnement (KRR) constituent un domaine clé de
                    l'intelligence
                    artificielle. La KRR est largement utilisée dans des domaines tels que la planification, la
                    représentation du langage naturel, la gestion des connaissances, les systèmes experts, etc.</p>
                <h4>Représentation des Connaissances</h4>
                <p>La représentation des connaissances implique la création d'une représentation lisible par machine de
                    la
                    connaissance d'un monde ou d'un domaine particulier. Cela vise à permettre aux systèmes
                    informatiques de
                    comprendre, interpréter et raisonner sur l'information.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">142
                    <a class="prev" href="#slide141"></a>
                    <a class="next" href="#slide143"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide143">
            <div class="header">
                <h1>4.9. Représentation des connaissances et raisonnement</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
                <h5>Exemples</h5>
                <ul>
                    <li><b>Réseaux Sémantiques</b> : Utilisent des relations sémantiques pour connecter des entités et
                        représenter la connaissance.</li>
                    <li><b>Ontologies</b> : Définissent des concepts, des propriétés et des relations dans un domaine
                        spécifique.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">143
                    <a class="prev" href="#slide142"></a>
                    <a class="next" href="#slide144"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide144">
            <div class="header">
                <h1>4.9. Représentation des connaissances et raisonnement</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Représentation des connaissances et raisonnement (KRR)</h3>
                <h4>Raisonnement</h4>
                <p>Le raisonnement consiste à déduire de nouvelles informations à partir des connaissances existantes.
                    C'est
                    le processus par lequel les systèmes tirent des conclusions logiques.</p>
                <h4>Compromis entre expressivité et praticité</h4>
                <ul>
                    <li><b>Expressivité</b> : La capacité à représenter une variété de connaissances de manière
                        détaillée.
                    </li>
                    <li><b>Praticité</b> : La facilité d'utilisation et de manipulation de la représentation des
                        connaissances.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">144
                    <a class="prev" href="#slide143"></a>
                    <a class="next" href="#slide145"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide145">
            <div class="header">
                <h1>4.10. Web sémantique (Semantic Web)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web sémantique</h3>
                <figure>
                    <img src="../../2021/MachineLearning/Semantic_web_stack.svg" height="400vh" />
                    <figcaption style="text-align:center">Semantic Web Stack
                        (https://commons.wikimedia.org/wiki/File:Semantic_web_stack.svg)</figcaption>
                </figure>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">145
                    <a class="prev" href="#slide144"></a>
                    <a class="next" href="#slide146"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide146">
            <div class="header">
                <h1>4.10. Web sémantique (Semantic Web)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web sémantique</h3>
                <p>Le Semantic Web Stack, également connu sous le nom de pile sémantique, représente une série de
                    technologies et de standards interconnectés qui ont été développés pour réaliser la vision du World
                    Wide
                    Web sémantique. Cette vision, initiée par Tim Berners-Lee, vise à rendre le contenu du web plus
                    compréhensible par les machines en ajoutant une sémantique aux données, permettant ainsi une
                    meilleure
                    interopérabilité et une utilisation plus avancée des informations sur le web.</p>
                <p>Les principales couches de la pile sémantique (du bas vers le haut) :</p>
                <ul>
                    <li><b>XML (eXtensible Markup Language)</b> : La base de la pile sémantique est constituée par XML,
                        un
                        langage de balisage extensible qui permet de structurer et de représenter des données de manière
                        lisible par les machines.</li>
                    <li><b>XML Namespaces</b> : Les espaces de noms XML sont utilisés pour éviter les conflits de noms
                        entre
                        les éléments XML provenant de différentes sources.</li>
                    <ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">146
                    <a class="prev" href="#slide145"></a>
                    <a class="next" href="#slide147"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide147">
            <div class="header">
                <h1>4.10. Web sémantique (Semantic Web)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web sémantique</h3>
                <ul>
                    <li><b>RDF (Resource Description Framework)</b> : RDF est un langage standardisé permettant de
                        décrire
                        des ressources du web de manière à faciliter le partage et la réutilisation d'informations entre
                        applications. Il repose sur un modèle de graphe pour représenter les relations entre les
                        entités.
                    </li>
                    <li><b>RDF Schema (RDFS)</b> : RDFS est une extension de RDF qui fournit des classes et des
                        propriétés
                        permettant de définir la structure des données RDF, facilitant ainsi la création de schémas et
                        de
                        taxonomies.</li>
                    <li><b>OWL (Web Ontology Language)</b> : OWL est une extension de RDF qui permet de définir des
                        ontologies, c'est-à-dire des modèles de connaissances formels décrivant les relations entre les
                        concepts. Il offre une expressivité accrue par rapport à RDFS.</li>
                    <li><b>SPARQL (SPARQL Protocol and RDF Query Language)</b> : - SPARQL est un langage de requête
                        permettant d'interroger des données RDF. Il offre une puissante capacité de recherche et
                        d'interrogation des informations sémantiques.</li>
                    <ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">147
                    <a class="prev" href="#slide146"></a>
                    <a class="next" href="#slide148"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide148">
            <div class="header">
                <h1>4.10. Web sémantique (Semantic Web)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web sémantique</h3>
                <ul>
                    <li><b>RDFa et Microformats</b> : RDFa (RDF in attributes) et Microformats sont des approches pour
                        intégrer des données sémantiques dans des documents HTML, permettant aux agents intelligents
                        d'extraire des informations à partir de pages web.</li>
                    <li><b>Linked Data</b> : Le concept de Linked Data encourage la publication de données structurées
                        sur
                        le web, avec des liens entre les ensembles de données, facilitant la découverte et l'intégration
                        des
                        informations.</li>
                </ul>
                <p>En utilisant cette pile sémantique, le World Wide Web sémantique vise à créer une infrastructure où
                    les
                    machines peuvent comprendre, interpréter et exploiter le contenu du web de manière plus avancée,
                    ouvrant
                    ainsi la porte à de nombreuses applications intelligentes et à une meilleure interconnexion des
                    données.
                </p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">148
                    <a class="prev" href="#slide147"></a>
                    <a class="next" href="#slide149"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide149">
            <div class="header">
                <h1>4.10. Web sémantique (Semantic Web)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web sémantique</h3>
                <ul>
                    <li><b>RIF (Rule Interchange Format)</b> : RIF fournit un cadre standard pour l'échange de règles
                        entre
                        différentes applications. Il s'agit d'un langage formel permettant de spécifier des règles
                        logiques,
                        ce qui est crucial pour l'automatisation de l'inférence et du raisonnement sur les données
                        sémantiques.</li>
                    <li><b>Unifying Logic</b> : Cette couche englobe les différentes logiques formelles utilisées dans
                        le
                        World Wide Web sémantique, y compris les logiques de description telles que OWL et les logiques
                        de
                        règles comme celles représentées par RIF. L'objectif est de fournir un cadre logique unifié pour
                        décrire les connaissances.</li>
                </ul>
                <p></p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">149
                    <a class="prev" href="#slide148"></a>
                    <a class="next" href="#slide150"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide150">
            <div class="header">
                <h1>4.10. Web sémantique (Semantic Web)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web sémantique</h3>
                <ul>
                    <li><b>Proof and Trust (Preuve et Confiance)</b> : Cette couche implique la capacité de fournir des
                        preuves formelles pour les déclarations sémantiques. Elle intègre également des mécanismes de
                        confiance, permettant d'évaluer la fiabilité des sources d'informations et d'inférer la
                        crédibilité
                        des données sémantiques.</li>
                    <li><b>User Applications (Applications Utilisateur)</b> : Au sommet de la pile, nous avons les
                        applications utilisateur qui tirent parti des données sémantiques pour offrir des
                        fonctionnalités
                        avancées. Cela pourrait inclure des applications d'analyse des sentiments, de recherche
                        sémantique,
                        de recommandation personnalisée, etc.</li>
                </ul>
                <p></p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">150
                    <a class="prev" href="#slide149"></a>
                    <a class="next" href="#slide151"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide151">
            <div class="header">
                <h1>4.11. Moteur de règles (Rule Engines)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Moteur de règles</h3>
                <p>Un moteur de règles, également connu sous le nom de moteur d'inférence, est un composant logiciel qui
                    traite et applique un ensemble de règles et de contraintes définies.</p>
                <ul>
                    <li><b>Règles :</b> Les règles sont des instructions conditionnelles qui décrivent les relations
                        entre
                        différentes entités ou éléments dans un système. Chaque règle a une condition qui, si elle est
                        remplie, déclenche une action spécifique.</li>
                    <li><b>Contraintes :</b> Les contraintes définissent des limitations ou des exigences spécifiques
                        sur
                        les données ou les actions dans un système. Elles contribuent à garantir la cohérence et la
                        conformité des opérations.</li>
                    <li><b>Traitement des données :</b> Le moteur de règles traite les données en fonction des règles
                        définies. Il examine l'état actuel du système et évalue si les conditions des règles sont
                        satisfaites.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">151
                    <a class="prev" href="#slide150"></a>
                    <a class="next" href="#slide152"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide152">
            <div class="header">
                <h1>4.11. Moteur de règles (Rule Engines)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Moteur de règles</h3>
                <ul>
                    <li><b>Évaluation des conditions :</b> Le moteur de règles évalue les conditions de chaque règle
                        pour
                        déterminer si elles sont vraies ou fausses. Si une condition est vraie, la règle associée est
                        déclenchée.</li>
                    <li><b>Action :</b> Lorsqu'une règle est déclenchée, une action spécifique est exécutée. Cette
                        action
                        peut inclure la modification de données, la génération de notifications, ou d'autres opérations
                        définies dans la logique métier.</li>
                    <li><b>Cohérence du système :</b> Le moteur de règles garantit la cohérence du système en appliquant
                        les
                        règles de manière séquentielle et en veillant à ce que les modifications apportées respectent
                        l'ensemble des contraintes.</li>
                    <li><b>Cycle d'évaluation :</b> Le moteur de règles peut fonctionner de manière répétée, évaluant en
                        continu l'état du système en fonction des règles. Cela permet une gestion dynamique et
                        adaptative
                        des opérations en réponse aux changements dans le système.</li>
                </ul>
                <p>Les moteurs de règles sont largement utilisés dans divers domaines, tels que la gestion des
                    workflows, la
                    logique métier, les systèmes d'alerte, et d'autres applications où des règles spécifiques doivent
                    être
                    appliquées pour maintenir la cohérence et automatiser les processus. Ils offrent une approche
                    déclarative pour spécifier la logique métier sans nécessiter une programmation explicite.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">152
                    <a class="prev" href="#slide151"></a>
                    <a class="next" href="#slide153"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide153">
            <div class="header">
                <h1>4.11. Moteur de règles (Rule Engines)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Moteur de règles : Applications</h3>
                <ul>
                    <li><b>Systèmes de gestion de workflow :</b> Un moteur de règles peut être utilisé pour orchestrer
                        le
                        flux de travail dans une entreprise. Par exemple, il peut déclencher des étapes spécifiques en
                        fonction de certaines conditions, comme l'approbation d'une demande ou la disponibilité de
                        ressources.</li>
                    <li><b>Systèmes d'alerte :</b> Dans le domaine des systèmes de surveillance, un moteur de règles
                        peut
                        être employé pour déclencher des alertes en cas de dépassement de seuils prédéfinis. Cela
                        pourrait
                        inclure des alertes de performance, de sécurité, ou d'autres métriques.</li>
                    <li><b>Logique métier dans les applications :</b> Les applications métier complexes peuvent utiliser
                        des
                        moteurs de règles pour gérer la logique métier. Par exemple, dans un système bancaire, des
                        règles
                        peuvent être définies pour gérer les conditions d'octroi de prêts.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">153
                    <a class="prev" href="#slide152"></a>
                    <a class="next" href="#slide154"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide154">
            <div class="header">
                <h1>4.11. Moteur de règles (Rule Engines)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Moteur de règles : Applications</h3>
                <ul>
                    <li><b>Systèmes de gestion de la chaîne d'approvisionnement :</b> Les moteurs de règles peuvent être
                        intégrés aux systèmes de gestion de la chaîne d'approvisionnement pour automatiser des décisions
                        telles que la sélection des fournisseurs en fonction de critères prédéfinis.</li>
                    <li><b>Systèmes de tarification dynamique :</b> Dans le commerce électronique, un moteur de règles
                        peut
                        être utilisé pour ajuster dynamiquement les prix en fonction de divers facteurs tels que la
                        demande
                        du marché, la disponibilité des produits, ou d'autres conditions commerciales.</li>
                    <li><b>Systèmes de gestion des fraudes :</b> Les moteurs de règles sont souvent utilisés dans les
                        systèmes de détection de fraude. Ils peuvent déclencher des alertes lorsque des modèles de
                        comportement suspects sont détectés, aidant ainsi à prévenir les activités frauduleuses.</li>
                    <li><b>Systèmes de traitement des demandes :</b> Les moteurs de règles peuvent être intégrés aux
                        systèmes de traitement des demandes pour automatiser la prise de décision en fonction de
                        critères
                        prédéfinis, accélérant ainsi le processus global.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">154
                    <a class="prev" href="#slide153"></a>
                    <a class="next" href="#slide155"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide155">
            <div class="header">
                <h1>4.11. Moteur de règles (Rule Engines)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Moteur de règles : Langages</h3>
                <ul>
                    <li><b>Drools (langage Drools Rule Language - DRL) :</b> - Drools est un moteur de règles open
                        source
                        basé sur le langage DRL. Il utilise une syntaxe déclarative pour définir des règles métier.</li>
                    <li><b>RuleML (Rule Markup Language) :</b> - RuleML est un langage de balisage spécialement conçu
                        pour
                        représenter des règles métier sous une forme lisible par machine.</li>
                    <li><b>Jess (Java Expert System Shell) :</b> - Jess est un moteur de règles pour Java qui utilise
                        son
                        propre langage basé sur le Lisp pour définir les règles.</li>
                    <li><b>CLIPS (C Language Integrated Production System) :</b> - CLIPS est un système expert qui
                        inclut un
                        moteur de règles. Il utilise un langage de programmation déclaratif pour spécifier des règles et
                        des
                        faits.</li>
                    <li><b>Rete :</b> - Rete est un algorithme utilisé dans plusieurs moteurs de règles, y compris CLIPS
                        et
                        Drools. Il est conçu pour optimiser l'évaluation des règles.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">155
                    <a class="prev" href="#slide154"></a>
                    <a class="next" href="#slide156"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide156">
            <div class="header">
                <h1>4.11. Moteur de règles (Rule Engines)</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Moteur de règles : Logiciels</h3>
                <ul>
                    <li><b>Drools :</b> Drools, mentionné ci-dessus, est un moteur de règles open source développé par
                        Red
                        Hat. Il offre des fonctionnalités avancées pour la gestion de règles métier.</li>
                    <li><b>IBM Operational Decision Manager (ODM) :</b> ODM est une solution d'IBM qui fournit un moteur
                        de
                        règles pour automatiser et gérer les décisions métier dans les applications.</li>
                    <li><b>Corticon :</b> Progress Corticon est une plateforme de gestion de règles métier qui permet
                        aux
                        utilisateurs de modéliser, exécuter et optimiser des règles métier.</li>
                    <li><b>InRule :</b> InRule est une plateforme de gestion de règles métier qui permet aux entreprises
                        de
                        définir, gérer et automatiser les règles métier.</li>
                    <li><b>Camunda :</b> Camunda propose un moteur de règles dans le cadre de son ensemble de solutions
                        BPMN
                        (Business Process Model and Notation) open source.</li>
                    <li><b>JBoss Rules (anciennement JRules) :</b> JBoss Rules, maintenant intégré à Drools, était un
                        moteur
                        de règles développé par IBM avant d'être open source.</li>
                    <li><b>Microsoft BizTalk Rules Engine :</b> Microsoft BizTalk Server inclut un moteur de règles qui
                        permet aux entreprises de définir et de gérer les règles métier dans leurs processus.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">156
                    <a class="prev" href="#slide155"></a>
                    <a class="next" href="#slide157"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide157">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Programmation logique</h3>
                <p>La programmation logique, en particulier la logique propositionnelle et la logique du premier ordre
                    (FOL), joue un rôle fondamental dans le domaine de l'intelligence artificielle. </p>
                <ul>
                    <li><b>Logique propositionnelle</b> : En IA, la logique propositionnelle est souvent utilisée pour
                        modéliser des systèmes de connaissance simples. Elle est adaptée pour représenter des faits, des
                        règles et des relations de manière formelle.</li>
                    <li><b>Logique du premier ordre (FOL, logique des prédicats)</b> : La logique du premier ordre est
                        largement utilisée en IA pour modéliser des connaissances plus riches et des raisonnements plus
                        sophistiqués. Elle permet de décrire des entités, leurs attributs et les relations entre elles
                        de
                        manière plus expressive.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">157
                    <a class="prev" href="#slide156"></a>
                    <a class="next" href="#slide158"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide158">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog</h3>
                <p>Prolog (Programming in Logic) est un langage de programmation déclaratif, basé sur la logique du
                    premier
                    ordre.</p>
                <ul>
                    <li>Prolog est souvent qualifié de <b>langage de programmation déclaratif</b>, car les programmes
                        Prolog
                        décrivent les relations entre les entités plutôt que de spécifier explicitement les étapes à
                        suivre
                        pour atteindre un résultat.</li>
                    <li>Prolog repose sur la <b>logique du premier ordre</b>, ce qui signifie qu'il permet de décrire
                        des
                        relations logiques complexes à l'aide de prédicats, de variables et de quantificateurs. Les
                        programmes Prolog sont composés de faits (des déclarations vraies) et de règles (des relations
                        logiques).</li>
                    <li>Prolog a été développé par <b>Alain Colmerauer</b> dans les années 1970 à l'Université de
                        Marseille.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">158
                    <a class="prev" href="#slide157"></a>
                    <a class="next" href="#slide159"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide159">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog</h3>
                <ul>
                    <li>Les programmes Prolog sont exprimés en termes de <b>relations</b>. Les faits décrivent des
                        relations
                        qui sont toujours vraies, tandis que les règles décrivent des relations qui sont vraies dans
                        certaines conditions. L'exécution d'une requête en Prolog revient à chercher des solutions aux
                        relations spécifiées.</li>
                    <li>Prolog a été largement utilisé dans des domaines tels que la <b>démonstration de théorèmes</b>,
                        les
                        systèmes experts et le traitement du langage naturel. En raison de sa nature déclarative et de
                        son
                        approche logique, Prolog est bien adapté pour représenter et résoudre des problèmes impliquant
                        des
                        relations complexes.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">159
                    <a class="prev" href="#slide158"></a>
                    <a class="next" href="#slide160"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide160">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: types de données</h3>
                <p>En Prolog, les types de données fondamentaux incluent les atomes, les nombres, les variables et les
                    termes composés.</p>
                <h4>Atome :</h4>
                <p>Un atome est une chaîne de caractères qui représente un nom. Il commence généralement par une lettre
                    minuscule et peut contenir des lettres, des chiffres et des caractères de soulignement. Les atomes
                    sont
                    utilisés pour représenter des constantes et des noms dans Prolog.</p>
                <div class="highlight">
                    <pre><span></span><span class="nf">animal</span><span class="p">(</span><span class="s s-Atom">chien</span><span class="p">).</span>
<span class="nf">couleur</span><span class="p">(</span><span class="s s-Atom">rouge</span><span class="p">).</span>
</pre>
                </div>

            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">160
                    <a class="prev" href="#slide159"></a>
                    <a class="next" href="#slide161"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide161">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: types de données</h3>
                <h4>Nombres :</h4>
                <p>Prolog prend en charge les entiers et les nombres à virgule flottante comme types de données
                    numériques.
                </p>
                <div class="highlight">
                    <pre><span></span><span class="nf">age</span><span class="p">(</span><span class="s s-Atom">personne1</span><span class="p">,</span> <span class="mi">23</span><span class="p">).</span>
<span class="nf">prix</span><span class="p">(</span><span class="s s-Atom">livre1</span><span class="p">,</span> <span class="mf">19.99</span><span class="p">).</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">161
                    <a class="prev" href="#slide160"></a>
                    <a class="next" href="#slide162"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide162">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: types de données</h3>
                <h4>Variables :</h4>
                <p>Les variables sont des symboles qui représentent des valeurs inconnues. Elles commencent généralement
                    par
                    une lettre majuscule ou un soulignement.</p>
                <div class="highlight">
                    <pre><span></span><span class="nf">personne</span><span class="p">(</span><span class="nv">X</span><span class="p">).</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">162
                    <a class="prev" href="#slide161"></a>
                    <a class="next" href="#slide163"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide163">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: types de données</h3>
                <h4>Terme Composé :</h4>
                <p>Les termes composés sont des structures de données complexes créées à partir de foncteurs (noms de
                    termes) et d'arguments. Ils sont utilisés pour représenter des entités composées.</p>
                <div class="highlight">
                    <pre><span></span><span class="nf">point</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">7</span><span class="p">).</span>
<span class="nf">livre</span><span class="p">(</span><span class="nf">titre</span><span class="p">(</span><span class="s s-Atom">&#39;Introduction à Prolog&#39;</span><span class="p">),</span> <span class="nf">auteur</span><span class="p">(</span><span class="s s-Atom">&#39;John Doe&#39;</span><span class="p">)).</span>
</pre>
                </div>

                <p>Les termes composés peuvent également être utilisés pour représenter des listes, des arbres et
                    d'autres
                    structures de données complexes.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">163
                    <a class="prev" href="#slide162"></a>
                    <a class="next" href="#slide164"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide164">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: règles</h3>
                <p>En Prolog, les règles sont des clauses qui définissent des relations entre différents termes. La
                    structure générale d'une règle est la suivante :</p>
                <code>
                          Tête : - Corps.
                        </code>
                <ul>
                    <li><b>Tête</b> : La tête de la règle spécifie la relation que nous voulons définir. Elle est
                        généralement composée d'un seul prédicat.</li>
                    <li><b>Corps</b> : Le corps de la règle contient une séquence de prédicats liés par des conjonctions
                        (,
                        pour ET) et des disjonctions (; pour OU). Le corps de la règle spécifie les conditions sous
                        lesquelles la relation spécifiée dans la tête est vraie.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">164
                    <a class="prev" href="#slide163"></a>
                    <a class="next" href="#slide165"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide165">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: faits</h3>
                <p>En Prolog, une clause avec un corps vide est en effet appelée un fait.</p>
                <div class="highlight">
                    <pre><span></span><span class="nf">personne</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">).</span>
<span class="nf">personne</span><span class="p">(</span><span class="s s-Atom">alice</span><span class="p">).</span>
</pre>
                </div>
                <p>Les faits sont des affirmations qui sont considérées comme vraies dans le monde que vous décrivez.
                    Ces
                    faits peuvent ensuite être utilisés dans des requêtes ou d'autres règles pour déduire des
                    informations
                    supplémentaires.</p>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">165
                    <a class="prev" href="#slide164"></a>
                    <a class="next" href="#slide166"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide166">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: Installation</h3>
                <p>GNU Prolog, également connu sous le nom de gprolog, est un compilateur Prolog basé sur le standard
                    ISO
                    Prolog.
                    Il est conçu pour être utilisé sur différentes plates-formes, y compris Linux, Windows et d'autres
                    systèmes d'exploitation.
                </p>
                <p>L'installation de gprolog sur une machine Ubuntu</p>
                <code>
                          $ sudo apt install gprolog
                        </code>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">166
                    <a class="prev" href="#slide165"></a>
                    <a class="next" href="#slide167"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide167">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog</h3>
                <code>
$ prolog</br>
GNU Prolog 1.4.5 (64 bits)</br>
Compiled Feb  5 2017, 10:30:08 with gcc</br>
By Daniel Diaz</br>
Copyright (C) 1999-2016 Daniel Diaz</br>
| ?- [user].</br>
compiling user for byte code...</br>
personne(tom).</br>
personne(alice).</br>
</br>
user compiled, 2 lines read - 241 bytes written, 12239 ms</br>
(4 ms) yes</br>
| ?- </br>
                        </code>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">167
                    <a class="prev" href="#slide166"></a>
                    <a class="next" href="#slide168"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide168">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <code>
?- personne(X).</br>
</br>
X = tom ? </br>
</br>
yes</br>
| ?- cat(bob).</br>
</br>
no</br>
                        </code>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">168
                    <a class="prev" href="#slide167"></a>
                    <a class="next" href="#slide169"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide169">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <code>
| ?- [user].                             </br>
compiling user for byte code...</br>
personne(tom).                           </br>
personne(alice).                         </br>
personnes(L) :- findall(X, personne(X), L).</br>
</br>
user compiled, 3 lines read - 490 bytes written, 10638 ms</br>
</br>
yes</br>
| ?- personnes(L).                         </br>
</br>
L = [tom,alice]</br>
</br>
yes</br>
                        </code>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">169
                    <a class="prev" href="#slide168"></a>
                    <a class="next" href="#slide170"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide170">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <code>
| ?- [user].              </br>
compiling user for byte code...</br>
friend(bob, alice).  </br>
friend(alice, kevin).</br>
friend(bob, thomas).                </br>
friend(bob, peter).  </br>
user compiled, 4 lines read - 486 bytes written, 77256 ms</br>
(10 ms) yes</br>
| ?- friend(bob, X).      </br>
</br>
X = alice ? a</br>
X = thomas</br>
X = peter</br>
(1 ms) yes</br>
                        </code>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">170
                    <a class="prev" href="#slide169"></a>
                    <a class="next" href="#slide171"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide171">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <pre>
			<code>
$ cat friend.pl
<div class="highlight"><pre><span></span><span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">alice</span><span class="p">).</span>
<span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">alice</span><span class="p">,</span> <span class="s s-Atom">kevin</span><span class="p">).</span>
<span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">thomas</span><span class="p">).</span>
<span class="nf">friend</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">peter</span><span class="p">).</span>
<span class="nf">human</span><span class="p">(</span><span class="nv">X</span><span class="p">):-</span><span class="nf">friend</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span><span class="k">_</span><span class="p">).</span>
<span class="nf">human</span><span class="p">(</span><span class="nv">Y</span><span class="p">):-</span><span class="nf">friend</span><span class="p">(</span><span class="k">_</span><span class="p">,</span><span class="nv">Y</span><span class="p">).</span>
</pre>
            </div>
            </code>
            </pre>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">171
                    <a class="prev" href="#slide170"></a>
                    <a class="next" href="#slide172"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide172">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <pre>
			<code>
$ prolog --consult-file friend.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
compiling /home/user/friend.pl for byte code...
/home/user/friend.pl compiled, 4 lines read - 515 bytes written, 22 ms
| ?- friend(bob,alice).

true ?

yes
                        </code>
			</pre>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">172
                    <a class="prev" href="#slide171"></a>
                    <a class="next" href="#slide173"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide173">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <pre>
			<code>
$ prolog --consult-file friend.pl
| ?- human(X).
X = bob ? a
X = alice
X = bob
X = bob
X = alice
X = kevin
X = thomas
X = peter

yes
| ?-
                        </code>
			</pre>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">173
                    <a class="prev" href="#slide172"></a>
                    <a class="next" href="#slide174"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide174">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <pre>
			<code>
$ cat ancetre.pl
<div class="highlight"><pre><span></span><span class="cm">/* Faits : Déclaration des relations parents */</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">jane</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">jim</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">ann</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">bob</span><span class="p">).</span>
<span class="nf">parent</span><span class="p">(</span><span class="s s-Atom">jim</span><span class="p">,</span> <span class="s s-Atom">pat</span><span class="p">).</span>

<span class="cm">/* Règle : X est l&#39;ancêtre de Y si X est le parent de Y ou si X est l&#39;ancêtre d&#39;un parent de Y. */</span>
<span class="nf">ancetre</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Y</span><span class="p">)</span> <span class="p">:-</span> <span class="nf">parent</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Y</span><span class="p">).</span>
<span class="nf">ancetre</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Y</span><span class="p">)</span> <span class="p">:-</span> <span class="nf">parent</span><span class="p">(</span><span class="nv">X</span><span class="p">,</span> <span class="nv">Z</span><span class="p">),</span> <span class="nf">ancetre</span><span class="p">(</span><span class="nv">Z</span><span class="p">,</span> <span class="nv">Y</span><span class="p">).</span>
</pre>
            </div>
            </code>
            </pre>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">174
                    <a class="prev" href="#slide173"></a>
                    <a class="next" href="#slide175"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide175">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <pre>
			<code>
$ prolog --consult-file friend.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
/home/user/ancetre.pl compiled, 11 lines read - 1119 bytes written, 14 ms
| ?- ancetre(kevin,X).
X = jane ? a
X = jim
X = ann
X = bob
X = pat

no
| ?-
                        </code>
			</pre>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">175
                    <a class="prev" href="#slide174"></a>
                    <a class="next" href="#slide176"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide176">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <div class="highlight">
                    <pre><span></span><span class="cm">/* Faits : Déclaration des élèves et de leurs notes */</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">math</span><span class="p">,</span> <span class="mi">85</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">kevin</span><span class="p">,</span> <span class="s s-Atom">anglais</span><span class="p">,</span> <span class="mi">90</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">math</span><span class="p">,</span> <span class="mi">92</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">jane</span><span class="p">,</span> <span class="s s-Atom">anglais</span><span class="p">,</span> <span class="mi">88</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">math</span><span class="p">,</span> <span class="mi">78</span><span class="p">).</span>
<span class="nf">note</span><span class="p">(</span><span class="s s-Atom">bob</span><span class="p">,</span> <span class="s s-Atom">anglais</span><span class="p">,</span> <span class="mi">85</span><span class="p">).</span>

<span class="cm">/* Règle : Calcul de la moyenne des notes d&#39;un élève */</span>
<span class="nf">moyenne</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Moyenne</span><span class="p">)</span> <span class="p">:-</span>
    <span class="nf">findall</span><span class="p">(</span><span class="nv">Note</span><span class="p">,</span> <span class="nf">note</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="k">_</span><span class="p">,</span> <span class="nv">Note</span><span class="p">),</span> <span class="nv">Notes</span><span class="p">),</span>
    <span class="nf">length</span><span class="p">(</span><span class="nv">Notes</span><span class="p">,</span> <span class="nv">N</span><span class="p">),</span>
    <span class="nf">somme_liste</span><span class="p">(</span><span class="nv">Notes</span><span class="p">,</span> <span class="nv">Sum</span><span class="p">),</span>
    <span class="nv">Moyenne</span> <span class="o">is</span> <span class="nv">Sum</span> <span class="o">/</span> <span class="nv">N</span><span class="p">.</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">176
                    <a class="prev" href="#slide175"></a>
                    <a class="next" href="#slide177"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide177">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <div class="highlight">
                    <pre><span></span>
<span class="cm">/* Prédicat auxiliaire : Calcul de la somme d&#39;une liste */</span>
<span class="nf">somme_liste</span><span class="p">([],</span> <span class="mi">0</span><span class="p">).</span>
<span class="nf">somme_liste</span><span class="p">([</span><span class="nv">X</span><span class="p">|</span><span class="nv">Xs</span><span class="p">],</span> <span class="nv">Sum</span><span class="p">)</span> <span class="p">:-</span>
    <span class="nf">somme_liste</span><span class="p">(</span><span class="nv">Xs</span><span class="p">,</span> <span class="nv">Reste</span><span class="p">),</span>
    <span class="nv">Sum</span> <span class="o">is</span> <span class="nv">X</span> <span class="o">+</span> <span class="nv">Reste</span><span class="p">.</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">177
                    <a class="prev" href="#slide176"></a>
                    <a class="next" href="#slide178"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide178">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <div class="highlight">
                    <pre><span></span>
<span class="cm">/* Règle : Affichage des élèves avec leur moyenne */</span>
<span class="nf">afficher_moyennes</span> <span class="o">:-</span>
    <span class="nf">setof</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Matiere</span><span class="s s-Atom">^</span><span class="nv">Note</span><span class="s s-Atom">^</span><span class="p">(</span><span class="nf">note</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Matiere</span><span class="p">,</span> <span class="nv">Note</span><span class="p">)),</span> <span class="nv">Eleves</span><span class="p">),</span>
    <span class="nf">member</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Eleves</span><span class="p">),</span>
    <span class="nf">moyenne</span><span class="p">(</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Moyenne</span><span class="p">),</span>
    <span class="nf">format</span><span class="p">(</span><span class="s s-Atom">&#39;Élève: ~w, Moyenne: ~2f~n&#39;</span><span class="p">,</span> <span class="p">[</span><span class="nv">Eleve</span><span class="p">,</span> <span class="nv">Moyenne</span><span class="p">]),</span>
    <span class="s s-Atom">fail</span><span class="p">.</span>
<span class="s s-Atom">afficher_moyennes</span><span class="p">.</span>
</pre>
                </div>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">178
                    <a class="prev" href="#slide177"></a>
                    <a class="next" href="#slide179"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide179">
            <div class="header">
                <h1>4.12. Programmation logique et IA</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Prolog: GNU Prolog: interrogation</h3>
                <pre>
			<code>
$ prolog --consult-file notes.pl
GNU Prolog 1.4.5 (64 bits)
Compiled Feb 23 2020, 20:14:50 with gcc
By Daniel Diaz
Copyright (C) 1999-2020 Daniel Diaz
/home/user/notes.pl compiled, 30 lines read - 3086 bytes written, 3 ms
| ?- afficher_moyennes.
Élève: bob, Moyenne: 81.50
Élève: jane, Moyenne: 90.00
Élève: kevin, Moyenne: 87.50

(1 ms) no
| ?-
                        </code>
			</pre>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">179
                    <a class="prev" href="#slide178"></a>
                    <a class="next" href="#slide180"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide180">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h1>Articles de recherche</h1>
                <ul>
                    <li>[Beel 2013a] Beel, Joeran, et al. “A Comparative Analysis of Offline and Online Evaluations and
                        Discussion of Research Paper Recommender System Evaluation.” Proceedings of the International
                        Workshop on Reproducibility and Replication in
                        Recommender Systems Evaluation, Association for Computing Machinery, 2013</li>
                    <li>[Beel 2013b] Beel, Joeran, et al. “Sponsored vs. Organic (Research Paper) Recommendations and
                        the
                        Impact of Labeling.” Research and Advanced Technology for Digital Libraries, edited by Trond
                        Aalberg
                        et al., Springer, 2013, pp. 391–95.</li>
                    <li>[Chrupała 2006] Chrupała, Grzegorz. Simple Data-Driven Context-Sensitive Lemmatization. 2006.
                        doras.dcu.ie, http://www.unizar.es/departamentos/filologia_inglesa/sepln2006/.</li>
                    <li>[Frakes 2003] Frakes, William B., and Christopher J. Fox. “Strength and Similarity of Affix
                        Removal
                        Stemming Algorithms.” ACM SIGIR Forum, vol. 37, no. 1, Apr. 2003, pp. 26–30. Spring 2003</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">180
                    <a class="prev" href="#slide179"></a>
                    <a class="next" href="#slide181"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide181">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h1>Articles de recherche</h1>
                <ul>
                    <li>[Gomez-Uribe 2016] Gomez-Uribe, Carlos A., and Neil Hunt. “The Netflix Recommender System:
                        Algorithms, Business Value, and Innovation.” ACM Transactions on Management Information Systems,
                        vol. 6, no. 4, Dec. 2016, p. 13:1–13:19. January
                        2016 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:
                        Short
                        Papers -
                        Volume 2, Association for Computational Linguistics, 2012, pp. 368–372.</li>
                    <li>[Gesmundo 2012] Gesmundo, Andrea, and Tanja Samardžić. “Lemmatisation as a Tagging Task.”
                    <li>[Herlocker 2000] Herlocker, Jonathan L., et al. “Explaining Collaborative Filtering
                        Recommendations.” Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work,
                        Association for Computing Machinery, 2000, pp. 241–250.
                        ACM
                    </li>
                    <li>[Konstan 2012] Konstan, Joseph A., and John Riedl. “Recommender Systems: From Algorithms to User
                        Experience.” User Modeling and User-Adapted Interaction, vol. 22, no. 1–2, Apr. 2012, pp.
                        101–123.
                    </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">181
                    <a class="prev" href="#slide180"></a>
                    <a class="next" href="#slide182"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide182">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h1>Articles de recherche</h1>
                <ul>
                    <li>[Màrquez 2000] Màrquez, Lluís, et al. “A Machine Learning Approach to POS Tagging.” Machine
                        Learning, vol. 39, no. 1, Apr. 2000, pp. 59–91.</li>
                    <li>[Mikolov 2013] Mikolov, Tomas, et al. “Efficient Estimation of Word Representations in Vector
                        Space.” ArXiv:1301.3781 [Cs], Sept. 2013.</li>
                    <li>[Miller 1995] Miller, George A. “WordNet: A Lexical Database for English.” Communications of the
                        ACM, vol. 38, no. 11, Nov. 1995, pp. 39–41. Nov. 1995</li>
                    <li>[Pazzani 2007] Pazzani, Michael J., and Daniel Billsus. “Content-Based Recommendation Systems.”
                        The
                        Adaptive Web: Methods and Strategies of Web Personalization, edited by Peter Brusilovsky et al.,
                        Springer, 2007, pp. 325–41. </li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">182
                    <a class="prev" href="#slide181"></a>
                    <a class="next" href="#slide183"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide183">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h1>Articles de recherche</h1>
                <ul>
                    <li>[Porter 1980] Porter, M. F. “An Algorithm for Suffix Stripping.” Program, vol. 14, no. 3, Jan.
                        1980,
                        pp. 130–37. Emerald Insight</li>
                    <li>[Pu 2012] Pu, Pearl, et al. “Evaluating Recommender Systems from the User’s Perspective: Survey
                        of
                        the State of the Art.” User Modeling and User-Adapted Interaction, vol. 22, no. 4, Oct. 2012,
                        pp.
                        317–55.
                    </li>
                    <li>[Ricci 2011] Ricci, Francesco, et al. “Introduction to Recommender Systems Handbook.”
                        Recommender
                        Systems Handbook, edited by Francesco Ricci et al., Springer US, 2011, pp. 1–35. </li>
                    <li>[Ziegler 2005] Ziegler, Cai-Nicolas, et al. “Improving Recommendation Lists through Topic
                        Diversification.” Proceedings of the 14th International Conference on World Wide Web,
                        Association
                        for Computing Machinery, 2005, pp. 22–32.</li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">183
                    <a class="prev" href="#slide182"></a>
                    <a class="next" href="#slide184"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide184">
            <div class="header">
                <h1>Références</h1>
            </div>
            <div class="content">
                <h3 class="topicsubheading">Web</h3>
                <ul>
                    <li><a
                            href="https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation">https://fr.wikipedia.org/wiki/Syst%C3%A8me_de_recommandation</a>
                    </li>
                    <li><a
                            href="https://fr.wikipedia.org/wiki/Racinisation">https://fr.wikipedia.org/wiki/Racinisation</a>
                    </li>
                    <li><a
                            href="https://fr.wikipedia.org/wiki/Intelligence_artificielle">https://fr.wikipedia.org/wiki/Intelligence_artificielle</a>
                    </li>
                    <li><a
                            href="https://fr.wikipedia.org/wiki/Programmation_logique">https://fr.wikipedia.org/wiki/Programmation_logique</a>
                    </li>
                    <li><a
                            href="https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances">https://fr.wikipedia.org/wiki/Repr%C3%A9sentation_des_connaissances</a>
                    </li>
                    <li><a
                            href="https://en.wikipedia.org/wiki/Morphology_(linguistics)">https://en.wikipedia.org/wiki/Morphology_(linguistics)</a>
                    </li>
                    <li><a
                            href="https://en.wikipedia.org/wiki/Word_embedding">https://en.wikipedia.org/wiki/Word_embedding</a>
                    </li>
                    <li><a href="https://en.wikipedia.org/wiki/Word2vec">https://en.wikipedia.org/wiki/Word2vec</a></li>
                    <li><a href="https://fr.wikipedia.org/wiki/Prolog">https://fr.wikipedia.org/wiki/Prolog</a></li>
                    <li><a href="https://www.nltk.org/howto/stem.html">https://www.nltk.org/howto/stem.html</a></li>
                    <li><a href="http://www.nltk.org/book/ch05.html">http://www.nltk.org/book/ch05.html</a></li>
                    <li><a href="https://spacy.io/usage/spacy-101">https://spacy.io/usage/spacy-101</a></li>
                    <li><a href="https://spacy.io/usage/visualizers">https://spacy.io/usage/visualizers</a></li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">184
                    <a class="prev" href="#slide183"></a>
                    <a class="next" href="#slide185"></a>
                </div>
            </div>
        </section>
        <section class="slide" id="slide185">
            <div class="header">
                <h1>Références:</h1>
            </div>
            <div class="content">
                <h1>Couleurs</h1>
                <ul>
                    <li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
                </ul>
                <h1>Images</h1>
                <ul>
                    <li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
                </ul>
            </div>
            <div class="footer">
                <div class="contact">Intelligence artificielle et apprentissage machine | John Samuel</div>
                <div class="navigation">185
                    <a class="prev" href="#slide184"></a>
                </div>
            </div>
        </section>

        <script>
            function changeCurrentURLSlideNumber(isIncrement) {
                url = window.location.href;
                position = url.indexOf("#slide");
                if (position != -1) { // Not on the first page
                    slideIdString = url.substr(position + 6);
                    if (!Number.isNaN(slideIdString)) {
                        slideId = parseInt(slideIdString);
                        if (isIncrement) {
                            if (slideId < 185) {
                                slideId = slideId + 1;
                            }
                        } else {
                            if (slideId > 1) {
                                slideId = slideId - 1;
                            }
                        }
                        /* regexp */
                        url = url.replace(/#slide\d+/g, "#slide" + slideId);
                        window.location.href = url;
                    }
                } else {
                    window.location.href = url + "#slide2";
                }
            }
            document.onkeydown = function (event) {

                event.preventDefault();
                /* This will ensure the default behavior of
                                                                page scroll behaviour (up, down, right, left)*/

                event = event || window.event;
                /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
                if (event.keyCode == '37') {
                    // left
                    changeCurrentURLSlideNumber(false);
                } else if (event.keyCode == '38') {
                    // up
                    changeCurrentURLSlideNumber(false);
                } else if (event.keyCode == '39') {
                    // right
                    changeCurrentURLSlideNumber(true);
                } else if (event.keyCode == '40') {
                    // down
                    changeCurrentURLSlideNumber(true);
                }
            }
            document.body.onmouseup = function (event) {
                event = event || window.event;
                event.preventDefault();
                changeCurrentURLSlideNumber(true);
            }
        </script>
    </body>

</html>