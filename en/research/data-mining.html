<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="Content-Language" content="en" />
    <link rel="shortcut icon" href="../../images/logo/favicon.png" />
    <title>Introduction to Data Mining: John Samuel</title>
    <style type="text/css">
        body {
            background-color: #FFFFFF;
        }

        #sidebar {
            position: fixed;
            background-color: #1B80CF;
            top: 0;
            left: 0;
            bottom: 0;
            width: 30vw;
        }

        #sidebar .title {
            position: relative;
            text-align: center;
            line-height: 4vmax;
            font-size: 1.4vmax;
            font-family: 'Arial';
            margin-top: 25vh;
        }

        #sidebar .title a:link,
        #sidebar .title a:visited {
            color: #FFFFFF;
            text-decoration: none;
        }

        .subtitle {
            top: 50vh;
            text-align: center;
            line-height: 1.3vmax;
            font-family: 'Arial';
            font-size: 1.2vmax;
            color: #FFFFFF;
        }

        .subtitle a:link,
        .subtitle a:visited {
            color: #FFFFFF;
            text-decoration: none;
        }

        .licence {
            position: fixed;
            text-align: right;
            bottom: 0;
            right: 0;
        }

        .home {
            position: fixed;
            text-align: left;
            font-family: 'Arial';
            color: #D3D3D3;
            z-index: 100;
            width: 100%;
            background-color: #FFFFFF;
            top: 0px;
            margin-bottom: 10px;
            padding-bottom: 10px;
        }

        .home a:link,
        .home a:visited {
            text-decoration: none;
            color: #D3D3D3;
        }

        .home ul {
            margin: 0;
            padding: 0;
            text-align: left;
            list-style: none;
        }

        .home li {
            position: relative;
            float: left;
            padding-top: 15px;
            margin-right: 1em;
            font-family: 'Arial';
        }

        .home li:hover {
            display: block;
        }

        .home li:hover a:link,
        .home li:hover a:visited {
            text-decoration: none;
            padding: 15px;
            color: #FFFFFF;
            background-color: #1B80CF;
        }

        .content {
            line-height: 1.4vmax;
            font-size: 1.2vmax;
            font-family: 'Arial';
            margin-top: 15vh;
            width: 90%;
        }

        .content h2,
        h3,
        h4 {
            color: #1B80CF;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
        }

        .content h3 {
            color: #1B80CF;
        }

        .content h2::before,
        .content h3::before {
            display: block;
            content: " ";
            visibility: hidden;
            height: 50px;
            margin-top: -50px;
            pointer-events: none;
            background-color: #FFFFFF;
        }

        .content a:link,
        .content a:visited {
            color: #1B80CF;
        }

        .home a:link,
        .home a:visited {
            color: #D3D3D3;
        }

        .page {
            width: 65vw;
            height: 100%;
            margin-left: 30vw;
            overflow: hidden;
            padding: 0 1em;
            font-family: 'Arial';
        }

        .page img {
            max-width: 100%;
            max-height: 100%;
        }

        @media (max-width: 640px),
        screen and (orientation: portrait) {
            body {
                max-width: 100%;
                max-height: 100%;
            }

            #sidebar {
                position: fixed;
                background-color: #1B80CF;
                top: 0;
                left: 0;
                bottom: 80vh;
                width: 100vw;
            }

            #sidebar .title {
                text-align: center;
                position: fixed;
                margin-top: 6vh;
                left: 0px;
                right: 0px;
                line-height: 3.5vmax;
                font-size: 1.5vmax;
                font-family: 'Arial';
            }

            #sidebar .subtitle {
                text-align: center;
                top: 5vh;
                left: 0px;
                right: 0px;
                position: fixed;
                margin-top: 10vh;
                font-size: 1.5vmax;
            }

            #sidebar .title a:link,
            #sidebar .title a:visited {
                text-align: center;
                color: #FFFFFF;
            }

            #sidebar .subtitle a:link,
            #sidebar .subtitle a:visited {
                text-align: center;
                color: #FFFFFF;
            }

            .home {
                z-index: 100;
                width: 100%;
                background-color: #1B80CF;
                font-size: 1.5vmax;
            }

            .home a:link,
            .home a:visited {
                text-decoration: none;
                color: #FFFFFF;
            }

            .content {
                line-height: 3.8vmax;
                font-size: 1.8vmax;
                font-family: 'Arial';
                margin-top: 22vh;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .page {
                top: 40vh;
                width: 95%;
                margin-left: 0vw;
            }

            .page img {
                max-width: 100%;
                max-height: 100%;
                border: 0;
            }
        }

        @media print {
            #sidebar {
                width: 100%;
                top: 0;
                position: relative;
                padding-bottom: 3vh;
            }

            #sidebar .title {
                margin-top: 0;
            }

            .home {
                display: none;
            }

            .page {
                margin-left: 2vw;
                width: 100%;
                font-size: 1.2vmax;
            }

            .content {
                line-height: 1.2vmax;
                font-size: 1vmax;
            }
        }
    </style>
</head>

<body vocab="http://schema.org/">
    <div class="page">
        <div class="home">
            <ul typeof="BreadcrumbList">
                <li property="itemListElement" typeof="ListItem">
                    <a property="item" typeof="WebPage" href="../index.html">
                        <span property="name">Home</span>
                    </a>
                </li>
                <li property="itemListElement" typeof="ListItem">
                    <a property="item" typeof="WebPage" href="../research/research.html">
                        <span property="name">Research</span>
                    </a>
                </li>
            </ul>
        </div>
        <div id="sidebar">
            <div class="title">
                <h1><a href="./data-analysis.html">Introduction to Data Mining</a></h1>
            </div>
            <div class="subtitle">
                <h3><a href="../about.html">John Samuel</a></h3>
            </div>
        </div>
        <div class="content">
            <p style="color:red;text-align:center">NOTE: Article in Progress</p>
            <p>From data to insights</p>
            <h2>1. Lifecycle of data</h2>
            <p>Lifecycle of data: data, knowledge, insights, action</p>
            <p>Data to knowledge</p>
            <ol>
                <li>Data acquisition</li>
                <li>Data Extraction</li>
                <li>Data Cleaning</li>
                <li>Data Transformation</li>
                <li>ETL</li>
                <li>Data analysis modeling</li>
                <li>Data Storage</li>
                <li>Analysis</li>
                <li>Visualisation</li>
            </ol>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/dataanalysissteps.svg" height="400vh"
                    width="600vw" />
                <figcaption style="text-align:center">Major steps of data analysis</figcaption>
            </figure>
            <p>Knowledge to Insights</p>
            <p>Knowledge to Knowledge</p>
            <p>Insights to Action</p>
            <p>Insights to Knowledge</p>
            <p>Action to Data</p>
            <p>Data analysis</p>
            <p>Data visualisation</p>
            <p>finding known and unknown patterns from data</p>
            <h4>ETL (Extraction Transformation and Loading)</h4>
            <ol>
                <li>Data Extraction</li>
                <li>Data Cleaning</li>
                <li>Data Transformation</li>
                <li>Loading data to information stores</li>
            </ol>
            <h2>2. Data Acquistion and Storage</h2>
            <h3>2.1. Data acquisition</h3>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/CashRegister.svg" height="200vh"
                    width="300vw" />
                <img src="../../fr/enseignement/cours/2017/BigData/images/640px-Surveillance_video_cameras,_Gdynia.jpeg"
                    height="200vh" width="300vw" />
                <img src="../../fr/enseignement/cours/2017/BigData/images/2013-10-14_12_27_21_Nimbus_electronic_temperature_sensor_housing.JPG"
                    height="200vh" width="300vw" />
                <img src="../../fr/enseignement/cours/2017/BigData/images/Socialmedia-pm.png" height="200vh"
                    width="300vw" />
            </figure>
            <ol>
                <li>Surveys</li>
                <ul>
                    <li>Manual surveys</li>
                    <li>Online surveys</li>
                </ul>
                <li>Sensors<sup><a href="#sensors">1</a></sup></li>
                <ul>
                    <li>Temperature, pressure, humidity, rainfall</li>
                    <li>Acoustic, navigation</li>
                    <li>Proximity, presence sensors</li>
                </ul>
                <li>Social networks</li>
                <li>Video surveillance cameras</li>
                <li>Web</li>
            </ol>
            <ol>
                <li id="sensors"><a
                        href="https://en.wikipedia.org/wiki/List_of_sensors">https://en.wikipedia.org/wiki/List_of_sensors</a>
                </li>
            </ol>
            <h3>2.2. Data storage formats</h3>
            <ul>
                <li>Binary and Textual Files</li>
                <li>CSV/TSV</li>
                <li>XML</li>
                <li>JSON</li>
                <li>Media (Images/Audio/Video)</li>
            </ul>
            <h3>2.3 Types of data stores</h3>
            <ol>
                <li>Structured data stores
                    <ul>
                        <li>Relational databases</li>
                        <li>Object-oriented databases</li>
                    </ul>
                </li>
                <li>Unstructured data stores
                    <ul>
                        <li>Filesystems</li>
                        <li>Content-management systems</li>
                        <li>Document collections</li>
                    </ul>
                </li>
                <li>Semi-structured data stores
                    <ul>
                        <li>Filesystems</li>
                        <li>NoSQL data stores</li>
                    </ul>
                </li>
            </ol>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/datarepresentation.svg" height="350vh"
                    width="600vw" />
                <figcaption style="text-align:center">Unstructured vs. Structured vs. Semi-structured</figcaption>
            </figure>
            <p>NoSQL versus SQL<sup><a href="#sqlnosql">5</a>: no strict schemas and no horizontal scaling for NoSQL
                    data stores.</sup>
            </p>
            <h4>2.3.1. ACID Transactions<sup>1</sup></h4>
            <ul>
                <li><b>Atomicity</b>: Each transaction must be "all or nothing".</li>
                <li><b>Consistency</b>: Any transaction must bring database from one valid state to another.</li>
                <li><b>Isolation</b>: Both concurrent execution and sequential execution of transactions must bring the
                    database to same state.</li>
                <li><b>Durability</b>: Irrespective of power losses, crashes, a transaction once committed to the
                    database must remain in that state.</li>
            </ul>
            <ol>
                <li>https://en.wikipedia.org/wiki/ACID</li>
            </ol>
            <ul>
                <li>Ensure validity of databases even in case of errors, power failures</li>
                <li>Important in banking sector</li>
            </ul>
            <h4>2.3.2. Types of data stores</h4>
            <ul>
                <li>Relational databases</li>
                <li>Object-oriented databases</li>
                <li>NoSQL (Not only SQL) data stores</li>
                <li>NewSQL</li>
            </ul>
            <h4>2.3.3. NoSQL</h4>
            <ul>
                <li>Comprises consistency</li>
                <li>Focus on availability and speed</li>
            </ul>
            <h4>2.3.4. Types of NoSQL stores</h4>
            <ul>
                <li>Column-oriented database</li>
                <li>Document-oriented database</li>
                <li>Key-value database</li>
                <li>Graph-oriented database</li>
            </ul>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/Column_(data_store).png" height="180vh" />
                <img src="../teaching/courses/2017/DataMining/images/Basex-6.6.3-GUI.png" height="180vh" />
            </figure>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/KeyValue.PNG" height="180vh" />
                <img src="../../fr/enseignement/cours/2017/BigData/images/GraphDatabase_PropertyGraph.png"
                    height="180vh" />
            </figure>
            <h2>3. Data Extraction and Integration</h2>
            <h3>3.1. Data extraction techniques</h3>
            <ul>
                <li>Data dumps
                    <ul>
                        <li>Downloading complete data dumps</li>
                        <li>Downloading selective data dumps</li>
                    </ul>
                </li>
                <li>Periodical polling of data feeds (e.g., blogs, news feeds)</li>
                <li>Data streams
                    <ul>
                        <li>Subscrbing to data streams (push notifications)</li>
                    </ul>
                </li>
            </ul>
            <h3>3.2. Query interfaces</h3>
            <ul>
                <li>Query endpoints supporting declarative languages
                    <ul>
                        <li>SQL</li>
                        <li>SPARQL</li>
                    </ul>
                </li>
                <li>Automated Manual search (and filter) options</li>
            </ul>
            <h3>3.3. Crawlers for web pages</h3>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/WebCrawlerArchitecture.svg.png"
                    height="300vh" />
                <figcaption>Web crawlers: navigating the entire using hyperlinks</figcaption>
            </figure>
            <h3>3.4. Application Programming Interface (API)</h3>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/Apisssss.gif" height="400vh" />
                <figcaption>API (Interface de programmation)</figcaption>
            </figure>
            <ul>
                <li>Web operations (CRUD) to manipulate externally managed resources</li>
                <li>Requires programmers to develop wrappers for web service integration</li>
            </ul>
            <h2>4. Pre-treatement of Data</h2>
            <h3>4.1 Data Cleaning: Types of Errors</h3>
            <ul>
                <li>Syntactical errors</li>
                <li>Semantical errors</li>
                <li>Data coverage errors</li>
            </ul>
            <h4>4.1.1. Syntactical errors</h4>
            <ul>
                <li>Lexical errors (e.g., user entered a string instead of a number)</li>
                <li>Data format errors (e.g, order of last name, first name)</li>
                <li>Irregular data errors (e.g., usage of different metrics)</li>
            </ul>
            <h4>4.1.2. Semantic errors</h4>
            <ul>
                <li>Violation of integrity constraints</li>
                <li>Contradiction</li>
                <li>Duplication</li>
                <li>Invalid data (unable to detect despite presence of triggers and integrity constraints)</li>
            </ul>
            <h4>4.1.3. Coverage errors</h4>
            <ul>
                <li>Missing values</li>
                <li>Missing data</li>
            </ul>
            <h3>4.2. Data Cleaning: Handling Errors</h3>
            <h4>4.2.1. Handling Syntactical errors</h4>
            <ul>
                <li>Validation using schema (e.g., XSD, JSONP)</li>
                <li>Data transformation</li>
            </ul>
            <h4>4.2.2. Handling Semantic errors</h4>
            <ul>
                <li>Duplicate elimination using techniques like specifying integrity constraints like functional
                    dependencies</li>
            </ul>
            <h4>4.2.3. Handling Coverage errors</h4>
            <ul>
                <li>Interpolation techniques</li>
                <li>External data sources</li>
            </ul>
            <h4>4.2.4. Administrators and handling errors</h4>
            <ul>
                <li>User feedback</li>
                <li>Alerts and triggers</li>
            </ul>
            <h2>5. Data Transformation</h2>
            <h3>Languages</h3>
            <ul>
                <li>Template languages</li>
                <li>XSLT</li>
                <li>AWK</li>
                <li>Sed</li>
                <li>Programming languages like PERL</li>
            </ul>
            <h2>6. ETL</h2>
            <h3>6.1. ETL (Extraction Transformation and Loading)</h3>
            <ol>
                <li>Data Extraction</li>
                <li>Data Cleaning</li>
                <li>Data Transformation</li>
                <li>Loading data to information stores</li>
            </ol>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/ETL.svg" height="400vh" width="600vw" />
                <figcaption style="text-align:center">ETL (Extraction, Transformation and Loading)</figcaption>
            </figure>
            <h4>6.2.1. Models for data analysis</h4>
            <ul>
                <li>Multidimensional data analysis</li>
                <li>Dimensions
                    <ul>
                        <li>Attributes</li>
                        <li>Levels</li>
                        <li>Hierarchies</li>
                    </ul>
                </li>
                <li>Facts
                    <ul>
                        <li>Measures</li>
                    </ul>
                </li>
            </ul>
            <ul>
                <li>Multidimensional data analysis: Examples</li>
                <ul>
                    <li>Dimensions (e.g.Spatio-temporal dimensions, Product)
                        <ul>
                            <li>Attributes (e.g. Name, Manufactures etc.)</li>
                            <li>Levels (e.g., Day, Month, Quarter, Store, City, Country etc.)</li>
                            <li>Hierarchies (e.g., Day-Month-Quarter-Year, Store-City-Country etc.)</li>
                        </ul>
                    </li>
                    <li>Facts
                        <ul>
                            <li>Measures (e.g., Number of products sold/unsold)</li>
                        </ul>
                    </li>
                </ul>
            </ul>
            <h4>6.2.2. Star Schema</h4>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Star-schema-example.png" height="400vh" />
                <figcaption></figcaption>
            </figure>
            <h4>6.2.3. Data Cubes</h4>
            <ul>
                <li>Data cubes for online analytical processing (OLAP)</li>
                <li>OLAP Cube operations
                    <ul>
                        <li>Slice</li>
                        <li>Dice</li>
                        <li>Drill up/down</li>
                        <li>Pivot</li>
                    </ul>
                </li>
            </ul>
            <h4>6.2.4. Snow Schema</h4>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/800px-Snowflake-schema-example.png"
                    height="400vh" />
                <figcaption></figcaption>
            </figure>
            <h3>6.2. ETL: From one data store to another</h3>
            <ul>
                <ul>
                    <li>From: Data sources
                        <ul>
                            <li>Internal or external databases</li>
                            <li>Web Services</li>
                        </ul>
                    </li>
                    <li>To: Data warehouses
                        <ul>
                            <li>Enterprise warehouses</li>
                            <li>Web warehouses</li>
                        </ul>
                    </li>
                </ul>
            </ul>
            <h2>7. Data Analysis</h2>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/640px-OpenOffice.org_Calc.png" height="400vh" />
                <figcaption style='text-align:center'>1.1.3. Data analysis</figcaption>
            </figure>
            <h3>Activities of data analysis</h3>
            <ol>
                <li>Retrieving values</li>
                <li>Filter</li>
                <li>Compute derived values</li>
                <li>Find extremum</li>
                <li>Sort</li>
                <li>Determine range</li>
                <li>Characterize distribution</li>
                <li>Find analysis</li>
                <li>Cluster</li>
                <li>Correlate</li>
                <li>Contextualization</li>
            </ol>
            <ol>
                <li id="analysis"><a
                        href="https://en.wikipedia.org/wiki/Data_analysis">https://en.wikipedia.org/wiki/Data_analysis</a>
                </li>
            </ol>
            <h2>8. Data Visualization</h2>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/Intersection_over_Union_-_object_detection_bounding_boxes.jpg"
                    height="300vh" />
                <img src="../../fr/enseignement/cours/2017/BigData/images/800px-Eyetracking_heat_map_Wikipedia.jpg"
                    height="300vh" />
            </figure>
            <h3>8.1. Types of Data Visualization</h3>
            <ol>
                <li>Time-series</li>
                <li>Ranking</li>
                <li>Part-to-whole</li>
                <li>Deviation</li>
                <li>Sort</li>
                <li>Frequency distribution</li>
                <li>Correlation</li>
                <li>Nominal comparison</li>
                <li>Geographic or geospatial</li>
            </ol>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/545px-Temperatures_across_the_world_in_the_1880s_and_the_1980s.jpg"
                    height="300vh" />
                <img src="../../fr/enseignement/cours/2017/BigData/images/Second_Generation_Thermal_Counter.png"
                    height="300vh" />
            </figure>
            <ol>
                <li id="analysis"><a
                        href="https://en.wikipedia.org/wiki/Data_visualization">https://en.wikipedia.org/wiki/Data_visualization</a>
                </li>
            </ol>
            <h3>8.2. Data Visualization: Examples</h3>
            <ol>
                <li>Bar-chart (Nominal comparison)</li>
                <li>Pie-chart (part-to-whole)</li>
                <li>Histograms (frequency-distribution)</li>
                <li>Scatter-plot (correlation)</li>
                <li>Network</li>
                <li>Line-chart (time-series)</li>
                <li>Treemap</li>
                <li>Gantt chart</li>
                <li>Heatmap</li>
            </ol>
            <h4>Pie Chart</h4>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/piechart.png" height="360px" />
                <img src="../teaching/courses/2017/DataMining/images/flower.jpg" height="450px" />
            </figure>
            <figure>
                <img src="../slides/2018/SWIB/WDQS-ParadigmLang.png" height="450px" />
                <figcaption>Programming Language Paradigms (Bubble Chart)</figcaption>
            </figure>
            <figure>
                <img src="../slides/2018/SWIB/Screenshot_2018-11-13-Histropedia Programming languages.png"
                    height="450px" />
                <figcaption>Timeline of Programming Languages (using Histropedia)</figcaption>
            </figure>
            <figure>
                <img src="../teaching/courses/2018/DataMining/Screenshot_2019-03-06 Wikidata Query Service.png"
                    height="450px" />
                <figcaption>Influence Graph of Programming Languages</figcaption>
            </figure>
            <h4>k Predominant colours</h4>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/kmeans.png" height="450px" />
                <img src="../teaching/courses/2017/DataMining/images/flower.jpg" height="450px" />
            </figure>
            <h4>RGB Scatter plots (Comparison)</h4>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/kmeansminibatchcomparison.png" height="450px" />
                <img src="../teaching/courses/2017/DataMining/images/flower.jpg" height="450px" />
            </figure>
            <h2>9. Patterns</h2>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="300px"
                    width="300px" />
                <img src="../teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg"
                    height="300px" width="300px" />
                <img src="../teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="300px"
                    width="300px" />
                <img src="../teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG"
                    height="300px" width="300px" />
            </figure>
            <h3>9.1. Patterns in Nature</h3>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/320px-Kittyply_edit1.jpg" height="150px"
                    width="150px" />
                <img src="../teaching/courses/2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG"
                    height="150px" width="150px" />
                <img src="../teaching/courses/2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="150px"
                    width="150px" />
                <img src="../teaching/courses/2017/DataMining/images/Rio_Negro_meanders.JPG" height="150px"
                    width="150px" />
            </figure>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="150px"
                    width="150px" />
                <img src="../teaching/courses/2017/DataMining/images/320px-Apis_florea_nest_closeup2.jpg" height="150px"
                    width="150px" />
                <img src="../teaching/courses/2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg"
                    height="150px" width="150px" />
                <img src="../teaching/courses/2017/DataMining/images/320px-Equus_grevyi_(aka).jpg" height="150px"
                    width="150px" />
            </figure>
            <ul>
                <li>Symmetry</li>
                <li>Trees, Fractals</li>
                <li>Spirals</li>
                <li>Chaos</li>
                <li>Waves</li>
                <li>Bubbles, Foam</li>
                <li>Tesselations</li>
                <li>Cracks</li>
                <li>Spots, stripes</li>
            </ul>
            <h3>9.2. Patterns by Humans</h3>
            <ul>
                <li>Buildings (Symmetry)</li>
                <li>Cities</li>
                <li>Virtual environments (e.g., video games)</li>
                <li>Human artifacts</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Roman_geometric_mosaic.jpg" height="300px"
                    width="600px" />
            </figure>
            <h4>Pattern creation</h4>
            <ul>
                <li>Repitition</li>
                <li>Fractals
                    <ul>
                        <li>Julia set: <i>f(z) = z<sup>2</sup> + c</i></li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/400px-Tiling_Dual_Semiregular_V3-3-3-3-6_Floret_Pentagonal.svg.png"
                    height="300px" width="300px" />
                <img src="../teaching/courses/2017/DataMining/images/Finite_subdivision_of_a_radial_link.png"
                    height="300px" width="300px" />
                <img src="../teaching/courses/2017/DataMining/images/Julia_set_(indigo).png" height="300px"
                    width="300px" />
            </figure>
            <h4>Synonyms</h4>
            <ul>
                <li>Pattern recognition</li>
                <li>Knowledge discovery in databases</li>
                <li>Data mining<sup><a href="#mining">2</a></sup></li>
                <li>Machine learning</li>
            </ul>
            <p>
                Data mining trends<sup><a href="#mining">2</a></sup> future (2007)<sup><a href="#trends">3</a></sup>
                finding patterns in data<sup><a href="#patterns">4</a></sup>
            </p>
            <h4>Pattern Recognition</h4>
            <ul>
                <li>Goal is to detect patterns and regularities in data</li>
                <li>Approaches
                    <ol>
                        <li><b>Supervised learning</b>: Availability of labeled training data</li>
                        <li><b>Unsupervised learning</b>: No labeled training data available</li>
                        <li><b>Semi-supervised learning</b>: Small set of labeled training data and a large amount of
                            unlabeled data</li>
                        <li><b>Self-supervised learning</b>: automated generation of labels for training</li>
                    </ol>
                </li>
            </ul>
            <h4>Formalization</h4>
            <ul>
                <li><b>Euclidean vector</b>: geometric object with magnitude and direction</li>
                <li><b>Vector space</b>: collection of vectors that can be added together and multiplied by numbers.
                </li>
                <li><b>Feature vector</b>: n-dimensional vector</li>
                <li><b>Feature space</b>: Vector space associated with the vectors</li>
            </ul>
            <h3>Examples: Features</h3>
            <ul>
                <li><b>Images</b>: pixel values.</li>
                <li><b>Texts</b>: Frequency of occurence of textual phrases.</li>
            </ul>
            <h4>Formalization</h4>
            <ul>
                <li><b>Feature construction<sup>1</sup></b>: construction of new features from already available
                    features</li>
                <li><b>Feature construction operators</b>
                    <ul>
                        <li>Equality operators, arithmetic operators, array operators (min, max, average etc.)...</li>
                    </ul>
                </li>
            </ul>
            <h3>Example</h3>
            <ul>
                <li>Let <b>Year of Birth</b> and <b>Year of Death</b> be two existing features.</li>
                <li>A new feature called <b>Age</b> = Year of Birth - Year of Death</li>
            </ul>
            <ol style="font-size:2vh">
                <li>https://en.wikipedia.org/wiki/Feature_vector</li>
            </ol>
            <h4>Formalization: Supervised learning</h4>
            <ul>
                <li>Let <i><b>N</b></i> be the number of training examples</li>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                <li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub><b>N</b></sub>, y<sub><b>N</b></sub>)</i>} be the
                    <i><b>N</b></i> training examples, where
                    <ul>
                        <li><i>x<sub>i</sub></i> is the feature vector of <i>i<sup>th</sup></i> training example.</li>
                        <li><i>y<sub>i</sub></i> is its label.</li>
                    </ul>
                </li>
                <li>The goal of supervised learning algorithm is to find <i>g: <b>X</b> &#8594; <b>Y</b></i>, where
                    <ul>
                        <li><i>g</i> is one of the functions from the set of possible functions <i>G</i> (hypotheses
                            space)</li>
                    </ul>
                </li>
                <li><b>Scoring function <i>F</i></b> denote the space of scoring functions, where
                    <ul>
                        <li><i>f: <b>X</b> &#215; <b>Y</b> &#8594; <b>R</b></i> such that <i>g</i> returns the highest
                            scoring function.</li>
                    </ul>
                </li>
            </ul>
            <h4>Formalization: Unsupervised learning</h4>
            <ul>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                <li>The goal of unsupervised learning algorithm is to
                    <ul>
                        <li>find mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
                    </ul>
                </li>
            </ul>
            <h4>Formalization: Semi-supervised learning</h4>
            <ul>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                <li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>l</sub>)</i>} be the
                    <i><b>l</b></i> be the set of labeled training examples
                </li>
                <li>Let {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>} be the <i><b>u</b></i> be the set of unlabeled
                    feature vectors of <i><b>X</b></i>.</li>
                <li>The goal of semi-supervised learning algorithm is to do
                    <ul>
                        <li><b>Transductive learning</b>, i.e., find correct labels for
                            {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>}. OR</li>
                        <li><b>Inductive learning</b>, i.e., find correct mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
                    </ul>
                </li>
            </ul>
            <h2>10. Data Mining</h2>
            <h4>Tasks in Data Mining</h4>
            <ol>
                <li>Classification</li>
                <li>Clustering</li>
                <li>Regression</li>
                <li>Sequence Labeling</li>
                <li>Association Rules</li>
                <li>Anomaly Detection</li>
                <li>Summarization</li>
            </ol>
            <h3>10.1. Classification</h3>
            <ul>
                <li>Generalizing known structure to apply to new data</li>
                <li>Identifying the set of categories to which an object belongs</li>
                <li>Binary vs. Multiclass classification</li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Spam vs Non-spam</li>
                <li>Document classification</li>
                <li>Handwriting recognition</li>
                <li>Speech Recognition</li>
                <li>Internet Search Engines</li>
            </ul>
            <h4>Formal definition</h4>
            <ul>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                <li>The goal of classification algorithm (or classifier) is to find {
                    <i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>k</sub>)</i>}, i.e., assigning a known
                    label to every input feature vector, where
                    <ul>
                        <li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
                        <li><i>y<sub>i</sub> &#8712; <b>Y</b></i></li>
                        <li>|<i><b>X</b> </i>| <i>= l</i></li>
                        <li>|<i><b>Y</b> </i>| <i>= k</i></li>
                        <li>l &gt;= k</li>
                    </ul>
                </li>
            </ul>
            <h4>Classifiers</h4>
            <ul>
                <li>Classifying Algorithm</li>
                <li>Two types of classifiers:
                    <ul>
                        <li><b>Binary classifiers</b> assigning an object to any of two classes</li>
                        <li><b>Multiclass classifiers</b> assigning an object to one of several classes</li>
                    </ul>
                </li>
            </ul>
            <h4>Linear Classifiers</h4>
            <ul>
                <li>A linear function assigning a score to each possible category by combining the feature vector of an
                    instance with a vector of weights, using a dot product.</li>
                <li>Formalization:
                    <ul>
                        <li>Let <i><b>X</b></i> be the input feature space and <i><b>x</b><sub>i</sub> &#8712;
                                <b>X</b></i></li>
                        <li>Let <i><b>&#946;</b><sub>k</sub></i> be vector of weights for category <i>k</i></li>
                        <li><i>score(<b>x</b><sub>i</sub>, k) = <b>x</b><sub>i</sub>.<b>&#946;</b><sub>k</sub></i>,
                            score for assigning category <i>k</i> to instance <i><b>x</b><sub>i</sub></i>. The category
                            that gives the highest score is assigned as the
                            category of the instance.</li>
                    </ul>
                </li>
            </ul>
            <h3>Classifiers</h3>
            <figure>
                <img src="../teaching/courses/2018/DataMining/positivenegative.svg" height="500px" width="500px" />
            </figure>
            <figure>
                <img src="../teaching/courses/2018/DataMining/Precisionrecall.svg" height="500px" width="500px" />
            </figure>
            <p>Let</p>
            <ul>
                <li><i>tp</i>: number of true postives</li>
                <li><i>fp</i>: number of false postives</li>
                <li><i>fn</i>: number of false negatives</li>
            </ul>
            <p>Then</p>
            <ul>
                <li>Precision <i>p = tp / (tp + fp)</i></li>
                <li>Recall <i>r = tp / (tp + fn)</i></li>
                <li>F1-score <i>f1 = 2 * ((p * r) / (p + r))</i></li>
            </ul>
            <figure>
                <img src="../teaching/courses/2019/DataMining/confusionmatrix.png" height="400px" />
                <figcaption>Confusion Matrix for a SVM classifier of handwritten digits (MNIST)</figcaption>
            </figure>
            <figure>
                <img src="../teaching/courses/2019/MachineLearning/multiclassclassifier.svg" height="400px" />
                <figcaption>Multiclass classification</figcaption>
            </figure>
            <figure>
                <img src="../teaching/courses/2019/MachineLearning/onevsall.svg" height="400px" />
                <figcaption>One-vs.-rest strategy for Multiclass classification</figcaption>
            </figure>
            <figure>
                <img src="../teaching/courses/2019/MachineLearning/onevsone.svg" height="400px" />
                <figcaption>One-vs.-one strategy for Multiclass classification</figcaption>
            </figure>
            <h3>10.2. Clustering</h3>
            <ul>
                <li>Discovering groups and structures in the data without using known structures in the data</li>
                <li>Objects in a cluster are more similar to each other than the objects in the other cluster</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/320px-Cluster-2.svg.png" height="300px"
                    width="300px" />
            </figure>
            <h4>Applications</h4>
            <ul>
                <li>Social network analysis</li>
                <li>Image segmentation</li>
                <li>Recommender systems</li>
                <li>Grouping of shopping items</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/K_Means_Example_Step_4.svg.png" height="300px"
                    width="300px" />
                <img src="../teaching/courses/2017/DataMining/images/Iris_dendrogram.png" height="300px"
                    width="300px" />
            </figure>
            <h4>Formal definition</h4>
            <ul>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>The goal of clustering is to find k subsets of <i><b>X</b></i>, in such a way that
                    <ul>
                        <li><i>C<sub>1</sub>.. &#8746; ..C<sub>k</sub> &#8746; C<sub>outliers</sub> = <b>X</b> </i> and
                        </li>
                        <li><i>C<sub>i</sub> &#8745; C<sub>j</sub> = &#981;, i &#8800; j; 1 &lt;i,j &lt;k</i></li>
                        <li><i>C<sub>outliers</sub></i> may consist of outlier instances (data anomaly)</li>
                    </ul>
                </li>
            </ul>
            <h4>Cluster models</h4>
            <ul>
                <li><b>Centroid models</b>: cluster represented by a single mean vector</li>
                <li><b>Connectivity models</b>: distance connectivity</li>
                <li>Distribution models: clusters modeled using statistical distributions</li>
                <li>Density models: clusters as connected dense regions in the data space</li>
                <li>Subspace models</li>
                <li>Group models</li>
                <li>Graph-based models</li>
                <li>Neural models</li>
            </ul>
            <h3>10.3. Regression</h3>
            <ul>
                <li>Finding a function which models the data</li>
                <li>Assigns a real-valued output to each input</li>
                <li>Estimating the relationships among variables</li>
                <li>Relationship between a dependent variable ('criterion variable') and one or more independent
                    variables ('predictors').</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Linear_regression.svg" height="400px"
                    width="400px" />
            </figure>
            <h4>Applications</h4>
            <ul>
                <li>Prediction</li>
                <li>Forecasting</li>
                <li>Machine learning</li>
                <li>Finance</li>
            </ul>
            <h4>Formal definition</h4>
            <ul>
                <li>A function that maps a data item to a prediction variable</li>
                <li>Let <i><b>X</b></i> be the independent variables</li>
                <li>Let <i><b>Y</b></i> be the dependent variables</li>
                <li>Let <i><b>&#946;</b></i> be the unknown parameters (scalar or vector)</li>
                <li>The goal of regression model is to approximate <i><b>Y</b></i> with <i><b>X</b>,<b>&#946;</b></i>,
                    i.e.,
                    <ul>
                        <li><i><b>Y</b> &#8773; f(<b>X</b>,<b>&#946;</b>)</i></li>
                    </ul>
                </li>
            </ul>
            <h4>Linear regression</h4>
            <ul>
                <li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> +
                        &#949;<sub>i</sub></i> OR</li>
                <li>parabola: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> +
                        &#946;<sub>1</sub>x<sub>i</sub><sup>2</sup> +&#949;<sub>i</sub></i></li>
            </ul>
            <h4>Linear regression</h4>
            <ul>
                <li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> +
                        &#949;<sub>i</sub></i> OR</li>
                <li><i>ŷ<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub><sub>i</sub></i> OR</li>
                <li>Residual: <i>e<sub>i</sub> = ŷ<sub>i</sub> - y<sub>i</sub></i></li>
                <li>Sum of squared residuals, SSE = &#931; e<sub>i</sub>, where 1 &lt; i &lt; n</li>
                <li>The goal is to minimize SSE</li>
            </ul>
            <h3>10.4. Sequence Labeling</h3>
            <ul>
                <li>Assigning a class to each member of a sequence of values </li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Part of speech tagging</li>
                <li>Linguistic translation</li>
                <li>Video analysis</li>
                <li>Handwriting recognition</li>
                <li>Information extraction</li>
            </ul>
            <h4>Formal definition</h4>
            <ul>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                <li>Let <i>&#12296;x<sub>1</sub>,...,x<sub>T</sub>&#12297;</i> be a sequence of length <i>T</i>.</li>
                <li>The goal of sequence labeling is to generate a corresponding sequnce
                    <ul>
                        <li><i>&#12296;y<sub>1</sub>,...,y<sub>T&#12297;</sub></i> of labels</li>
                        <li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
                        <li><i>y<sub>j</sub> &#8712; <b>Y</b></i></li>
                    </ul>
                </li>
            </ul>
            <h3>10.5. Association Rules</h3>
            <h4>Association Rules</h4>
            <ul>
                <li>Searches for relationships between variables</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/associationruletable.png" height="400px"
                    width="400px" />
            </figure>
            <h4>Applications</h4>
            <ul>
                <li>Web usage mining</li>
                <li>Intrusion detection</li>
                <li>Affinity analysis</li>
            </ul>
            <h4>Formal definition</h4>
            <ul>
                <li>Let <i><b>I</b></i> be a set of <i>n</i> binary attributes called items</li>
                <li>Let <i><b>T</b></i> be a set of <i>m</i> transactions called database</li>
                <li>Let <i><b>I</b></i> = {<i>(i<sub>1</sub>,...,i<sub>n</sub>)</i>} and <i><b>T</b></i> =
                    {<i>(t<sub>1</sub>,...,t<sub>m</sub>)</i>}</li>
                <li>The goal of association rule learning is to find
                    <ul>
                        <li><i><b>X</b> &#8658; <b>Y</b></i>, where <i><b>X</b> &#8658; <b>Y</b> &#8838; <b>I</b></i>
                        </li>
                        <li><i><b>X</b></i> is the antecedent</li>
                        <li><i><b>Y</b></i> is the consequent</li>
                    </ul>
                </li>
            </ul>
            <h4>Formal definition</h4>
            <ul>
                <li>Support: how frequently an itemset appears in the database
                    <ul>
                        <li><i>supp(<b>X</b>) = |t &#8712;<b>T</b></i>; <i><b>X</b> &#8838; t| / |<b>T</b>|</i></li>
                    </ul>
                </li>
                <li>Confidence: how frequently the rule has been found to be true.
                    <ul>
                        <li><i>conf(<b>X</b> &#8658; <b>Y</b>) = supp(<b>X</b> &#8746; <b>Y</b>)/supp(<b>X</b>)</i></li>
                    </ul>
                </li>
                <li>Lift: the ratio of the observed support to that of the expected if X and Y were independent
                    <ul>
                        <li><i>lift(<b>X</b> &#8658; <b>Y</b>) = supp(<b>X</b> &#8746; <b>Y</b>)/(supp(<b>X</b>)
                                &#10761; supp(<b>Y</b>))</i></li>
                    </ul>
                </li>
            </ul>
            <h4>Example</h4>
            <ul>
                <li>{<b>bread</b>, <b>butter</b>} &#8658; {<b>milk</b>}</li>
            </ul>
            <h3>10.6. Anomaly Detection</h3>
            <ul>
                <li>Identification of unusual data records</li>
                <li>Approaches
                    <ol>
                        <li>Unsupervised anomaly detection</li>
                        <li>Supervised anomaly detection</li>
                        <li>Semi-supervised anomaly detection</li>
                    </ol>
                </li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Intrusion detection</li>
                <li>Fraud detection</li>
                <li>Remove anomalous data</li>
                <li>System health monitoring</li>
                <li>Event detection in sensor networks</li>
                <li>Misuse detection</li>
            </ul>
            <h4>Characteristics</h4>
            <ul>
                <li>Unexpected bursts</li>
            </ul>
            <h4>Formalization</h4>
            <ul>
                <li>Let <i><b>Y</b></i> be a set of measurements</li>
                <li>Let <i>P<sub><b>Y</b></sub>(y)</i> be a statistical model for the distribution of <i><b>Y</b></i>
                    under 'normal' conditions.</li>
                <li>Let <i><b>T</b></i> be a user-defined threshold.</li>
                <li>A measurement <i>x</i> is an outlier if <i>P<sub><b>Y</b></sub>(x) &lt; <b>T</b></i></li>
            </ul>
            <h3>10.7. Summarization</h3>
            <ul>
                <li>Providing a more compact representation of the data set</li>
                <li>Report Generation</li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Keyphrase extraction</li>
                <li>Document summarization</li>
                <li>Search engines</li>
                <li>Image summarization</li>
                <li>Video summarization: Finding important events from videos</li>
            </ul>
            <h4>Formalization: Multidocument summarization</h4>
            <ul>
                <li>Let {<i><b>D</b> = D<sub>1</sub>, ..., D<sub>k</sub></i>} be a document collection of k documents
                </li>
                <li>A Document {<i>D = t<sub>1</sub>, ..., t<sub>m</sub></i>} consists of m textual units (words,
                    sentences, paragraphs etc.) </li>
                <li>Let {<i><b>D</b> = t<sub>1</sub>, ..., t<sub>n</sub></i>} be the complete set of all textual units
                    from all documents, where
                    <ul>
                        <li><i>t<sub>i</sub> &#8712; <b>D</b>,</i> if and only if <i>&#8707; D<sub>j</sub></i> such that
                            <i>t<sub>i</sub> &#8712; D<sub>j</sub></i>
                        </li>
                    </ul>
                </li>
                <li><i>S &#8838; <b>D</b></i> constitutes a summary</li>
                <li> Two scoring functions
                    <ul>
                        <li><i>Rel(i)</i>: relevance of textual unit <i>i</i> in the summary</li>
                        <li><i>Red(i,j)</i>: Redundancy between two textual units <i>t<sub>i</sub></i>, t<sub>j</sub>
                        </li>
                    </ul>
                </li>
            </ul>
            <ul>
                <li>Scoring for a summary S
                    <ul>
                        <li><i>s(S)</i> score of summary S</li>
                        <li><i>l(i)</i> is the length of the textual unit i</li>
                        <li><i>K</i> is the fixed maximum length of the summary</li>
                    </ul>
                </li>
            </ul>
            <figure>
                <img src="../teaching/courses/2018/DataMining/scoringfunction.png" height="200px" width="500px" />
            </figure>
            <ul>
                <li>Finding a subset from the entire subset</li>
                <li>Approaches
                    <ol>
                        <li><b>Extraction</b>: Selecting a subset of existing words, phrases, or sentences in the
                            original text without any modification</li>
                        <li><b>Abstraction</b>: Build an internal semantic representation and then use natural language
                            generation techniques</li>
                    </ol>
                </li>
            </ul>
            <h4>Extractive summarization</h4>
            <ul>
                <li>Approaches
                    <ol>
                        <li><b>Generic summarization</b>: Obtaining a generic summary</li>
                        <li><b>Query relevant summarization</b>: Summary relevant to a query</li>
                    </ol>
                </li>
            </ul>
            <h2>11. Algorithms</h2>
            <ol>
                <li>Support Vector Machines (SVM)</li>
                <li>Stochastic Gradient Descent (SGD)</li>
                <li>Nearest-Neighbours</li>
                <li>Naive Bayes</li>
                <li>Decision Trees</li>
                <li>Ensemble Methods (Random Forest)</li>
            </ol>
            <h3>11.1. Support Vector Machines (SVM)</h3>
            <h4>Introduction</h4>
            <ul>
                <li>Supervised learning approach</li>
                <li>Binary classification algorithm</li>
                <li>Constructs a hyperplane ensuring the maximum separation between two classes</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/SVM Separating Hyperplanes.svg" height="500px"
                    width="500px" />
            </figure>
            <h4>Hyperplane</h4>
            <ul>
                <li>Hyperplane of n-dimensional space is a subspace of dimension <i>n-1</i></li>
                <li>Examples
                    <ul>
                        <li>Hyperplane of a 2-dimensional space is 1-dimensional line</li>
                        <li>Hyperplane of a 3-dimensional space is 2-dimensional plane</li>
                    </ul>
                </li>
            </ul>
            <h4>Formal definition</h4>
            <ul>
                <li>The goal of a SVM is to estimate a function <i>f: R<sup>N</sup> &#10761; {+1,-1}</i>, i.e.,
                    <ul>
                        <li>If <i>x<sub>1</sub>,...,x<sub>l</sub></i> &#8712; <i>R<sup>N</sup></i> are the <i>N</i>
                            input data points,</li>
                        <li>the goal is to find <i>(x<sub>1</sub>,y<sub>1</sub>),...,(x<sub>l</sub>,y<sub>l</sub>)</i>
                            &#8712; <i>R<sup>N</sup> &#10761; {+1,-1}</i></li>
                    </ul>
                </li>
                <li>Any hyperplane can be written by the equation using set of input points <i><b>x</b></i>
                    <ul>
                        <li><i><b>w</b>.<b>x</b> - b = 0</i>, where</li>
                        <li><i><b>w</b> &#8712; R<sup>N</sup></i>, a normal vector to the plane</li>
                        <li><i>b &#8712; R</i></li>
                    </ul>
                </li>
                <li>A decision function is given by <i>f(x) = sign(<b>w</b>.<b>x</b> - b )</i>
                </li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Surface_normal_illustration.svg" height="400px"
                    width="500px" />
                <figcaption>Normal vector</figcaption>
            </figure>
            <h4>Formal definition</h4>
            <ul>
                <li>If the training data are linearly separable, two hyperplanes can be selected</li>
                <li>They separate the two classes of data, so that distance between them is as large as possible.</li>
                <li>The hyperplanes can be given by the equations
                    <ul>
                        <li><i><b>w</b>.<b>x</b> - b = 1</i></li>
                        <li><i><b>w</b>.<b>x</b> - b = -1</i></li>
                    </ul>
                </li>
                <li>The distance between the two hyperplanes can be given by <i>2/||<b>w</b>||</i></li>
                <li>Region between these two hyperplanes is called margin.</li>
                <li>Maximum-margin hyperplane is the hyperplane that lies halfway between them.</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png"
                    height="500px" width="500px" />
            </figure>
            <h4>Formal definition</h4>
            <ul>
                <li>In order to prevent data points from falling into the margin, following constraints are added
                    <ul>
                        <li><i><b>w</b>.<b>x</b><sub>i</sub> - b &gt;= 1</i>, if <i>y<sub>i</sub> = 1</i></li>
                        <li><i><b>w</b>.<b>x</b><sub>i</sub> - b &lt;= -1</i>, if <i>y<sub>i</sub> = -1</i></li>
                    </ul>
                <li><i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b) &gt;= 1</i> for 1&lt;= i &lt;= n</li>
                </li>
                <li>The goal is to minimize ||<b>w</b>|| subject to <i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b)
                        &gt;= 1</i> for 1&lt;= i &lt;= n</li>
                <li>Solving for both <i><b>w</b></i> and <i>b</i> gives our classifier
                    <i>f(x) = sign(<b>w</b>.<b>x</b> - b)</i>
                </li>
                <li>Max-margin hyperplane is completely determined by the points that lie nearest to it, called the
                    <b>support vectors</b>
                </li>
            </ul>
            <h4>Data mining tasks</h4>
            <ul>
                <li>Classification (Multi-class classification)</li>
                <li>Regression</li>
                <li>Anomaly detection</li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Text and hypertext categorization</li>
                <li>Image classification</li>
                <li>Handwriting recognition</li>
            </ul>
            <h3>11.2. Stochastic Gradient Descent (SGD)</h3>
            <ul>
                <li>A stochastic approximation of the gradient descent optimization</li>
                <li>Iterative method for minimizing an objective function that is written as a sum of differentiable
                    functions.</li>
                <li>Finds minima or maxima by iteration</li>
            </ul>
            <h4>Gradient</h4>
            <ul>
                <li>Multi-variable generalization of the derivative. </li>
                <li>Gives slope of the tangent of the graph of a function</li>
                <li>Gradient points in the direction of the greatest rate of increase of a function</li>
                <li>Magnitude of gradient is the slope of the graph in that direction</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Gradient2.svg" height="500px" width="500px" />
            </figure>
            <h4>Gradient vs Derivative</h4>
            <ul>
                <li>Derivatives defined on functions of single variable</li>
                <li>Gradient defined on functions of multiple variables</li>
                <li>Gradient is a vector-valued function (range is a vector)</li>
                <li>Derivative is a scalar-valued function</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Gradient2.svg" height="500px" width="500px" />
            </figure>
            <h4>Gradient descent</h4>
            <ul>
                <li>First-order iterative optimization algorithm for finding the minimum of a function.</li>
                <li>Finding a local minima involves taking steps proportional to the negative of the gradient of the
                    function at the current point.</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Gradient_descent.svg" height="500px"
                    width="500px" />
            </figure>
            <h4>Standard gradient descent method</h4>
            <ul>
                <li>Let's take the problem of minimizing an objective function
                    <ul>
                        <li><i>Q(w) = 1/n (&#931;Q<sub>i</sub>(w)), 1&lt;=i&lt;n</i></li>
                        <li>Summand function <i>Q<sub>i</sub></i> associated with <i>i<sup>th</sup></i> observation in
                            the data set.</li>
                    </ul>
                </li>
                <li><i>w = w - &#951;.&#8711; Q(w)</i></li>
            </ul>
            <h4>Iterative method</h4>
            <ul>
                <li>Choose an initial vector of parameters w and learning rate &#951;.</li>
                <li>Repeat until an approximate minimum is obtained:
                    <ul>
                        <li>Randomly shuffle examples in the training set.</li>
                        <li><i>w = w - &#951;.&#8711; Q<sub>i</sub>(w),</i> for i=1...n</li>
                    </ul>
                </li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Classification</li>
                <li>Regression</li>
            </ul>
            <h3>11.3. Nearest-Neighbours</h3>
            <h4>k-nearest neighbors algorithm</h4>
            <ul>
                <li>k-NN classification: output is a class membership
                    <br /> (object is classified by a majority vote of its neighbors.)
                </li>
                <li>k-NN regression: output is the property value for the object (average values of its k nearest
                    neighbors)</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/KnnClassification.svg" height="500px"
                    width="500px" />
            </figure>
            <h4>Applications</h4>
            <ul>
                <li>Regression</li>
                <li>Anomaly detection</li>
            </ul>
            <h3>11.4. Naive Bayes classifiers</h3>
            <ul>
                <li>Collection of simple probabilistic classifiers based on applying Bayes' theorem with strong
                    independence assumption between the features.</li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Document classification (spam/non-spam)</li>
            </ul>
            <h4>Bayes' Theorem</h4>
            <ul>
                <li>If A and B are events.</li>
                <li>P(A), P(B) are probabilities of observing A and B independently of each other..</li>
                <li>P(A|B) is conditional probability, the likelihood of event A occurring given that B is true</li>
                <li>P(B|A) is conditional probability, the likelihood of event B occurring given that A is true</li>
                <li> P(B) &#8800; 0</li>
                <li><i>P(A|B) = (P(B|A).P(A))/P(B)</i></li>
            </ul>
            <h3>11.5. Decision Trees</h3>
            <ul>
                <li>Decision support tool</li>
                <li>Tree-like model of decisions and their possible consequences</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Decision_tree_model.png" height="400px"
                    width="400px" />
            </figure>
            <h4>Applications</h4>
            <ul>
                <li>Classification</li>
                <li>Regression</li>
                <li>Decision Analysis: identifying strategies to reach a goal</li>
                <li>Operations Research</li>
            </ul>
            <h3>11.6. Ensemble Methods (Random Forest)</h3>
            <h4>Defintion</h4>
            <ul>
                <li>Collection of multiple learning algorithms to obtain better predictive performance than could be
                    obtained from one of the constituting algorithms alone.</li>
                <li>Random forests are obtained by building multiple decision trees at training time</li>
            </ul>
            <ul>
                <li>Multiclass classification</li>
                <li>Multilabel classification (the problem of assigning one or more label to each instance. There is no
                    limit on the number of classes an instance can be assigned to.)</li>
                <li>Regression</li>
                <li>Anomaly detection</li>
            </ul>
            <h2>12. Feature Selection</h2>
            <h4>Definition</h4>
            <ul>
                <li>Process of selecting a subset of relevant features</li>
                <li>Used in domains with large number of features and comparatively few sample points</li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Analysis of written texts</li>
                <li>Analysis of DNA microarray data</li>
            </ul>
            <h4>Formal defintion[8]</h4>
            <ul>
                <li>Let <i>X</i> be the original set of n features, i.e., |<i>X</i>| = n</li>
                <li>Let <i>w<sub>i</sub></i> be the weight assigned to feature <i>x<sub>i</sub></i>&#8712; <i>X</i></li>
                <li>Binary feature selection assigns binary weights whereas continuous feature selection assigns weights
                    preserving the order of its relevance.</li>
                <li>Let <i>J(X')</i> be an evaluation measure, defined as <i>J: X' &#8838; X &#8594; R</i></li>
                <li> Feature selection problem may be defined in three following ways
                    <ol>
                        <li>|<i>X'</i>| = <i>m &lt; n</i>. Find <i>X'</i> &#8834; <i>X</i> such that <i>J(X')</i> is
                            maximum</li>
                        <li>Choose <i>J<sub>0</sub></i>, Find <i>X'</i> &#8838; <i>X</i>, such that <i>J(X') &gt;=
                                J<sub>0</sub></i></li>
                        <li>Find a compromise among minimizing |<i>X'</i>| and maximizing <i>J(X')</i></li>
                    </ol>
                </li>
            </ul>
            <h4>Data Mining</h4>
            <h4>Goals</h4>
            <ol>
                <li>Artifical Neural Networks</li>
                <li>Deep Learning</li>
                <li>Reinforcement Learning</li>
                <li>Data Licences, Ethics and Privacy</li>
            </ol>
            <h2>13. Artificial Neural Networks</h2>
            <ul>
                <li>Inspired by biological neural networks</li>
                <li>Collection of connected nodes called artificial neurons.</li>
                <li>Artificial neurons can transmit signal from one to another (like in a synapse).</li>
                <li>Signal between artificial neurons is a real number</li>
                <li>The output of a neuron is the sum of weighted inputs.</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Colored_neural_network.svg" />
                <figcaption>Artificial neural networks</figcaption>
            </figure>
            <h4>Perceptron</h4>
            <ul>
                <li>Algorithm for supervised learning of binary classifiers</li>
                <li>Binary classifier</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Perceptron.svg" />
                <figcaption>Artificial neural networks</figcaption>
            </figure>
            <h4>Perceptron: Formal definition</h4>
            <ul>
                <li>Let <i>y = f(z)</i> be output of perceptron for an input vector <i>z</i></li>
                <li>Let <i><b>N</b></i> be the number of training examples</li>
                <li>Let <i><b>X</b></i> be the input feature space</li>
                <li>Let {<i>(x<sub>1</sub>, d<sub>1</sub>),...,(x<sub><b>N</b></sub>, d<sub><b>N</b></sub>)</i>} be the
                    <i><b>N</b></i> training examples, where
                    <ul>
                        <li><i>x<sub>i</sub></i> is the feature vector of <i>i<sup>th</sup></i> training example.</li>
                        <li><i>d<sub>i</sub></i> is the desired output value.</li>
                        <li><i>x<sub>j,i</sub></i> be the <i>i<sup>th</sup></i> feature of <i>j<sup>th</sup></i>
                            training example.</li>
                        <li><i>x<sub>j,0</sub></i> = 1</li>
                    </ul>
                </li>
            </ul>
            <ul>
                <li> Weights are represented in the following manner:
                    <ul>
                        <li><i>w<sub>i</sub></i> is the <i>i<sup>th</sup></i> value of weight vector.</li>
                        <li><i>w<sub>i</sub>(t)</i> is the <i>i<sup>th</sup></i> value of weight vector at a given time
                            t.</li>
                    </ul>
                </li>
            </ul>
            <h4>Perceptron: Steps</h4>
            <ol>
                <li>Initialize weights and threshold</li>
                <li> For each example <i>(x<sub>j</sub>, d<sub>j</sub>)</i> in training set<i></i>
                    <ul>
                        <li>Calculate the weight: <i>y<sub>j</sub>(t)=f[w(t).x<sub>j</sub>]</i></li>
                        <li>Update the weights: <i>w<sub>i</sub>(t + 1) = w<sub>i</sub>(t) +
                                (d<sub>j</sub>-y<sub>j</sub>(t))x<sub>j,i</sub></i></li>
                    </ul>
                </li>
                <li>Repeat step 2 until the iteration error <i>1/s (&#931; |d<sub>j</sub> - y<sub>j</sub>(t)|)</i> is
                    less than user-specified threshold.</li>
            </ol>
            <h4>Backpropagation</h4>
            <ul>
                <li>Backward propagation of errors</li>
                <li>Adjust the weight of neurons by calculating the gradient of the loss function</li>
                <li>Error is calculated and propagated back to the network layers</li>
            </ul>
            <h2>14. Deep Learning</h2>
            <h4>Deep neural networks</h4>
            <ul>
                <li>Multiple hidden layers between the input and output layers</li>
            </ul>
            <h4>Applications</h4>
            <ul>
                <li>Computer vision</li>
                <li>Speech recognition</li>
                <li>Drug design</li>
                <li>Natural language processing</li>
                <li>Machine translation</li>
            </ul>
            <h4>Convolutional deep neural networks</h4>
            <ul>
                <li>Analysis of images</li>
                <li>Inspired by neurons in the virtual cortex</li>
                <li>Network learns the filters</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Typical_cnn.png" />
            </figure>
            <h2>15. Reinforcement Learning</h2>
            <ul>
                <li>Inspired by behaviourist psychology</li>
                <li>Actions to be taken in order to maximize the cumulative award.</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/DataMining/images/Reinforcement_learning_diagram.svg"
                    height="400vh" />
            </figure>
            <h2>16. Data Licences, Ethics and Privacy</h2>
            <ul>
                <li>Data usage licences</li>
                <li>Confidentiality and Privacy</li>
                <li>Ethics</li>
            </ul>
            <figure>
                <img src="../teaching/courses/2017/ArchitectureInformationSystems/images/Privacy_written_in_tiles.jpg"
                    height="400vh" />
                <figcaption>Privacy</figcaption>
            </figure>
            <h4>Big Data</h4>
            <ul>
                <li>Volume</li>
                <li>Variety</li>
                <li>Velocity</li>
                <li>Veracity</li>
                <li>Value</li>
            </ul>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/Open_Definition_logo.png" height="100vh" />
                <figcaption>Open Data</figcaption>
            </figure>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/LOD_Cloud_2014.svg.png" height="400vh" />
                <figcaption>Linked Open data cloud</figcaption>
            </figure>
            <figure>
                <img src="../../fr/enseignement/cours/2017/BigData/images/Internet_Archive_logo_and_wordmark.svg.png"
                    height="400vh" />
                <figcaption>Archived data</figcaption>
            </figure>
            <h3>References</h3>
            <ol>
                <li id="#course"><a href="../../en/teaching/courses/2017/DataMining/index.html">Data Mining course by
                        John Samuel (2017)</a></li>
                <li id="mining">Piatetsky-Shapiro, Gregory. “Data Mining and Knowledge Discovery 1996 to 2005:
                    Overcoming the Hype and Moving from ‘University’ to ‘Business’ and ‘Analytics.’” Data Mining and
                    Knowledge Discovery, vol. 15, no. 1, July 2007, pp. 99–105.
                    DOI.org (Crossref), doi:10.1007/s10618-006-0058-2.</li>
                <li id="trends">Kriegel, Hans-Peter, et al. “Future Trends in Data Mining.” Data Mining and Knowledge
                    Discovery, vol. 15, no. 1, July 2007, pp. 87–97. DOI.org (Crossref), doi:10.1007/s10618-007-0067-9.
                </li>
                <li id="patterns">Fayyad, Usama, et al. “Knowledge Discovery and Data Mining: Towards a Unifying
                    Framework.” Proceedings of the Second International Conference on Knowledge Discovery and Data
                    Mining, AAAI Press, 1996, pp. 82–88.</li>
                <li id="sqlnosql"><a href="https://cacm.acm.org/blogs/blog-cacm/250843-nosql-vs-sql/fulltext">NoSQL vs.
                        SQL</a></li>
            </ol>
        </div>
    </div>
</body>

</html>