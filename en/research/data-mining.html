<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <meta http-equiv="Content-Language" content="en"/>
    <link rel="shortcut icon" href="../../images/logo/favicon.png"/>
    <title>Data Mining: John Samuel</title>
    <style type="text/css">
    body{
      background-color: #FFFFFF;
    }
    #sidebar {
      position: fixed;
      background-color: #1B80CF;
      top: 0;
      left: 0;
      bottom: 0;
      width:30vw;
    }
    #sidebar .title {
      position:relative;
      text-align: center;
      line-height: 4vmax;
      font-size: 1.4vmax;
      font-family: 'Arial';
      margin-top: 25vh;
    }
    #sidebar .title a:link,
    #sidebar .title a:visited{
     color: #FFFFFF;
     text-decoration:none;
    }
    .subtitle {
      top: 50vh;
      text-align: center;
      line-height: 1.3vmax;
      font-family: 'Arial';
      font-size: 1.5vmax;
      color: #FFFFFF;
    }
    .subtitle a:link,
    .subtitle a:visited{
     color: #FFFFFF;
     text-decoration:none;
    }
    .licence {
      position:fixed;
      text-align: right;
      bottom:0;
      right:0;
    }
    .home {
     position:fixed;
     text-align: left;
     font-family: 'Arial';
     color: #D3D3D3;
     z-index:100;
     width:100%;
     background-color:#FFFFFF;
     top:0px;
     margin-bottom:10px;
     padding-bottom:10px;
    }
    .home a:link,
    .home a:visited{
     text-decoration:none;
     color: #D3D3D3;
    }
    .home ul{
      margin: 0;
      padding: 0;
      text-align: left;
      list-style:none;
    }
    .home li{
     position: relative;
     float: left;
     padding-top:15px;
     margin-right: 1em;
     font-family: 'Arial';
    }
    .home li:hover {
      display:block;
    }
    .home a:link,
    .home a:visited{
     color: #D3D3D3;
    }
    .home li:hover a:link,
    .home li:hover a:visited{
      text-decoration:none;
      padding:15px;
      color:#FFFFFF;
      background-color: #1B80CF;
    }
    .content {
     line-height: 1.8vmax;
     font-size: 1.2vmax;
     font-family: 'Arial';
     margin-top: 15vh;
     width:90%;
    }
    .content h2, h3, h4{
     color:#1B80CF;
    }
    .content a:link,
    .content a:visited{
     color: #1B80CF;
    }
    .content h3 {
     color: #1B80CF;
    }
    .content h2::before,
    .content h3::before{
       display: block;
       content : " ";
       visibility:hidden;
       height:50px;
       margin-top:-50px;
       pointer-events: none;
       background-color:#FFFFFF;
    }
    .content a:link,
    .content a:visited{
     color:#1B80CF;
    }
    .content li {
      margin:5px;
    }
    .page {
      width:65vw;
      height:100%;
      margin-left:30vw;
      overflow: hidden;
      padding: 0 1em;
      font-family: 'Arial';
    }
   .page img {
     max-width:100%;
     max-height:100%;
   }
    @media (max-width: 640px), screen and (orientation: portrait) {
      body {
        max-width:100%;
        max-height:100%;
      }
      #sidebar {
        position: fixed;
        background-color: #1B80CF;
        top: 0;
        left: 0;
        bottom: 80vh;
        width:100vw;
      }
      #sidebar .title {
        text-align: center;
        position: fixed;
        margin-top: 6vh;
        left:0px;
        right:0px;
        line-height: 3.5vmax;
        font-size: 1.5vmax;
        font-family: 'Arial';
      }
      #sidebar .subtitle {
        text-align:center;
        top: 5vh;
        left:0px;
        right:0px;
        position: fixed;
        margin-top: 10vh;
        font-size: 1.5vmax;
      }
      #sidebar .title a:link,
      #sidebar .title a:visited{
        text-align:center;
        color:#FFFFFF;
      }
      #sidebar .subtitle a:link,
      #sidebar .subtitle a:visited{
        text-align:center;
        color:#FFFFFF;
      }
      .home{
        z-index:100;
        width:100%;
        background-color:#1B80CF;
        font-size:1.5vmax;
      }
      .home a:link,
      .home a:visited{
        text-decoration:none;
        color:#FFFFFF;
      }
      .content {
        line-height: 3.8vmax;
        font-size: 1.8vmax;
        font-family: 'Arial';
        margin-top:22vh;
      }
      .content a:link,
      .content a:visited{
        color:#1B80CF;
      }
      .page {
        top: 40vh;
        width:95%;
        margin-left:0vw;
      }
      .page img {
        max-width:100%;
        max-height:100%;
        border:0;
      }
    }
    </style>
  </head>
  <body vocab="http://schema.org/">
    <div class="page">
      <div class="home">
       <ul typeof="BreadcrumbList">
        <li property="itemListElement" typeof="ListItem">
            <a property="item" typeof="WebPage" href="../index.html">
              <span property="name">Home</span>
            </a>
        </li>
        <li property="itemListElement" typeof="ListItem">
            <a property="item" typeof="WebPage" href="../research/research.html">
              <span property="name">Research</span>
            </a>
        </li>
       </ul>
      </div>
      <div id="sidebar">
        <div class="title">
          <h1><a href="./data-analysis.html">Data Mining</a></h1>
        </div>
        <div class="subtitle">
          <h3><a href="../about.html">John Samuel</a></h3>
        </div>
      </div>
      
        <p style="color:red;text-align:center">NOTE: Article in Progress</p>
        <p>From data to insights</p>
	<h2>Lifecycle of data</h2>
        <p>Lifecycle of data: data, knowledge, insights, action</p>
        <p>Data to knowledge</p>
        <ol>
	  <li>Data acquisition</li>
	  <li>Data Extraction</li>
	  <li>Data Cleaning</li>
	  <li>Data Transformation</li>
          <li>ETL</li>
	  <li>Data analysis modeling</li>
	  <li>Data Storage</li>
	  <li>Analysis</li>
	  <li>Visualisation</li>
        </ol>
        <p>Knowledge to Insights</p>
        <p>Knowledge to Knowledge</p>
        <p>Insights to Action</p>
        <p>Insights to Knowledge</p>
        <p>Action to Data</p>
        <p>Data analysis</p>
        <p>Data visualisation</p>
        <p>finding known and unknown patterns from data</p>
			<h1>1. Lifecycle of data</h1>
	            <h1>1.1.2. ETL (Extraction Transformation and Loading)</h1>
		    <ol>
			  <li>Data Extraction</li>
			  <li>Data Cleaning</li>
			  <li>Data Transformation</li>
			  <li>Loading data to information stores</li>
		    </ol>
			<h1>1. Lifecycle of data</h1>
			<h1>1. Lifecycle of data</h1>
			<h1>2. Data Acquistion and Storage</h1>
	            <h1>2.1. Data acquisition</h1>
		    <ol>
			  <li>Surveys</li>
		          <ul>
                            <li>Manual surveys</li>
                            <li>Online surveys</li>
		          </ul>
			  <li>Sensors<sup><a href="#sensors">1</a></sup></li>
		          <ul>
                            <li>Temperature, pressure, humidity, rainfall</li>
                            <li>Acoustic, navigation</li>
                            <li>Proximity, presence sensors</li>
		          </ul>
			  <li>Social networks</li>
			  <li>Video surveillance cameras</li>
			  <li>Web</li>
		    </ol>
		    <ol>
                      <li id="sensors"><a href="https://en.wikipedia.org/wiki/List_of_sensors">https://en.wikipedia.org/wiki/List_of_sensors</a></li>
		    </ol>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.2. Data storage formats</h1>
			<ul>
                           <li>Binary and Textual Files</li>
                           <li>CSV/TSV</li>
                           <li>XML</li>
                           <li>JSON</li>
                           <li>Media (Images/Audio/Video)</li>
			</ul>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.2 Types of data stores</h1>
			<ol>
                           <li>Structured data stores
                             <ul>
                               <li>Relational databases</li>
                               <li>Object-oriented databases</li>
                             </ul>
                           </li>
                           <li>Unstructured data stores
                             <ul>
                               <li>Filesystems</li>
                               <li>Content-management systems</li>
                               <li>Document collections</li>
                             </ul>
                           </li>
                           <li>Semi-structured data stores
                             <ul>
                               <li>Filesystems</li>
                               <li>NoSQL data stores</li>
                             </ul>
                           </li>
			</ol>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.3.1. ACID Transactions<sup>1</sup></h1>
			<ul>
			  <li><b>Atomicity</b>: Each transaction must be "all or nothing".</li>
			  <li><b>Consistency</b>: Any transaction must bring database from one valid state to another.</li>
			  <li><b>Isolation</b>: Both concurrent execution and sequential execution of transactions must bring the database to same state.</li>
			  <li><b>Durability</b>: Irrespective of power losses, crashes, a transaction once committed to the database must remain in that state.</li>
			</ul>
                        <ol>
                         <li>https://en.wikipedia.org/wiki/ACID</li>
                        </ol>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.3.1. ACID Transactions</h1>
			<ul>
			  <li>Ensure validity of databases even in case of errors, power failures</li>
			  <li>Important in banking sector</li>
			</ul>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.3.2. Types of data stores</h1>
			<ul>
			  <li>Relational databases</li>
			  <li>Object-oriented databases</li>
			  <li>NoSQL (Not only SQL) data stores</li>
			  <li>NewSQL</li>
			</ul>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.3.3. NoSQL</h1>
			<ul>
			  <li>Comprises consistency</li>
			  <li>Focus on availability and speed</li>
			</ul>
			<h1>2. Data Acquistion and Storage</h1>
			<h1>2.3.3. Types of NoSQL stores</h1>
			<ul>
			  <li>Column-oriented database</li>
			  <li>Document-oriented database</li>
			  <li>Key-value database</li>
			  <li>Graph-oriented database</li>
			</ul>
			<h1>3. Data Extraction and Integration</h1>
			<h1>3.1. Data extraction techniques</h1>
			<ul>
                          <li>Data dumps
			   <ul>
                            <li>Downloading complete data dumps</li>
                            <li>Downloading selective data dumps</li>
			   </ul>
                          </li>
                          <li>Periodical polling of data feeds (e.g., blogs, news feeds)</li>
                          <li>Data streams
			   <ul>
                            <li>Subscrbing to data streams (push notifications)</li>
			   </ul>
                          </li>
			</ul>
			<h1>3. Data Extraction and Integration</h1>
			<h1>3.2. Query interfaces</h1>
			<ul>
			  <li>Query endpoints supporting declarative languages
                            <ul>
			     <li>SQL</li>
			     <li>SPARQL</li>
			    </ul>
                           </li>
			  <li>Automated Manual search (and filter) options</li>
                         </ul>
			<h1>3. Data Extraction and Integration</h1>
			<h3>3.3. Crawlers for web pages</h3>
			<h1>3. Data Extraction and Integration</h1>
			<h1>3.4. Application Programming Interface (API)</h1>
			<ul>
			  <li>Web operations (CRUD) to manipulate externally managed resources</li>
                          <li>Requires programmers to develop wrappers for web service integration</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.1 Data Cleaning: Types of Errors</h1>
			<ul>
			  <li>Syntactical errors</li>
			  <li>Semantical errors</li>
			  <li>Data coverage errors</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.1.1. Syntactical errors</h1>
			<ul>
			  <li>Lexical errors (e.g., user entered a string instead of a number)</li>
			  <li>Data format errors (e.g, order of last name, first name)</li>
			  <li>Irregular data errors (e.g., usage of different metrics)</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.1.2. Semantic errors</h1>
			<ul>
			  <li>Violation of integrity constraints</li>
			  <li>Contradiction</li>
			  <li>Duplication</li>
			  <li>Invalid data (unable to detect despite presence of triggers and integrity constraints)</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.1.3. Coverage errors</h1>
			<ul>
			  <li>Missing values</li>
			  <li>Missing data</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.2.1. Handling Syntactical errors</h1>
			<ul>
			  <li>Validation using schema (e.g., XSD, JSONP)</li>
			  <li>Data transformation</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.2.2. Handling Semantic errors</h1>
			<ul>
			  <li>Duplicate elimination using techniques like specifying integrity constraints like functional dependencies</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.2.3. Handling Coverage errors</h1>
			<ul>
			  <li>Interpolation techniques</li>
			  <li>External data sources</li>
			</ul>
			<h1>4. Pre-treatement of Data</h1>
			<h1>4.2.4. Administrators and handling errors</h1>
			<ul>
			  <li>User feedback</li>
			  <li>Alerts and triggers</li>
			</ul>
			<h1>5. Data Transformation</h1>
			<h1>5.1 Languages</h1>
			<ul>
			  <li>Template languages</li>
			  <li>XSLT</li>
			  <li>AWK</li>
			  <li>Sed</li>
			  <li>Programming languages like PERL</li>
			</ul>
			<h1>6. ETL</h1>
	            <h1>6.1. ETL (Extraction Transformation and Loading)</h1>
		    <ol>
			  <li>Data Extraction</li>
			  <li>Data Cleaning</li>
			  <li>Data Transformation</li>
			  <li>Loading data to information stores</li>
		    </ol>
			<h1>6. ETL</h1>
	            <h1>6.2.1. Models for data analysis</h1>
		    <ul>
		       <li>Multidimensional data analysis</li>
		       <li>Dimensions
		         <ul>
		           <li>Attributes</li>
		           <li>Levels</li>
		           <li>Hierarchies</li>
		         </ul>
                       </li>
		       <li>Facts
		         <ul>
		           <li>Measures</li>
		         </ul>
                       </li>
		    </ul>
			<h1>6. ETL</h1>
	            <h1>6.2.1. Models for data analysis</h1>
		    <ul>
		       <li>Multidimensional data analysis: Examples</li>
		       <ul>
		         <li>Dimensions (e.g.Spatio-temporal dimensions, Product)
		           <ul>
		             <li>Attributes (e.g. Name, Manufactures etc.)</li>
		             <li>Levels (e.g., Day, Month, Quarter, Store, City, Country etc.)</li>
		             <li>Hierarchies (e.g., Day-Month-Quarter-Year, Store-City-Country etc.)</li>
		           </ul>
                         </li>
		         <li>Facts
		           <ul>
		             <li>Measures (e.g., Number of products sold/unsold)</li>
		           </ul>
                         </li>
		       </ul>
		    </ul>
			<h1>6. ETL</h1>
	            <h1>6.2.3. Star Schema</h1>
			<h1>6. ETL</h1>
	            <h1>6.2.3. Data Cubes</h1>
		    <ul>
		       <li>Data cubes for online analytical processing (OLAP)</li>
		       <li>OLAP Cube operations
                         <ul>
                           <li>Slice</li>
                           <li>Dice</li>
                           <li>Drill up/down</li>
                           <li>Pivot</li>
                         </ul>
                       </li>
		    </ul>
			<h1>6. ETL</h1>
	            <h1>6.2.4. Snow Schema</h1>
			<h1>6. ETL</h1>
	            <h1>6.2. ETL: From one data store to another</h1>
		    <ul>
		       <ul>
		         <li>From: Data sources
		           <ul>
		             <li>Internal or external databases</li>
		             <li>Web Services</li>
		           </ul>
                         </li>
		         <li>To: Data warehouses
		           <ul>
		             <li>Enterprise warehouses</li>
		             <li>Web warehouses</li>
		           </ul>
                         </li>
		       </ul>
		    </ul>
			<h1>7. Data Analysis</h1>
	            <h3>Activities of data analysis</h3>
		    <ol>
			  <li>Retrieving values</li>
			  <li>Filter</li>
			  <li>Compute derived values</li>
			  <li>Find extremum</li>
			  <li>Sort</li>
			  <li>Determine range</li>
			  <li>Characterize distribution</li>
			  <li>Find analysis</li>
			  <li>Cluster</li>
			  <li>Correlate</li>
			  <li>Contextualization</li>
		    </ol>
		    <ol>
                      <li id="analysis"><a href="https://en.wikipedia.org/wiki/Data_analysis">https://en.wikipedia.org/wiki/Data_analysis</a></li>
		    </ol>
			<h1>8. Data Visualization</h1>
	            <h3>8.1. Data Visualization</h3>
		    <ol>
			  <li>Time-series</li>
			  <li>Ranking</li>
			  <li>Part-to-whole</li>
			  <li>Deviation</li>
			  <li>Sort</li>
			  <li>Frequency distribution</li>
			  <li>Correlation</li>
			  <li>Nominal comparison</li>
			  <li>Geographic or geospatial</li>
		    </ol>
		    <ol>
                      <li id="analysis"><a href="https://en.wikipedia.org/wiki/Data_visualization">https://en.wikipedia.org/wiki/Data_visualization</a></li>
		    </ol>
			<h1>8. Data Visualization</h1>
	            <h1>8.2. Data Visualization: Examples</h1>
		    <ol>
			  <li>Bar-chart (Nominal comparison)</li>
			  <li>Pie-chart (part-to-whole)</li>
			  <li>Histograms (frequency-distribution)</li>
			  <li>Scatter-plot (correlation)</li>
			  <li>Network</li>
			  <li>Line-chart (time-series)</li>
			  <li>Treemap</li>
			  <li>Gantt chart</li>
			  <li>Heatmap</li>
		    </ol>
			<h1>1. Patterns</h1>
			<h1>Patterns in Nature</h1>
			<ul>
                           <li>Symmetry</li>
                           <li>Trees, Fractals</li>
                           <li>Spirals</li>
                           <li>Chaos</li>
                           <li>Waves</li>
                           <li>Bubbles, Foam</li>
                           <li>Tesselations</li>
                           <li>Cracks</li>
                           <li>Spots, stripes</li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Patterns by Humans</h1>
			<ul>
                           <li>Buildings (Symmetry)</li>
                           <li>Cities</li>
                           <li>Virtual environments (e.g., video games)</li>
                           <li>Human artifacts</li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Pattern creation</h1>
			<ul>
                           <li>Repitition</li>
                           <li>Fractals
                             <ul>
                              <li>Julia set: <i>f(z) = z<sup>2</sup> + c</i></li>
                             </ul>
                           </li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Synonyms</h1>
			<ul>
                           <li>Pattern recognition</li>
                           <li>Knowledge discovery in databases</li>
                           <li>Data mining</li>
                           <li>Machine learning</li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Pattern Recognition</h1>
			<ul>
                           <li>Goal is to detect patterns and regularities in data</li>
                           <li>Approaches
                             <ol>
                              <li><b>Supervised learning</b>: Availability of labeled training data</li>
                              <li><b>Unsupervised learning</b>: No labeled training data available</li>
                              <li><b>Semi-supervised learning</b>: Small set of labeled training data and a large amount of unlabeled data</li>
                             </ol>
                           </li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Formalization</h1>
			<ul>
                           <li><b>Euclidean vector</b>: geometric object with magnitude and direction</li>
                           <li><b>Vector space</b>: collection of vectors that can be added together and multiplied by numbers.</li>
                           <li><b>Feature vector</b>: n-dimensional vector</li>
                           <li><b>Feature space</b>: Vector space associated with the vectors</li>
			</ul>
			<h3>Examples: Features</h3>
			<ul>
                           <li><b>Images</b>: pixel values.</li>
                           <li><b>Texts</b>: Frequency of occurence of textual phrases.</li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Formalization</h1>
			<ul>
                           <li><b>Feature construction<sup>1</sup></b>: construction of new features from already available features</li>
                           <li><b>Feature construction operators</b>
                             <ul>
                               <li>Equality operators, arithmetic operators, array operators (min, max, average etc.)...</li>
                             </ul>
                           </li>
			</ul>
			<h3>Example</h3>
			<ul>
                           <li>Let <b>Year of Birth</b> and <b>Year of Death</b> be two existing features.</li>
                           <li>A new feature called <b>Age</b> =  Year of Birth - Year of Death</li>
			</ul>
                        <ol style="font-size:2vh">
                           <li>https://en.wikipedia.org/wiki/Feature_vector</li>
                        </ol>
			<h1>1. Patterns</h1>
			<h1>Formalization: Supervised learning</h1>
			<ul>
                           <li>Let <i><b>N</b></i> be the number of training examples</li>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub><b>N</b></sub>, y<sub><b>N</b></sub>)</i>} be the <i><b>N</b></i> training examples, where
                            <ul>
                             <li><i>x<sub>i</sub></i> is the feature vector of <i>i<sup>th</sup></i> training example.</li>
                             <li><i>y<sub>i</sub></i> is its label.</li>
                            </ul>
                           </li>
                           <li>The goal of supervised learning algorithm is to find <i>g: <b>X</b> &#8594; <b>Y</b></i>, where
                            <ul>
                              <li><i>g</i> is one of the functions from the set of possible functions <i>G</i> (hypotheses space)</li>
                            </ul>
                           </li>
                           <li><b>Scoring function <i>F</i></b> denote the space of scoring functions, where
                            <ul>
                              <li><i>f: <b>X</b> &#215; <b>Y</b> &#8594; <b>R</b></i> such that <i>g</i> returns the highest scoring function.</li>
                            </ul>
                           </li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Formalization: Unsupervised learning</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>The goal of unsupervised learning algorithm is to
                            <ul>
                              <li>find mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
                            </ul>
                           </li>
			</ul>
			<h1>1. Patterns</h1>
			<h1>Formalization: Semi-supervised learning</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>l</sub>)</i>} be the <i><b>l</b></i> be the set of labeled training examples</li>
                           <li>Let {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>} be the <i><b>u</b></i> be the set of unlabeled feature vectors of <i><b>X</b></i>.</li>
                           <li>The goal of semi-supervised learning algorithm is to do
                            <ul>
                              <li><b>Transductive learning</b>, i.e., find correct labels for {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>}. OR</li>
                              <li><b>Inductive learning</b>, i.e., find correct mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
                            </ul>
                           </li>
			</ul>
			<h1>2. Data Mining</h1>
			<h1>Tasks in Data Mining</h1>
			<ol>
                           <li>Classification</li>
                           <li>Clustering</li>
                           <li>Regression</li>
                           <li>Sequence Labeling</li>
                           <li>Association Rules</li>
                           <li>Anomaly Detection</li>
                           <li>Summarization</li>
			</ol>
			<h1>2.1. Classification</h1>
			<h1></h1>
			<ul>
                           <li>Generalizing known structure to apply to new data</li>
                           <li>Identifying the set of categories to which an object belongs</li>
                           <li>Binary vs. Multiclass classification</li>
			</ul>
			<h1>2.1. Classification</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Spam vs Non-spam</li>
                           <li>Document classification</li>
                           <li>Handwriting recognition</li>
                           <li>Speech Recognition</li>
                           <li>Internet Search Engines</li>
			</ul>
			<h1>2.1. Classification</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>The goal of classification algorithm (or classifier) is to find
                             {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>k</sub>)</i>}, i.e., assigning a known label to every input feature vector, where
                            <ul>
                             <li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
                             <li><i>y<sub>i</sub> &#8712; <b>Y</b></i></li>
                             <li>|<i><b>X</b> </i>| <i>= l</i></li>
                             <li>|<i><b>Y</b> </i>| <i>= k</i></li>
                             <li>l &gt;= k</li>
                            </ul>
                           </li>
                         </ul>
			<h1>2.1. Classification</h1>
			<h1>Classifiers</h1>
			<ul>
                           <li>Classifying Algorithm</li>
                           <li>Two types of classifiers:
                            <ul>
                             <li><b>Binary classifiers</b> assigning an object to any of two classes</li>
                             <li><b>Multiclass classifiers</b> assigning an object to one of several classes</li>
                            </ul>
                           </li>
			</ul>
			<h1>2.1. Classification</h1>
			<h1>Linear Classifiers</h1>
			<ul>
                           <li>A linear function assigning a score to each possible category by combining the feature vector of an instance with a vector of weights, using a dot product.</li>
                           <li>Formalization:
			   <ul>
                             <li>Let <i><b>X</b></i> be the input feature space and <i><b>x</b><sub>i</sub> &#8712; <b>X</b></i></li>
                             <li>Let <i><b>&#946;</b><sub>k</sub></i> be vector of weights for category <i>k</i></li>
                             <li><i>score(<b>x</b><sub>i</sub>, k) = <b>x</b><sub>i</sub>.<b>&#946;</b><sub>k</sub></i>, score for assigning category <i>k</i> to instance <i><b>x</b><sub>i</sub></i>. The category that gives the highest score is assigned as the category of the instance.</li>
                            </ul>
                           </li>
			</ul>
			<h1>2.2. Classifiers</h1>
			<h1>2.2. Classifiers</h1>
			<h1>2.2. Classifiers</h1>
                   <p>Let</p>
                   <ul>
                    <li><i>tp</i>: number of true postives</li>
                    <li><i>fp</i>: number of false postives</li>
                    <li><i>fn</i>: number of false negatives</li>
                   </ul>
                   <p>Then</p>
                   <ul>
                    <li>Precision <i>p  = tp / (tp + fp)</i></li>
                    <li>Recall <i>r  = tp / (tp + fn)</i></li>
                    <li>F1-score <i>f1  = 2 * ((p * r) / (p + r))</i></li>
                   </ul>
			<h1>2.2. Clustering</h1>
			<h1></h1>
			<ul>
                           <li>Discovering groups and structures in the data without using known structures in the data</li>
                           <li>Objects in a cluster are more similar to each other than the objects in the other cluster</li>
			</ul>
			<h1>2.2. Clustering</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Social network analysis</li>
                           <li>Image segmentation</li>
                           <li>Recommender systems</li>
                           <li>Grouping of shopping items</li>
			</ul>
			<h1>2.2. Clustering</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>The goal of clustering is to find k subsets of <i><b>X</b></i>, in such a way that
                            <ul>
                              <li><i>C<sub>1</sub>.. &#8746; ..C<sub>k</sub> &#8746; C<sub>outliers</sub> = <b>X</b> </i> and</li>
                              <li><i>C<sub>i</sub> &#8745; C<sub>j</sub> = &#981;, i &#8800; j; 1 &lt;i,j &lt;k</i></li>
                              <li><i>C<sub>outliers</sub></i> may consist of outlier instances (data anomaly)</li>
                            </ul>
                           </li>
                         </ul>
			<h1>2.2. Clustering</h1>
			<h1>Cluster models</h1>
			<ul>
                           <li><b>Centroid models</b>: cluster represented by a single mean vector</li>
                           <li><b>Connectivity models</b>: distance connectivity</li>
                           <li>Distribution models: clusters modeled using statistical distributions</li>
                           <li>Density models: clusters as connected dense regions in the data space</li>
                           <li>Subspace models</li>
                           <li>Group models</li>
                           <li>Graph-based models</li>
                           <li>Neural models</li>
			</ul>
			<h1>2.3. Regression</h1>
			<h1></h1>
			<ul>
                           <li>Finding a function which models the data</li>
                           <li>Assigns a real-valued output to each input</li>
                           <li>Estimating the relationships among variables</li>
                           <li>Relationship between a dependent variable ('criterion variable') and one or more independent variables ('predictors').</li>
			</ul>
			<h1>2.3. Regression</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Prediction</li>
                           <li>Forecasting</li>
                           <li>Machine learning</li>
                           <li>Finance</li>
			</ul>
			<h1>2.3. Regression</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>A function that maps a data item to a prediction variable</li>
                           <li>Let <i><b>X</b></i> be the independent variables</li>
                           <li>Let <i><b>Y</b></i> be the dependent variables</li>
                           <li>Let <i><b>&#946;</b></i> be the unknown parameters (scalar or vector)</li>
                           <li>The goal of regression model is to approximate <i><b>Y</b></i> with <i><b>X</b>,<b>&#946;</b></i>, i.e.,
                           <ul>
                             <li><i><b>Y</b> &#8773; f(<b>X</b>,<b>&#946;</b>)</i></li>
                           </ul>
                           </li>
			</ul>
			<h1>2.3. Regression</h1>
			<h1>Linear regression</h1>
			<ul>
                           <li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> + &#949;<sub>i</sub></i> OR</li>
                           <li>parabola: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> + &#946;<sub>1</sub>x<sub>i</sub><sup>2</sup> +&#949;<sub>i</sub></i></li>
			</ul>
			<h1>2.3. Regression</h1>
			<h1>Linear regression</h1>
			<ul>
                           <li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> + &#949;<sub>i</sub></i> OR</li>
                           <li><i>ŷ<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub><sub>i</sub></i> OR</li>
                           <li>Residual: <i>e<sub>i</sub> = ŷ<sub>i</sub> - y<sub>i</sub></i></li>
                           <li>Sum of squared residuals, SSE  = &#931; e<sub>i</sub>, where 1 &lt; i &lt; n</li>
                           <li>The goal is to minimize SSE</li>
			</ul>
			<h1>2.4. Sequence Labeling</h1>
			<h1></h1>
			<ul>
                           <li>Assigning a class to each member of a sequence of values </li>
			</ul>
			<h1>2.4. Sequence Labeling</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Part of speech tagging</li>
                           <li>Linguistic translation</li>
                           <li>Video analysis</li>
                           <li>Handwriting recognition</li>
                           <li>Information extraction</li>
			</ul>
			<h1>2.4. Sequence Labeling</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>Let <i>&#12296;x<sub>1</sub>,...,x<sub>T</sub>&#12297;</i> be a sequence of length <i>T</i>.</li>
                           <li>The goal of sequence labeling is to generate a corresponding sequnce
                            <ul>
                             <li><i>&#12296;y<sub>1</sub>,...,y<sub>T&#12297;</sub></i> of labels</li>
                             <li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
                             <li><i>y<sub>j</sub> &#8712; <b>Y</b></i></li>
                            </ul>
                           </li>
			</ul>
			<h1>2.5. Association Rules</h1>
			<h1>Association Rules</h1>
			<ul>
                           <li>Searches for relationships between variables</li>
			</ul>
			<h1>2.5. Association Rules</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Web usage mining</li>
                           <li>Intrusion detection</li>
                           <li>Affinity analysis</li>
			</ul>
			<h1>2.5. Association Rules</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>I</b></i> be a set of <i>n</i> binary attributes called items</li>
                           <li>Let <i><b>T</b></i> be a set of <i>m</i> transactions called database</li>
                           <li>Let <i><b>I</b></i> = {<i>(i<sub>1</sub>,...,i<sub>n</sub>)</i>} and <i><b>T</b></i> = {<i>(t<sub>1</sub>,...,t<sub>m</sub>)</i>}</li>
                           <li>The goal of association rule learning is to find
                            <ul>
                             <li><i><b>X</b> &#8658; <b>Y</b></i>, where <i><b>X</b> &#8658; <b>Y</b> &#8838; <b>I</b></i></li>
                             <li><i><b>X</b></i> is the antecedent</li>
                             <li><i><b>Y</b></i> is the consequent</li>
                            </ul>
                           </li>
			</ul>
			<h1>2.5. Association Rules</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>Support: how frequently an itemset appears in the database
                            <ul>
                             <li><i>supp(<b>X</b>) = |t &#8712;<b>T</b></i>; <i><b>X</b> &#8838; t| / |<b>T</b>|</i></li>
                            </ul>
                           </li>
                           <li>Confidence:  how frequently the rule has been found to be true.
                            <ul>
                             <li><i>conf(<b>X</b> &#8658; <b>Y</b>) = supp(<b>X</b> &#8746; <b>Y</b>)/supp(<b>X</b>)</i></li>
                            </ul>
                           </li>
                           <li>Lift: the ratio of the observed support to that of the expected if X and Y were independent
                            <ul>
                             <li><i>lift(<b>X</b> &#8658; <b>Y</b>) = supp(<b>X</b> &#8746; <b>Y</b>)/(supp(<b>X</b>) &#10761; supp(<b>Y</b>))</i></li>
                            </ul>
                           </li>
			</ul>
			<h1>2.5. Association Rules</h1>
			<h1>Example</h1>
			<ul>
                          <li>{<b>bread</b>, <b>butter</b>} &#8658; {<b>milk</b>}</li>
			</ul>
			<h1>2.6. Anomaly Detection</h1>
			<h1></h1>
			<ul>
                           <li>Identification of unusual data records</li>
                           <li>Approaches
                             <ol>
                               <li>Unsupervised anomaly detection</li>
                               <li>Supervised anomaly detection</li>
                               <li>Semi-supervised anomaly detection</li>
                             </ol>
                           </li>
			</ul>
			<h1>2.6. Anomaly Detection</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Intrusion detection</li>
                           <li>Fraud detection</li>
                           <li>Remove anomalous data</li>
                           <li>System health monitoring</li>
                           <li>Event detection in sensor networks</li>
                           <li>Misuse detection</li>
			</ul>
			<h1>2.6. Anomaly Detection</h1>
			<h1>Characteristics</h1>
			<ul>
                           <li>Unexpected bursts</li>
			</ul>
			<h1>2.6. Anomaly Detection</h1>
			<h1>Formalization</h1>
			<ul>
                           <li>Let <i><b>Y</b></i> be a set of measurements</li>
                           <li>Let <i>P<sub><b>Y</b></sub>(y)</i> be a statistical model for the distribution of <i><b>Y</b></i> under 'normal' conditions.</li>
                           <li>Let <i><b>T</b></i> be a user-defined threshold.</li>
                           <li>A measurement <i>x</i> is an outlier if <i>P<sub><b>Y</b></sub>(x) &lt; <b>T</b></i></li>
			</ul>
			<h1>2.7. Summarization</h1>
			<h1></h1>
			<ul>
                           <li>Providing a more compact representation of the data set</li>
                           <li>Report Generation</li>
			</ul>
			<h1>2.7. Summarization</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Keyphrase extraction</li>
                           <li>Document summarization</li>
                           <li>Search engines</li>
                           <li>Image summarization</li>
                           <li>Video summarization: Finding important events from videos</li>
			</ul>
			<h1>2.7. Summarization</h1>
			<h1>Formalization: Multidocument summarization</h1>
			<ul>
                           <li>Let {<i><b>D</b> = D<sub>1</sub>, ..., D<sub>k</sub></i>} be a document collection of k documents </li>
                           <li>A Document {<i>D = t<sub>1</sub>, ..., t<sub>m</sub></i>} consists of m textual units (words, sentences, paragraphs etc.) </li>
                           <li>Let {<i><b>D</b> = t<sub>1</sub>, ..., t<sub>n</sub></i>} be the complete set of all textual units from all documents, where
                            <ul>
                             <li><i>t<sub>i</sub> &#8712; <b>D</b>,</i> if and only if <i>&#8707; D<sub>j</sub></i> such that <i>t<sub>i</sub> &#8712; D<sub>j</sub></i></li>
                            </ul>
                           </li>
                           <li><i>S &#8838; <b>D</b></i> constitutes a summary</li>
                           <li> Two scoring functions
                            <ul>
                             <li><i>Rel(i)</i>: relevance of textual unit <i>i</i> in the summary</li>
                             <li><i>Red(i,j)</i>: Redundancy between two textual units <i>t<sub>i</sub></i>, t<sub>j</sub></li>
                            </ul>
                           </li>
			</ul>
			<h1>2.7. Summarization</h1>
			<h1>Formalization: Multidocument summarization</h1>
			<ul>
                           <li>Scoring for a summary S
                            <ul>
                             <li><i>s(S)</i> score of summary S</li>
                             <li><i>l(i)</i> is the length of the textual unit i</li>
                             <li><i>K</i> is the fixed maximum length of the summary</li>
                            </ul>
                           </li>
			</ul>
			<h1>2.7. Summarization</h1>
			<h1></h1>
			<ul>
                           <li>Finding a subset from the entire subset</li>
                           <li>Approaches
                             <ol>
                               <li><b>Extraction</b>:  Selecting a subset of existing words, phrases, or sentences in the original text without any modification</li>
                               <li><b>Abstraction</b>: Build an internal semantic representation and then use natural language generation techniques</li>
                             </ol>
                           </li>
			</ul>
			<h1>2.7. Summarization</h1>
			<h1>Extractive summarization</h1>
			<ul>
                           <li>Approaches
                             <ol>
                               <li><b>Generic summarization</b>:  Obtaining a generic summary</li>
                               <li><b>Query relevant summarization</b>: Summary relevant to a query</li>
                             </ol>
                           </li>
			</ul>
			<h1>3. Algorithms</h1>
			<h1></h1>
			<ol>
                           <li>Support Vector Machines (SVM)</li>
                           <li>Stochastic Gradient Descent (SGD)</li>
                           <li>Nearest-Neighbours</li>
                           <li>Naive Bayes</li>
                           <li>Decision Trees</li>
                           <li>Ensemble Methods (Random Forest)</li>
			</ol>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Introduction</h1>
			<ul>
                           <li>Supervised learning approach</li>
                           <li>Binary classification algorithm</li>
                           <li>Constructs a hyperplane ensuring the maximum separation between two classes</li>
			</ul>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Hyperplane</h1>
			<ul>
                           <li>Hyperplane of n-dimensional space is a subspace of dimension <i>n-1</i></li>
                           <li>Examples
                            <ul>
                              <li>Hyperplane of a 2-dimensional space is 1-dimensional line</li>
                              <li>Hyperplane of a 3-dimensional space is 2-dimensional plane</li>
                            </ul>
                           </li>
			</ul>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Formal definition</h1>
			<ul>
                           <li>The goal of a SVM is to estimate a function <i>f: R<sup>N</sup> &#10761; {+1,-1}</i>, i.e.,
                            <ul>
                             <li>If <i>x<sub>1</sub>,...,x<sub>l</sub></i>  &#8712; <i>R<sup>N</sup></i> are the <i>N</i> input data points,</li>
                             <li>the goal is to find <i>(x<sub>1</sub>,y<sub>1</sub>),...,(x<sub>l</sub>,y<sub>l</sub>)</i>  &#8712; <i>R<sup>N</sup> &#10761; {+1,-1}</i></li>
                            </ul>
                           </li>
                           <li>Any hyperplane can be written by the equation using set of input points <i><b>x</b></i>
                            <ul>
                              <li><i><b>w</b>.<b>x</b> - b = 0</i>, where</li>
                              <li><i><b>w</b></i> &#8712; R<sup>N</sup></i>, a normal vector to the plane</li>
                              <li><i>b &#8712; R</i></li>
                            </ul>
                           </li>
                           <li>A decision function is given by <i>f(x) = sign(<b>w</b>.<b>x</b> - b )</i>
                           </li>
			</ul>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Formal definition</h1>
			<ul>
                          <li>If the training data are linearly separable, two hyperplanes can be selected</li>
                          <li>They separate the two classes of data, <br>so that distance between them is as large as possible.</li>
                          <li>The hyperplanes can be given by the equations
                           <ul>
                             <li><i><b>w</b>.<b>x</b> - b = 1</i></li>
                             <li><i><b>w</b>.<b>x</b> - b = -1</i></li>
                           </ul>
                          </li>
                          <li>The distance between the two hyperplanes can be given by <i>2/||<b>w</b>||</i></li>
                          <li>Region between these two hyperplanes is called margin.</li>
                          <li>Maximum-margin hyperplane is the hyperplane <br> that lies halfway between them.</li>
			</ul>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Formal definition</h1>
			<ul>
                          <li>In order to prevent data points from falling into the margin, following constraints are added
                           <ul>
                             <li><i><b>w</b>.<b>x</b><sub>i</sub> - b &gt;= 1</i>, if <i>y<sub>i</sub> = 1</i></li>
                             <li><i><b>w</b>.<b>x</b><sub>i</sub> - b &lt;= -1</i>, if <i>y<sub>i</sub> = -1</i></li>
                           </ul>
                           <li><i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b) &gt;= 1</i> for  1&lt;= i &lt;= n</li>
                          </li>
                          <li>The goal is to minimize ||<b>w</b>|| subject to <i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b) &gt;= 1</i> for  1&lt;= i &lt;= n</li>
                          <li>Solving for both <i><b>w</b></i> and <i>b</i> gives our classifier
                                   <i>f(x) = sign(<b>w</b>.<b>x</b> - b)</i></li>
                          <li>Max-margin hyperplane is completely determined by the points that lie nearest to it, called the <b>support vectors</b></li>
			</ul>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Data mining tasks</h1>
			<ul>
                           <li>Classification (Multi-class classification)</li>
                           <li>Regression</li>
                           <li>Anomaly detection</li>
			</ul>
			<h1>3.1. Support Vector Machines (SVM)</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Text and hypertext categorization</li>
                           <li>Image classification</li>
                           <li>Handwriting recognition</li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent (SGD)</h1>
			<h1></h1>
			<ul>
                           <li>A stochastic approximation of the gradient descent optimization</li>
                           <li>Iterative method for minimizing an objective function that is written as a sum of differentiable functions.</li>
                           <li>Finds minima or maxima by iteration</li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent</h1>
			<h1>Gradient</h1>
			<ul>
                           <li>Multi-variable generalization of the derivative. </li>
                           <li>Gives slope of the tangent of the graph of a function</li>
                           <li>Gradient points in the direction of the greatest rate of increase of a function</li>
                           <li>Magnitude of gradient is the slope of the graph in that direction</li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent</h1>
			<h1>Gradient vs Derivative</h1>
			<ul>
                           <li>Derivatives defined on functions of single variable</li>
                           <li>Gradient defined on functions of multiple variables</li>
                           <li>Gradient is a vector-valued function (range is a vector)</li>
                           <li>Derivative is a scalar-valued function</li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent</h1>
			<h1>Gradient descent</h1>
			<ul>
                           <li>First-order iterative optimization algorithm for finding the minimum of a function.</li>
                           <li>Finding a local minima involves taking steps proportional to</br> the negative of the gradient of the function at the current point.</li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent</h1>
			<h1>Standard gradient descent method</h1>
			<ul>
                           <li>Let's take the problem of minimizing an objective function
                            <ul>
		             <li><i>Q(w) = 1/n (&#931;Q<sub>i</sub>(w)), 1&lt;=i&lt;n</i></li>
                             <li>Summand function <i>Q<sub>i</sub></i> associated with <i>i<sup>th</sup></i> observation in the data set.</li>
                            </ul>
                           </li>
                           <li><i>w = w - &#951;.&#8711; Q(w)</i></li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent</h1>
			<h1>Iterative method</h1>
			<ul>
                           <li>Choose an initial vector of parameters <w> and learning rate &#951;.</li>
                           <li>Repeat until an approximate minimum is obtained:
                            <ul>
                             <li>Randomly shuffle examples in the training set.</li>
                             <li><i>w = w - &#951;.&#8711; Q<sub>i</sub>(w),</i> for i=1...n</li>
                            </ul>
                           </li>
			</ul>
			<h1>3.2. Stochastic Gradient Descent</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Classification</li>
                           <li>Regression</li>
			</ul>
			<h1>3.3. Nearest-Neighbours</h1>
			<h1>k-nearest neighbors algorithm</h1>
			<ul>
                           <li>k-NN classification: output is a class membership
                                 <br/> (object is classified by a majority vote of its neighbors.)</li>
                           <li>k-NN regression: output is the property value for the object </br>
                                (average values of its k nearest neighbors)</li>
			</ul>
			<h1>3.3. Nearest-Neighbours</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Regression</li>
                           <li>Anomaly detection</li>
			</ul>
			<h1>3.4. Naive Bayes classifiers</h1>
			<h1></h1>
			<ul>
                           <li>Collection of simple probabilistic classifiers based on applying Bayes' theorem with strong independence assumption between the features.</li>
			</ul>
			<h1>3.4. Naive Bayes classifiers</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Document classification (spam/non-spam)</li>
			</ul>
			<h1>3.4. Naive Bayes classifiers</h1>
			<h1>Bayes' Theorem</h1>
			<ul>
                           <li>If A and B are events.</li>
                           <li>P(A), P(B) are probabilities of observing A and B independently of each other..</li>
                           <li>P(A|B) is conditional probability, the likelihood of event A occurring given that B is true</li>
                           <li>P(B|A) is conditional probability, the likelihood of event B occurring given that A is true</li>
                           <li> P(B) &#8800; 0</li>
                           <li><i>P(A|B) = (P(B|A).P(A))/P(B)</i></li>
			</ul>
			<h1>3.5. Decision Trees</h1>
			<h1></h1>
			<ul>
                           <li>Decision support tool</li>
                           <li>Tree-like model of decisions and their possible consequences</li>
			</ul>
			<h1>3.5. Decision Trees</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Classification</li>
                           <li>Regression</li>
                           <li>Decision Analysis: identifying strategies to reach a goal</li>
                           <li>Operations Research</li>
			</ul>
			<h1>3.6. Ensemble Methods (Random Forest)</h1>
			<h1>Defintion</h1>
			<ul>
                           <li>Collection of multiple learning algorithms to obtain better predictive performance than could be obtained from one of the constituting algorithms alone.</li>
                           <li>Random forests are obtained by building multiple decision trees at training time</li>
			</ul>
			<h1>3.6. Ensemble Methods (Random Forest)</h1>
			<h1></h1>
			<ul>
                           <li>Multiclass classification</li>
                           <li>Multilabel classification (the problem of assigning one or more label to each instance. There is no limit on the number of classes an instance can be assigned to.)</li>
                           <li>Regression</li>
                           <li>Anomaly detection</li>
			</ul>
			<h1>4. Feature Selection</h1>
			<h1>Definition</h1>
			<ul>
                           <li>Process of selecting a subset of relevant features</li>
                           <li>Used in domains with large number of features and comparatively few sample points</li>
			</ul>
			<h1>4. Feature Selection</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Analysis of written texts</li>
                           <li>Analysis of DNA microarray data</li>
			</ul>
			<h1>4. Feature Selection</h1>
			<h1>Formal defintion[8]</h1>
			<ul>
                           <li>Let <i>X</i> be the original set of n features, i.e., |<i>X</i>| = n</li>
                           <li>Let <i>w<sub>i</sub></i> be the weight assigned to feature <i>x<sub>i</sub></i>&#8712; <i>X</i></li>
                           <li>Binary feature selection assigns binary weights whereas continuous feature selection assigns weights preserving the order of its relevance.</li>
                           <li>Let <i>J(X')</i> be an evaluation measure, defined as <i>J: X' &#8838; X &#8594; R</i></li>
                           <li> Feature selection problem may be defined in three following ways
                            <ol>
                              <li>|<i>X'</i>| = <i>m &lt; n</i>. Find <i>X'</i> &#8834; <i>X</i> such that <i>J(X')</i> is maximum</li>
                              <li>Choose <i>J<sub>0</sub></i>, Find <i>X'</i> &#8838; <i>X</i>, such that <i>J(X') &gt;= J<sub>0</sub></i></li>
                              <li>Find a compromise among minimizing |<i>X'</i>| and maximizing <i>J(X')</i></li>
                            </ol>
                           </li>
			</ul>
			<h1>Data Mining</h1>
			<h1>Goals</h1>
			<ol>
                           <li>Artifical Neural Networks</li>
                           <li>Deep Learning</li>
                           <li>Reinforcement Learning</li>
                           <li>Data Licences, Ethics and Privacy</li>
			</ol>
			<h1>1. Artificial Neural Networks</h1>
			<h1></h1>
			<ul>
                           <li>Inspired by biological neural networks</li>
                           <li>Collection of connected nodes called artificial neurons.</li>
                           <li>Artificial neurons can transmit signal from one to another (like in a synapse).</li>
                           <li>Signal between artificial neurons is a real number</li>
                           <li>The output of a neuron is the sum of weighted inputs.</li>
			</ul>
			<h1>1. Artificial Neural Networks</h1>
			<h1>Perceptron</h1>
			<ul>
                           <li>Algorithm for supervised learning of binary classifiers</li>
                           <li>Binary classifier</li>
			</ul>
			<h1>1. Artificial Neural Networks</h1>
			<h1>Perceptron: Formal definition</h1>
			<ul>
                           <li>Let <i>y = f(z)</i> be output of perceptron for an input vector <i>z</i></li>
                           <li>Let <i><b>N</b></i> be the number of training examples</li>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let {<i>(x<sub>1</sub>, d<sub>1</sub>),...,(x<sub><b>N</b></sub>, d<sub><b>N</b></sub>)</i>} be the <i><b>N</b></i> training examples, where
                            <ul>
                             <li><i>x<sub>i</sub></i> is the feature vector of <i>i<sup>th</sup></i> training example.</li>
                             <li><i>d<sub>i</sub></i> is the desired output value.</li>
                             <li><i>x<sub>j,i</sub></i> be the <i>i<sup>th</sup></i> feature of <i>j<sup>th</sup></i> training example.</li>
                             <li><i>x<sub>j,0</sub></i> = 1</li>
                            </ul>
                           </li>
			</ul>
			<h1>1. Artificial Neural Networks</h1>
			<h1>Perceptron: Formal definition</h1>
			<ul>
                           <li> Weights are represented in the following manner:
                            <ul>
                             <li><i>w<sub>i</sub></i> is the <i>i<sup>th</sup></i> value of weight vector.</li>
                             <li><i>w<sub>i</sub>(t)</i> is the <i>i<sup>th</sup></i> value of weight vector at a given time t.</li>
                            </ul>
                           </li>
			</ul>
			<h1>1. Artificial Neural Networks</h1>
			<h1>Perceptron: Steps</h1>
			<ol>
                           <li>Initialize weights and threshold</li>
                           <li> For each example <i>(x<sub>j</sub>, d<sub>j</sub>)</i> in training set<i></i>
                            <ul>
                             <li>Calculate the weight: <i>y<sub>j</sub>(t)=f[w(t).x<sub>j</sub>]</i></li>
                             <li>Update the weights: <i>w<sub>i</sub>(t + 1) = w<sub>i</sub>(t) + (d<sub>j</sub>-y<sub>j</sub>(t))x<sub>j,i</sub></i></li>
                            </ul>
                           </li>
                           <li>Repeat step 2 until the iteration error <i>1/s (&#931; |d<sub>j</sub> - y<sub>j</sub>(t)|)</i> is less than user-specified threshold.</li>
			</ol>
			<h1>1. Artificial Neural Networks</h1>
			<h1>Backpropagation</h1>
			<ul>
                           <li>Backward propagation of errors</li>
                           <li>Adjust the weight of neurons by calculating the gradient of the loss function</li>
                           <li>Error is calculated and propagated back to the network layers</li>
			</ul>
			<h1>2. Deep Learning</h1>
			<h1>Deep neural networks</h1>
			<ul>
                           <li>Multiple hidden layers between the input and output layers</li>
			</ul>
			<h1>2. Deep Learning</h1>
			<h1>Applications</h1>
			<ul>
                           <li>Computer vision</li>
                           <li>Speech recognition</li>
                           <li>Drug design</li>
                           <li>Natural language processing</li>
                           <li>Machine translation</li>
			</ul>
			<h1>2. Deep Learning</h1>
			<h1>Convolutional deep neural networks</h1>
			<ul>
                           <li>Analysis of images</li>
                           <li>Inspired by neurons in the virtual cortex</li>
                           <li>Network learns the filters</li>
			</ul>
			<h1>3. Reinforcement Learning</h1>
			<h1></h1>
			<ul>
                           <li>Inspired by behaviourist psychology</li>
                           <li>Actions to be taken in order to maximize the cumulative award.</li>
			</ul>
			<h1>4. Data Licences, Ethics and Privacy</h1>
			<h1></h1>
			<ul>
                           <li>Data usage licences</li>
                           <li>Confidentiality and Privacy</li>
                           <li>Ethics</li>
			</ul>
			<h1>4. Data Licences, Ethics and Privacy</h1>
			<h1>Big Data</h1>
			<ul>
			  <li>Volume</li>
			  <li>Variety</b></li>
			  <li>Velocity</li>
			  <li>Veracity</li>
			  <li>Value</li>
			</ul>
			<h1>4. Data Licences, Ethics and Privacy</h1>
			<h1>4. Data Licences, Ethics and Privacy</h1>
			<h1>4. Data Licences, Ethics and Privacy</h1>
			<h1>4. Data Licences, Ethics and Privacy</h1>
        <h3>References</h3>
        <ol>
         <li id="#course"><a href="../../en/teaching/courses/2017/DataMining/index.html">Data Mining course by John Samuel (2017)</a></li>
        </ol>
     </div>
  </body>
</html>
