<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8" />
        <meta http-equiv="Content-Language" content="en" />
        <link rel="shortcut icon" href="../../images/logo/favicon.png" />
        <title>Large Language Models: John Samuel</title>
        <style type="text/css">
            body {
                background-color: #FFFFFF;
            }

            #sidebar {
                position: fixed;
                background-color: #1B80CF;
                top: 0;
                left: 0;
                bottom: 0;
                width: 30vw;
            }

            #sidebar .title {
                position: relative;
                text-align: center;
                line-height: 4vmax;
                font-size: 1.4vmax;
                font-family: 'Arial';
                margin-top: 25vh;
            }

            #sidebar .title a:link,
            #sidebar .title a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .subtitle {
                top: 50vh;
                text-align: center;
                line-height: 1.3vmax;
                font-family: 'Arial';
                font-size: 1.5vmax;
                color: #FFFFFF;
            }

            .subtitle a:link,
            .subtitle a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .licence {
                position: fixed;
                text-align: right;
                bottom: 0;
                right: 0;
            }

            .home {
                position: fixed;
                text-align: left;
                font-family: 'Arial';
                color: #D3D3D3;
                z-index: 100;
                width: 100%;
                background-color: #FFFFFF;
                top: 0px;
                margin-bottom: 10px;
                padding-bottom: 10px;
            }

            .home a:link,
            .home a:visited {
                text-decoration: none;
                color: #D3D3D3;
            }

            .home ul {
                margin: 0;
                padding: 0;
                text-align: left;
                list-style: none;
            }

            .home li {
                position: relative;
                float: left;
                padding-top: 15px;
                margin-right: 1em;
                font-family: 'Arial';
            }

            .home li:hover {
                display: block;
            }

            .home a:link,
            .home a:visited {
                color: #D3D3D3;
            }

            .home li:hover a:link,
            .home li:hover a:visited {
                text-decoration: none;
                padding: 15px;
                color: #FFFFFF;
                background-color: #1B80CF;
            }

            .content {
                line-height: 1.8vmax;
                font-size: 1.2vmax;
                font-family: 'Arial';
                margin-top: 15vh;
                width: 90%;
            }

            .content h2,
            h3,
            h4 {
                color: #1B80CF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content h3 {
                color: #1B80CF;
            }

            .content h2::before,
            .content h3::before {
                display: block;
                content: " ";
                visibility: hidden;
                height: 50px;
                margin-top: -50px;
                pointer-events: none;
                background-color: #FFFFFF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content li {
                margin: 5px;
            }

            .page {
                width: 65vw;
                height: 100%;
                margin-left: 30vw;
                overflow: hidden;
                padding: 0 1em;
                font-family: 'Arial';
            }

            .page img {
                max-width: 100%;
                max-height: 100%;
            }

            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                #sidebar {
                    position: fixed;
                    background-color: #1B80CF;
                    top: 0;
                    left: 0;
                    bottom: 80vh;
                    width: 100vw;
                }

                #sidebar .title {
                    text-align: center;
                    position: fixed;
                    margin-top: 6vh;
                    left: 0px;
                    right: 0px;
                    line-height: 3.5vmax;
                    font-size: 1.5vmax;
                    font-family: 'Arial';
                }

                #sidebar .subtitle {
                    text-align: center;
                    top: 5vh;
                    left: 0px;
                    right: 0px;
                    position: fixed;
                    margin-top: 10vh;
                    font-size: 1.5vmax;
                }

                #sidebar .title a:link,
                #sidebar .title a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                #sidebar .subtitle a:link,
                #sidebar .subtitle a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                .home {
                    z-index: 100;
                    width: 100%;
                    background-color: #1B80CF;
                    font-size: 1.5vmax;
                }

                .home a:link,
                .home a:visited {
                    text-decoration: none;
                    color: #FFFFFF;
                }

                .content {
                    line-height: 3.8vmax;
                    font-size: 1.8vmax;
                    font-family: 'Arial';
                    margin-top: 22vh;
                }

                .content a:link,
                .content a:visited {
                    color: #1B80CF;
                }

                .page {
                    top: 40vh;
                    width: 95%;
                    margin-left: 0vw;
                }

                .page img {
                    max-width: 100%;
                    max-height: 100%;
                    border: 0;
                }
            }
        </style>
    </head>

    <body vocab="http://schema.org/">
        <div class="page">
            <div class="home">
                <ul typeof="BreadcrumbList">
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../index.html">
                            <span property="name">Home</span>
                        </a>
                    </li>
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../research/research.html">
                            <span property="name">Research</span>
                        </a>
                    </li>
                </ul>

            </div>
            <div id="sidebar">
                <div class="title">
                    <h1><a href="./large-language-models.html">Large Language Models</a></h1>
                </div>
                <div class="subtitle">
                    <h4><a href="../about.html">John Samuel</a></h4>
                </div>
            </div>

            <div class="content">
                <h2>Large Language Models</h2>
                <div
                    style="background-color: #f0f8ff; border-left: 4px solid #1e90ff; padding: 1em; margin-bottom: 1.5em;">
                    <strong>This article is part of a series on <a href="./artificial-intelligence.html">Artificial
                            Intelligence</a>.</strong>
                </div>
                <p>Large Language Models (LLMs) are a category of artificial intelligence systems designed
                    to process, understand, and generate human language. These models are trained on vast
                    corpora of textual data and are capable of performing diverse natural language
                    processing (NLP) tasks, including answering questions, summarizing content, generating
                    human-like text, translation, and more.</p>
                <p>LLMs are typically built using a neural network architecture known as the <a
                        href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a>,
                    which allows them to process sequences of words or tokens in parallel. This design
                    improves computational efficiency and enables the modeling of long-range dependencies in
                    text—an advantage over earlier models like recurrent neural networks (RNNs) and long
                    short-term memory (LSTM) networks.</p>
                <p>Applications of LLMs span across chatbots, virtual assistants (e.g., Siri, Alexa), search
                    engines, machine translation systems, sentiment analysis, legal document review, and
                    creative writing tools. Their capabilities are increasingly embedded in enterprise and
                    consumer-facing technologies.</p>
                <p>As these models continue to advance, they are playing a pivotal role in enhancing
                    human-computer interaction and driving new applications across domains such as
                    education, healthcare, programming, and scientific research.</p>

                <h3>Understanding Large Language Models</h3>
                <p>LLMs comprise multiple components that function in unison to interpret and generate
                    language. The core building blocks include:</p>
                <ul>
                    <li><strong>Word vectors:</strong> Numerical representations of words that capture
                        semantic and syntactic relationships. Techniques like <a
                            href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>, <a
                            href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)">GloVe</a>, and
                        contextual embeddings from models like BERT are examples.</li>
                    <li><strong>Transformers:</strong> A deep learning architecture introduced in the paper
                        "<a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>",
                        transformers rely on self-attention mechanisms to model dependencies across
                        sequences and support large-scale training.</li>
                    <li><strong>Feed-forward neural networks:</strong> Dense neural layers that process
                        intermediate embeddings and compute output predictions such as classification
                        probabilities or token logits.</li>
                    <li><strong>Attention mechanisms:</strong> Methods that allow models to dynamically
                        focus on different parts of the input, improving context-awareness and performance
                        on tasks like translation and summarization.</li>
                </ul>

                <h4>Transformers</h4>
                <p>Transformers are the foundation of modern LLMs. Unlike traditional sequential models,
                    transformers use self-attention to process input data in parallel, drastically improving
                    training efficiency. Each layer of a transformer consists of a self-attention block and
                    a feed-forward network, both equipped with residual connections and layer normalization.
                </p>
                <p>Transformers power virtually all state-of-the-art models today, including BERT, GPT, T5,
                    and more. They enable both encoder-based models (for understanding tasks) and
                    decoder-based or encoder-decoder models (for generation and translation).</p>

                <h4>Feed-forward Neural Networks</h4>
                <p>These are the standard fully connected layers found within each transformer block. They
                    are responsible for refining the intermediate token representations after the attention
                    layers. The feed-forward networks contribute non-linearity and further abstraction in
                    representation learning.</p>

                <h4>Attention Mechanisms</h4>
                <p>Attention mechanisms assign weights to input tokens based on their relevance to the
                    current context.Self-attention allows every
                    token to attend to every other token in the sequence, enhancing the model’s
                    understanding of syntactic and semantic structures.</p>
                <p>This mechanism is especially crucial for capturing long-range dependencies and contextual
                    nuances in language, which are essential for high-quality generation and interpretation.
                </p>

                <h4>Limitations of Large Language Models</h4>
                <p>Despite their capabilities, LLMs inherit various limitations. A major concern is the
                    perpetuation of societal and linguistic <a
                        href="https://en.wikipedia.org/wiki/Algorithmic_bias">biases</a>, as these models
                    are trained on human-generated data. If the training data contains biased, toxic, or
                    misleading content, the models may reflect and amplify these issues in their output.</p>
                <p>LLMs are often described as <strong>stochastic parrots</strong>—a term popularized by <a
                        href="https://dl.acm.org/doi/10.1145/3442188.3445922">Emily Bender et al.</a> to
                    emphasize that while models can generate fluent and coherent text, they do not possess
                    true understanding or intentionality. Their outputs are based on statistical patterns in
                    data rather than comprehension of meaning.</p>
                <p>Other challenges include hallucination (producing plausible but false information), lack
                    of transparency in model decisions, environmental costs from large-scale training, and
                    difficulties in evaluating outputs rigorously.</p>

                <h4>Examples of Large Language Models</h4>
                <ul>
                    <li><a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> (Generative Pre-trained
                        Transformer 3)</li>
                    <li><a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a>
                        (Bidirectional Encoder Representations from Transformers)</li>
                    <li><a href="https://arxiv.org/abs/1910.10683">T5</a> (Text-to-Text Transfer
                        Transformer)</li>
                    <li><a href="https://arxiv.org/abs/1906.08237">XLNet</a> (Generalized Autoregressive
                        Pretraining)</li>
                    <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> (Robustly Optimized BERT)
                    </li>
                    <li><a href="https://arxiv.org/abs/1909.11942">ALBERT</a> (A Lite BERT)</li>
                    <li><a href="https://arxiv.org/abs/1904.09223">ERNIE</a> (by Baidu)</li>
                    <li><a href="https://openai.com/chatgpt/overview/">ChatGPT</a> (by OpenAI) (a fine-tuned variant of
                        GPT-3.5/GPT-4 for conversation)</li>
                    <li><a href="https://arxiv.org/abs/2204.02311">PaLM</a> (Pathways Language Model by
                        Google)</li>
                    <li><a href="https://arxiv.org/abs/2211.09085">Galactica</a> (by Meta, designed for
                        scientific knowledge)</li>
                    <li><a href="https://arxiv.org/abs/2205.01068">OPT</a> (Open Pre-trained Transformer by
                        Meta)</li>
                    <li><a href="https://arxiv.org/abs/2112.11446">Gopher</a> (by DeepMind)</li>
                    <li><a href="https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1/">Jurassic-1</a> (by
                        AI21
                        Labs)</li>
                    <li><a href="https://arxiv.org/abs/2201.11990">Megatron-Turing NLG</a> (by NVIDIA and
                        Microsoft)</li>
                    <li><a href="https://huggingface.co/bigscience/bloom">BLOOM</a> (multilingual model by
                        BigScience)</li>
                    <li><a href="https://openai.com/codex/">Codex</a> (OpenAI model for programming
                        tasks)</li>
                </ul>

                <h4>Related Articles</h4>
                <ul>
                    <li><a href="./language-models.html">Language models</a></li>
                    <li><a href="./large-language-models-language-communities.html">Large language
                            models versus language communities</a></li>
                    <li><a href="./large-language-models-energy.html">Large language models - Energy
                            Consumption</a></li>
                    <li><a href="./large-language-models-search-engine-optimization.html">Large language
                            models based search engine optimization</a></li>
                    <li><a href="./nogenai-nofilter.html">NoGenAI is the new NoFilter</a></li>
                </ul>

                <h4>References</h4>
                <ol>
                    <li id="languagemodels">Bender, Emily M., et al. “<a
                            href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">On the Dangers of
                            Stochastic Parrots: Can Language Models Be Too Big?</a>” Proceedings of the 2021
                        ACM Conference on Fairness, Accountability, and Transparency, 2021, pp. 610–23.</li>
                    <li>Vaswani, Ashish, et al. “<a href="https://arxiv.org/abs/1706.03762">Attention Is All
                            You Need</a>.” arXiv preprint arXiv:1706.03762 (2017).</li>
                    <li><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer
                            (Wikipedia)</a></li>
                    <li><a
                            href="https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/">A
                            Jargon-Free Explanation of How AI Large Language Models Work</a> – Ars Technica
                    </li>
                </ol>
                </main>
            </div>
        </div>
    </body>

</html>