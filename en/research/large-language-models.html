<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta http-equiv="Content-Language" content="en">
        <link rel="shortcut icon" href="../../images/logo/favicon.png">
        <title>Large Language Models | John Samuel</title>
        <style>
            /* ===== Design Tokens ===== */
            :root {
                /* Neural Network & Transformers Theme */
                --color-base-bg: #f8f9fb;
                --color-base-surface: #ffffff;
                --color-base-text: #1a202c;
                --color-base-text-muted: #4a5568;

                /* Technical AI Accent Colors */
                --color-accent-1: #10b981;
                /* Emerald - neural connections */
                --color-accent-2: #059669;
                /* Deep emerald - processing depth */
                --color-accent-3: #14b8a6;
                /* Teal - data flow */
                --color-accent-4: #06b6d4;
                /* Cyan - attention mechanisms */
                --color-accent-light: #ecfdf5;

                /* Functional Colors */
                --color-shadow: rgba(16, 185, 129, 0.12);
                --color-border: rgba(16, 185, 129, 0.15);
                --color-hover: rgba(16, 185, 129, 0.08);

                /* Gradients */
                --gradient-primary: linear-gradient(135deg, #10b981 0%, #14b8a6 50%, #06b6d4 100%);
                --gradient-subtle: linear-gradient(180deg, #f8f9fb 0%, #ecfdf5 100%);
                --gradient-hero: linear-gradient(135deg, #059669 0%, #10b981 100%);

                /* Typography */
                --font-sans: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Helvetica Neue', Arial, sans-serif;
                --font-mono: 'SF Mono', Monaco, 'Cascadia Code', 'Consolas', 'Courier New', monospace;
                --text-xs: clamp(0.75rem, 0.7rem + 0.25vw, 0.875rem);
                --text-sm: clamp(0.875rem, 0.8rem + 0.35vw, 1rem);
                --text-base: clamp(1rem, 0.95rem + 0.25vw, 1.125rem);
                --text-lg: clamp(1.125rem, 1.05rem + 0.35vw, 1.25rem);
                --text-xl: clamp(1.25rem, 1.15rem + 0.5vw, 1.5rem);
                --text-2xl: clamp(1.5rem, 1.3rem + 1vw, 2rem);
                --text-3xl: clamp(2rem, 1.6rem + 2vw, 2.5rem);
                --text-4xl: clamp(2.5rem, 2rem + 2.5vw, 3.5rem);

                /* Spacing */
                --space-xs: 0.25rem;
                --space-sm: 0.5rem;
                --space-md: 1rem;
                --space-lg: 1.5rem;
                --space-xl: 2rem;
                --space-2xl: 3rem;
                --space-3xl: 4rem;
                --space-4xl: 6rem;

                /* Border Radius */
                --radius-sm: 4px;
                --radius-md: 8px;
                --radius-lg: 12px;
                --radius-xl: 16px;
                --radius-2xl: 24px;
                --radius-full: 9999px;

                /* Shadows */
                --shadow-sm: 0 1px 2px rgba(0, 0, 0, 0.04), 0 1px 3px rgba(0, 0, 0, 0.06);
                --shadow-md: 0 4px 6px rgba(0, 0, 0, 0.05), 0 2px 4px rgba(0, 0, 0, 0.06);
                --shadow-lg: 0 10px 24px rgba(0, 0, 0, 0.08), 0 4px 8px rgba(0, 0, 0, 0.06);
                --shadow-theme: 0 10px 30px var(--color-shadow), 0 4px 10px rgba(0, 0, 0, 0.06);
            }

            /* ===== Reset & Base ===== */
            *,
            *::before,
            *::after {
                box-sizing: border-box;
                margin: 0;
                padding: 0;
            }

            html {
                scroll-behavior: smooth;
                scroll-padding-top: 80px;
            }

            body {
                font-family: var(--font-sans);
                font-size: var(--text-base);
                line-height: 1.7;
                color: var(--color-base-text);
                background: var(--gradient-subtle);
                background-attachment: fixed;
                -webkit-font-smoothing: antialiased;
                -moz-osx-font-smoothing: grayscale;
                overflow-x: hidden;
            }

            /* ===== Skip Link ===== */
            .skip-link {
                position: absolute;
                top: -40px;
                left: 0;
                background: var(--color-accent-1);
                color: white;
                padding: var(--space-sm) var(--space-md);
                text-decoration: none;
                border-radius: var(--radius-md);
                z-index: 100;
            }

            .skip-link:focus {
                top: var(--space-sm);
                left: var(--space-sm);
            }

            /* ===== Navigation ===== */
            .main-nav {
                position: fixed;
                top: 0;
                left: 0;
                right: 0;
                z-index: 50;
                background: rgba(255, 255, 255, 0.85);
                -webkit-backdrop-filter: blur(12px) saturate(180%);
                backdrop-filter: blur(12px) saturate(180%);
                border-bottom: 1px solid var(--color-border);
                box-shadow: var(--shadow-sm);
            }

            .nav-container {
                max-width: 1400px;
                margin: 0 auto;
                padding: var(--space-md) var(--space-xl);
                display: flex;
                align-items: center;
                gap: var(--space-lg);
            }

            .nav-brand {
                font-size: var(--text-lg);
                font-weight: 700;
                color: var(--color-accent-1);
                text-decoration: none;
                letter-spacing: -0.01em;
                background: var(--gradient-primary);
                -webkit-background-clip: text;
                -webkit-text-fill-color: transparent;
                background-clip: text;
                transition: transform 0.3s ease;
            }

            .nav-brand:hover {
                transform: scale(1.05);
            }

            .breadcrumb {
                display: flex;
                list-style: none;
                gap: var(--space-sm);
                flex-wrap: wrap;
                align-items: center;
            }

            .breadcrumb li {
                display: flex;
                align-items: center;
                gap: var(--space-sm);
            }

            .breadcrumb li:not(:last-child)::after {
                content: '→';
                color: var(--color-base-text-muted);
                font-size: var(--text-sm);
            }

            .breadcrumb a {
                color: var(--color-base-text-muted);
                text-decoration: none;
                font-size: var(--text-sm);
                transition: color 0.3s ease;
            }

            .breadcrumb a:hover {
                color: var(--color-accent-1);
            }

            /* ===== Hero Section ===== */
            .hero-section {
                margin-top: 80px;
                padding: var(--space-4xl) var(--space-xl);
                background: var(--gradient-hero);
                position: relative;
                overflow: hidden;
            }

            .hero-section::before {
                content: '';
                position: absolute;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                background:
                    radial-gradient(circle at 20% 50%, rgba(20, 184, 166, 0.15) 0%, transparent 50%),
                    radial-gradient(circle at 80% 50%, rgba(6, 182, 212, 0.15) 0%, transparent 50%);
                pointer-events: none;
            }

            .hero-container {
                max-width: 1400px;
                margin: 0 auto;
                position: relative;
                z-index: 1;
            }

            .hero-content {
                max-width: 700px;
            }

            .hero-title {
                font-size: var(--text-4xl);
                font-weight: 800;
                color: white;
                letter-spacing: -0.03em;
                line-height: 1.2;
                margin-bottom: var(--space-lg);
            }

            .hero-author {
                font-size: var(--text-xl);
                color: rgba(255, 255, 255, 0.9);
                font-weight: 500;
            }

            .hero-author a {
                color: white;
                text-decoration: none;
                border-bottom: 2px solid rgba(255, 255, 255, 0.5);
                transition: border-color 0.3s ease;
            }

            .hero-author a:hover {
                border-color: white;
            }

            /* ===== SVG Transformer Illustration ===== */
            .hero-illustration {
                position: absolute;
                right: 5%;
                top: 50%;
                transform: translateY(-50%);
                width: 450px;
                height: 450px;
                opacity: 0.12;
                pointer-events: none;
            }

            @keyframes dataFlow {

                0%,
                100% {
                    opacity: 0.3;
                }

                50% {
                    opacity: 1;
                }
            }

            @keyframes matrixPulse {

                0%,
                100% {
                    transform: scale(1);
                    opacity: 0.4;
                }

                50% {
                    transform: scale(1.05);
                    opacity: 0.8;
                }
            }

            @keyframes rotateAttention {
                from {
                    transform: rotate(0deg);
                }

                to {
                    transform: rotate(360deg);
                }
            }

            /* ===== Main Content ===== */
            .main-content {
                max-width: 1400px;
                margin: 0 auto;
                padding: var(--space-4xl) var(--space-xl);
            }

            .series-notice {
                background: linear-gradient(135deg, rgba(16, 185, 129, 0.1) 0%, rgba(20, 184, 166, 0.05) 100%);
                border-left: 4px solid var(--color-accent-1);
                border-radius: var(--radius-lg);
                padding: var(--space-lg);
                margin-bottom: var(--space-3xl);
                box-shadow: var(--shadow-sm);
            }

            .series-notice strong {
                color: var(--color-accent-1);
            }

            .series-notice a {
                color: var(--color-accent-1);
                text-decoration: none;
                border-bottom: 2px solid var(--color-accent-1);
                transition: opacity 0.3s ease;
            }

            .series-notice a:hover {
                opacity: 0.7;
            }

            /* ===== Content Sections ===== */
            .content-section {
                margin-bottom: var(--space-3xl);
            }

            .content-section p {
                margin-bottom: var(--space-xl);
                line-height: 1.8;
                color: var(--color-base-text);
            }

            .content-section h2 {
                font-size: var(--text-3xl);
                font-weight: 700;
                color: var(--color-accent-1);
                margin-bottom: var(--space-xl);
                letter-spacing: -0.02em;
                position: relative;
                padding-bottom: var(--space-md);
            }

            .content-section h2::after {
                content: '';
                position: absolute;
                bottom: 0;
                left: 0;
                width: 80px;
                height: 3px;
                background: var(--gradient-primary);
                border-radius: var(--radius-full);
            }

            .content-section h3 {
                font-size: var(--text-2xl);
                font-weight: 700;
                color: var(--color-accent-2);
                margin: var(--space-2xl) 0 var(--space-lg);
                letter-spacing: -0.01em;
            }

            .content-section h4 {
                font-size: var(--text-xl);
                font-weight: 600;
                color: var(--color-accent-2);
                margin: var(--space-xl) 0 var(--space-md);
            }

            .content-section a {
                color: var(--color-accent-1);
                text-decoration: none;
                border-bottom: 1px solid transparent;
                transition: border-color 0.3s ease;
            }

            .content-section a:hover {
                border-color: var(--color-accent-1);
            }

            .content-section strong {
                color: var(--color-accent-2);
                font-weight: 600;
            }

            /* ===== Lists ===== */
            .content-section ul,
            .content-section ol {
                margin-left: var(--space-xl);
                margin-bottom: var(--space-xl);
            }

            .content-section li {
                margin-bottom: var(--space-md);
                line-height: 1.7;
            }

            .content-section ul li {
                position: relative;
                padding-left: var(--space-md);
            }

            .content-section ul li::before {
                content: '▸';
                position: absolute;
                left: 0;
                color: var(--color-accent-1);
                font-weight: 700;
            }

            /* ===== Section Cards ===== */
            .section-card {
                background: rgba(255, 255, 255, 0.7);
                -webkit-backdrop-filter: blur(12px) saturate(180%);
                backdrop-filter: blur(12px) saturate(180%);
                border: 1px solid var(--color-border);
                border-radius: var(--radius-2xl);
                padding: var(--space-2xl);
                margin-bottom: var(--space-3xl);
                box-shadow: var(--shadow-theme);
                transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            }

            .section-card:hover {
                transform: translateY(-4px);
                box-shadow: 0 20px 40px rgba(16, 185, 129, 0.15);
            }

            /* ===== Dividers ===== */
            .section-divider {
                height: 2px;
                background: var(--gradient-primary);
                border-radius: var(--radius-full);
                margin: var(--space-3xl) auto;
                max-width: 120px;
                position: relative;
            }

            .section-divider::before,
            .section-divider::after {
                content: '';
                position: absolute;
                top: 50%;
                width: 8px;
                height: 8px;
                background: var(--color-accent-1);
                border-radius: 50%;
                transform: translateY(-50%);
                animation: pulse 2s ease-in-out infinite;
            }

            .section-divider::before {
                left: -12px;
            }

            .section-divider::after {
                right: -12px;
                animation-delay: 1s;
            }

            @keyframes pulse {

                0%,
                100% {
                    opacity: 0.3;
                    transform: translateY(-50%) scale(1);
                }

                50% {
                    opacity: 1;
                    transform: translateY(-50%) scale(1.3);
                }
            }

            /* ===== Footer ===== */
            .main-footer {
                margin-top: var(--space-4xl);
                padding: var(--space-3xl) var(--space-xl);
                background: var(--gradient-hero);
                color: white;
                position: relative;
                overflow: hidden;
            }

            .main-footer::before {
                content: '';
                position: absolute;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                background:
                    radial-gradient(circle at 30% 50%, rgba(20, 184, 166, 0.2) 0%, transparent 50%),
                    radial-gradient(circle at 70% 50%, rgba(6, 182, 212, 0.2) 0%, transparent 50%);
                pointer-events: none;
            }

            .footer-content {
                max-width: 1400px;
                margin: 0 auto;
                position: relative;
                z-index: 1;
                display: flex;
                justify-content: space-between;
                align-items: center;
                flex-wrap: wrap;
                gap: var(--space-xl);
            }

            .footer-copyright {
                font-size: var(--text-sm);
                opacity: 0.9;
            }

            .language-selector {
                display: flex;
                gap: var(--space-md);
                flex-wrap: wrap;
            }

            .language-selector span {
                opacity: 0.7;
                font-size: var(--text-sm);
            }

            .language-selector a {
                color: white;
                text-decoration: none;
                font-size: var(--text-sm);
                padding: var(--space-xs) var(--space-md);
                background: rgba(255, 255, 255, 0.1);
                border-radius: var(--radius-full);
                border: 1px solid rgba(255, 255, 255, 0.2);
                transition: all 0.3s ease;
            }

            .language-selector a:hover {
                background: rgba(255, 255, 255, 0.2);
                transform: translateY(-2px);
            }

            /* ===== Background Grid Pattern ===== */
            .background-pattern {
                position: fixed;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                pointer-events: none;
                z-index: -1;
                opacity: 0.03;
                background-image:
                    linear-gradient(var(--color-accent-1) 1px, transparent 1px),
                    linear-gradient(90deg, var(--color-accent-1) 1px, transparent 1px);
                background-size: 50px 50px;
            }

            /* ===== Floating Nodes ===== */
            .background-nodes {
                position: fixed;
                top: 0;
                left: 0;
                right: 0;
                bottom: 0;
                pointer-events: none;
                z-index: -1;
                overflow: hidden;
            }

            .node {
                position: absolute;
                width: 6px;
                height: 6px;
                background: var(--color-accent-1);
                border-radius: 50%;
                opacity: 0.2;
                animation: floatNode 20s ease-in-out infinite;
            }

            .node-1 {
                top: 15%;
                left: 10%;
                animation-delay: 0s;
            }

            .node-2 {
                top: 30%;
                right: 15%;
                animation-delay: 3s;
            }

            .node-3 {
                bottom: 25%;
                left: 20%;
                animation-delay: 6s;
            }

            .node-4 {
                bottom: 40%;
                right: 25%;
                animation-delay: 9s;
            }

            @keyframes floatNode {

                0%,
                100% {
                    transform: translate(0, 0);
                }

                25% {
                    transform: translate(20px, -20px);
                }

                50% {
                    transform: translate(-15px, 15px);
                }

                75% {
                    transform: translate(15px, 10px);
                }
            }

            /* ===== Responsive Design ===== */
            @media (max-width: 1024px) {
                .hero-illustration {
                    opacity: 0.08;
                    width: 350px;
                    height: 350px;
                }
            }

            @media (max-width: 768px) {
                .nav-container {
                    flex-direction: column;
                    align-items: flex-start;
                    gap: var(--space-sm);
                }

                .hero-section {
                    padding: var(--space-3xl) var(--space-md);
                }

                .hero-illustration {
                    display: none;
                }

                .main-content {
                    padding: var(--space-2xl) var(--space-md);
                }

                .section-card {
                    padding: var(--space-lg);
                }

                .footer-content {
                    flex-direction: column;
                    text-align: center;
                }

                .language-selector {
                    justify-content: center;
                }
            }

            /* ===== Reduced Motion ===== */
            @media (prefers-reduced-motion: reduce) {

                *,
                *::before,
                *::after {
                    animation-duration: 0.01ms !important;
                    animation-iteration-count: 1 !important;
                    transition-duration: 0.01ms !important;
                }
            }

            /* ===== Focus Styles ===== */
            a:focus-visible,
            button:focus-visible {
                outline: 3px solid var(--color-accent-1);
                outline-offset: 2px;
                border-radius: var(--radius-sm);
            }
        </style>
    </head>

    <body>
        <!-- Skip Link -->
        <a href="#main-content" class="skip-link">Skip to main content</a>

        <!-- Background Pattern -->
        <div class="background-pattern" aria-hidden="true"></div>

        <!-- Background Nodes -->
        <div class="background-nodes" aria-hidden="true">
            <div class="node node-1"></div>
            <div class="node node-2"></div>
            <div class="node node-3"></div>
            <div class="node node-4"></div>
        </div>

        <!-- Navigation -->
        <nav class="main-nav" role="navigation" aria-label="Main navigation">
            <div class="nav-container">
                <a href="../about.html" class="nav-brand">John Samuel</a>
                <ul class="breadcrumb" vocab="http://schema.org/" typeof="BreadcrumbList">
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../index.html">
                            <span property="name">Home</span>
                        </a>
                    </li>
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../research/research.html">
                            <span property="name">Research</span>
                        </a>
                    </li>
                    <li property="itemListElement" typeof="ListItem">
                        <span property="name">Large Language Models</span>
                    </li>
                </ul>
            </div>
        </nav>

        <!-- Hero Section -->
        <header class="hero-section">
            <div class="hero-container">
                <div class="hero-content">
                    <h1 class="hero-title">Large Language Models</h1>
                    <p class="hero-author">by <a href="../about.html">John Samuel</a></p>
                </div>
            </div>

            <!-- SVG Transformer Architecture Illustration -->
            <svg class="hero-illustration" viewBox="0 0 450 450" xmlns="http://www.w3.org/2000/svg" aria-hidden="true">
                <!-- Transformer Layers -->
                <g opacity="0.8">
                    <!-- Input Layer -->
                    <rect x="100" y="50" width="250" height="40" rx="8" fill="white" opacity="0.4">
                        <animate attributeName="opacity" values="0.4;0.7;0.4" dur="3s" repeatCount="indefinite" />
                    </rect>

                    <!-- Self-Attention Layer -->
                    <rect x="100" y="120" width="250" height="60" rx="8" fill="white" opacity="0.5">
                        <animate attributeName="opacity" values="0.5;0.8;0.5" dur="3.5s" repeatCount="indefinite" />
                    </rect>
                    <text x="225" y="155" text-anchor="middle" fill="white" font-size="14"
                        opacity="0.9">Attention</text>

                    <!-- Feed-Forward Layer -->
                    <rect x="100" y="210" width="250" height="60" rx="8" fill="white" opacity="0.5">
                        <animate attributeName="opacity" values="0.5;0.8;0.5" dur="4s" repeatCount="indefinite" />
                    </rect>
                    <text x="225" y="245" text-anchor="middle" fill="white" font-size="14"
                        opacity="0.9">Feed-Forward</text>

                    <!-- Output Layer -->
                    <rect x="100" y="300" width="250" height="40" rx="8" fill="white" opacity="0.4">
                        <animate attributeName="opacity" values="0.4;0.7;0.4" dur="4.5s" repeatCount="indefinite" />
                    </rect>

                    <!-- Connection Lines -->
                    <line x1="225" y1="90" x2="225" y2="120" stroke="white" stroke-width="2" opacity="0.5">
                        <animate attributeName="opacity" values="0.3;0.7;0.3" dur="3s" repeatCount="indefinite" />
                    </line>
                    <line x1="225" y1="180" x2="225" y2="210" stroke="white" stroke-width="2" opacity="0.5">
                        <animate attributeName="opacity" values="0.3;0.7;0.3" dur="3.5s" repeatCount="indefinite" />
                    </line>
                    <line x1="225" y1="270" x2="225" y2="300" stroke="white" stroke-width="2" opacity="0.5">
                        <animate attributeName="opacity" values="0.3;0.7;0.3" dur="4s" repeatCount="indefinite" />
                    </line>
                </g>

                <!-- Attention Mechanism Visualization -->
                <g transform="translate(225, 150)">
                    <circle r="30" fill="none" stroke="white" stroke-width="2" opacity="0.3">
                        <animate attributeName="r" values="30;35;30" dur="4s" repeatCount="indefinite" />
                    </circle>

                    <!-- Attention Heads -->
                    <circle cx="0" cy="-25" r="4" fill="white" opacity="0.8">
                        <animate attributeName="cy" values="-25;-28;-25" dur="2s" repeatCount="indefinite" />
                    </circle>
                    <circle cx="22" cy="-12" r="4" fill="white" opacity="0.8">
                        <animate attributeName="cx" values="22;25;22" dur="2.5s" repeatCount="indefinite" />
                    </circle>
                    <circle cx="22" cy="12" r="4" fill="white" opacity="0.8">
                        <animate attributeName="cy" values="12;15;12" dur="3s" repeatCount="indefinite" />
                    </circle>
                    <circle cx="0" cy="25" r="4" fill="white" opacity="0.8">
                        <animate attributeName="cy" values="25;28;25" dur="3.5s" repeatCount="indefinite" />
                    </circle>
                    <circle cx="-22" cy="12" r="4" fill="white" opacity="0.8">
                        <animate attributeName="cx" values="-22;-25;-22" dur="4s" repeatCount="indefinite" />
                    </circle>
                    <circle cx="-22" cy="-12" r="4" fill="white" opacity="0.8">
                        <animate attributeName="cy" values="-12;-15;-12" dur="2.2s" repeatCount="indefinite" />
                    </circle>
                </g>

                <!-- Data Flow Particles -->
                <g>
                    <circle r="3" fill="white" opacity="0.6">
                        <animateMotion dur="5s" repeatCount="indefinite"
                            path="M 225 90 L 225 120 L 225 180 L 225 210 L 225 270 L 225 300" />
                        <animate attributeName="opacity" values="0;0.8;0" dur="5s" repeatCount="indefinite" />
                    </circle>
                    <circle r="3" fill="white" opacity="0.6">
                        <animateMotion dur="5s" repeatCount="indefinite" begin="1.5s"
                            path="M 225 90 L 225 120 L 225 180 L 225 210 L 225 270 L 225 300" />
                        <animate attributeName="opacity" values="0;0.8;0" dur="5s" repeatCount="indefinite"
                            begin="1.5s" />
                    </circle>
                    <circle r="3" fill="white" opacity="0.6">
                        <animateMotion dur="5s" repeatCount="indefinite" begin="3s"
                            path="M 225 90 L 225 120 L 225 180 L 225 210 L 225 270 L 225 300" />
                        <animate attributeName="opacity" values="0;0.8;0" dur="5s" repeatCount="indefinite"
                            begin="3s" />
                    </circle>
                </g>

                <!-- Neural Network Connections -->
                <g opacity="0.3">
                    <line x1="80" y1="150" x2="225" y2="150" stroke="white" stroke-width="1">
                        <animate attributeName="opacity" values="0.2;0.5;0.2" dur="4s" repeatCount="indefinite" />
                    </line>
                    <line x1="370" y1="150" x2="225" y2="150" stroke="white" stroke-width="1">
                        <animate attributeName="opacity" values="0.2;0.5;0.2" dur="4.5s" repeatCount="indefinite" />
                    </line>
                    <line x1="80" y1="240" x2="225" y2="240" stroke="white" stroke-width="1">
                        <animate attributeName="opacity" values="0.2;0.5;0.2" dur="5s" repeatCount="indefinite" />
                    </line>
                    <line x1="370" y1="240" x2="225" y2="240" stroke="white" stroke-width="1">
                        <animate attributeName="opacity" values="0.2;0.5;0.2" dur="5.5s" repeatCount="indefinite" />
                    </line>
                </g>
            </svg>
        </header>

        <!-- Main Content -->
        <main id="main-content" class="main-content">
            <div class="series-notice">
                <strong>This article is part of a series on <a href="./artificial-intelligence.html">Artificial
                        Intelligence</a>.</strong>
            </div>

            <section class="content-section">
                <h2>Large Language Models</h2>

                <p>Large Language Models (LLMs) are a category of artificial intelligence systems designed to process,
                    understand, and generate human language. These models are trained on vast corpora of textual data
                    and are capable of performing diverse natural language processing (NLP) tasks, including answering
                    questions, summarizing content, generating human-like text, translation, and more.</p>

                <p>LLMs are typically built using a neural network architecture known as the <a
                        href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">transformer</a>, which
                    allows them to process sequences of words or tokens in parallel. This design improves computational
                    efficiency and enables the modeling of long-range dependencies in text—an advantage over earlier
                    models like recurrent neural networks (RNNs) and long short-term memory (LSTM) networks.</p>

                <p>Applications of LLMs span across chatbots, virtual assistants (e.g., Siri, Alexa), search engines,
                    machine translation systems, sentiment analysis, legal document review, and creative writing tools.
                    Their capabilities are increasingly embedded in enterprise and consumer-facing technologies.</p>

                <p>As these models continue to advance, they are playing a pivotal role in enhancing human-computer
                    interaction and driving new applications across domains such as education, healthcare, programming,
                    and scientific research.</p>
            </section>

            <div class="section-divider" aria-hidden="true"></div>

            <div class="section-card">
                <section class="content-section">
                    <h3>Understanding Large Language Models</h3>

                    <p>LLMs comprise multiple components that function in unison to interpret and generate language. The
                        core building blocks include:</p>

                    <ul>
                        <li><strong>Word vectors:</strong> Numerical representations of words that capture semantic and
                            syntactic relationships. Techniques like <a
                                href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>, <a
                                href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)">GloVe</a>, and contextual
                            embeddings from models like BERT are examples.</li>
                        <li><strong>Transformers:</strong> A deep learning architecture introduced in the paper "<a
                                href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a>", transformers
                            rely on self-attention mechanisms to model dependencies across sequences and support
                            large-scale training.</li>
                        <li><strong>Feed-forward neural networks:</strong> Dense neural layers that process intermediate
                            embeddings and compute output predictions such as classification probabilities or token
                            logits.</li>
                        <li><strong>Attention mechanisms:</strong> Methods that allow models to dynamically focus on
                            different parts of the input, improving context-awareness and performance on tasks like
                            translation and summarization.</li>
                    </ul>
                </section>
            </div>

            <div class="section-card">
                <section class="content-section">
                    <h4>Transformers</h4>

                    <p>Transformers are the foundation of modern LLMs. Unlike traditional sequential models,
                        transformers use self-attention to process input data in parallel, drastically improving
                        training efficiency. Each layer of a transformer consists of a self-attention block and a
                        feed-forward network, both equipped with residual connections and layer normalization.</p>

                    <p>Transformers power virtually all state-of-the-art models today, including BERT, GPT, T5, and
                        more. They enable both encoder-based models (for understanding tasks) and decoder-based or
                        encoder-decoder models (for generation and translation).</p>
                </section>
            </div>

            <div class="section-card">
                <section class="content-section">
                    <h4>Feed-forward Neural Networks</h4>

                    <p>These are the standard fully connected layers found within each transformer block. They are
                        responsible for refining the intermediate token representations after the attention layers. The
                        feed-forward networks contribute non-linearity and further abstraction in representation
                        learning.</p>
                </section>
            </div>

            <div class="section-card">
                <section class="content-section">
                    <h4>Attention Mechanisms</h4>

                    <p>Attention mechanisms assign weights to input tokens based on their relevance to the current
                        context. Self-attention allows every token to attend to every other token in the sequence,
                        enhancing the model's understanding of syntactic and semantic structures.</p>

                    <p>This mechanism is especially crucial for capturing long-range dependencies and contextual nuances
                        in language, which are essential for high-quality generation and interpretation.</p>
                </section>
            </div>

            <div class="section-divider" aria-hidden="true"></div>

            <div class="section-card">
                <section class="content-section">
                    <h4>Limitations of Large Language Models</h4>

                    <p>Despite their capabilities, LLMs inherit various limitations. A major concern is the perpetuation
                        of societal and linguistic <a href="https://en.wikipedia.org/wiki/Algorithmic_bias">biases</a>,
                        as these models are trained on human-generated data. If the training data contains biased,
                        toxic, or misleading content, the models may reflect and amplify these issues in their output.
                    </p>

                    <p>LLMs are often described as <strong>stochastic parrots</strong>—a term popularized by <a
                            href="https://dl.acm.org/doi/10.1145/3442188.3445922">Emily Bender et al.</a> to emphasize
                        that while models can generate fluent and coherent text, they do not possess true understanding
                        or intentionality. Their outputs are based on statistical patterns in data rather than
                        comprehension of meaning.</p>

                    <p>Other challenges include hallucination (producing plausible but false information), lack of
                        transparency in model decisions, environmental costs from large-scale training, and difficulties
                        in evaluating outputs rigorously.</p>
                </section>
            </div>

            <div class="section-card">
                <section class="content-section">
                    <h4>Examples of Large Language Models</h4>

                    <ul>
                        <li><a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> (Generative Pre-trained Transformer
                            3)</li>
                        <li><a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a> (Bidirectional
                            Encoder Representations from Transformers)</li>
                        <li><a href="https://arxiv.org/abs/1910.10683">T5</a> (Text-to-Text Transfer Transformer)</li>
                        <li><a href="https://arxiv.org/abs/1906.08237">XLNet</a> (Generalized Autoregressive
                            Pretraining)</li>
                        <li><a href="https://arxiv.org/abs/1907.11692">RoBERTa</a> (Robustly Optimized BERT)</li>
                        <li><a href="https://arxiv.org/abs/1909.11942">ALBERT</a> (A Lite BERT)</li>
                        <li><a href="https://arxiv.org/abs/1904.09223">ERNIE</a> (by Baidu)</li>
                        <li><a href="https://openai.com/chatgpt/overview/">ChatGPT</a> (by OpenAI) (a fine-tuned variant
                            of GPT-3.5/GPT-4 for conversation)</li>
                        <li><a href="https://arxiv.org/abs/2204.02311">PaLM</a> (Pathways Language Model by Google)</li>
                        <li><a href="https://arxiv.org/abs/2211.09085">Galactica</a> (by Meta, designed for scientific
                            knowledge)</li>
                        <li><a href="https://arxiv.org/abs/2205.01068">OPT</a> (Open Pre-trained Transformer by Meta)
                        </li>
                        <li><a href="https://arxiv.org/abs/2112.11446">Gopher</a> (by DeepMind)</li>
                        <li><a href="https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1/">Jurassic-1</a>
                            (by AI21 Labs)</li>
                        <li><a href="https://arxiv.org/abs/2201.11990">Megatron-Turing NLG</a> (by NVIDIA and Microsoft)
                        </li>
                        <li><a href="https://huggingface.co/bigscience/bloom">BLOOM</a> (multilingual model by
                            BigScience)</li>
                        <li><a href="https://openai.com/codex/">Codex</a> (OpenAI model for programming tasks)</li>
                    </ul>
                </section>
            </div>

            <div class="section-card">
                <section class="content-section">
                    <h4>Related Articles</h4>

                    <ul>
                        <li><a href="./language-models.html">Language models</a></li>
                        <li><a href="./large-language-models-language-communities.html">Large language models versus
                                language communities</a></li>
                        <li><a href="./large-language-models-energy.html">Large language models - Energy Consumption</a>
                        </li>
                        <li><a href="./large-language-models-search-engine-optimization.html">Large language models
                                based search engine optimization</a></li>
                        <li><a href="./nogenai-nofilter.html">NoGenAI is the new NoFilter</a></li>
                    </ul>
                </section>
            </div>

            <div class="section-card">
                <section class="content-section">
                    <h4>References</h4>

                    <ol>
                        <li id="languagemodels">Bender, Emily M., et al. "<a
                                href="https://dl.acm.org/doi/pdf/10.1145/3442188.3445922">On the Dangers of Stochastic
                                Parrots: Can Language Models Be Too Big?</a>" Proceedings of the 2021 ACM Conference on
                            Fairness, Accountability, and Transparency, 2021, pp. 610–23.</li>
                        <li>Vaswani, Ashish, et al. "<a href="https://arxiv.org/abs/1706.03762">Attention Is All You
                                Need</a>." arXiv preprint arXiv:1706.03762 (2017).</li>
                        <li><a href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer
                                (Wikipedia)</a></li>
                        <li><a
                                href="https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/">A
                                Jargon-Free Explanation of How AI Large Language Models Work</a> – Ars Technica</li>
                    </ol>
                </section>
            </div>
        </main>

        <!-- Footer -->
        <footer class="main-footer">
            <div class="footer-content">
                <div class="footer-copyright">
                    <p>© 2024 John Samuel. All content licensed under Creative Commons CC-BY-SA.</p>
                </div>
                <div class="language-selector">
                    <span>Languages:</span>
                    <a href="./large-language-models.html" hreflang="en">English </a>
                </div>
            </div>
        </footer>
    </body>

</html>