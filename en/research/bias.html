<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8" />
        <meta http-equiv="Content-Language" content="en" />
        <link rel="shortcut icon" href="../../images/logo/favicon.png" />
        <title>Bias in Data Science: John Samuel</title>
        <style type="text/css">
            body {
                background-color: #FFFFFF;
            }

            #sidebar {
                position: fixed;
                background-color: #1B80CF;
                top: 0;
                left: 0;
                bottom: 0;
                width: 30vw;
            }

            #sidebar .title {
                position: relative;
                text-align: center;
                line-height: 4vmax;
                font-size: 1.4vmax;
                font-family: 'Arial';
                margin-top: 25vh;
            }

            #sidebar .title a:link,
            #sidebar .title a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .subtitle {
                top: 50vh;
                text-align: center;
                line-height: 1.3vmax;
                font-family: 'Arial';
                font-size: 1.5vmax;
                color: #FFFFFF;
            }

            .subtitle a:link,
            .subtitle a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .licence {
                position: fixed;
                text-align: right;
                bottom: 0;
                right: 0;
            }

            .home {
                position: fixed;
                text-align: left;
                font-family: 'Arial';
                color: #D3D3D3;
                z-index: 100;
                width: 100%;
                background-color: #FFFFFF;
                top: 0px;
                margin-bottom: 10px;
                padding-bottom: 10px;
            }

            .home a:link,
            .home a:visited {
                text-decoration: none;
                color: #D3D3D3;
            }

            .home ul {
                margin: 0;
                padding: 0;
                text-align: left;
                list-style: none;
            }

            .home li {
                position: relative;
                float: left;
                padding-top: 15px;
                margin-right: 1em;
                font-family: 'Arial';
            }

            .home li:hover {
                display: block;
            }

            .home a:link,
            .home a:visited {
                color: #D3D3D3;
            }

            .home li:hover a:link,
            .home li:hover a:visited {
                text-decoration: none;
                padding: 15px;
                color: #FFFFFF;
                background-color: #1B80CF;
            }

            .content {
                line-height: 1.8vmax;
                font-size: 1.2vmax;
                font-family: 'Arial';
                margin-top: 15vh;
                width: 90%;
            }

            .content h2,
            h3,
            h4 {
                color: #1B80CF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content h3 {
                color: #1B80CF;
            }

            .content h2::before,
            .content h3::before {
                display: block;
                content: " ";
                visibility: hidden;
                height: 50px;
                margin-top: -50px;
                pointer-events: none;
                background-color: #FFFFFF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content li {
                margin: 5px;
            }

            .page {
                width: 65vw;
                height: 100%;
                margin-left: 30vw;
                overflow: hidden;
                padding: 0 1em;
                font-family: 'Arial';
            }

            .page img {
                max-width: 100%;
                max-height: 100%;
            }

            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                #sidebar {
                    position: fixed;
                    background-color: #1B80CF;
                    top: 0;
                    left: 0;
                    bottom: 80vh;
                    width: 100vw;
                }

                #sidebar .title {
                    text-align: center;
                    position: fixed;
                    margin-top: 6vh;
                    left: 0px;
                    right: 0px;
                    line-height: 3.5vmax;
                    font-size: 1.5vmax;
                    font-family: 'Arial';
                }

                #sidebar .subtitle {
                    text-align: center;
                    top: 5vh;
                    left: 0px;
                    right: 0px;
                    position: fixed;
                    margin-top: 10vh;
                    font-size: 1.5vmax;
                }

                #sidebar .title a:link,
                #sidebar .title a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                #sidebar .subtitle a:link,
                #sidebar .subtitle a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                .home {
                    z-index: 100;
                    width: 100%;
                    background-color: #1B80CF;
                    font-size: 1.5vmax;
                }

                .home a:link,
                .home a:visited {
                    text-decoration: none;
                    color: #FFFFFF;
                }

                .content {
                    line-height: 3.8vmax;
                    font-size: 1.8vmax;
                    font-family: 'Arial';
                    margin-top: 22vh;
                }

                .content a:link,
                .content a:visited {
                    color: #1B80CF;
                }

                .page {
                    top: 40vh;
                    width: 95%;
                    margin-left: 0vw;
                }

                .page img {
                    max-width: 100%;
                    max-height: 100%;
                    border: 0;
                }
            }
        </style>
    </head>

    <body vocab="http://schema.org/">
        <div class="page">
            <div class="home">
                <ul typeof="BreadcrumbList">
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../index.html">
                            <span property="name">Home</span>
                        </a>
                    </li>
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../research/research.html">
                            <span property="name">Research</span>
                        </a>
                    </li>
                </ul>

            </div>
            <div id="sidebar">
                <div class="title">
                    <h1><a href="./bias.html">Bias in Data Science</a></h1>
                </div>
                <div class="subtitle">
                    <h3><a href="../about.html">John Samuel</a></h3>
                </div>
            </div>

            <div class="content">
                <h2>Bias in Data Science: Impacts, Causes, and Solutions</h2>
                <div
                    style="background-color: #f0f8ff; border-left: 4px solid #1e90ff; padding: 1em; margin-bottom: 1.5em;">
                    <strong>This article is part of a series on <a href="./data-science.html">Data
                            Science</a>.</strong>
                </div>
                <p>
                    Machine learning algorithms are designed to identify patterns from available data and use
                    these patterns to make predictions or inform decisions. While technological advances have
                    greatly increased the use of machine learning in everyday life, they also carry significant
                    risks when it comes to bias. Human biases and prejudices are well documented and have led to
                    the development of laws and regulations worldwide, such as anti-discrimination statutes, to
                    ensure fair treatment of all individuals. However, when these same biases seep into the
                    technologies we create, it brings about serious consequences with far-reaching social
                    impacts</h3>.
                </p>

                <h3>Societal Impacts of Machine Learning Predictions</h3>
                <p>
                    Machine learning is frequently used in high-stakes decision-making that can deeply affect
                    people's lives. Examples include:
                </p>
                <ul>
                    <li>Predicting the likelihood of an individual committing a crime (risk assessment in
                        criminal justice)</li>
                    <li>Determining who is likely to be a productive employee (automated recruitment and hiring
                        tools)</li>
                    <li>Assessing who will excel as a student (educational admissions and interventions)</li>
                    <li>Evaluating customer worthiness (credit scoring and lending)</li>
                    <li>Predicting “good” citizenship (public policy, welfare eligibility)</li>
                    <li>Assessing who will receive optimal healthcare (predictive algorithms in medicine)</li>
                </ul>

                <p>
                    If predictions generated by these systems are based on biased data, they can
                    <strong>disadvantage</strong> entire groups, leading to discrimination, systematic
                    exclusion, and deepening of existing inequities</h3>.
                </p>

                <h3>The Roots and Results of Bias in Technology</h3>
                <p>
                    Many current machine learning algorithms are trained using historical data. This process can
                    inadvertently reinforce human biases and prejudices present in the data, rather than
                    reducing them</h3>. As a result, technology may not
                    support the progress of a diverse community, and can, in fact, perpetuate and amplify
                    inequalities.
                </p>

                <p>
                    While issues of bias in data-driven systems have been documented for decades, they have
                    become <strong>more pronounced and widespread</strong> as machine learning is increasingly
                    integrated into various sectors, such as healthcare, credit scoring, employment, and
                    criminal justice</h3>. The use of biased training
                    data almost inevitably results in unfair or discriminatory outcomes for marginalized
                    populations</h3>.
                </p>

                <h3>Dimensions of Data: Diversity, Quantity, and Quality</h3>
                <p>
                    To achieve equitable outcomes in machine learning, it is crucial to consider:</p>
                <ul>
                    <li><strong>Data diversity:</strong> Is each group in the population adequately represented?
                    </li>
                    <li><strong>Data quantity:</strong> Is there enough data for reliable modeling for each
                        demographic?</li>
                    <li><strong>Data quality:</strong> Are the data collection methods accurate and unbiased?
                    </li>
                </ul>
                <p>
                    Without sufficient and diverse representation in the training data, even the most
                    sophisticated algorithms will underperform or produce skewed results</h3>.
                </p>

                <h3>Historical Data ≠ Future Progress</h3>
                <p>
                    While it is tempting to assume that our historical data and decisions can be used to
                    accurately predict the future, this can limit societal progress. These systems often learn
                    rules based on past patterns – and if these patterns reflect prejudice or structural
                    inequality, predictions based on them will continue to reinforce the status quo rather than
                    advance humanity</h3>.
                </p>

                <h3>Beyond Technical Challenges: Social and Ethical Dimensions</h3>
                <p>
                    Bias in data science is not just a technical issue; it is also a <strong>social and ethical
                        one</strong>. The consequences of machine learning bias can deeply affect individuals
                    and communities, shaping opportunities and outcomes in domains with major societal
                    impact</h3>. Recognition of these risks has led experts to call for
                    transparency, accountability, and the application of ethical principles in the development
                    and deployment of data-driven systems.
                </p>

                <h3>Ensuring Inclusive and Representative Data</h3>
                <p>
                    To reduce bias, sample data used for training and analysis must be
                    <strong>comprehensive</strong> and <strong>representative</strong> of the broad population,
                    not just a specific subset. This includes ensuring diversity across race, gender,
                    socioeconomic status, geographic location, age, and more</h3>.
                </p>
                <p>
                    Efforts to improve data diversity, quantity, and quality enhance both the <em>accuracy</em>
                    and <em>fairness</em> of machine learning models, especially in high-stakes applications
                    like healthcare and criminal justice</h3>.
                </p>

                <h3>Correlation Does Not Imply Causation</h3>
                <p>
                    A frequent pitfall in data-driven decision-making is assuming that correlation between two
                    variables means that one causes the other. This is a logical error; a statistical
                    relationship can exist due to other factors or confounding variables. Failing to distinguish
                    between correlation and causation has led to numerous incorrect conclusions and harmful
                    decisions in data science</h3>. For more, see the concept of <a
                        href="https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation">Correlation
                        does not imply causation</a>.
                </p>

                <h3>Discrimination-Aware Data Mining</h3>
                <p>
                    Addressing bias in machine learning requires the application of <strong>discrimination-aware
                        data mining</strong> techniques. These methods are specifically developed to detect,
                    measure, and mitigate different types of unfairness in data and outcomes. Strategies include
                    modifying data (preprocessing), adjusting algorithms (in-processing), and correcting results
                    after data mining (postprocessing). Research indicates that such methods can produce models
                    that are both accurate and non-discriminatory, leading to more equitable
                    technology</h3>.
                </p>

                <h3>Principles and Steps for Ensuring Fairness</h3>
                <ul>
                    <li><strong>Principles for Accountability:</strong> Adopt frameworks like the <a
                            href="https://www.fatml.org/resources/principles-for-accountable-algorithms">Principles
                            for Accountable Algorithms</a>, promoting transparency, contestability, and
                        explanation of algorithmic decisions</h3>.</li>
                    <li><strong>Demographic Parity and Equalized Odds:</strong> Use fairness metrics to assess
                        whether algorithms are making equitable predictions across different
                        groups</h3>.</li>
                    <li><strong>External Audits and Bias Audits:</strong> Regularly review algorithms for
                        disparate impact and implement corrective measures</h3>.</li>
                    <li><strong>Stakeholder Engagement:</strong> Work with affected communities and domain
                        experts to identify biases and establish fair practices</h3>.</li>
                    <li><strong>Ongoing Monitoring:</strong> Continuously monitor deployed systems for emergent
                        biases as data, society, and models evolve</h3>.</li>
                </ul>

                <h3>Conclusion: Towards Fair and Ethical Data Science</h3>
                <p>
                    Bias in data science cannot be solved by technical fixes alone. It is critical to address
                    both the technical roots of bias (in data, algorithms, and system design) and the broader
                    social and ethical context in which these tools operate. Ensuring fairness, inclusivity, and
                    transparency in data science benefits not only individuals and marginalized groups but
                    society as a whole</h3>.
                </p>

                <h3>References</h3>

                <ol>
                    <li id="bias-technology"><a
                            href="https://fivethirtyeight.com/features/technology-is-biased-too-how-do-we-fix-it/">Technology
                            Is Biased Too. How Do We Fix It?</a></li>
                    <li id="racism-software"><a
                            href="https://www.politico.com/agenda/story/2018/02/07/algorithmic-bias-software-recommendations-000631">Is
                            your software racist?</a></li>
                    <li id="confirmation-bias"><a href="https://en.wikipedia.org/wiki/Confirmation_bias">Confirmation
                            bias</a></li>
                    <li id="digital-decisions"><a href="https://cdt.org/issue/privacy-data/digital-decisions/">Digital
                            Decisions</a></li>
                    <li id="principles-accountability"><a
                            href="https://www.fatml.org/resources/principles-for-accountable-algorithms">Principles
                            for
                            Accountable Algorithms and a Social Impact Statement for Algorithms</a></li>
                </ol>
            </div>
        </div>
    </body>

</html>