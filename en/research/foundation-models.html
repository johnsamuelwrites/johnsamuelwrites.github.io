<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8" />
        <meta http-equiv="Content-Language" content="en" />
        <link rel="shortcut icon" href="../../images/logo/favicon.png" />
        <title>Foundation Models: John Samuel</title>
        <style type="text/css">
            body {
                background-color: #FFFFFF;
            }

            #sidebar {
                position: fixed;
                background-color: #1B80CF;
                top: 0;
                left: 0;
                bottom: 0;
                width: 30vw;
            }

            #sidebar .title {
                position: relative;
                text-align: center;
                line-height: 4vmax;
                font-size: 1.4vmax;
                font-family: 'Arial';
                margin-top: 25vh;
            }

            #sidebar .title a:link,
            #sidebar .title a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .subtitle {
                top: 50vh;
                text-align: center;
                line-height: 1.3vmax;
                font-family: 'Arial';
                font-size: 1.5vmax;
                color: #FFFFFF;
            }

            .subtitle a:link,
            .subtitle a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .licence {
                position: fixed;
                text-align: right;
                bottom: 0;
                right: 0;
            }

            .home {
                position: fixed;
                text-align: left;
                font-family: 'Arial';
                color: #D3D3D3;
                z-index: 100;
                width: 100%;
                background-color: #FFFFFF;
                top: 0px;
                margin-bottom: 10px;
                padding-bottom: 10px;
            }

            .home a:link,
            .home a:visited {
                text-decoration: none;
                color: #D3D3D3;
            }

            .home ul {
                margin: 0;
                padding: 0;
                text-align: left;
                list-style: none;
            }

            .home li {
                position: relative;
                float: left;
                padding-top: 15px;
                margin-right: 1em;
                font-family: 'Arial';
            }

            .home li:hover {
                display: block;
            }

            .home a:link,
            .home a:visited {
                color: #D3D3D3;
            }

            .home li:hover a:link,
            .home li:hover a:visited {
                text-decoration: none;
                padding: 15px;
                color: #FFFFFF;
                background-color: #1B80CF;
            }

            .content {
                line-height: 1.8vmax;
                font-size: 1.2vmax;
                font-family: 'Arial';
                margin-top: 15vh;
                width: 90%;
            }

            .content h2,
            h3,
            h4 {
                color: #1B80CF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content h3 {
                color: #1B80CF;
            }

            .content h2::before,
            .content h3::before {
                display: block;
                content: " ";
                visibility: hidden;
                height: 50px;
                margin-top: -50px;
                pointer-events: none;
                background-color: #FFFFFF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content li {
                margin: 5px;
            }

            .page {
                width: 65vw;
                height: 100%;
                margin-left: 30vw;
                overflow: hidden;
                padding: 0 1em;
                font-family: 'Arial';
            }

            .page img {
                max-width: 100%;
                max-height: 100%;
            }

            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                #sidebar {
                    position: fixed;
                    background-color: #1B80CF;
                    top: 0;
                    left: 0;
                    bottom: 80vh;
                    width: 100vw;
                }

                #sidebar .title {
                    text-align: center;
                    position: fixed;
                    margin-top: 6vh;
                    left: 0px;
                    right: 0px;
                    line-height: 3.5vmax;
                    font-size: 1.5vmax;
                    font-family: 'Arial';
                }

                #sidebar .subtitle {
                    text-align: center;
                    top: 5vh;
                    left: 0px;
                    right: 0px;
                    position: fixed;
                    margin-top: 10vh;
                    font-size: 1.5vmax;
                }

                #sidebar .title a:link,
                #sidebar .title a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                #sidebar .subtitle a:link,
                #sidebar .subtitle a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                .home {
                    z-index: 100;
                    width: 100%;
                    background-color: #1B80CF;
                    font-size: 1.5vmax;
                }

                .home a:link,
                .home a:visited {
                    text-decoration: none;
                    color: #FFFFFF;
                }

                .content {
                    line-height: 3.8vmax;
                    font-size: 1.8vmax;
                    font-family: 'Arial';
                    margin-top: 22vh;
                }

                .content a:link,
                .content a:visited {
                    color: #1B80CF;
                }

                .page {
                    top: 40vh;
                    width: 95%;
                    margin-left: 0vw;
                }

                .page img {
                    max-width: 100%;
                    max-height: 100%;
                    border: 0;
                }
            }
        </style>
    </head>

    <body vocab="http://schema.org/">
        <div class="page">
            <div class="home">
                <ul typeof="BreadcrumbList">
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../index.html">
                            <span property="name">Home</span>
                        </a>
                    </li>
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../research/research.html">
                            <span property="name">Research</span>
                        </a>
                    </li>
                </ul>

            </div>
            <div id="sidebar">
                <div class="title">
                    <h1><a href="./foundation-models.html">Foundation Models</a></h1>
                </div>
                <div class="subtitle">
                    <h3><a href="../about.html">John Samuel</a></h3>
                </div>
            </div>

            <div class="content">
                <article>
                    <div
                        style="background-color: #f0f8ff; border-left: 4px solid #1e90ff; padding: 1em; margin-bottom: 1.5em;">
                        <strong>This article is part of a series on <a href="./artificial-intelligence.html">Artificial
                                Intelligence</a>.</strong>
                    </div>

                    <h2>Foundation Models: Architecture, Domains, and Impact in Artificial Intelligence</h2>

                    <h3>1. Introduction</h3>
                    <p>
                        Over the past decade, artificial intelligence has evolved dramatically, driven by the rise of
                        <strong>foundation models</strong>—a new class of large-scale, general-purpose models capable of
                        performing a wide variety of tasks. These models are typically trained on vast datasets using
                        scalable architectures such as the <a
                            href="https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)">Transformer</a>,
                        and are adapted through fine-tuning or prompting for downstream applications.
                    </p>
                    <p>
                        Foundation models mark a departure from traditional task-specific systems. They are designed to
                        generalize across domains—ranging from <a
                            href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language
                            processing</a> and <a href="https://en.wikipedia.org/wiki/Computer_vision">computer
                            vision</a> to <a href="https://en.wikipedia.org/wiki/Speech_recognition">speech
                            recognition</a>, <a href="https://en.wikipedia.org/wiki/Robotics">robotics</a>, and <a
                            href="https://en.wikipedia.org/wiki/Multimodal_learning">multimodal learning</a>.
                    </p>

                    <h3>2. Defining Characteristics of Foundation Models</h3>
                    <ol>
                        <li><strong>Pretraining on Broad Data</strong> – Foundation models are pretrained on large,
                            diverse datasets that span multiple modalities, including web-scale text corpora, images,
                            audio recordings, video, and sensor streams.</li>
                        <li><strong>Scalability</strong> – These models utilize billions of parameters and are trained
                            using high-performance compute infrastructure, enabling them to scale in both capacity and
                            performance.</li>
                        <li><strong>Adaptability</strong> – Once pretrained, foundation models can be fine-tuned or
                            prompted to handle a range of downstream tasks with minimal additional data or training.
                        </li>
                    </ol>
                    <p>
                        In contrast to earlier models that were retrained for every task, <strong>foundation
                            models</strong> support transfer learning at scale. For instance, <strong>GPT-4</strong> by
                        <a href="https://openai.com/gpt-4">OpenAI</a> can perform translation, summarization,
                        programming assistance, and more, all from a single pretrained model. Likewise,
                        <strong>CLIP</strong> by <a href="https://openai.com/research/clip">OpenAI</a> aligns images and
                        text for tasks such as classification and semantic search. Other notable examples include
                        <strong>BERT</strong> from Google, which excels in natural language understanding tasks, and
                        <strong>DALL-E 3</strong>, also by OpenAI, which generates high-quality images from text
                        descriptions. Additionally, <strong>Llama</strong> models from Meta have significantly
                        contributed to the open-source large language model landscape. These advancements demonstrate
                        the versatility and efficiency of foundation models across various domains.
                    </p>
                    <h3>3. Historical Evolution of Foundation Models</h3>
                    <p>
                        The evolution of foundation models is rooted in early advances in deep learning and
                        representation learning:
                    </p>
                    <ul>
                        <li><strong>2013:</strong> <a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a>
                            introduced distributed word representations.</li>
                        <li><strong>2018:</strong> <strong>BERT</strong> (Bidirectional Encoder Representations from
                            Transformers) by Google demonstrated masked language modeling and fine-tuning for multiple
                            NLP tasks.</li>
                        <li><strong>2020:</strong> <strong>GPT-3</strong> showed that few-shot learning was possible
                            through in-context prompting, with 175 billion parameters.</li>
                        <li><strong>2021:</strong> <strong>CLIP</strong> and <strong>DALL·E</strong> opened the door to
                            vision-language and generative multimodal models.</li>
                        <li><strong>2022–2024:</strong> <strong>Whisper</strong> (speech), <strong>RT-1</strong>
                            (robotics), and <strong>GPT-4</strong> (multimodal) generalized this approach across
                            domains.</li>
                    </ul>

                    <h3>4. Key Foundation Models Across Domains</h3>
                    <table border="1" cellspacing="0" cellpadding="8">
                        <thead>
                            <tr>
                                <th>Domain</th>
                                <th>Model</th>
                                <th>Developer</th>
                                <th>Architecture</th>
                                <th>Year</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Natural Language Processing</td>
                                <td><a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT</a></td>
                                <td>Google</td>
                                <td>Transformer (Encoder)</td>
                                <td>2018</td>
                            </tr>
                            <tr>
                                <td>Natural Language Processing</td>
                                <td><a href="https://openai.com/gpt-4">GPT-4</a></td>
                                <td>OpenAI</td>
                                <td>Transformer (Decoder)</td>
                                <td>2023</td>
                            </tr>
                            <tr>
                                <td>Computer Vision</td>
                                <td><a href="https://en.wikipedia.org/wiki/ResNet">ResNet</a></td>
                                <td>Microsoft</td>
                                <td>Convolutional Neural Network</td>
                                <td>2015</td>
                            </tr>
                            <tr>
                                <td>Computer Vision</td>
                                <td><a
                                        href="https://research.google/blog/transformers-for-image-recognition-at-scale/">ViT</a>
                                </td>
                                <td>Google</td>
                                <td>Vision Transformer</td>
                                <td>2020</td>
                            </tr>
                            <tr>
                                <td>Speech and Audio</td>
                                <td><a href="https://openai.com/research/whisper">Whisper</a></td>
                                <td>OpenAI</td>
                                <td>Transformer (Encoder-Decoder)</td>
                                <td>2022</td>
                            </tr>
                            <tr>
                                <td>Robotics</td>
                                <td><a href="https://robotics-transformer1.github.io/">RT-1</a></td>
                                <td>Google DeepMind</td>
                                <td>Transformer + Multimodal Inputs</td>
                                <td>2022</td>
                            </tr>
                            <tr>
                                <td>Multimodal AI</td>
                                <td><a href="https://openai.com/research/clip">CLIP</a></td>
                                <td>OpenAI</td>
                                <td>Contrastive Vision-Language Model (Transformer-based)</td>
                                <td>2021</td>
                            </tr>
                            <tr>
                                <td>Multimodal AI</td>
                                <td><a href="https://openai.com/dall-e">DALL·E</a></td>
                                <td>OpenAI</td>
                                <td>Diffusion Model (various iterations)</td>
                                <td>2021 (DALL-E 1), 2022 (DALL-E 2), 2023 (DALL-E 3)</td>
                            </tr>
                        </tbody>
                    </table>

                    <h3>5. Challenges and Limitations</h3>
                    <ul>
                        <li><strong>Bias and Fairness:</strong> Foundation models often reflect biases present in their
                            training data, leading to potentially unfair or unsafe outputs.</li>
                        <li><strong>Resource Consumption:</strong> Training large models requires significant energy and
                            compute resources, raising environmental and economic concerns.</li>
                        <li><strong>Hallucination and Reliability:</strong> Language models may generate plausible but
                            factually incorrect outputs—a phenomenon known as <a
                                href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">hallucination</a>.
                        </li>
                        <li><strong>Opacity and Explainability:</strong> Due to their scale and complexity, foundation
                            models often behave as black boxes, challenging interpretability.</li>
                        <li><strong>Security and Misuse:</strong> Malicious use cases (e.g., disinformation, deepfakes)
                            underscore the need for robust safety mechanisms and governance.</li>
                    </ul>

                    <h3>6. Future Directions</h3>
                    <p>
                        The future of foundation models is likely to be shaped by trends such as <strong>efficient
                            fine-tuning</strong> (e.g., LoRA, adapters), <strong>open-weight models</strong> (e.g.,
                        LLaMA), and <strong>alignment with human values</strong> through techniques like <a
                            href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Reinforcement
                            Learning from Human Feedback (RLHF)</a>. Emerging research in <strong>multimodal
                            agents</strong> and <strong>neurosymbolic architectures</strong> may further extend the
                        capabilities of foundation models into reasoning and decision-making.
                    </p>

                    <h3>7. References</h3>
                    <ol>
                        <li><a href="https://en.wikipedia.org/wiki/Foundation_model">Foundation model – Wikipedia</a>
                        </li>
                        <li><a href="https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)">Transformer
                                (deep learning architecture) – Wikipedia</a></li>
                        <li><a href="https://openai.com/gpt-4">GPT-4 – OpenAI</a></li>
                        <li><a href="https://openai.com/research/whisper">Whisper – OpenAI</a></li>
                        <li><a href="https://robotics-transformer1.github.io/">RT-1 – Robotics Transformer</a></li>
                        <li><a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT – Wikipedia</a></li>
                        <li><a href="https://en.wikipedia.org/wiki/ResNet">ResNet – Wikipedia</a></li>
                        <li><a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">ViT –
                                Google AI Blog</a></li>
                        <li><a
                                href="https://research.google/blog/rt-1-robotics-transformer-for-real-world-control-at-scale/">RT-1:
                                Robotics Transformer for real-world control at scale</a>
                        </li>
                        <li><a href="https://openai.com/research/clip">CLIP – OpenAI</a></li>
                        <li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">RLHF –
                                Wikipedia</a></li>
                        <li>Bommasani, R., Hudson, D. A., et al. (2021). <a href="https://arxiv.org/abs/2108.07258">On
                                the Opportunities and Risks of Foundation Models</a>. arXiv:2108.07258.</li>
                    </ol>
                </article>
            </div>
        </div>
    </body>

</html>