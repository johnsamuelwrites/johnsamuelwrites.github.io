<!DOCTYPE html>
<html lang="en">

    <head>
        <meta charset="utf-8" />
        <meta http-equiv="Content-Language" content="en" />
        <link rel="shortcut icon" href="../../images/logo/favicon.png" />
        <title>Language Models: John Samuel</title>
        <style type="text/css">
            body {
                background-color: #FFFFFF;
            }

            #sidebar {
                position: fixed;
                background-color: #1B80CF;
                top: 0;
                left: 0;
                bottom: 0;
                width: 30vw;
            }

            #sidebar .title {
                position: relative;
                text-align: center;
                line-height: 4vmax;
                font-size: 1.4vmax;
                font-family: 'Arial';
                margin-top: 25vh;
            }

            #sidebar .title a:link,
            #sidebar .title a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .subtitle {
                top: 50vh;
                text-align: center;
                line-height: 1.3vmax;
                font-family: 'Arial';
                font-size: 1.5vmax;
                color: #FFFFFF;
            }

            .subtitle a:link,
            .subtitle a:visited {
                color: #FFFFFF;
                text-decoration: none;
            }

            .licence {
                position: fixed;
                text-align: right;
                bottom: 0;
                right: 0;
            }

            .home {
                position: fixed;
                text-align: left;
                font-family: 'Arial';
                color: #D3D3D3;
                z-index: 100;
                width: 100%;
                background-color: #FFFFFF;
                top: 0px;
                margin-bottom: 10px;
                padding-bottom: 10px;
            }

            .home a:link,
            .home a:visited {
                text-decoration: none;
                color: #D3D3D3;
            }

            .home ul {
                margin: 0;
                padding: 0;
                text-align: left;
                list-style: none;
            }

            .home li {
                position: relative;
                float: left;
                padding-top: 15px;
                margin-right: 1em;
                font-family: 'Arial';
            }

            .home li:hover {
                display: block;
            }

            .home a:link,
            .home a:visited {
                color: #D3D3D3;
            }

            .home li:hover a:link,
            .home li:hover a:visited {
                text-decoration: none;
                padding: 15px;
                color: #FFFFFF;
                background-color: #1B80CF;
            }

            .content {
                line-height: 1.8vmax;
                font-size: 1.2vmax;
                font-family: 'Arial';
                margin-top: 15vh;
                width: 90%;
            }

            .content h2,
            h3,
            h4 {
                color: #1B80CF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content h3 {
                color: #1B80CF;
            }

            .content h2::before,
            .content h3::before {
                display: block;
                content: " ";
                visibility: hidden;
                height: 50px;
                margin-top: -50px;
                pointer-events: none;
                background-color: #FFFFFF;
            }

            .content a:link,
            .content a:visited {
                color: #1B80CF;
            }

            .content li {
                margin: 5px;
            }

            .page {
                width: 65vw;
                height: 100%;
                margin-left: 30vw;
                overflow: hidden;
                padding: 0 1em;
                font-family: 'Arial';
            }

            .page img {
                max-width: 100%;
                max-height: 100%;
            }

            @media (max-width: 640px),
            screen and (orientation: portrait) {
                body {
                    max-width: 100%;
                    max-height: 100%;
                }

                #sidebar {
                    position: fixed;
                    background-color: #1B80CF;
                    top: 0;
                    left: 0;
                    bottom: 80vh;
                    width: 100vw;
                }

                #sidebar .title {
                    text-align: center;
                    position: fixed;
                    margin-top: 6vh;
                    left: 0px;
                    right: 0px;
                    line-height: 3.5vmax;
                    font-size: 1.5vmax;
                    font-family: 'Arial';
                }

                #sidebar .subtitle {
                    text-align: center;
                    top: 5vh;
                    left: 0px;
                    right: 0px;
                    position: fixed;
                    margin-top: 10vh;
                    font-size: 1.5vmax;
                }

                #sidebar .title a:link,
                #sidebar .title a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                #sidebar .subtitle a:link,
                #sidebar .subtitle a:visited {
                    text-align: center;
                    color: #FFFFFF;
                }

                .home {
                    z-index: 100;
                    width: 100%;
                    background-color: #1B80CF;
                    font-size: 1.5vmax;
                }

                .home a:link,
                .home a:visited {
                    text-decoration: none;
                    color: #FFFFFF;
                }

                .content {
                    line-height: 3.8vmax;
                    font-size: 1.8vmax;
                    font-family: 'Arial';
                    margin-top: 22vh;
                }

                .content a:link,
                .content a:visited {
                    color: #1B80CF;
                }

                .page {
                    top: 40vh;
                    width: 95%;
                    margin-left: 0vw;
                }

                .page img {
                    max-width: 100%;
                    max-height: 100%;
                    border: 0;
                }
            }
        </style>
    </head>

    <body vocab="http://schema.org/">
        <div class="page">
            <div class="home">
                <ul typeof="BreadcrumbList">
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../index.html">
                            <span property="name">Home</span>
                        </a>
                    </li>
                    <li property="itemListElement" typeof="ListItem">
                        <a property="item" typeof="WebPage" href="../research/research.html">
                            <span property="name">Research</span>
                        </a>
                    </li>
                </ul>

            </div>
            <div id="sidebar">
                <div class="title">
                    <h1><a href="./language-models.html">Language Models</a></h1>
                </div>
                <div class="subtitle">
                    <h3><a href="../about.html">John Samuel</a></h3>
                </div>
            </div>

            <div class="content">
                <div
                    style="background-color: #f0f8ff; border-left: 4px solid #1e90ff; padding: 1em; margin-bottom: 1.5em;">
                    <strong>This article is part of a series on <a href="./artificial-intelligence.html">Artificial
                            Intelligence</a>.</strong>
                </div>
                <article>
                    <section>
                        <p>Language models are a class of artificial intelligence systems designed to process,
                            understand, and generate human language. These models are foundational to the field
                            of Natural Language Processing (NLP), enabling tasks such as text classification,
                            machine translation, question answering, and information retrieval.</p>

                        <p>Traditionally, language models have been categorized into statistical, neural, and
                            hybrid approaches. Classical language models include rule-based systems, n-gram
                            models, and probabilistic methods, while more recent developments feature
                            lightweight neural networks capable of delivering high performance without the
                            resource demands of large-scale architectures.</p>
                    </section>

                    <section>
                        <h2>Classical Statistical Language Models</h2>
                        <p>Before the advent of neural networks, language modeling relied on statistical
                            approaches. One of the earliest and most influential techniques was the
                            <strong>n-gram model</strong>, which estimates the probability of a word based on
                            the preceding <em>n−1</em> words.
                        </p>
                        <p>For example, in a bigram model (<em>n = 2</em>), the probability of a word depends
                            only on the previous word. These models are simple, efficient, and interpretable,
                            but suffer from data sparsity and limited context.</p>

                        <p>More advanced statistical models include:</p>
                        <ul>
                            <li><strong>Hidden Markov Models (HMMs)</strong>: Used extensively in speech
                                recognition and part-of-speech tagging. HMMs model sequences where the system
                                being modeled is assumed to follow a Markov process with hidden states.</li>
                            <li><strong>Probabilistic Context-Free Grammars (PCFGs)</strong>: Extend traditional
                                grammars by associating probabilities with each production rule, enabling
                                probabilistic parsing of sentences.</li>
                        </ul>

                        <p>These models laid the groundwork for many early NLP applications.</p>
                    </section>

                    <section>
                        <h2>Early Neural Language Models</h2>
                        <p>The limitations of statistical models led to the development of neural language
                            models. These models learn dense vector representations of words and contexts,
                            enabling generalization beyond observed data.</p>

                        <h3>Word Embeddings</h3>
                        <p><strong>Word embeddings</strong> are numerical vector representations of words in a
                            continuous space where semantically similar words are mapped to nearby points. This
                            representation enables machines to process textual input more meaningfully.</p>

                        <p>Notable models include:</p>
                        <ul>
                            <li><strong>word2vec</strong>: Developed by Mikolov et al. (2013), it introduced two
                                architectures: Continuous Bag of Words (CBOW) and Skip-Gram. These models
                                predict context from a target word or vice versa using shallow neural networks.
                            </li>
                            <li><strong>GloVe (Global Vectors)</strong>: Introduced by Pennington et al., this
                                model combines global matrix factorization and local context windowing to
                                generate word vectors that capture both semantic and syntactic relationships.
                            </li>
                            <li><strong>ELMo (Embeddings from Language Models)</strong>: Developed by Peters et
                                al. (2018), ELMo introduced contextualized embeddings derived from a
                                bidirectional LSTM trained on a language modeling objective. Unlike static
                                embeddings, ELMo's word vectors change based on context.</li>
                        </ul>

                        <p>These models marked a significant shift in NLP, allowing for transfer learning and
                            better semantic understanding with relatively modest computational resources.</p>
                    </section>

                    <section>
                        <h2>Compact Transformer-Based Models</h2>
                        <p>While transformers are commonly associated with large language models, compact
                            transformer architectures offer the benefits of deep contextual representation with
                            significantly reduced computational overhead.</p>

                        <ul>
                            <li><strong>DistilBERT</strong>: A smaller version of BERT created using knowledge
                                distillation. It retains ~97% of BERT's performance while being 60% faster and
                                lighter, making it suitable for resource-constrained environments.</li>
                            <li><strong>ALBERT (A Lite BERT)</strong>: Introduces parameter reduction techniques
                                such as cross-layer parameter sharing and factorized embedding parameterization,
                                allowing for more scalable models without loss in accuracy.</li>
                        </ul>

                        <p>These models are increasingly popular in mobile applications and real-time systems,
                            where inference speed and model size are critical.</p>
                    </section>

                    <section>
                        <h2>Applications of Classical and Small Language Models</h2>
                        <p>Despite the attention given to large-scale models, small and classical language
                            models remain crucial in many domains due to their transparency, efficiency, and
                            ease of deployment. Common use cases include:</p>
                        <ul>
                            <li><strong>Spell checking and autocorrection</strong> in text editors</li>
                            <li><strong>Speech recognition and part-of-speech tagging</strong> using HMMs</li>
                            <li><strong>Sentiment analysis</strong> with bag-of-words or word embedding-based
                                classifiers</li>
                            <li><strong>Information retrieval and ranking</strong> using TF-IDF or probabilistic
                                relevance models</li>
                        </ul>
                        <p>Moreover, in educational and low-resource settings, classical models serve as
                            interpretable and computationally feasible alternatives to larger architectures.</p>
                    </section>

                    <section>
                        <h2>Word Vectors Summary</h2>
                        <p>Word vectors are integral to many language models. Whether derived from neural
                            embeddings or matrix factorization techniques, they enable machines to capture
                            relationships such as analogies (<em>Car - Wheels + Wings ≈ Airplane</em>).</p>

                        <p>They serve as foundational input for both classical models and compact transformer
                            architectures, bridging the gap between symbolic and sub-symbolic AI.</p>
                    </section>

                    <section>
                        <h3>Related Articles</h3>
                        <ul>
                            <li><a href="./large-language-models.html">Large language models</a></li>
                            <li><a href="./large-language-models-language-communities.html">Large language
                                    models versus language communities</a></li>
                            <li><a href="./large-language-models-energy.html">Large language models - Energy
                                    Consumption</a></li>
                            <li><a href="./large-language-models-search-engine-optimization.html">Large language
                                    models based search engine optimization</a></li>
                            <li><a href="./nogenai-nofilter.html">NoGenAI is the new NoFilter</a></li>
                        </ul>
                    </section>

                    <section>
                        <h3>References</h3>
                        <ol>
                            <li>Manning, C. D., Raghavan, P., & Schütze, H. (2008). <em>Introduction to
                                    Information Retrieval</em>. Cambridge University Press. [<a
                                    href="https://nlp.stanford.edu/IR-book/">Link</a>]</li>
                            <li>Jurafsky, D., & Martin, J. H. (2023). <em>Speech and Language Processing</em>
                                (3rd ed.). [<a href="https://web.stanford.edu/~jurafsky/slp3/">Link</a>]</li>
                            <li><a href="https://en.wikipedia.org/wiki/N-gram">Wikipedia: N-gram</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/Word2vec">Wikipedia: word2vec</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">Wikipedia: Hidden
                                    Markov Model</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/ELMo_(language_model)">Wikipedia:
                                    ELMo</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)">Wikipedia:
                                    GloVe</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/DistilBERT">Wikipedia: DistilBERT</a>
                            </li>
                            <li><a href="https://en.wikipedia.org/wiki/ALBERT_(language_model)">Wikipedia:
                                    ALBERT</a></li>
                        </ol>
                    </section>
                </article>

            </div>
        </div>
    </body>

</html>