<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <meta http-equiv="Content-Language" content="en"/>
    <link rel="shortcut icon" href="../../images/logo/favicon.png"/>
    <title>Data Mining: John Samuel</title>
    <style type="text/css">
    body{
      background-color: #FFFFFF;
    }
    #sidebar {
      position: fixed;
      background-color: #00363a;
      top: 0;
      left: 0;
      bottom: 0;
      width:25vw;
    }
    .title {
      position:relative;
      text-align: center;
      line-height: 9vh;
      font-size: 1.5vmax;
      font-family: 'Arial';
      margin-top: 40vh;
    }
    .title a:link, 
    .title a:visited{
     color: #FFFFFF;
     text-decoration:none;
    }
    .subtitle {
      top: 50vh;
      text-align: center;
      line-height: 1.3em;
      font-family: 'Arial';
      font-size: 1.5vmax;
      color: #FFFFFF;
    }
    a:link, a:visited {
     color:#00363a;
    }
    .subtitle a:link,
    .subtitle a:visited{
     color: #FFFFFF;
     text-decoration:none;
    }
    .licence {
      position:fixed;
      text-align: right;
      bottom:0;
      right:0;
    }
    .home {
     font-family: 'Arial';
     text-align: left;
    }
    .codeexample {
      background-color:#eeeeee;
    }
    .home ul{
      margin: 0;
      padding: 0;
      text-align: left;
      list-style:none;
    }
    .home li{
     position: relative;
     float: left;
     margin-right: 1em;
    }
    .content {
     line-height: 1.6em;
     font-size: 1.3em;
     font-family: 'Arial';
     margin-top: 8vh;
    }
    .content h3,h2,h4{
     color:#00363a;
    }
    .exercise {
     margin-left:2vw;
    }
    .exercise p{
     margin-left:1vw;
    }
    .exercise img{
     width:100%;
    }
    .content a:link,
    .content a:visited{
     color:#00363a;
    }
    .home a:link,
    .home a:visited{
     color: #D3D3D3;
    }
    .page {
      width:65vw;
      height:100%;
      margin-left:25vw;
      overflow: auto;
      padding: 0 1em;
    }
    img {
     max-width:100%;
     max-height:100%;
    }
  </style>
  </head>
  <body vocab="http://schema.org/">
    <div id="sidebar">
     <div class="title">
      <h1><a href="./index.html">Practicals: Data Mining</a></h1>
     </div>
     <div class="subtitle">
      <h3><a href="../../../../about.html">John Samuel</a></h3>
     </div>
    </div>
    <div class="licence"><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="./../../../../../images/license.png"/></a>
    </div>
    <div class="page">
      <div class="home">
       <ul typeof="BreadcrumbList">
        <li property="itemListElement" typeof="ListItem">
          <a property="item" typeof="WebPage" href="../../../../index.html">
            <span property="name">Home</span>
          </a>
        </li>
        <li property="itemListElement" typeof="ListItem">
         <a property="item" typeof="WebPage" href="index.html">
          <span property="name">Data Mining</span>
         </a>
        </li>
        <li property="itemListElement" typeof="ListItem">
         <a property="item" typeof="WebPage" href="../../../index.html">
          <span property="name">Teaching</span>
         </a>
        </li>
       </ul>
      </div>
      <div class="content">
        <h3>Goals</h3>
        <ol>
          <li>Continue working with <a href="http://scikit-learn.org/stable/modules/clustering.html">clustering</a> and <a href="http://scikit-learn.org/stable/modules/svm.html">classification</a> algorithms</li>
          <li>Work on <a href="http://scikit-learn.org/stable/modules/linear_model.html">linear regression models</a></li>
          <li>Start working on <a href="http://scikit-learn.org/stable/modules/neural_networks_supervised.html">neural network models</a> including single and multilayered perceptrons.</li>
          <li>Continue working on the <a href="https://en.wikipedia.org/wiki/Recommender_system">recommender system</a></li>
        </ol>
        <h3>Scoring</h3>
        <p>Every exercise has an associated difficulty level. Easy and medium-difficult exercises help you understand the fundamentals and give you ideas to work on difficult exercises. It is highly recommended that you finish easy and medium-difficult exercises to have a good score. Given below is the difficulty scale that will be marked with every exercise: </p>
        <ol>
          <li><span style="color:red">&#9733;</span>: Easy</li>
          <li><span style="color:red">&#9733;&#9733;</span>: Medium</li>
          <li><span style="color:red">&#9733;&#9733;&#9733;</span>: Difficult</li>
        </ol>
        <h3>Guidelines</h3>
        <ol>
          <li>To get complete guidance from the mentors, it is highly recommended that you work on today's practical session and not on the preceding ones.</li>
          <li>Make sure that you rename your submission properly and correctly. Double-check your submission.</li>
          <li>Please check the <a href="./references.html">references</a>.</li>
          <li>There are several ways to achieve a task. Hence there are many possible solutions. But try to make maximum use of the libraries that have been suggested to you for your exercises.</li>
        </ol>
        <h4>Installation</h4>
        <div class="exercise">
          <p>Please refer <a href="./installation.html">installation</a> page.</p>
        </div>
        <h4>Exercise 3.1 <span style="color:red">&#9733;</span></h4>
        <div class="exercise">
          <p>During practical session 2, we saw a clustering algorithm called KMeans. In this practical session, we see some more clustering algorithms. We will try to get more clusters and also check the time taken by each of these algorithms.</p>
          <p>Let's start once again with <b>KMeans</b> and try to get clusters of size between 2 and 11.</p> 
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import KMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             fig, axes = plot.subplots(nrows=5, ncols=2, figsize=(20,25))<br/>
             <br/>
             xaxis = 0<br/>
             yaxis = 0<br/>
             for x in range(2, 12):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = KMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;npbins = numpy.arange(0, cluster_count + 1)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;histogram = numpy.histogram(clusters.labels_, bins=npbins)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;labels = numpy.unique(clusters.labels_)<br/>
             <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;barlist = axes[xaxis, yaxis].bar(labels, histogram[0])<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;if(yaxis == 0):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;else:<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  xaxis = xaxis + 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 0<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;for i in range(cluster_count):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  barlist[i].set_color('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[i][0]),<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      math.ceil(clusters.cluster_centers_[i][1]), math.ceil(clusters.cluster_centers_[i][2])))<br/>
             <br/>
             <br/>
             plot.show()<br/>
            </code>
          </p>
          <p>Your next goal is to test the above code for cluster sizes between 2 and 21 which will give you the figure given below.</p> 
          <p><span style='color:"red"'>Note:</span> The following image was generated after 6 minutes. Optionally, you can add <i>print</i> statements to test whether your code is working fine.</p>
          <p>
            <img style="width:50%" src="./images/kmeans.png"></img>
          </p>
          <p>Now we modify the above algorithm to use <b>MiniBatchKMeans</b> clustering algorithm. Observe the changes.</p>
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import MiniBatchKMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             fig, axes = plot.subplots(nrows=5, ncols=2, figsize=(20,25))<br/>
             <br/>
             xaxis = 0<br/>
             yaxis = 0<br/>
             for x in range(2, 12):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;npbins = numpy.arange(0, cluster_count + 1)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;histogram = numpy.histogram(clusters.labels_, bins=npbins)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;labels = numpy.unique(clusters.labels_)<br/>
             <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;barlist = axes[xaxis, yaxis].bar(labels, histogram[0])<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;if(yaxis == 0):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;else:<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  xaxis = xaxis + 1<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  yaxis = 0<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;for i in range(cluster_count):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  barlist[i].set_color('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[i][0]),<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      math.ceil(clusters.cluster_centers_[i][1]), math.ceil(clusters.cluster_centers_[i][2])))<br/>
             <br/>
             <br/>
             plot.show()<br/>
            </code>
          </p>
          <p>What did you observe? Your next goal is to test the above code for cluster sizes between 2 and 21 which will give you the figure given below.</p> 
          <p>What did you observe? What are your conclusions?</p> 
          <p>
            <img style="width:50%" src="./images/minibatchkmeans.png"></img>
          </p>
          <p>In order to understand time taken by each of these algorithms, we will repeat the above experiment, but this time we will plot the time taken to obtain clusters of different sizes.</p> 
          <p>We start with <b>KMeans</b>.</p>
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import time<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import KMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             for x in range(1, 20):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;start_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = KMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;end_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;total_time = end_time - start_time<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;print("Total time: ", x, ":", total_time)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;X.append(x)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;Y.append(total_time)<br/>
             <br/>
             plot.bar(X, Y)<br/>
             plot.show()<br/>
            </code>
          </p>
          <p>You may get a graph similar to the following.</p>
          <p>
            <img style="width:50%" src="./images/kmeanstime.png"></img>
          </p>
          <p>We now use <b>MiniBatchKMeans</b>.</p>
          <p class="codeexample">
            <code>
             from PIL import Image<br/>
             import numpy<br/>
             import math<br/>
             import time<br/>
             import matplotlib.pyplot as plot<br/>
             from sklearn.cluster import MiniBatchKMeans<br/>
             <br/>
             imgfile = Image.open("flower.jpg")<br/>
             numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
             <br/>
             X = []<br/>
             Y = []<br/>
             <br/>
             for x in range(1, 20):<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;cluster_count = x <br/>
             &nbsp;&nbsp;&nbsp;&nbsp;<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;start_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;clusters.fit(numarray)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;end_time = time.time()<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;total_time = end_time - start_time<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;print("Total time: ", x, ":", total_time)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;X.append(x)<br/>
             &nbsp;&nbsp;&nbsp;&nbsp;Y.append(total_time)<br/>
             <br/>
             plot.bar(X, Y)<br/>
             plot.show()<br/>
            </code>
          </p>
          <p>You may get a graph similar to the following.</p>
          <p>
            <img style="width:40%" src="./images/minibatchkmeanstime.png"></img>
          </p>
          <p>Now test the above code using <b>MiniBatchKMeans</b> algorithm with cluster sizes between 2 and 50. What are your observations? </p>
          <p>Finally we want to see whether we get the same cluster centers from both the algorithms. Run the following program to see the cluster centers produced by the two algorithms. We use two different colors (red and black) to distinguish the cluster centers from the two algorithms.</p>
          <p>
          <p class="codeexample">
            <code>
              from PIL import Image<br/>
              import numpy<br/>
              import math<br/>
              import matplotlib.pyplot as plot<br/>
              from sklearn.cluster import KMeans<br/>
              from sklearn.cluster import MiniBatchKMeans<br/>
              <br/>
              imgfile = Image.open("flower.jpg")<br/>
              numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
              <br/>
              cluster_count = 10<br/>
              <br/>
              clusters = KMeans(n_clusters = cluster_count)<br/>
              clusters.fit(numarray)<br/>
              <br/>
              mclusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
              mclusters.fit(numarray)<br/>
              <br/>
              fig, axes = plot.subplots(nrows=3, ncols=1, figsize=(20,25))<br/>
              #Scatter plot for RG (RGB)<br/>
              axes[0].scatter(numarray[:,0],numarray[:,1])<br/>
              axes[0].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,1], c='red')<br/>
              axes[0].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,1], c='black')<br/>
              <br/>
              #Scatter plot of RB (RGB)<br/>
              axes[1].scatter(numarray[:,0],numarray[:,2])<br/>
              axes[1].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,2], c='red')<br/>
              axes[1].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,2], c='black')<br/>
              <br/>
              #Scatter plot of GB (RGB)<br/>
              axes[2].scatter(numarray[:,1],numarray[:,2])<br/>
              axes[2].scatter(clusters.cluster_centers_[:,1], clusters.cluster_centers_[:,2], c='red')<br/>
              axes[2].scatter(mclusters.cluster_centers_[:,1], mclusters.cluster_centers_[:,2], c='black')<br/>
            </code>
          </p>
          <p>
            <img style="width:40%" src="./images/scatterplots.png"></img>
          </p>
          <p>We would like to see how the individual pixel values have been clustered. Run the following program a couple of times.</p>
          <p class="codeexample">
            <code>
              from PIL import Image<br/>
              import numpy<br/>
              import math<br/>
              import time<br/>
              import matplotlib.pyplot as plot<br/>
              from sklearn.cluster import KMeans<br/>
              from sklearn.cluster import MiniBatchKMeans<br/>
              <br/>
              imgfile = Image.open("flower.jpg")<br/>
              numarray = numpy.array(imgfile.getdata(), numpy.uint8)<br/>
              <br/>
              cluster_count = 10<br/>
              <br/>
              mclusters = MiniBatchKMeans(n_clusters = cluster_count)<br/>
              mclusters.fit(numarray)<br/>
              <br/>
              npbins = numpy.arange(0, cluster_count + 1)<br/>
              histogram = numpy.histogram(mclusters.labels_, bins=npbins)<br/>
              labels = numpy.unique(mclusters.labels_)<br/>
              <br/>
              fig, axes = plot.subplots(nrows=3, ncols=2, figsize=(20,25))<br/>
              <br/>
              #Scatter plot for RG (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = mclusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(mclusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       math.ceil(mclusters.cluster_centers_[j][1]), 0))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[0,0].scatter(numarray[:,0],numarray[:,1], c=colors)<br/>
              axes[0,0].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,1], marker="+", c='red')<br/>
              <br/>
              #Scatter plot for RB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = mclusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(mclusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       0, math.ceil(mclusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[1,0].scatter(numarray[:,0],numarray[:,2], c=colors)<br/>
              axes[1,0].scatter(mclusters.cluster_centers_[:,0], mclusters.cluster_centers_[:,2], marker="+", c='white')<br/>
              <br/>
              #Scatter plot for GB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = mclusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (0, math.ceil(mclusters.cluster_centers_[j][1]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;        math.ceil(mclusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[2,0].scatter(numarray[:,1],numarray[:,2], c=colors)<br/>
              axes[2,0].scatter(mclusters.cluster_centers_[:,1], mclusters.cluster_centers_[:,2], marker="+", c='yellow')<br/>
              <br/>
              clusters = KMeans(n_clusters = cluster_count)<br/>
              clusters.fit(numarray)<br/>
              <br/>
              npbins = numpy.arange(0, cluster_count + 1)<br/>
              histogram = numpy.histogram(clusters.labels_, bins=npbins)<br/>
              labels = numpy.unique(clusters.labels_)<br/>
              <br/>
              #Scatter plot for RG (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = clusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       math.ceil(clusters.cluster_centers_[j][1]), 0))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[0,1].scatter(numarray[:,0],numarray[:,1], c=colors)<br/>
              axes[0,1].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,1], marker="+", c='red')<br/>
              <br/>
              #Scatter plot for RB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = clusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (math.ceil(clusters.cluster_centers_[j][0]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;       0, math.ceil(clusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[1,1].scatter(numarray[:,0],numarray[:,2], c=colors)<br/>
              axes[1,1].scatter(clusters.cluster_centers_[:,0], clusters.cluster_centers_[:,2], marker="+", c='white')<br/>
              <br/>
              #Scatter plot for GB (RGB)<br/>
              colors = []<br/>
              for i in range(len(numarray)):<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;j = clusters.labels_[i]<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;colors.append('#%02x%02x%02x' % (0, math.ceil(clusters.cluster_centers_[j][1]),<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;        math.ceil(clusters.cluster_centers_[j][2])))<br/>
              &nbsp;&nbsp;&nbsp;&nbsp;              <br/>
              axes[2,1].scatter(numarray[:,1],numarray[:,2], c=colors)<br/>
              axes[2,1].scatter(clusters.cluster_centers_[:,1], clusters.cluster_centers_[:,2], marker="+", c='yellow')<br/>
              plot.show()<br/>
            </code>
          </p>
          <p>
            <img style="width:40%" src="./images/kmeansminibatchcomparison.png"></img>
          </p>
          <p>What are your conclusions? </p>
        </div>
        <h4>Exercise 3.2 <span style="color:red">&#9733;</span></h4>
        <div class="exercise">
         <p>Download the file <a href="./population.csv">population.csv</a> (source: query given in <a href="./references.html">references</a>). We will first plot this multi-annual population.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset)<br/>
           df.plot(x='year', y='population', kind='scatter')<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="./images/populationscatterplot.png"></img>
         </p>
         <p>We will focus on data starting from 1960 (why?). Our goal is to use regression techniques to predict population. But we don't know how to verify. So with the available data, we create two categories: training data and test data.</p>
         <p>We will first start with <b>linear regression</b>. We split data into two: training data and test data. We will plot the actual population values and the predicted values.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           from sklearn.linear_model import LinearRegression<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset[4:])<br/>
           <br/>
           #training data<br/>
           x_train = df['year'][:40].values.reshape(-1, 1)<br/>
           y_train = df['population'][:40].values.reshape(-1, 1)<br/>
           <br/>
           #training<br/>
           lr = LinearRegression()<br/>
           lr.fit(x_train, y_train)<br/>
           <br/>
           #printing coefficients<br/>
           print(lr.intercept_, lr.coef_)<br/>
           <br/>
           #prediction<br/>
           x_predict = x_train = df['year'][41:].values.reshape(-1, 1)<br/>
           y_actual = df['population'][41:].values.reshape(-1, 1) <br/>
           y_predict = lr.predict(x_predict)<br/>
           <br/>
           plot.scatter(x_predict, y_actual)<br/>
           plot.plot(x_predict, y_predict, color='red', linewidth=2)<br/>
           plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="./images/populationlinearregression.png"></img>
         </p>
         <p>Now test the above program including the data before 1960. What did you notice? You may have got the following graph.</p>
         <p>
           <img style="width:40%" src="./images/allpopulationlinearregression.png"></img>
         </p>
         <p>What are your observations? So the above program using linear regression perfectly fit for a subset of data. Let's now try with <b>polynomial regression</b> with degree 2.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           from sklearn.linear_model import LinearRegression<br/>
           from sklearn.preprocessing import PolynomialFeatures<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset[4:])<br/>
           <br/>
           #training data<br/>
           <br/>
           x_train = df['year'][:50].values.reshape(-1, 1)<br/>
           y_train = df['population'][:50].values.reshape(-1, 1)<br/>
           <br/>
           pf = PolynomialFeatures(degree=2)<br/>
           x_poly = pf.fit_transform(x_train)<br/>
           <br/>
           #training<br/>
           lr = LinearRegression()<br/>
           lr.fit(x_poly, y_train)<br/>
           <br/>
           #printing coefficients<br/>
           print(lr.intercept_, lr.coef_)<br/>
           <br/>
           #prediction<br/>
           x_predict = x_train = df['year'][41:].values.reshape(-1, 1)<br/>
           y_actual = df['population'][41:].values.reshape(-1, 1) <br/>
           y_predict = lr.predict(pf.fit_transform(x_predict))<br/>
           <br/>
           plot.scatter(x_predict, y_actual)<br/>
           plot.plot(x_predict, y_predict, color='red', linewidth=2)<br/>
           plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="./images/populationpolynomialregression.png"></img>
         </p>
         <p>Before jumping into a conclusion, let's consider the entire data and see.</p>
         <p class="codeexample">
          <code>
           import numpy as np<br/>
           import matplotlib.pyplot as plot<br/>
           import pandas as pd<br/>
           from sklearn.linear_model import LinearRegression<br/>
           from sklearn.preprocessing import PolynomialFeatures<br/>
           dataset = np.loadtxt("population.csv", dtype={'names': ('year', 'population'), 'formats': ('i4', 'i')},<br/>
           &nbsp;&nbsp;&nbsp;&nbsp;skiprows=1, delimiter=",", encoding="UTF-8")<br/>
           <br/>
           df = pd.DataFrame(dataset)<br/>
           <br/>
           #training data<br/>
           <br/>
           x_train = df['year'][:40].values.reshape(-1, 1)<br/>
           y_train = df['population'][:40].values.reshape(-1, 1)<br/>
           <br/>
           pf = PolynomialFeatures(degree=2)<br/>
           x_poly = pf.fit_transform(x_train)<br/>
           <br/>
           #training<br/>
           lr = LinearRegression()<br/>
           lr.fit(x_poly, y_train)<br/>
           <br/>
           #printing coefficients<br/>
           print(lr.intercept_, lr.coef_)<br/>
           <br/>
           #prediction<br/>
           x_predict = x_train = df['year'][41:].values.reshape(-1, 1)<br/>
           <br/>
           # Let's add some more years<br/>
           x_predict = np.append(range(1900, 1959), x_predict)<br/>
           x_predict = x_predict.reshape(-1, 1)<br/>
           <br/>
           y_actual = df['population'][41:].values.reshape(-1, 1) <br/>
           y_predict = lr.predict(pf.fit_transform(x_predict))<br/>
           <br/>
           plot.scatter(df['year'], df['population'])<br/>
           plot.plot(x_predict, y_predict, color='red', linewidth=2)<br/>
           plot.show()<br/>
          </code>
         </p>
         <p>
           <img style="width:40%" src="./images/allpopulationpolynomialregression.png"></img>
         </p>
         <p>What do you think? Can we use this program to predict the missing data (especially in the absence of other external source of information)? Try the above program with different degrees.</p>
        </div>
        <h4>Exercise 3.3 <span style="color:red">&#9733;&#9733;</span></h4>
        <div class="exercise">
          <ul>
            <li></li>
          </ul>
        </div>
        <h4>Exercise 3.4 <span style="color:red">&#9733;&#9733;</span></h4>
        <div class="exercise">
          <ul>
            <li></li>
          </ul>
        </div>
        <h4>Exercise 3.5 <span style="color:red">&#9733;&#9733;&#9733;</span></h4>
        <div class="exercise">
          <ul>
            <li></li>
          </ul>
        </div>
        <h4>Submission</h4>
        <ul>
          <li>Rename your notebook as Name1_Name2_[Name3].ipynb, where Name1, Name2 are your names.</li>
          <li>Submit your notebook online.</li>
          <li>Please <b>don't</b> submit your JSON, TSV and CSV files.</li>
        </ul>
        <h4>References</h4>
        <p><a href="./references.html">Link</a></p>
      </div>
  </body>
</html>
