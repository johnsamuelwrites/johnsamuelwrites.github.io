<html>
<head>
<meta charset="utf-8"/>
<title>Data Mining</title>
<style type="text/css">
    body {
      height: 100%;
      width: 100%;
      background-color: #b4ffff;
      margin: 0;
      overflow: hidden;
      font-family: Arial;
    }
    .slide {
      height: 100%;
      width: 100%;
    }
    
    .content {
      height: 62%;
      width: 90vw;
      display: flex;
      flex-direction: column;
      color: #424242;
      text-align: left;
      padding: 5vw;
      overflow-x: auto;
      font-size: 3vmin;
      flex-wrap: wrap;
    }
    .content h1,
    .content h2 {
      color: #00363a;
    }
    
    .content table {
      color: #424242;
      font-size: 120%;
      width: 90%;
    }
    .content a:link,
    .content a:visited{
      color: #00363a;
      text-decoration:none;
    }
    .content th {
      color: #424242;
      background-color: #81b9bf;
      font-size: 120%;
      padding: 15px;
    }
    .content figure {
      max-width:90%;
      max-height:90%;
    }
    .content figure img{
      max-width:90%;
      max-height:90%;
    }
    .content figure figcaption {
        max-width:90%;
        max-height:90%;
    }
    .content td {
      color: #424242;
      font-size: 11w;
      width: 8%;
      padding: 10px;
    }
    .content li {
      line-height: 4vh;
    }
    .header {
      color: #ffffff;
      background-color: #00363a;
      height: 5vw;
    }
    .header h1 {
      text-align: center;
      font-size: 3vw;
      margin: 0;
    }
    .footer {
      height: 3vw;
      color: #ffffff;
      background-color: #00363a;
      margin: 0;
      padding: .3vw;
      overflow: hidden;
    }
    .footer .contact {
      float: left;
      color: #ffffff;
      text-align: left;
      font-size: 3.2vmin;
    }
    .footer .navigation {
      float: right;
      text-align: right;
      width: 8vw;
      font-size: 3vmin;
    }
    
    .footer .navigation .next,.prev {
      font-size: 3vmin;
      color: #ffffff;
      text-decoration: none;
    }
    
    .footer .navigation .next::after {
      content: "| >";
    }
    
    .footer .navigation .prev::after {
      content: "< ";
    }
    @media (max-width: 640px), screen and (orientation: portrait) {
       body {
         max-width:100%;
         max-height:100%;
       }
       .slide {
          height: 100%;
          width: 100%;
       } 
       .content {
          width: 90vw;
          height:92%;
          display: flex;
          flex-direction: column;
          text-align: left;
          padding: 1vw;
          line-height: 3.8vmax;
          font-size: 1.8vmax;
          flex-wrap: wrap;
       }
       .content figure {
         max-width:90%;
         max-height:90%;
       }
       .content figure img{
         max-width:70%;
         max-height:70%;
       }
       .content figure figcaption {
           max-width:90%;
           max-height:90%;
       }
    }
    
    @media print {
      body {
        max-width:100%;
        max-height:100%;
      }
      .content {
        height: 76%;
        width: 90vw;
        display: flex;
        flex-direction: column;
        color: #424242;
        text-align: left;
        padding: 5vw;
        font-size: 3vmin;
        flex-wrap: wrap;
      }
      .content figure img{
        max-width:80%;
        max-height:80%;
      }
      .content figcaption {
        max-width:80%;
        max-height:80%;
      }
    }

    </style>
</head>
<body>
	<section class="slide" id="slide1">
		<div class="header">
		</div>
		<div class="content">
			<h1 style="font-size:3.5vw">Data Mining</h1>
                        <p><b>John Samuel</b><br/>
                          CPE Lyon<br/><br/>
                        <b>Year</b>: 2018-2019<br/>
                        <b>Email</b>: john(dot)samuel(at)cpe(dot)fr<br/><br/>
                        <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a></p>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">1

                                <a class="next" href="#slide2"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide2">
		<div class="header">
			<h1>Data Mining</h1>
		</div>
		<div class="content">
			<h1>Goals</h1>
			<ul>
                           <li>Understanding Patterns</li>
                           <li>Data mining tasks</li>
                           <li>Algorithms for data mining</li>
                           <li>Feature Selection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">2
				<a class="prev" href="#slide1"></a>
                                <a class="next" href="#slide3"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide3">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
                  <figure>
                    <img src="../../2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="300px" width="300px"/>
                    <img src="../../2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg" height="300px" width="300px"/>
                    <img src="../../2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="300px" width="300px"/>
                    <img src="../../2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG" height="300px" width="300px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">3
				<a class="prev" href="#slide2"></a>
                                <a class="next" href="#slide4"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide4">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Patterns in Nature</h1>
			<ul>
                           <li>Symmetry</li>
                           <li>Trees, Fractals</li>
                           <li>Spirals</li>
                           <li>Chaos</li>
                           <li>Waves</li>
                           <li>Bubbles, Foam</li>
                           <li>Tesselations</li>
                           <li>Cracks</li>
                           <li>Spots, stripes</li>
			</ul>
                  <figure>
                    <img src="../../2017/DataMining/images/320px-Kittyply_edit1.jpg" height="150px" width="150px"/>
                    <img src="../../2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG" height="150px" width="150px"/>
                    <img src="../../2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="150px" width="150px"/>
                    <img src="../../2017/DataMining/images/Rio_Negro_meanders.JPG" height="150px" width="150px"/>
                  </figure>
                  <figure>
                    <img src="../../2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="150px" width="150px"/>
                    <img src="../../2017/DataMining/images/320px-Apis_florea_nest_closeup2.jpg" height="150px" width="150px"/>
                    <img src="../../2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg" height="150px" width="150px"/>
                    <img src="../../2017/DataMining/images/320px-Equus_grevyi_(aka).jpg" height="150px" width="150px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">4
				<a class="prev" href="#slide3"></a>
                                <a class="next" href="#slide5"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide5">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Patterns by Humans</h1>
			<ul>
                           <li>Buildings (Symmetry)</li>
                           <li>Cities</li>
                           <li>Virtual environments (e.g., video games)</li>
                           <li>Human artifacts</li>
			</ul>
                  <figure>
                    <img src="../../2017/DataMining/images/Roman_geometric_mosaic.jpg" height="300px" width="600px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">5
				<a class="prev" href="#slide4"></a>
                                <a class="next" href="#slide6"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide6">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Pattern creation</h1>
			<ul>
                           <li>Repitition</li>
                           <li>Fractals
                             <ul>
                              <li>Julia set: <i>f(z) = z<sup>2</sup> + c</i></li>
                             </ul>
                           </li>
			</ul>
                  <figure>
                    <img src="../../2017/DataMining/images/400px-Tiling_Dual_Semiregular_V3-3-3-3-6_Floret_Pentagonal.svg.png" height="300px" width="300px"/>
                    <img src="../../2017/DataMining/images/Finite_subdivision_of_a_radial_link.png" height="300px" width="300px"/>
                    <img src="../../2017/DataMining/images/Julia_set_(indigo).png" height="300px" width="300px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">6
				<a class="prev" href="#slide5"></a>
                                <a class="next" href="#slide7"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide7">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Synonyms</h1>
			<ul>
                           <li>Pattern recognition</li>
                           <li>Knowledge discovery in databases</li>
                           <li>Data mining</li>
                           <li>Machine learning</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">7
				<a class="prev" href="#slide6"></a>
                                <a class="next" href="#slide8"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide8">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Pattern Recognition</h1>
			<ul>
                           <li>Goal is to detect patterns and regularities in data</li>
                           <li>Approaches
                             <ol>
                              <li><b>Supervised learning</b>: Availability of labeled training data</li>
                              <li><b>Unsupervised learning</b>: No labeled training data available</li>
                              <li><b>Semi-supervised learning</b>: Small set of labeled training data and a large amount of unlabeled data</li>
                             </ol>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">8
				<a class="prev" href="#slide7"></a>
                                <a class="next" href="#slide9"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide9">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization</h1>
			<ul>
                           <li><b>Euclidean vector</b>: geometric object with magnitude and direction</li>
                           <li><b>Vector space</b>: collection of vectors that can be added together and multiplied by numbers.</li>
                           <li><b>Feature vector</b>: n-dimensional vector</li>
                           <li><b>Feature space</b>: Vector space associated with the vectors</li>
			</ul>
			<h3>Examples: Features</h3>
			<ul>
                           <li><b>Images</b>: pixel values.</li>
                           <li><b>Texts</b>: Frequency of occurence of textual phrases.</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">9
				<a class="prev" href="#slide8"></a>
                                <a class="next" href="#slide10"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide10">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization</h1>
			<ul>
                           <li><b>Feature construction<sup>1</sup></b>: construction of new features from already available features</li>
                           <li><b>Feature construction operators</b>
                             <ul>
                               <li>Equality operators, arithmetic operators, array operators (min, max, average etc.)...</li>
                             </ul>
                           </li>
			</ul>
			<h3>Example</h3>
			<ul>
                           <li>Let <b>Year of Birth</b> and <b>Year of Death</b> be two existing features.</li>
                           <li>A new feature called <b>Age</b> =  Year of Birth - Year of Death</li>
			</ul>
                        <ol style="font-size:2vh">
                           <li>https://en.wikipedia.org/wiki/Feature_vector</li>
                        </ol>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">10
				<a class="prev" href="#slide9"></a>
                                <a class="next" href="#slide11"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide11">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization: Supervised learning</h1>
			<ul>
                           <li>Let <i><b>N</b></i> be the number of training examples</li>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub><b>N</b></sub>, y<sub><b>N</b></sub>)</i>} be the <i><b>N</b></i> training examples, where
                            <ul>
                             <li><i>x<sub>i</sub></i> is the feature vector of <i>i<sup>th</sup></i> training example.</li>
                             <li><i>y<sub>i</sub></i> is its label.</li>
                            </ul>
                           </li>
                           <li>The goal of supervised learning algorithm is to find <i>g: <b>X</b> &#8594; <b>Y</b></i>, where
                            <ul>
                              <li><i>g</i> is one of the functions from the set of possible functions <i>G</i> (hypotheses space)</li>
                            </ul>
                           </li>
                           <li><b>Scoring function <i>F</i></b> denote the space of scoring functions, where
                            <ul>
                              <li><i>f: <b>X</b> &#215; <b>Y</b> &#8594; <b>R</b></i> such that <i>g</i> returns the highest scoring function.</li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">11
				<a class="prev" href="#slide10"></a>
                                <a class="next" href="#slide12"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide12">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization: Unsupervised learning</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>The goal of unsupervised learning algorithm is to
                            <ul>
                              <li>find mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">12
				<a class="prev" href="#slide11"></a>
                                <a class="next" href="#slide13"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide13">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization: Semi-supervised learning</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>l</sub>)</i>} be the <i><b>l</b></i> be the set of labeled training examples</li>
                           <li>Let {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>} be the <i><b>u</b></i> be the set of unlabeled feature vectors of <i><b>X</b></i>.</li>
                           <li>The goal of semi-supervised learning algorithm is to do
                            <ul>
                              <li><b>Transductive learning</b>, i.e., find correct labels for {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>}. OR</li>
                              <li><b>Inductive learning</b>, i.e., find correct mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">13
				<a class="prev" href="#slide12"></a>
                                <a class="next" href="#slide14"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide14">
		<div class="header">
			<h1>2. Data Mining</h1>
		</div>
		<div class="content">
			<h1>Tasks in Data Mining</h1>
			<ol>
                           <li>Classification</li>
                           <li>Clustering</li>
                           <li>Regression</li>
                           <li>Sequence Labeling</li>
                           <li>Association Rules</li>
                           <li>Anomaly Detection</li>
                           <li>Summarization</li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">14
				<a class="prev" href="#slide13"></a>
                                <a class="next" href="#slide15"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide15">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Generalizing known structure to apply to new data</li>
                           <li>Identifying the set of categories to which an object belongs</li>
                           <li>Binary vs. Multiclass classification</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">15
				<a class="prev" href="#slide14"></a>
                                <a class="next" href="#slide16"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide16">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Spam vs Non-spam</li>
                           <li>Document classification</li>
                           <li>Handwriting recognition</li>
                           <li>Speech Recognition</li>
                           <li>Internet Search Engines</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">16
				<a class="prev" href="#slide15"></a>
                                <a class="next" href="#slide17"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide17">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>The goal of classification algorithm (or classifier) is to find
                             {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>k</sub>)</i>}, i.e., assigning a known label to every input feature vector, where
                            <ul>
                             <li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
                             <li><i>y<sub>i</sub> &#8712; <b>Y</b></i></li>
                             <li>|<i><b>X</b> </i>| <i>= l</i></li>
                             <li>|<i><b>Y</b> </i>| <i>= k</i></li>
                             <li>l &gt;= k</li>
                            </ul>
                           </li>
                         </ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">17
				<a class="prev" href="#slide16"></a>
                                <a class="next" href="#slide18"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide18">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Classifiers</h1>
			<ul>
                           <li>Classifying Algorithm</li>
                           <li>Two types of classifiers:
                            <ul>
                             <li><b>Binary classifiers</b> assigning an object to any of two classes</li>
                             <li><b>Multiclass classifiers</b> assigning an object to one of several classes</li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">18
				<a class="prev" href="#slide17"></a>
                                <a class="next" href="#slide19"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide19">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Linear Classifiers</h1>
			<ul>
                           <li>A linear function assigning a score to each possible category by combining the feature vector of an instance with a vector of weights, using a dot product.</li>
                           <li>Formalization:
			   <ul>
                             <li>Let <i><b>X</b></i> be the input feature space and <i><b>x</b><sub>i</sub> &#8712; <b>X</b></i></li>
                             <li>Let <i><b>&#946;</b><sub>k</sub></i> be vector of weights for category <i>k</i></li>
                             <li><i>score(<b>x</b><sub>i</sub>, k) = <b>x</b><sub>i</sub>.<b>&#946;</b><sub>k</sub></i>, score for assigning category <i>k</i> to instance <i><b>x</b><sub>i</sub></i>. The category that gives the highest score is assigned as the category of the instance.</li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">19
				<a class="prev" href="#slide18"></a>
                                <a class="next" href="#slide20"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide20">
		<div class="header">
			<h1>2.2. Classifiers</h1>
		</div>
		<div class="content">
                  <figure>
                    <img src="./positivenegative.svg" height="500px" width="500px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">20
				<a class="prev" href="#slide19"></a>
                                <a class="next" href="#slide21"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide21">
		<div class="header">
			<h1>2.2. Classifiers</h1>
		</div>
		<div class="content">
                  <figure>
                    <img src="./Precisionrecall.svg" height="500px" width="500px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">21
				<a class="prev" href="#slide20"></a>
                                <a class="next" href="#slide22"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide22">
		<div class="header">
			<h1>2.2. Classifiers</h1>
		</div>
		<div class="content">
                   <p>Let</p>
                   <ul>
                    <li><i>tp</i>: number of true postives</li>
                    <li><i>fp</i>: number of false postives</li>
                    <li><i>fn</i>: number of false negatives</li>
                   </ul>
                   <p>Then</p>
                   <ul>
                    <li>Precision <i>p  = tp / (tp + fp)</i></li>
                    <li>Recall <i>r  = tp / (tp + fn)</i></li>
                    <li>F1-score <i>f1  = 2 * ((p * r) / (p + r))</i></li>
                   </ul>
                  <figure>
                    <img src="./Precisionrecall.svg" height="500px" width="300px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">22
				<a class="prev" href="#slide21"></a>
                                <a class="next" href="#slide23"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide23">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Discovering groups and structures in the data without using known structures in the data</li>
                           <li>Objects in a cluster are more similar to each other than the objects in the other cluster</li>
			</ul>
                  <figure>
                    <img src="../../2017/DataMining/images/320px-Cluster-2.svg.png" height="300px" width="300px"/>
                  </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">23
				<a class="prev" href="#slide22"></a>
                                <a class="next" href="#slide24"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide24">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Social network analysis</li>
                           <li>Image segmentation</li>
                           <li>Recommender systems</li>
                           <li>Grouping of shopping items</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">24
				<a class="prev" href="#slide23"></a>
                                <a class="next" href="#slide25"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide25">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>The goal of clustering is to find k subsets of <i><b>X</b></i>, in such a way that
                            <ul>
                              <li><i>C<sub>1</sub>.. &#8746; ..C<sub>k</sub> &#8746; C<sub>outliers</sub> = <b>X</b> </i> and</li>
                              <li><i>C<sub>i</sub> &#8745; C<sub>j</sub> = &#981;, i &#8800; j; 1 &lt;i,j &lt;k</i></li>
                              <li><i>C<sub>outliers</sub></i> may consist of outlier instances (data anomaly)</li>
                            </ul>
                           </li>
                         </ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">25
				<a class="prev" href="#slide24"></a>
                                <a class="next" href="#slide26"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide26">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1>Cluster models</h1>
			<ul>
                           <li><b>Centroid models</b>: cluster represented by a single mean vector</li>
                           <li><b>Connectivity models</b>: distance connectivity</li>
                           <li>Distribution models: clusters modeled using statistical distributions</li>
                           <li>Density models: clusters as connected dense regions in the data space</li>
                           <li>Subspace models</li>
                           <li>Group models</li>
                           <li>Graph-based models</li>
                           <li>Neural models</li>
			</ul>
                       <figure>
                         <img src="../../2017/DataMining/images/K_Means_Example_Step_4.svg.png" height="300px" width="300px"/>
                         <img src="../../2017/DataMining/images/Iris_dendrogram.png" height="300px" width="300px"/>
                       </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">26
				<a class="prev" href="#slide25"></a>
                                <a class="next" href="#slide27"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide27">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Finding a function which models the data</li>
                           <li>Assigns a real-valued output to each input</li>
                           <li>Estimating the relationships among variables</li>
                           <li>Relationship between a dependent variable ('criterion variable') and one or more independent variables ('predictors').</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">27
				<a class="prev" href="#slide26"></a>
                                <a class="next" href="#slide28"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide28">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Prediction</li>
                           <li>Forecasting</li>
                           <li>Machine learning</li>
                           <li>Finance</li>
			</ul>
                       <figure>
                         <img src="../../2017/DataMining/images/Linear_regression.svg" height="400px" width="400px"/>
                       </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">28
				<a class="prev" href="#slide27"></a>
                                <a class="next" href="#slide29"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide29">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>A function that maps a data item to a prediction variable</li>
                           <li>Let <i><b>X</b></i> be the independent variables</li>
                           <li>Let <i><b>Y</b></i> be the dependent variables</li>
                           <li>Let <i><b>&#946;</b></i> be the unknown parameters (scalar or vector)</li>
                           <li>The goal of regression model is to approximate <i><b>Y</b></i> with <i><b>X</b>,<b>&#946;</b></i>, i.e.,
                           <ul>
                             <li><i><b>Y</b> &#8773; f(<b>X</b>,<b>&#946;</b>)</i></li>
                           </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">29
				<a class="prev" href="#slide28"></a>
                                <a class="next" href="#slide30"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide30">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Linear regression</h1>
			<ul>
                           <li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> + &#949;<sub>i</sub></i> OR</li>
                           <li>parabola: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> + &#946;<sub>1</sub>x<sub>i</sub><sup>2</sup> +&#949;<sub>i</sub></i></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">30
				<a class="prev" href="#slide29"></a>
                                <a class="next" href="#slide31"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide31">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Linear regression</h1>
			<ul>
                           <li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> + &#949;<sub>i</sub></i> OR</li>
                           <li><i>ŷ<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub><sub>i</sub></i> OR</li>
                           <li>Residual: <i>e<sub>i</sub> = ŷ<sub>i</sub> - y<sub>i</sub></i></li>
                           <li>Sum of squared residuals, SSE  = &#931; e<sub>i</sub>, where 1 &lt; i &lt; n</li>
                           <li>The goal is to minimize SSE</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">31
				<a class="prev" href="#slide30"></a>
                                <a class="next" href="#slide32"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide32">
		<div class="header">
			<h1>2.4. Sequence Labeling</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Assigning a class to each member of a sequence of values </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">32
				<a class="prev" href="#slide31"></a>
                                <a class="next" href="#slide33"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide33">
		<div class="header">
			<h1>2.4. Sequence Labeling</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Part of speech tagging</li>
                           <li>Linguistic translation</li>
                           <li>Video analysis</li>
                           <li>Handwriting recognition</li>
                           <li>Information extraction</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">33
				<a class="prev" href="#slide32"></a>
                                <a class="next" href="#slide34"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide34">
		<div class="header">
			<h1>2.4. Sequence Labeling</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>X</b></i> be the input feature space</li>
                           <li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
                           <li>Let <i>&#12296;x<sub>1</sub>,...,x<sub>T</sub>&#12297;</i> be a sequence of length <i>T</i>.</li>
                           <li>The goal of sequence labeling is to generate a corresponding sequnce
                            <ul>
                             <li><i>&#12296;y<sub>1</sub>,...,y<sub>T&#12297;</sub></i> of labels</li>
                             <li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
                             <li><i>y<sub>j</sub> &#8712; <b>Y</b></i></li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">34
				<a class="prev" href="#slide33"></a>
                                <a class="next" href="#slide35"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide35">
		<div class="header">
			<h1>2.5. Association Rules</h1>
		</div>
		<div class="content">
			<h1>Association Rules</h1>
			<ul>
                           <li>Searches for relationships between variables</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">35
				<a class="prev" href="#slide34"></a>
                                <a class="next" href="#slide36"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide36">
		<div class="header">
			<h1>2.5. Association Rules</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Web usage mining</li>
                           <li>Intrusion detection</li>
                           <li>Affinity analysis</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">36
				<a class="prev" href="#slide35"></a>
                                <a class="next" href="#slide37"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide37">
		<div class="header">
			<h1>2.5. Association Rules</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>Let <i><b>I</b></i> be a set of <i>n</i> binary attributes called items</li>
                           <li>Let <i><b>T</b></i> be a set of <i>m</i> transactions called database</li>
                           <li>Let <i><b>I</b></i> = {<i>(i<sub>1</sub>,...,i<sub>n</sub>)</i>} and <i><b>T</b></i> = {<i>(t<sub>1</sub>,...,t<sub>m</sub>)</i>}</li>
                           <li>The goal of association rule learning is to find
                            <ul>
                             <li><i><b>X</b> &#8658; <b>Y</b></i>, where <i><b>X</b> &#8658; <b>Y</b> &#8838; <b>I</b></i></li>
                             <li><i><b>X</b></i> is the antecedent</li>
                             <li><i><b>Y</b></i> is the consequent</li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">37
				<a class="prev" href="#slide36"></a>
                                <a class="next" href="#slide38"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide38">
		<div class="header">
			<h1>2.5. Association Rules</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>Support: how frequently an itemset appears in the database
                            <ul>
                             <li><i>supp(<b>X</b>) = |t &#8712;<b>T</b></i>; <i><b>X</b> &#8838; t| / |<b>T</b>|</i></li>
                            </ul>
                           </li>
                           <li>Confidence:  how frequently the rule has been found to be true.
                            <ul>
                             <li><i>conf(<b>X</b> &#8658; <b>Y</b>) = supp(<b>X</b> &#8746; <b>Y</b>)/supp(<b>X</b>)</i></li>
                            </ul>
                           </li>
                           <li>Lift: the ratio of the observed support to that of the expected if X and Y were independent
                            <ul>
                             <li><i>lift(<b>X</b> &#8658; <b>Y</b>) = supp(<b>X</b> &#8746; <b>Y</b>)/(supp(<b>X</b>) &#10761; supp(<b>Y</b>))</i></li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">38
				<a class="prev" href="#slide37"></a>
                                <a class="next" href="#slide39"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide39">
		<div class="header">
			<h1>2.5. Association Rules</h1>
		</div>
		<div class="content">
			<h1>Example</h1>
			<ul>
                          <li>{<b>bread</b>, <b>butter</b>} &#8658; {<b>milk</b>}</li>
			</ul>
                       <figure>
                         <img src="../../2017/DataMining/images/associationruletable.png" height="400px" width="400px"/>
                       </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">39
				<a class="prev" href="#slide38"></a>
                                <a class="next" href="#slide40"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide40">
		<div class="header">
			<h1>2.6. Anomaly Detection</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Identification of unusual data records</li>
                           <li>Approaches
                             <ol>
                               <li>Unsupervised anomaly detection</li>
                               <li>Supervised anomaly detection</li>
                               <li>Semi-supervised anomaly detection</li>
                             </ol>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">40
				<a class="prev" href="#slide39"></a>
                                <a class="next" href="#slide41"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide41">
		<div class="header">
			<h1>2.6. Anomaly Detection</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Intrusion detection</li>
                           <li>Fraud detection</li>
                           <li>Remove anomalous data</li>
                           <li>System health monitoring</li>
                           <li>Event detection in sensor networks</li>
                           <li>Misuse detection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">41
				<a class="prev" href="#slide40"></a>
                                <a class="next" href="#slide42"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide42">
		<div class="header">
			<h1>2.6. Anomaly Detection</h1>
		</div>
		<div class="content">
			<h1>Characteristics</h1>
			<ul>
                           <li>Unexpected bursts</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">42
				<a class="prev" href="#slide41"></a>
                                <a class="next" href="#slide43"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide43">
		<div class="header">
			<h1>2.6. Anomaly Detection</h1>
		</div>
		<div class="content">
			<h1>Formalization</h1>
			<ul>
                           <li>Let <i><b>Y</b></i> be a set of measurements</li>
                           <li>Let <i>P<sub><b>Y</b></sub>(y)</i> be a statistical model for the distribution of <i><b>Y</b></i> under 'normal' conditions.</li>
                           <li>Let <i><b>T</b></i> be a user-defined threshold.</li>
                           <li>A measurement <i>x</i> is an outlier if <i>P<sub><b>Y</b></sub>(x) &lt; <b>T</b></i></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">43
				<a class="prev" href="#slide42"></a>
                                <a class="next" href="#slide44"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide44">
		<div class="header">
			<h1>2.7. Summarization</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Providing a more compact representation of the data set</li>
                           <li>Report Generation</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">44
				<a class="prev" href="#slide43"></a>
                                <a class="next" href="#slide45"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide45">
		<div class="header">
			<h1>2.7. Summarization</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Keyphrase extraction</li>
                           <li>Document summarization</li>
                           <li>Search engines</li>
                           <li>Image summarization</li>
                           <li>Video summarization: Finding important events from videos</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">45
				<a class="prev" href="#slide44"></a>
                                <a class="next" href="#slide46"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide46">
		<div class="header">
			<h1>2.7. Summarization</h1>
		</div>
		<div class="content">
			<h1>Formalization: Multidocument summarization</h1>
			<ul>
                           <li>Let {<i><b>D</b> = D<sub>1</sub>, ..., D<sub>k</sub></i>} be a document collection of k documents </li>
                           <li>A Document {<i>D = t<sub>1</sub>, ..., t<sub>m</sub></i>} consists of m textual units (words, sentences, paragraphs etc.) </li>
                           <li>Let {<i><b>D</b> = t<sub>1</sub>, ..., t<sub>n</sub></i>} be the complete set of all textual units from all documents, where
                            <ul>
                             <li><i>t<sub>i</sub> &#8712; <b>D</b>,</i> if and only if <i>&#8707; D<sub>j</sub></i> such that <i>t<sub>i</sub> &#8712; D<sub>j</sub></i></li>
                            </ul>
                           </li>
                           <li><i>S &#8838; <b>D</b></i> constitutes a summary</li>
                           <li> Two scoring functions
                            <ul>
                             <li><i>Rel(i)</i>: relevance of textual unit <i>i</i> in the summary</li>
                             <li><i>Red(i,j)</i>: Redundancy between two textual units <i>t<sub>i</sub></i>, t<sub>j</sub></li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">46
				<a class="prev" href="#slide45"></a>
                                <a class="next" href="#slide47"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide47">
		<div class="header">
			<h1>2.7. Summarization</h1>
		</div>
		<div class="content">
			<h1>Formalization: Multidocument summarization</h1>
			<ul>
                           <li>Scoring for a summary S
                            <ul>
                             <li><i>s(S)</i> score of summary S</li>
                             <li><i>l(i)</i> is the length of the textual unit i</li>
                             <li><i>K</i> is the fixed maximum length of the summary</li>
                            </ul>
                           </li>
			</ul>
                       <figure>
                         <img src="./scoringfunction.png" height="200px" width="500px"/>
                       </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">47
				<a class="prev" href="#slide46"></a>
                                <a class="next" href="#slide48"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide48">
		<div class="header">
			<h1>2.7. Summarization</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Finding a subset from the entire subset</li>
                           <li>Approaches
                             <ol>
                               <li><b>Extraction</b>:  Selecting a subset of existing words, phrases, or sentences in the original text without any modification</li>
                               <li><b>Abstraction</b>: Build an internal semantic representation and then use natural language generation techniques</li>
                             </ol>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">48
				<a class="prev" href="#slide47"></a>
                                <a class="next" href="#slide49"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide49">
		<div class="header">
			<h1>2.7. Summarization</h1>
		</div>
		<div class="content">
			<h1>Extractive summarization</h1>
			<ul>
                           <li>Approaches
                             <ol>
                               <li><b>Generic summarization</b>:  Obtaining a generic summary</li>
                               <li><b>Query relevant summarization</b>: Summary relevant to a query</li>
                             </ol>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">49
				<a class="prev" href="#slide48"></a>
                                <a class="next" href="#slide50"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide50">
		<div class="header">
			<h1>3. Algorithms</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ol>
                           <li>Support Vector Machines (SVM)</li>
                           <li>Stochastic Gradient Descent (SGD)</li>
                           <li>Nearest-Neighbours</li>
                           <li>Naive Bayes</li>
                           <li>Decision Trees</li>
                           <li>Ensemble Methods (Random Forest)</li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">50
				<a class="prev" href="#slide49"></a>
                                <a class="next" href="#slide51"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide51">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Introduction</h1>
			<ul>
                           <li>Supervised learning approach</li>
                           <li>Binary classification algorithm</li>
                           <li>Constructs a hyperplane ensuring the maximum separation between two classes</li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/SVM Separating Hyperplanes.svg" height="500px" width="500px"/>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">51
				<a class="prev" href="#slide50"></a>
                                <a class="next" href="#slide52"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide52">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Hyperplane</h1>
			<ul>
                           <li>Hyperplane of n-dimensional space is a subspace of dimension <i>n-1</i></li>
                           <li>Examples
                            <ul>
                              <li>Hyperplane of a 2-dimensional space is 1-dimensional line</li>
                              <li>Hyperplane of a 3-dimensional space is 2-dimensional plane</li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">52
				<a class="prev" href="#slide51"></a>
                                <a class="next" href="#slide53"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide53">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                           <li>The goal of a SVM is to estimate a function <i>f: R<sup>N</sup> &#10761; {+1,-1}</i>, i.e.,
                            <ul>
                             <li>If <i>x<sub>1</sub>,...,x<sub>l</sub></i>  &#8712; <i>R<sup>N</sup></i> are the <i>N</i> input data points,</li>
                             <li>the goal is to find <i>(x<sub>1</sub>,y<sub>1</sub>),...,(x<sub>l</sub>,y<sub>l</sub>)</i>  &#8712; <i>R<sup>N</sup> &#10761; {+1,-1}</i></li>
                            </ul>
                           </li>
                           <li>Any hyperplane can be written by the equation using set of input points <i><b>x</b></i>
                            <ul>
                              <li><i><b>w</b>.<b>x</b> - b = 0</i>, where</li>
                              <li><i><b>w</b></i> &#8712; R<sup>N</sup></i>, a normal vector to the plane</li>
                              <li><i>b &#8712; R</i></li>
                            </ul>
                           </li>
                           <li>A decision function is given by <i>f(x) = sign(<b>w</b>.<b>x</b> - b )</i>
                           </li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/Surface_normal_illustration.svg" height="400px" width="500px"/>
                          <figcaption>Normal vector</figcaption>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">53
				<a class="prev" href="#slide52"></a>
                                <a class="next" href="#slide54"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide54">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                          <li>If the training data are linearly separable, two hyperplanes can be selected</li>
                          <li>They separate the two classes of data, <br>so that distance between them is as large as possible.</li>
                          <li>The hyperplanes can be given by the equations
                           <ul>
                             <li><i><b>w</b>.<b>x</b> - b = 1</i></li>
                             <li><i><b>w</b>.<b>x</b> - b = -1</i></li>
                           </ul>
                          </li>
                          <li>The distance between the two hyperplanes can be given by <i>2/||<b>w</b>||</i></li>
                          <li>Region between these two hyperplanes is called margin.</li>
                          <li>Maximum-margin hyperplane is the hyperplane <br> that lies halfway between them.</li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png" height="500px" width="500px"/>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">54
				<a class="prev" href="#slide53"></a>
                                <a class="next" href="#slide55"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide55">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
                          <li>In order to prevent data points from falling into the margin, following constraints are added
                           <ul>
                             <li><i><b>w</b>.<b>x</b><sub>i</sub> - b &gt;= 1</i>, if <i>y<sub>i</sub> = 1</i></li>
                             <li><i><b>w</b>.<b>x</b><sub>i</sub> - b &lt;= -1</i>, if <i>y<sub>i</sub> = -1</i></li>
                           </ul>
                           <li><i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b) &gt;= 1</i> for  1&lt;= i &lt;= n</li>
                          </li>
                          <li>The goal is to minimize ||<b>w</b>|| subject to <i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b) &gt;= 1</i> for  1&lt;= i &lt;= n</li>
                          <li>Solving for both <i><b>w</b></i> and <i>b</i> gives our classifier
                                   <i>f(x) = sign(<b>w</b>.<b>x</b> - b)</i></li>
                          <li>Max-margin hyperplane is completely determined by the points that lie nearest to it, called the <b>support vectors</b></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">55
				<a class="prev" href="#slide54"></a>
                                <a class="next" href="#slide56"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide56">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Data mining tasks</h1>
			<ul>
                           <li>Classification (Multi-class classification)</li>
                           <li>Regression</li>
                           <li>Anomaly detection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">56
				<a class="prev" href="#slide55"></a>
                                <a class="next" href="#slide57"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide57">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Text and hypertext categorization</li>
                           <li>Image classification</li>
                           <li>Handwriting recognition</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">57
				<a class="prev" href="#slide56"></a>
                                <a class="next" href="#slide58"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide58">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent (SGD)</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>A stochastic approximation of the gradient descent optimization</li>
                           <li>Iterative method for minimizing an objective function that is written as a sum of differentiable functions.</li>
                           <li>Finds minima or maxima by iteration</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">58
				<a class="prev" href="#slide57"></a>
                                <a class="next" href="#slide59"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide59">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent</h1>
		</div>
		<div class="content">
			<h1>Gradient</h1>
			<ul>
                           <li>Multi-variable generalization of the derivative. </li>
                           <li>Gives slope of the tangent of the graph of a function</li>
                           <li>Gradient points in the direction of the greatest rate of increase of a function</li>
                           <li>Magnitude of gradient is the slope of the graph in that direction</li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/Gradient2.svg" height="500px" width="500px"/>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">59
				<a class="prev" href="#slide58"></a>
                                <a class="next" href="#slide60"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide60">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent</h1>
		</div>
		<div class="content">
			<h1>Gradient vs Derivative</h1>
			<ul>
                           <li>Derivatives defined on functions of single variable</li>
                           <li>Gradient defined on functions of multiple variables</li>
                           <li>Gradient is a vector-valued function (range is a vector)</li>
                           <li>Derivative is a scalar-valued function</li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/Gradient2.svg" height="500px" width="500px"/>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">60
				<a class="prev" href="#slide59"></a>
                                <a class="next" href="#slide61"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide61">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent</h1>
		</div>
		<div class="content">
			<h1>Gradient descent</h1>
			<ul>
                           <li>First-order iterative optimization algorithm for finding the minimum of a function.</li>
                           <li>Finding a local minima involves taking steps proportional to</br> the negative of the gradient of the function at the current point.</li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/Gradient_descent.svg" height="500px" width="500px"/>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">61
				<a class="prev" href="#slide60"></a>
                                <a class="next" href="#slide62"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide62">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent</h1>
		</div>
		<div class="content">
			<h1>Standard gradient descent method</h1>
			<ul>
                           <li>Let's take the problem of minimizing an objective function
                            <ul>
		             <li><i>Q(w) = 1/n (&#931;Q<sub>i</sub>(w)), 1&lt;=i&lt;n</i></li>
                             <li>Summand function <i>Q<sub>i</sub></i> associated with <i>i<sup>th</sup></i> observation in the data set.</li>
                            </ul>
                           </li>
                           <li><i>w = w - &#951;.&#8711; Q(w)</i></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">62
				<a class="prev" href="#slide61"></a>
                                <a class="next" href="#slide63"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide63">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent</h1>
		</div>
		<div class="content">
			<h1>Iterative method</h1>
			<ul>
                           <li>Choose an initial vector of parameters <w> and learning rate &#951;.</li>
                           <li>Repeat until an approximate minimum is obtained:
                            <ul>
                             <li>Randomly shuffle examples in the training set.</li>
                             <li><i>w = w - &#951;.&#8711; Q<sub>i</sub>(w),</i> for i=1...n</li>
                            </ul>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">63
				<a class="prev" href="#slide62"></a>
                                <a class="next" href="#slide64"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide64">
		<div class="header">
			<h1>3.2. Stochastic Gradient Descent</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Classification</li>
                           <li>Regression</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">64
				<a class="prev" href="#slide63"></a>
                                <a class="next" href="#slide65"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide65">
		<div class="header">
			<h1>3.3. Nearest-Neighbours</h1>
		</div>
		<div class="content">
			<h1>k-nearest neighbors algorithm</h1>
			<ul>
                           <li>k-NN classification: output is a class membership
                                 <br/> (object is classified by a majority vote of its neighbors.)</li>
                           <li>k-NN regression: output is the property value for the object </br>
                                (average values of its k nearest neighbors)</li>
			</ul>
                        <figure>
                          <img src="../../2017/DataMining/images/KnnClassification.svg" height="500px" width="500px"/>
                        </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">65
				<a class="prev" href="#slide64"></a>
                                <a class="next" href="#slide66"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide66">
		<div class="header">
			<h1>3.3. Nearest-Neighbours</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Regression</li>
                           <li>Anomaly detection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">66
				<a class="prev" href="#slide65"></a>
                                <a class="next" href="#slide67"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide67">
		<div class="header">
			<h1>3.4. Naive Bayes classifiers</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Collection of simple probabilistic classifiers based on applying Bayes' theorem with strong independence assumption between the features.</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">67
				<a class="prev" href="#slide66"></a>
                                <a class="next" href="#slide68"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide68">
		<div class="header">
			<h1>3.4. Naive Bayes classifiers</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Document classification (spam/non-spam)</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">68
				<a class="prev" href="#slide67"></a>
                                <a class="next" href="#slide69"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide69">
		<div class="header">
			<h1>3.4. Naive Bayes classifiers</h1>
		</div>
		<div class="content">
			<h1>Bayes' Theorem</h1>
			<ul>
                           <li>If A and B are events.</li>
                           <li>P(A), P(B) are probabilities of observing A and B independently of each other..</li>
                           <li>P(A|B) is conditional probability, the likelihood of event A occurring given that B is true</li>
                           <li>P(B|A) is conditional probability, the likelihood of event B occurring given that A is true</li>
                           <li> P(B) &#8800; 0</li>
                           <li><i>P(A|B) = (P(B|A).P(A))/P(B)</i></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">69
				<a class="prev" href="#slide68"></a>
                                <a class="next" href="#slide70"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide70">
		<div class="header">
			<h1>3.5. Decision Trees</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Decision support tool</li>
                           <li>Tree-like model of decisions and their possible consequences</li>
			</ul>
                       <figure>
                          <img src="../../2017/DataMining/images/Decision_tree_model.png" height="400px" width="400px"/>
                       </figure>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">70
				<a class="prev" href="#slide69"></a>
                                <a class="next" href="#slide71"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide71">
		<div class="header">
			<h1>3.5. Decision Trees</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Classification</li>
                           <li>Regression</li>
                           <li>Decision Analysis: identifying strategies to reach a goal</li>
                           <li>Operations Research</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">71
				<a class="prev" href="#slide70"></a>
                                <a class="next" href="#slide72"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide72">
		<div class="header">
			<h1>3.6. Ensemble Methods (Random Forest)</h1>
		</div>
		<div class="content">
			<h1>Defintion</h1>
			<ul>
                           <li>Collection of multiple learning algorithms to obtain better predictive performance than could be obtained from one of the constituting algorithms alone.</li>
                           <li>Random forests are obtained by building multiple decision trees at training time</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">72
				<a class="prev" href="#slide71"></a>
                                <a class="next" href="#slide73"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide73">
		<div class="header">
			<h1>3.6. Ensemble Methods (Random Forest)</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
                           <li>Multiclass classification</li>
                           <li>Multilabel classification (the problem of assigning one or more label to each instance. There is no limit on the number of classes an instance can be assigned to.)</li>
                           <li>Regression</li>
                           <li>Anomaly detection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">73
				<a class="prev" href="#slide72"></a>
                                <a class="next" href="#slide74"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide74">
		<div class="header">
			<h1>4. Feature Selection</h1>
		</div>
		<div class="content">
			<h1>Definition</h1>
			<ul>
                           <li>Process of selecting a subset of relevant features</li>
                           <li>Used in domains with large number of features and comparatively few sample points</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">74
				<a class="prev" href="#slide73"></a>
                                <a class="next" href="#slide75"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide75">
		<div class="header">
			<h1>4. Feature Selection</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
                           <li>Analysis of written texts</li>
                           <li>Analysis of DNA microarray data</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">75
				<a class="prev" href="#slide74"></a>
                                <a class="next" href="#slide76"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide76">
		<div class="header">
			<h1>4. Feature Selection</h1>
		</div>
		<div class="content">
			<h1>Formal defintion[8]</h1>
			<ul>
                           <li>Let <i>X</i> be the original set of n features, i.e., |<i>X</i>| = n</li>
                           <li>Let <i>w<sub>i</sub></i> be the weight assigned to feature <i>x<sub>i</sub></i>&#8712; <i>X</i></li>
                           <li>Binary feature selection assigns binary weights whereas continuous feature selection assigns weights preserving the order of its relevance.</li>
                           <li>Let <i>J(X')</i> be an evaluation measure, defined as <i>J: X' &#8838; X &#8594; R</i></li>
                           <li> Feature selection problem may be defined in three following ways
                            <ol>
                              <li>|<i>X'</i>| = <i>m &lt; n</i>. Find <i>X'</i> &#8834; <i>X</i> such that <i>J(X')</i> is maximum</li>
                              <li>Choose <i>J<sub>0</sub></i>, Find <i>X'</i> &#8838; <i>X</i>, such that <i>J(X') &gt;= J<sub>0</sub></i></li>
                              <li>Find a compromise among minimizing |<i>X'</i>| and maximizing <i>J(X')</i></li>
                            </ol>
                           </li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">76
				<a class="prev" href="#slide75"></a>
                                <a class="next" href="#slide77"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide77">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Research articles</h1>
			<ol>
		          <li>From data mining to knowledge discovery in databases, Usama Fayyad, Gregory Piatetsky-Shapiro, and Padhraic Smyth, AI Magazine Volume 17 Number 3 (1996)</li>
		          <li>Survey of Clustering Data Mining Techniques, Pavel Berkhin</li>
		          <li>Mining association rules between sets of items in large databases, Agrawal, Rakesh, Tomasz Imieliński, and Arun Swami. Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD 1993. p. 207. </li>
		          <li>Comparisons of Sequence Labeling Algorithms and Extensions, Nguyen, Nam, and Yunsong Guo. Proceedings of the 24th international conference on Machine learning. ACM, 2007. </li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">77
				<a class="prev" href="#slide76"></a>
                                <a class="next" href="#slide78"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide78">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Research articles</h1>
			<ol start="5">
		          <li>An Analysis of Active Learning Strategies for Sequence Labeling Tasks, Settles, Burr, and Mark Craven. Proceedings of the conference on empirical methods in natural language processing. Association for Computational Linguistics, 2008.</li>
		          <li>Anomaly detection in crowded scenes, Mahadevan; Vijay et al. Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on. IEEE, 2010</li>
		          <li>A Study of Global Inference Algorithms in Multi-Document Summarization. McDonald, Ryan. European Conference on Information Retrieval. Springer, Berlin, Heidelberg, 2007.</li>
		          <li>Feature selection algorithms: A survey and experimental evaluation., Molina, Luis Carlos, Lluís Belanche, and Àngela Nebot.  Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 2002.</li>
		          <li>Support vector machines, Hearst, Marti A., et al. IEEE Intelligent Systems and their applications 13.4 (1998): 18-28.</li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">78
				<a class="prev" href="#slide77"></a>
                                <a class="next" href="#slide79"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide79">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Online resources</h1>
			<ul>
				<li><a href="https://en.wikipedia.org/wiki/Patterns_in_nature">Patterns in Nature</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Statistical_classification">Statistical classification</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning">Association rule learning</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection">Anomaly detection</a></li>
			</ul>
			<ul>
				<li><a href="https://en.wikipedia.org/wiki/Sequence_labeling">Sequence labeling</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Automatic_summarization">Automatic summarization</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Pattern_recognition">Pattern recognition</a></li>
				<li><a href="http://scikit-learn.org/stable/">Scikit-learn</a></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">79
				<a class="prev" href="#slide78"></a>
                                <a class="next" href="#slide80"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide80">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Online resources</h1>
			<ul>
			<ul>
				<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient descent</a></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">80
				<a class="prev" href="#slide79"></a>
                                <a class="next" href="#slide81"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide81">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Colors</h1>
			<ul>
				<li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
			</ul>
			<h1>Images</h1>
			<ul>
				<li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Mining | John Samuel</div>
			<div class="navigation">81
				<a class="prev" href="#slide80"></a>
                                <a class="next" href="#slide82"></a>
			</div>
		</div>
	</section>

	<script>
		function changeCurrentURLSlideNumber(isIncrement) {
			url = window.location.href;
			position = url.indexOf("#slide");
			if (position != -1) { // Not on the first page
				slideIdString = url.substr(position + 6);
				if (!Number.isNaN(slideIdString)) {
					slideId = parseInt(slideIdString);
					if (isIncrement) {
                                               if (slideId  < 81) {
						slideId = slideId + 1;
                                               }
					} else {
						if (slideId > 1) {
							slideId = slideId - 1;
						}
					}
                                        /* regexp */
					url = url.replace(/#slide\d+/g, "#slide" + slideId);
					window.location.href = url;
				}
			} else {
				window.location.href = url + "#slide2";
			}
		}
		document.onkeydown = function(event) {

			event.preventDefault(); /* This will ensure the default behavior of
													        page scroll behaviour (up, down, right, left)*/

			event = event || window.event;
                        /*Codes de la touche sur le clavier: 37, 38, 39, 40*/
			if (event.keyCode == '37') {
				// left
				changeCurrentURLSlideNumber(false);
			} else if (event.keyCode == '38') {
				// up
				changeCurrentURLSlideNumber(false);
			} else if (event.keyCode == '39') {
				// right
				changeCurrentURLSlideNumber(true);
			} else if (event.keyCode == '40') {
				// down
				changeCurrentURLSlideNumber(true);
			}
		}
		document.body.onmouseup = function(event) {
			event = event || window.event;
			event.preventDefault();
			changeCurrentURLSlideNumber(true);
		}
	</script>
        <script src="MathJax.js"></script>
</body>
</html>
