<html>

<head>
	<meta charset="utf-8" />
	<title>Data Science for Chemists (2024): Introduction (ETI): John Samuel</title>
	<link rel="shortcut icon" href="../../../../../images/logo/favicon.png" />
	<style type="text/css">
		body {
			height: 100%;
			width: 100%;
			background-color: white;
			margin: 0;
			overflow: hidden;
			font-family: Arial;
		}

		.slide {
			height: 100%;
			width: 100%;
		}

		.content {
			height: 79%;
			width: 95vw;
			display: flex;
			line-height: 1.7em;
			flex-direction: column;
			align-items: flex-start;
			margin: 0 auto;
			color: #000000;
			text-align: left;
			padding-left: 1.5vmax;
			padding-top: 1.5vmax;
			overflow-x: auto;
			font-size: 2.8vmin;
			flex-wrap: wrap;
		}

		.content h1,
		h2,
		h3,
		h4 {
			color: #1B80CF;
		}

		.content .topichighlight {
			background-color: #78002E;
			color: #FFFFFF;
		}

		.content .topicheading {
			background-color: #1B80CF;
			color: #FFFFFF;
			vertical-align: middle;
			border-radius: 0 2vmax 2vmax 0%;
			height: 4vmax;
			line-height: 4vmax;
			padding-left: 1vmax;
			margin: 0.1vmax;
			width: 50%;
			margin-bottom: 1vmax;
		}

		.content .flexcontent {
			display: flex;
			overflow-y: auto;
			font-size: 2.8vmin;
			flex-wrap: wrap;
		}

		.content .gridcontent {
			display: grid;
			grid-template-columns: auto auto auto auto;
			grid-column-gap: 0px;
			grid-row-gap: 0px;
			grid-gap: 0px;
		}

		.content .topicsubheading {
			background-color: #1B80CF;
			color: #FFFFFF;
			vertical-align: middle;
			border-radius: 0 1.5vmax 1.5vmax 0%;
			height: 3vmax;
			margin: 0.1vmax;
			font-size: 90%;
			line-height: 3vmax;
			padding-left: 1vmax;
			width: 40%;
			margin-bottom: 1vmax;
		}

		.content table {
			color: #000000;
			font-size: 100%;
			width: 100%;
		}

		.content a:link,
		.content a:visited {
			color: #1B80CF;
			text-decoration: none;
		}

		.content th {
			color: #FFFFFF;
			background-color: #1B80CF;
			border-radius: 2vmax 2vmax 2vmax 2vmax;
			font-size: 120%;
			padding: 15px;
		}

		.content figure {
			max-width: 90%;
			max-height: 90%;
		}

		.content .fullwidth img {
			max-width: 90%;
			max-height: 90%;
		}

		.content figure img {
			max-width: 50vmin;
			max-height: 50vmin;
			display: block;
			margin-left: auto;
			margin-right: auto;
		}

		.content figure figcaption {
			max-width: 90%;
			max-height: 90%;
			margin: 0.1vmax;
			font-size: 90%;
			text-align: center;
			padding: 0.5vmax;
			background-color: #E1F5FE;
			border-radius: 2vmax 2vmax 2vmax 2vmax;
		}

		.content td {
			color: #000000;
			width: 8%;
			padding-left: 3vmax;
			padding-top: 1vmax;
			padding-bottom: 1vmax;
			background-color: #E1F5FE;
			border-radius: 2vmax 2vmax 2vmax 2vmax;
		}

		.content li {
			line-height: 1.7em;
		}

		.header {
			color: #ffffff;
			background-color: #00549d;
			height: 5vmax;
		}

		.header h1 {
			text-align: center;
			vertical-align: middle;
			font-size: 3vmax;
			line-height: 4vmax;
			margin: 0;
		}

		.footer {
			height: 3vmax;
			line-height: 3vmax;
			vertical-align: middle;
			color: #ffffff;
			background-color: #00549d;
			margin: 0;
			padding: .3vmax;
			overflow: hidden;
		}

		.footer .contact {
			float: left;
			color: #ffffff;
			text-align: left;
			font-size: 3.2vmin;
		}

		.footer .navigation {
			float: right;
			text-align: right;
			width: 8vw;
			font-size: 3vmin;
		}

		.footer .navigation .next,
		.prev {
			font-size: 3vmin;
			color: #ffffff;
			text-decoration: none;
		}

		.footer .navigation .next::after {
			content: "| >";
		}

		.footer .navigation .prev::after {
			content: "< ";
		}

		/* Using same Jupyter CSS
     */

		.highlight {
			background: #f8f8f8;
		}

		.highlight .c {
			color: #408080;
			font-style: italic
		}

		/* Comment */

		.highlight .err {
			border: 1px solid #FF0000
		}

		/* Error */

		.highlight .k {
			color: #008000;
			font-weight: bold
		}

		/* Keyword */

		.highlight .o {
			color: #666666
		}

		/* Operator */

		.highlight .ch {
			color: #408080;
			font-style: italic
		}

		/* Comment.Hashbang */

		.highlight .c1 {
			color: #408080;
			font-style: italic
		}

		/* Comment.Single */

		.highlight .cs {
			color: #408080;
			font-style: italic
		}

		/* Comment.Special */

		.highlight .cm {
			color: #408080;
			font-style: italic
		}

		/* Comment.Multiline */

		.highlight .nn {
			color: #0000FF;
			font-weight: bold
		}

		/* Name.Namespace */

		.highlight .k {
			color: #008000;
			font-weight: bold
		}

		/* Keyword */

		.highlight .s2 {
			color: #BA2121
		}

		/* Literal.String.Double */

		.highlight .s1 {
			color: #BA2121
		}

		/* Literal.String.Single */

		.highlight .kn {
			color: #008000;
			font-weight: bold
		}

		/* Keyword.Namespace */

		.highlight .nb {
			color: #008000
		}

		/* Name.Builtin */

		.highlight .mb {
			color: #666666
		}

		/* Literal.Number.Bin */

		.highlight .mf {
			color: #666666
		}

		/* Literal.Number.Float */

		.highlight .mh {
			color: #666666
		}

		/* Literal.Number.Hex */

		.highlight .mi {
			color: #666666
		}

		/* Literal.Number.Integer */

		.highlight .mo {
			color: #666666
		}

		/* Literal.Number.Oct */

		@media (max-width: 640px),
		screen and (orientation: portrait) {
			body {
				max-width: 100%;
				max-height: 100%;
			}

			.slide {
				height: 100%;
				width: 100%;
			}

			.content {
				width: 100%;
				height: 92%;
				display: flex;
				flex-direction: row;
				text-align: left;
				padding: 1vw;
				line-height: 3.8vmax;
				font-size: 1.8vmax;
				flex-wrap: wrap;
			}

			.content .topicheading {
				width: 90%;
			}

			.content h1,
			h2,
			h3,
			h4 {
				width: 100%;
			}

			.content figure img {
				max-width: 80vmin;
				max-height: 50vmin;
			}

			.content figure figcaption {
				max-width: 90%;
				max-height: 90%;
			}
		}

		@media print {
			body {
				max-width: 100%;
				max-height: 100%;
			}

			.content {
				font-size: 2.8vmin;
			}

			.content .flexcontent {
				font-size: 2.5vmin;
			}
		}
	</style>
</head>

<body>
	<section class="slide" id="slide1">
		<div class="header">
		</div>
		<div class="content">
			<h1 style="font-size:3.5vw">Data Science for Chemists</h1>
			<h2 style="font-size:1.5vw">IPL Summer School, CPE Lyon</h2>
			<p><b>John Samuel</b></br>
				CPE Lyon<br /><br />
				<b>Year</b>: 2023-2024<br />
				<b>Email</b>: john.samuel@cpe.fr</br>
				</br>
				<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img
						alt="Creative Commons License" style="border-width:0"
						src="../../../../../en/teaching/courses/2017/C/88x31.png" /></a>
			</p>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">1

				<a class="next" href="#slide2"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide2">
		<div class="header">
			<h1>Data Mining</h1>
		</div>
		<div class="content">
			<h1>Goals</h1>
			<ul>
				<li>Understanding Patterns</li>
				<li>Data mining tasks</li>
				<li>Algorithms for data mining</li>
				<li>Feature Selection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">2
				<a class="prev" href="#slide1"></a>
				<a class="next" href="#slide3"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide3">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<figure class="flexcontent">
				<img src="../../2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="300px" width="300px" />
				<img src="../../2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg" height="300px"
					width="300px" />
				<img src="../../2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="300px" width="300px" />
				<img src="../../2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG" height="300px"
					width="300px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">3
				<a class="prev" href="#slide2"></a>
				<a class="next" href="#slide4"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide4">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Patterns in Nature</h1>
			<ul>
				<li>Symmetry</li>
				<li>Trees, Fractals</li>
				<li>Spirals</li>
				<li>Chaos</li>
				<li>Waves</li>
				<li>Bubbles, Foam</li>
				<li>Tesselations</li>
				<li>Cracks</li>
				<li>Spots, stripes</li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/320px-Kittyply_edit1.jpg" height="150px" width="150px" />
				<img src="../../2017/DataMining/images/320px-Angelica_flowerhead_showing_pattern.JPG" height="150px"
					width="150px" />
				<img src="../../2017/DataMining/images/304px-Aloe_polyphylla_spiral.jpg" height="150px" width="150px" />
				<img src="../../2017/DataMining/images/Rio_Negro_meanders.JPG" height="150px" width="150px" />
			</figure>
			<figure>
				<img src="../../2017/DataMining/images/2006-01-14_Surface_waves.jpg" height="150px" width="150px" />
				<img src="../../2017/DataMining/images/320px-Apis_florea_nest_closeup2.jpg" height="150px"
					width="150px" />
				<img src="../../2017/DataMining/images/320px-Cracked_earth_in_the_Rann_of_Kutch.jpg" height="150px"
					width="150px" />
				<img src="../../2017/DataMining/images/320px-Equus_grevyi_(aka).jpg" height="150px" width="150px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">4
				<a class="prev" href="#slide3"></a>
				<a class="next" href="#slide5"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide5">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Patterns by Humans</h1>
			<ul>
				<li><b>Buildings (Symmetry)</b> : Human-made structures with symmetrical patterns.
					Example: Gothic cathedrals, modern skyscrapers.</li>

				<li><b>Cities</b> : Planned or organic agglomerations inhabited by humans.
					Example: Paris, New York.</li>

				<li><b>Virtual Environment (e.g., video games)</b> : Digitally created spaces for human interaction.
					Example: Open worlds in video games, virtual simulations.</li>

				<li><b>Human artifacts</b> : Objects made by humans in various fields.
					Example: Prehistoric tools, contemporary artworks.</li>

			</ul>
			<figure>
				<img src="../../2017/DataMining/images/Roman_geometric_mosaic.jpg" height="300px" width="600px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">5
				<a class="prev" href="#slide4"></a>
				<a class="next" href="#slide6"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide6">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Pattern creation</h1>
			<ul>
				<li>Repetition</li>

				<li><b>Fractals</b> : Self-similar mathematical structures at different scales.

					<ul>

						<li>Julia set: A fractal set defined by an iterative function <i>f(z) = z<sup>2</sup> + c</i>
						</li>

						<li><b>Characteristics</b> : produces complex repetitive patterns when visualized.</li>

					</ul>

				</li>

			</ul>
			<figure class="flexcontent">
				<img src="../../2017/DataMining/images/400px-Tiling_Dual_Semiregular_V3-3-3-3-6_Floret_Pentagonal.svg.png"
					height="300px" width="300px" />
				<img src="../../2017/DataMining/images/Finite_subdivision_of_a_radial_link.png" height="300px"
					width="300px" />
				<img src="../../2017/DataMining/images/Julia_set_(indigo).png" height="300px" width="300px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">6
				<a class="prev" href="#slide5"></a>
				<a class="next" href="#slide7"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide7">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Synonyms</h1>
			<ul>
				<li>Pattern recognition</li>
				<li>Knowledge discovery in databases</li>
				<li>Data mining</li>
				<li>Machine learning</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">7
				<a class="prev" href="#slide6"></a>
				<a class="next" href="#slide8"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide8">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Pattern Recognition: Approaches</h1>
			<ul>
				<li><b>Supervised Learning</b>: The model is trained on a labeled dataset where input examples are
					associated with desired outputs. The model learns to make predictions on new data based on these
					associations.</li>
				<li><b>Unsupervised Learning</b>: The model is exposed to unlabeled data and seeks to discover patterns,
					structures, or intrinsic relationships within the data.</li>
				<li><b>Semi-Supervised Learning</b>: A combination of the two above, using both labeled and unlabeled
					data for training.</li>
				<li><b>Reinforcement Learning</b>: The model learns to make decisions by interacting with its
					environment. It receives rewards or penalties based on its actions, which guides its learning.</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">8
				<a class="prev" href="#slide7"></a>
				<a class="next" href="#slide9"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide9">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization</h1>
			<ul>
				<li><b>Euclidean vector</b>:
					<ul>
						<li>An Euclidean vector is a geometric object characterized by its magnitude (length) and
							direction.</li>
						<li>Euclidean vectors are commonly used to represent data as points in a multidimensional space,
							where each dimension corresponds to a feature or variable.</li>
					</ul>
				</li>

				<li><b>Vector space</b>:
					<ul>
						<li>A vector space is a collection of vectors that can be added together and multiplied by
							numbers (scalars).</li>
					</ul>
				</li>

			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">9
				<a class="prev" href="#slide8"></a>
				<a class="next" href="#slide10"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide10">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization</h1>
			<ul>
				<li><b>Feature vector</b>:
					<ul>
						<li>A feature vector is an n-dimensional vector that represents the features or attributes of an
							entity.</li>
					</ul>
				</li>

				<li><b>Feature space</b>:
					<ul>
						<li>The feature space is the vector space associated with feature vectors.</li>
						<li>Each dimension of this space represents a particular feature, and vectors are used to
							position data in this space based on their features.</li>
					</ul>
				</li>

			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">10
				<a class="prev" href="#slide9"></a>
				<a class="next" href="#slide11"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide11">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Examples of Features</h1>
			<ul>
				<li><b>Images</b>: In the context of images, feature vectors can be constructed from pixel values. Each
					pixel can be considered as a dimension, and a feature vector will contain the values of all pixels,
					thus representing an image as a vector.</li>

				<li><b>Texts</b>: For texts, feature vectors are often constructed based on the frequency of words,
					phrases, or tokens in a document. This allows the representation of textual content using numerical
					values, which is essential for text analysis and information retrieval.</li>

			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">11
				<a class="prev" href="#slide10"></a>
				<a class="next" href="#slide12"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide12">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization</h1>
			<ul>
				<li><b>Feature construction<sup>1</sup></b>:
					<ul>
						<li>Feature construction involves creating new variables or attributes from those already
							present in the data.</li>
						<li>This step can be crucial for improving the performance of machine learning models by
							introducing relevant information and eliminating noise.</li>
					</ul>
				</li>

				<li><b>Feature construction operators</b>:
					<ul>
						<li>Construction operators are functions or mathematical operations used to create new features
							from existing ones.</li>
						<li>Commonly used operators include equality operators (comparisons), arithmetic operators
							(addition, subtraction, multiplication, division), array operators (min, max, mean, median,
							etc.), transformation functions, etc.</li>
					</ul>
				</li>
				<ol style="font-size:2vh">
					<li>https://en.wikipedia.org/wiki/Feature_vector</li>
				</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">12
				<a class="prev" href="#slide11"></a>
				<a class="next" href="#slide13"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide13">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Example</h1>
			<ul>
				<li>Let <b>Year of Birth</b> and <b>Year of Death</b> be two existing features.</li>
				<li>A new feature called <b>age</b> is created. <b>age</b> = <b>Year of Death</b> - <b>Year of Birth</b>
				</li>
			</ul>

			<p>Feature construction is an essential step in the data preprocessing pipeline in machine learning, as it
				can help make data more informative for learning algorithms.</p>

		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">13
				<a class="prev" href="#slide12"></a>
				<a class="next" href="#slide14"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide14">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization: Supervised learning</h1>
			<ul>
				<li>Let <i><b>N</b></i> be the number of training examples</li>
				<li>Let <i><b>X</b></i> be the input feature space</li>
				<li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
				<li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub><b>N</b></sub>, y<sub><b>N</b></sub>)</i>} be the
					<i><b>N</b></i> training examples, where
					<ul>
						<li><i>x<sub>i</sub></i> is the feature vector of <i>i<sup>th</sup></i> training example.</li>
						<li><i>y<sub>i</sub></i> is its label.</li>
					</ul>
				</li>
				<li>The goal of supervised learning algorithm is to find <i>g: <b>X</b> &#8594; <b>Y</b></i>, where
					<ul>
						<li><i>g</i> is one of the functions from the set of possible functions <i>G</i> (hypotheses
							space)</li>
					</ul>
				</li>
				<li><b>Scoring function <i>F</i></b> denote the space of scoring functions, where
					<ul>
						<li><i>f: <b>X</b> &#215; <b>Y</b> &#8594; <b>R</b></i> such that <i>g</i> returns the highest
							scoring function.</li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">14
				<a class="prev" href="#slide13"></a>
				<a class="next" href="#slide15"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide15">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization: Unsupervised learning</h1>
			<ul>
				<li>Let <i><b>X</b></i> be the input feature space</li>
				<li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
				<li>The goal of unsupervised learning algorithm is to
					<ul>
						<li>find mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">15
				<a class="prev" href="#slide14"></a>
				<a class="next" href="#slide16"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide16">
		<div class="header">
			<h1>1. Patterns</h1>
		</div>
		<div class="content">
			<h1>Formalization: Semi-supervised learning</h1>
			<ul>
				<li>Let <i><b>X</b></i> be the input feature space</li>
				<li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
				<li>Let {<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>l</sub>)</i>} be the
					<i><b>l</b></i> be the set of labeled training examples
				</li>
				<li>Let {<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>} be the <i><b>u</b></i> be the set of unlabeled
					feature vectors of <i><b>X</b></i>.</li>
				<li>The goal of semi-supervised learning algorithm is to do
					<ul>
						<li><b>Transductive learning</b>, i.e., find correct labels for
							{<i>x<sub>l+1</sub>,...,x<sub>l+u</sub></i>}. OR</li>
						<li><b>Inductive learning</b>, i.e., find correct mapping <i><b>X</b> &#8594; <b>Y</b></i></li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">16
				<a class="prev" href="#slide15"></a>
				<a class="next" href="#slide17"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide17">
		<div class="header">
			<h1>2. Data Mining</h1>
		</div>
		<div class="content">
			<h1>Common Tasks in Data Mining</h1>
			<ol>
				<li>Classification</li>
				<li>Clustering</li>
				<li>Regression</li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">17
				<a class="prev" href="#slide16"></a>
				<a class="next" href="#slide18"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide18">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Classifiers</h1>
			<ul>
				<li><b>Algorithmic categorization of objects</b> : Process of assigning classes or categories to objects
					via algorithms. The goal is to organize data into distinct groups to facilitate analysis and
					decision-making.</li>

				<li><b>Class assignment</b> : Assigning a class or category to each object (or individual).</li>

				<li><b>Types of classification:</b>
					<ul>
						<li><b>Binary classification</b> : Assignment to two classes.</li>
						<li><b>Multi-class classification</b> : Assignment to multiple classes simultaneously.</li>
					</ul>
				</li>

			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">18
				<a class="prev" href="#slide17"></a>
				<a class="next" href="#slide19"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide19">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
				<li>Spam vs Non-spam</li>
				<li>Document classification</li>
				<li>Handwriting recognition</li>
				<li>Speech Recognition</li>
				<li>Internet Search Engines</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">19
				<a class="prev" href="#slide18"></a>
				<a class="next" href="#slide20"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide20">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
				<li>Let <i><b>X</b></i> be the input feature space</li>
				<li>Let <i><b>Y</b></i> be the output feature space (of labels)</li>
				<li>The goal of classification algorithm (or classifier) is to find
					{<i>(x<sub>1</sub>, y<sub>1</sub>),...,(x<sub>l</sub>, y<sub>k</sub>)</i>}, i.e., assigning a known
					label to every input feature vector, where
					<ul>
						<li><i>x<sub>i</sub> &#8712; <b>X</b></i> </li>
						<li><i>y<sub>i</sub> &#8712; <b>Y</b></i></li>
						<li>|<i><b>X</b> </i>| <i>= l</i></li>
						<li>|<i><b>Y</b> </i>| <i>= k</i></li>
						<li>l &gt;= k</li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">20
				<a class="prev" href="#slide19"></a>
				<a class="next" href="#slide21"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide21">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h2 class="topicsubheading">Classifiers</h2>
			<figure>
				<img src="../../2018/DataMining/positivenegative.svg" height="500px" width="500px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">21
				<a class="prev" href="#slide20"></a>
				<a class="next" href="#slide22"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide22">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h2 class="topicsubheading">Classifiers</h2>
			<figure>
				<img src="../../2018/DataMining/Precisionrecall.svg" height="500px" width="500px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">22
				<a class="prev" href="#slide21"></a>
				<a class="next" href="#slide23"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide23">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<p>Let</p>
			<ul>
				<li><i>tp</i>: number of true postives</li>
				<li><i>fp</i>: number of false postives</li>
				<li><i>fn</i>: number of false negatives</li>
			</ul>
			<p>Then</p>
			<ul>
				<li>Accuracy <i>a = (tp + tn) / (tp + tn + fp + fn)</i></li>
				<li>Precision <i>p = tp / (tp + fp)</i></li>
				<li>Recall <i>r = tp / (tp + fn)</i></li>
				<li>Specificity <i>r = tn / (tp + fn)</i></li>
				<li>F1-score <i>f1 = 2 * ((p * r) / (p + r))</i></li>
			</ul>
			<figure>
				<img src="../../2018/DataMining/Precisionrecall.svg" height="500px" width="300px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">23
				<a class="prev" href="#slide22"></a>
				<a class="next" href="#slide24"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide24">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h2 class="topicsubheading">Confusion Matrix</h2>
			<figure>
				<img src="../../2019/DataMining/confusionmatrix.png" height="400px" />
				<figcaption>Confusion Matrix for a SVM classifier of handwritten digits (MNIST)</figcaption>
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">24
				<a class="prev" href="#slide23"></a>
				<a class="next" href="#slide25"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide25">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h2 class="topicsubheading">Confusion Matrix</h2>
			<figure>
				<img src="../../2019/DataMining/confusionmatrix1.png" height="400px" />
				<figcaption>Confusion Matrix plot of a Perception of handwritten digits (MNIST)</figcaption>
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">25
				<a class="prev" href="#slide24"></a>
				<a class="next" href="#slide26"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide26">
		<div class="header">
			<h1>2.1. Classification</h1>
		</div>
		<div class="content">
			<h2 class="topicsubheading">Multiclass classification</h2>
			<figure>
				<img src="../../2019/MachineLearning/multiclassclassifier.svg" height="400px" />
				<figcaption>Multiclass classification</figcaption>
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">26
				<a class="prev" href="#slide25"></a>
				<a class="next" href="#slide27"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide27">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
				<li>Discovering groups and structures in the data without using known structures in the data</li>
				<li>Objects in a cluster are more similar to each other than the objects in the other cluster</li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/320px-Cluster-2.svg.png" height="300px" width="300px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">27
				<a class="prev" href="#slide26"></a>
				<a class="next" href="#slide28"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide28">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
				<li>Social network analysis</li>
				<li>Image segmentation</li>
				<li>Recommender systems</li>
				<li>Grouping of shopping items</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">28
				<a class="prev" href="#slide27"></a>
				<a class="next" href="#slide29"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide29">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
				<li>Let <i><b>X</b></i> be the input feature space</li>
				<li>The goal of clustering is to find k subsets of <i><b>X</b></i>, in such a way that
					<ul>
						<li><i>C<sub>1</sub>.. &#8746; ..C<sub>k</sub> &#8746; C<sub>outliers</sub> = <b>X</b> </i> and
						</li>
						<li><i>C<sub>i</sub> &#8745; C<sub>j</sub> = &#981;, i &#8800; j; 1 &lt;i,j &lt;k</i></li>
						<li><i>C<sub>outliers</sub></i> may consist of outlier instances (data anomaly)</li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">29
				<a class="prev" href="#slide28"></a>
				<a class="next" href="#slide30"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide30">
		<div class="header">
			<h1>2.2. Clustering</h1>
		</div>
		<div class="content">
			<h1>Cluster models</h1>
			<ul>
				<li><b>Centroid models</b>: cluster represented by a single mean vector</li>
				<li><b>Connectivity models</b>: distance connectivity</li>
				<li>Distribution models: clusters modeled using statistical distributions</li>
				<li>Density models: clusters as connected dense regions in the data space</li>
				<li>Subspace models</li>
				<li>Group models</li>
				<li>Graph-based models</li>
				<li>Neural models</li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/K_Means_Example_Step_4.svg.png" height="300px" width="300px" />
				<img src="../../2017/DataMining/images/Iris_dendrogram.png" height="300px" width="300px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">30
				<a class="prev" href="#slide29"></a>
				<a class="next" href="#slide31"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide31">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
				<li>Finding a function which models the data</li>
				<li>Assigns a real-valued output to each input</li>
				<li>Estimating the relationships among variables</li>
				<li>Relationship between a dependent variable ('criterion variable') and one or more independent
					variables ('predictors').</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">31
				<a class="prev" href="#slide30"></a>
				<a class="next" href="#slide32"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide32">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
				<li>Prediction</li>
				<li>Forecasting</li>
				<li>Machine learning</li>
				<li>Finance</li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/Linear_regression.svg" height="400px" width="400px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">32
				<a class="prev" href="#slide31"></a>
				<a class="next" href="#slide33"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide33">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
				<li>A function that maps a data item to a prediction variable</li>
				<li>Let <i><b>X</b></i> be the independent variables</li>
				<li>Let <i><b>Y</b></i> be the dependent variables</li>
				<li>Let <i><b>&#946;</b></i> be the unknown parameters (scalar or vector)</li>
				<li>The goal of regression model is to approximate <i><b>Y</b></i> with <i><b>X</b>,<b>&#946;</b></i>,
					i.e.,
					<ul>
						<li><i><b>Y</b> &#8773; f(<b>X</b>,<b>&#946;</b>)</i></li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">33
				<a class="prev" href="#slide32"></a>
				<a class="next" href="#slide34"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide34">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Linear regression</h1>
			<ul>
				<li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> +
						&#949;<sub>i</sub></i> OR</li>
				<li>parabola: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> +
						&#946;<sub>1</sub>x<sub>i</sub><sup>2</sup> +&#949;<sub>i</sub></i></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">34
				<a class="prev" href="#slide33"></a>
				<a class="next" href="#slide35"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide35">
		<div class="header">
			<h1>2.3. Regression</h1>
		</div>
		<div class="content">
			<h1>Linear regression</h1>
			<ul>
				<li>straight line: <i>y<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub>x<sub>i</sub> +
						&#949;<sub>i</sub></i> OR</li>
				<li><i>ŷ<sub>i</sub> = &#946;<sub>0</sub> + &#946;<sub>1</sub><sub>i</sub></i> OR</li>
				<li>Residual: <i>e<sub>i</sub> = ŷ<sub>i</sub> - y<sub>i</sub></i></li>
				<li>Sum of squared residuals, SSE = &#931; e<sub>i</sub>, where 1 &lt; i &lt; n</li>
				<li>The goal is to minimize SSE</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">35
				<a class="prev" href="#slide34"></a>
				<a class="next" href="#slide36"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide36">
		<div class="header">
			<h1>3. Algorithms</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ol>
				<li>Support Vector Machines (SVM)</li>
				<li>Decision Trees</li>
				<li>Ensemble Methods (Random Forest)</li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">36
				<a class="prev" href="#slide35"></a>
				<a class="next" href="#slide37"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide37">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Introduction</h1>
			<ul style="width:40%">
				<li>Supervised learning approach</li>
				<li>Binary classification algorithm</li>
				<li>Constructs a hyperplane ensuring the maximum separation between two classes</li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/SVM Separating Hyperplanes.svg" height="500px" width="500px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">37
				<a class="prev" href="#slide36"></a>
				<a class="next" href="#slide38"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide38">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Hyperplane</h1>
			<ul style="width:40%">
				<li>Hyperplane of n-dimensional space is a subspace of dimension <i>n-1</i></li>
				<li>Examples
					<ul>
						<li>Hyperplane of a 2-dimensional space is 1-dimensional line</li>
						<li>Hyperplane of a 3-dimensional space is 2-dimensional plane</li>
					</ul>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">38
				<a class="prev" href="#slide37"></a>
				<a class="next" href="#slide39"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide39">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<ul style="width:40%">
				<li>The goal of a SVM is to estimate a function <i>f: R<sup>N</sup> &#10761; {+1,-1}</i>, i.e.,
					<ul>
						<li>If <i>x<sub>1</sub>,...,x<sub>l</sub></i> &#8712; <i>R<sup>N</sup></i> are the <i>N</i>
							input data points,</li>
						<li>the goal is to find <i>(x<sub>1</sub>,y<sub>1</sub>),...,(x<sub>l</sub>,y<sub>l</sub>)</i>
							&#8712; <i>R<sup>N</sup> &#10761; {+1,-1}</i></li>
					</ul>
				</li>
				<li>Any hyperplane can be written by the equation using set of input points <i><b>x</b></i>
					<ul>
						<li><i><b>w</b>.<b>x</b> - b = 0</i>, where</li>
						<li><i><b>w</b></i> &#8712; R<sup>N</sup></i>, a normal vector to the plane</li>
						<li><i>b &#8712; R</i></li>
					</ul>
				</li>
				<li>A decision function is given by <i>f(x) = sign(<b>w</b>.<b>x</b> - b )</i>
				</li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/Surface_normal_illustration.svg" height="400px" width="500px" />
				<figcaption>Normal vector</figcaption>
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">39
				<a class="prev" href="#slide38"></a>
				<a class="next" href="#slide40"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide40">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1 class="topicsubheading">Formalisation</h1>
			<ul style="width:40%">
				<li>If the training data are linearly separable, two hyperplanes can be selected</li>
				<li>They separate the two classes of data, <br>so that distance between them is as large as possible.
				</li>
				<li>The hyperplanes can be given by the equations
					<ul>
						<li><i><b>w</b>.<b>x</b> - b = 1</i></li>
						<li><i><b>w</b>.<b>x</b> - b = -1</i></li>
					</ul>
				</li>
				<li>The distance between the two hyperplanes can be given by <i>2/||<b>w</b>||</i></li>
			</ul>
			<figure>
				<img src="../../2017/DataMining/images/Svm_max_sep_hyperplane_with_margin.png" height="500px"
					height="500px" width="500px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">40
				<a class="prev" href="#slide39"></a>
				<a class="next" href="#slide41"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide41">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
				<li>Region between these two hyperplanes is called margin.</li>
				<li>Maximum-margin hyperplane is the hyperplane <br> that lies halfway between them.</li>
				<li>In order to prevent data points from falling into the margin, following constraints are added
					<ul>
						<li><i><b>w</b>.<b>x</b><sub>i</sub> - b &gt;= 1</i>, if <i>y<sub>i</sub> = 1</i></li>
						<li><i><b>w</b>.<b>x</b><sub>i</sub> - b &lt;= -1</i>, if <i>y<sub>i</sub> = -1</i></li>
					</ul>
				<li><i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b) &gt;= 1</i> for 1&lt;= i &lt;= n</li>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">41
				<a class="prev" href="#slide40"></a>
				<a class="next" href="#slide42"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide42">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Formal definition</h1>
			<ul>
				<li>The goal is to minimize ||<b>w</b>|| subject to <i>y<sub>i</sub>(<b>w</b>.<b>x</b><sub>i</sub> - b)
						&gt;= 1</i> for 1&lt;= i &lt;= n</li>
				<li>Solving for both <i><b>w</b></i> and <i>b</i> gives our classifier
					<i>f(x) = sign(<b>w</b>.<b>x</b> - b)</i>
				</li>
				<li>Max-margin hyperplane is completely determined by the points that lie nearest to it, called the
					<b>support vectors</b>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">42
				<a class="prev" href="#slide41"></a>
				<a class="next" href="#slide43"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide43">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Data mining tasks</h1>
			<ul>
				<li>Classification (Multi-class classification)</li>
				<li>Regression</li>
				<li>Anomaly detection</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">43
				<a class="prev" href="#slide42"></a>
				<a class="next" href="#slide44"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide44">
		<div class="header">
			<h1>3.1. Support Vector Machines (SVM)</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
				<li>Text and hypertext categorization</li>
				<li>Image classification</li>
				<li>Handwriting recognition</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">44
				<a class="prev" href="#slide43"></a>
				<a class="next" href="#slide45"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide45">
		<div class="header">
			<h1>3.2. Decision Trees</h1>
		</div>
		<div class="content">
			<h1>Decision Trees</h1>
			<figure>
				<img src="../../2017/DataMining/images/Decision_tree_model.png" height="400px" width="400px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">45
				<a class="prev" href="#slide44"></a>
				<a class="next" href="#slide46"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide46">
		<div class="header">
			<h1>3.2. Decision Trees</h1>
		</div>
		<div class="content">
			<h1>Decision Trees</h1>
			<p>Decision trees are a powerful decision support tool that uses a tree-like model to represent decisions
				and their possible consequences.</p>

			<ul>

				<li><b>Tree model:</b> Decision trees represent decisions in a tree structure, where each internal node
					represents a feature (or attribute), each branch represents a decision based on that feature, and
					each leaf represents an outcome or class.</li>

				<li><b>Easy to interpret:</b> Decision trees are easy to understand and interpret, making them popular
					for decision-making in many fields.</li>

				<li><b>Adaptability:</b> They can be used to model both classification and regression problems.</li>

				<li><b>Use of simple rules:</b> Decisions are made by following simple rules based on feature values,
					making interpretation of results easy even for non-experts.</li>

			</ul>

			<figure>
				<img src="../../2017/DataMining/images/Decision_tree_model.png" height="400px" width="400px" />
			</figure>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">46
				<a class="prev" href="#slide45"></a>
				<a class="next" href="#slide47"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide47">
		<div class="header">
			<h1>3.2. Decision Trees</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
				<li>Classification</li>
				<li>Regression</li>
				<li>Decision Analysis: identifying strategies to reach a goal</li>
				<li>Operations Research</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">47
				<a class="prev" href="#slide46"></a>
				<a class="next" href="#slide48"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide48">
		<div class="header">
			<h1>3.3. Ensemble Methods (Random Forest)</h1>
		</div>
		<div class="content">
			<h1>Ensemble learning</h1>
			<p>Ensemble learning, particularly decision tree forests, is a technique that combines multiple learning
				models to improve predictive performance compared to a single model. Decision tree forests are obtained
				by constructing multiple decision trees during the training phase.</p>

			<ul>

				<li><b>Construction of decision trees</b> : During the training phase, multiple decision trees are
					constructed using different subsets of data and/or features. Each tree is independently trained.
				</li>
				<li><b>Majority vote</b> : For classification, each decision tree votes for the predicted class of a new
					example. The final class assigned to the example is determined by a majority vote among all the
					trees in the forest. For regression, the predicted values by each tree are averaged to obtain the
					final value.</li>
				<li><b>Stability</b> : Decision tree forests are generally more stable than individual decision trees
					because they are less sensitive to variations in the training data.</li>


			</ul>

		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">48
				<a class="prev" href="#slide47"></a>
				<a class="next" href="#slide49"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide49">
		<div class="header">
			<h1>3.3. Ensemble Methods (Random Forest)</h1>
		</div>
		<div class="content">
			<h1></h1>
			<ul>
				<li><b>Multi-class classification</b> : Decision tree forests are used to classify instances into one of
					several mutually exclusive classes. For example, classifying images into different categories such
					as animals, vehicles, household objects, etc.</li>

				<li><b>Multi-label classification</b> : Unlike multi-class classification, multi-label classification
					allows an instance to be assigned to multiple classes simultaneously. For example, classifying
					documents where a document can be associated with multiple categories such as "science",
					"technology", "politics", etc.</li>

				<li><b>Regression</b> : Decision tree forests can also be used for regression tasks, where the output is
					a continuous value rather than a discrete class. For example, predicting the price of a house based
					on its features.</li>

				<li><b>Anomaly detection</b> : They can also be employed to detect anomalies or unusual behaviors in
					data. This can be useful in various domains such as detecting financial fraud, identifying failures
					in industrial systems, etc.</li>

			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">49
				<a class="prev" href="#slide48"></a>
				<a class="next" href="#slide50"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide50">
		<div class="header">
			<h1>4. Feature Selection</h1>
		</div>
		<div class="content">
			<p><b>Feature selection</b> is a process aimed at choosing a subset of relevant features from a large number
				of available features.</p>

			<ul>

				<li>This technique is widely used in fields where the number of features is high compared to the sample
					size, as this can lead to overfitting and high computation time.</li>

				<li>Feature selection is also considered a dimensionality reduction method, as it aims to reduce the
					number of dimensions in the feature space without losing discriminative information.</li>

				<li>Feature selection aims to:
					<ul>
						<li>Identify the most relevant features that contribute the most to data variability or model
							prediction capability.</li>

						<li>Reduce the dimensionality of the feature space to improve the performance of machine
							learning models in terms of computation time and overfitting prevention.</li>
					</ul>
				</li>

			</ul>

		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">50
				<a class="prev" href="#slide49"></a>
				<a class="next" href="#slide51"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide51">
		<div class="header">
			<h1>4. Feature Selection</h1>
		</div>
		<div class="content">
			<h1>Applications</h1>
			<ul>
				<li><b>Analysis of written texts</b> : In the analysis of written texts, feature selection is
					effectively used to extract the most relevant and informative elements from textual data. This can
					include identifying keywords, named entities, specific linguistic patterns, or other features
					essential for text analysis tasks such as document classification, information extraction, or
					summary generation. Feature selection in this context aims to reduce the dimensionality of textual
					data while preserving their relevance and expressiveness for subsequent analysis tasks.</li>

				<li><b>Analysis of DNA microarray data</b> : In genomics and bioinformatics, DNA microarrays generate
					highly dimensional datasets that often require dimensionality reduction to identify the most
					significant genes or DNA sequences associated with particular phenotypes, such as diseases or
					biological responses.</li>

			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">51
				<a class="prev" href="#slide50"></a>
				<a class="next" href="#slide52"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide52">
		<div class="header">
			<h1>4. Feature Selection</h1>
		</div>
		<div class="content">
			<h1>Formal defintion[8]</h1>
			<ul>
				<li>Let <i>X</i> be the original set of n features, i.e., |<i>X</i>| = n</li>
				<li>Let <i>w<sub>i</sub></i> be the weight assigned to feature <i>x<sub>i</sub></i>&#8712; <i>X</i></li>
				<li>Binary feature selection assigns binary weights whereas continuous feature selection assigns weights
					preserving the order of its relevance.</li>
				<li>Let <i>J(X')</i> be an evaluation measure, defined as <i>J: X' &#8838; X &#8594; R</i></li>
				<li> Feature selection problem may be defined in three following ways
					<ol>
						<li>|<i>X'</i>| = <i>m &lt; n</i>. Find <i>X'</i> &#8834; <i>X</i> such that <i>J(X')</i> is
							maximum</li>
						<li>Choose <i>J<sub>0</sub></i>, Find <i>X'</i> &#8838; <i>X</i>, such that <i>J(X') &gt;=
								J<sub>0</sub></i></li>
						<li>Find a compromise among minimizing |<i>X'</i>| and maximizing <i>J(X')</i></li>
					</ol>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">52
				<a class="prev" href="#slide51"></a>
				<a class="next" href="#slide53"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide53">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Research articles</h1>
			<ol>
				<li>From data mining to knowledge discovery in databases, Usama Fayyad, Gregory Piatetsky-Shapiro, and
					Padhraic Smyth, AI Magazine Volume 17 Number 3 (1996)</li>
				<li>Survey of Clustering Data Mining Techniques, Pavel Berkhin</li>
				<li>Mining association rules between sets of items in large databases, Agrawal, Rakesh, Tomasz
					Imieliński, and Arun Swami. Proceedings of the 1993 ACM SIGMOD international conference on
					Management of data - SIGMOD 1993. p. 207. </li>
				<li>Comparisons of Sequence Labeling Algorithms and Extensions, Nguyen, Nam, and Yunsong Guo.
					Proceedings of the 24th international conference on Machine learning. ACM, 2007. </li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">53
				<a class="prev" href="#slide52"></a>
				<a class="next" href="#slide54"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide54">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Research articles</h1>
			<ol start="5">
				<li>An Analysis of Active Learning Strategies for Sequence Labeling Tasks, Settles, Burr, and Mark
					Craven. Proceedings of the conference on empirical methods in natural language processing.
					Association for Computational Linguistics, 2008.</li>
				<li>Anomaly detection in crowded scenes, Mahadevan; Vijay et al. Computer Vision and Pattern Recognition
					(CVPR), 2010 IEEE Conference on. IEEE, 2010</li>
				<li>A Study of Global Inference Algorithms in Multi-Document Summarization. McDonald, Ryan. European
					Conference on Information Retrieval. Springer, Berlin, Heidelberg, 2007.</li>
				<li>Feature selection algorithms: A survey and experimental evaluation., Molina, Luis Carlos, Lluís
					Belanche, and Àngela Nebot. Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International
					Conference on. IEEE, 2002.</li>
				<li>Support vector machines, Hearst, Marti A., et al. IEEE Intelligent Systems and their applications
					13.4 (1998): 18-28.</li>
			</ol>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">54
				<a class="prev" href="#slide53"></a>
				<a class="next" href="#slide55"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide55">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Online resources (English Wikipedia)</h1>
			<ul>
				<li><a
						href="https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124">Accuracy,
						Recall, Precision, F-Score & Specificity, which to optimize on?</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Patterns_in_nature">Patterns in Nature</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Data_mining">Data Mining</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Statistical_classification">Statistical classification</a>
				</li>
				<li><a href="https://en.wikipedia.org/wiki/Regression_analysis">Regression analysis</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning">Association rule learning</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection">Anomaly detection</a></li>
			</ul>
			<ul>
				<li><a href="https://en.wikipedia.org/wiki/Sequence_labeling">Sequence labeling</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Automatic_summarization">Automatic summarization</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Pattern_recognition">Pattern recognition</a></li>
				<li><a href="http://scikit-learn.org/stable/">Scikit-learn</a></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">55
				<a class="prev" href="#slide54"></a>
				<a class="next" href="#slide56"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide56">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Online resources (English Wikipedia)</h1>
			<ul>
				<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine">Support Vector Machines</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning">Decision tree learning</a></li>
				<li><a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic gradient descent</a>
				</li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">56
				<a class="prev" href="#slide55"></a>
				<a class="next" href="#slide57"></a>
			</div>
		</div>
	</section>
	<section class="slide" id="slide57">
		<div class="header">
			<h1>References</h1>
		</div>
		<div class="content">
			<h1>Colors</h1>
			<ul>
				<li><a href="https://material.io/color/">Color Tool - Material Design</a></li>
			</ul>
			<h1>Images</h1>
			<ul>
				<li><a href="https://commons.wikimedia.org/">Wikimedia Commons</a></li>
			</ul>
		</div>
		<div class="footer">
			<div class="contact">Data Science for Chemists | John Samuel</div>
			<div class="navigation">57
				<a class="prev" href="#slide56"></a>
				<a class="next" href="#slide58"></a>
			</div>
		</div>
	</section>
	<script>
		function changeCurrentURLSlideNumber(isIncrement) {
			url = window.location.href;
			position = url.indexOf("#slide");
			if (position != -1) { // Not on the first page
				slideIdString = url.substr(position + 6);
				if (!Number.isNaN(slideIdString)) {
					slideId = parseInt(slideIdString);
					if (isIncrement) {
						if (slideId < 57) {
							slideId = slideId + 1;
						}
					} else {
						if (slideId > 1) {
							slideId = slideId - 1;
						}
					}
					/* regexp */
					url = url.replace(/#slide\d+/g, "#slide" + slideId);
					window.location.href = url;
				}
			} else {
				window.location.href = url + "#slide2";
			}
		}
		document.onkeydown = function (event) {

			event.preventDefault();
			/* This will ensure the default behavior of
															page scroll behaviour (up, down, right, left)*/

			event = event || window.event;
			/*Codes de la touche sur le clavier: 37, 38, 39, 40*/
			if (event.keyCode == '37') {
				// left
				changeCurrentURLSlideNumber(false);
			} else if (event.keyCode == '38') {
				// up
				changeCurrentURLSlideNumber(false);
			} else if (event.keyCode == '39') {
				// right
				changeCurrentURLSlideNumber(true);
			} else if (event.keyCode == '40') {
				// down
				changeCurrentURLSlideNumber(true);
			}
		}
		document.body.onmouseup = function (event) {
			event = event || window.event;
			event.preventDefault();
			changeCurrentURLSlideNumber(true);
		}
	</script>
</body>

</html>
